var documenterSearchIndex = {"docs":
[{"location":"Imputation.html#imputation_module","page":"Imputation","title":"The BetaML.Imputation Module","text":"","category":"section"},{"location":"Imputation.html","page":"Imputation","title":"Imputation","text":"Imputation","category":"page"},{"location":"Imputation.html#BetaML.Imputation","page":"Imputation","title":"BetaML.Imputation","text":"Imputation module\n\nProvide various imputation methods for missing data. Note that the interpretation of \"missing\" can be very wide. For example, reccomendation systems / collaborative filtering (e.g. suggestion of the film to watch) can well be representated as a missing data to impute problem.\n\nMeanImputer: Simple imputator using the features or the records means, with optional record normalisation (fastest)\nGMMImputer: Impute data using a Generative (Gaussian) Mixture Model (good trade off)\nRFImputer: Impute missing data using Random Forests, with optional replicable multiple imputations (most accurate).\n\nImputations for all these models can be optained by running fit!([Imputator model],X). The data with the missing values imputed can then be obtained with predict(m::Imputer). Useinfo(m::Imputer) to retrieve further information concerning the imputation. Note that if multiple imputations are run (for the supporting imputators) predict() will return a vector of predictions rather than a single one`.\n\nExample\n\njulia> using Statistics, BetaML\n\njulia> X            = [2 missing 10; 2000 4000 1000; 2000 4000 10000; 3 5 12 ; 4 8 20; 1 2 5]\n6×3 Matrix{Union{Missing, Int64}}:\n    2      missing     10\n 2000  4000          1000\n 2000  4000         10000\n    3     5            12\n    4     8            20\n    1     2             5\n\njulia> mod          = RFImputer(multipleImputations=10,  rng=copy(FIXEDRNG));\n\njulia> fit!(mod,X)\ntrue\n\njulia> vals         = predict(mod)\n10-element Vector{Matrix{Union{Missing, Int64}}}:\n [2 3 10; 2000 4000 1000; … ; 4 8 20; 1 2 5]\n [2 4 10; 2000 4000 1000; … ; 4 8 20; 1 2 5]\n [2 4 10; 2000 4000 1000; … ; 4 8 20; 1 2 5]\n [2 136 10; 2000 4000 1000; … ; 4 8 20; 1 2 5]\n [2 137 10; 2000 4000 1000; … ; 4 8 20; 1 2 5]\n [2 4 10; 2000 4000 1000; … ; 4 8 20; 1 2 5]\n [2 4 10; 2000 4000 1000; … ; 4 8 20; 1 2 5]\n [2 4 10; 2000 4000 1000; … ; 4 8 20; 1 2 5]\n [2 137 10; 2000 4000 1000; … ; 4 8 20; 1 2 5]\n [2 137 10; 2000 4000 1000; … ; 4 8 20; 1 2 5]\n\njulia> nR,nC        = size(vals[1])\n(6, 3)\n\njulia> medianValues = [median([v[r,c] for v in vals]) for r in 1:nR, c in 1:nC]\n6×3 Matrix{Float64}:\n    2.0     4.0     10.0\n 2000.0  4000.0   1000.0\n 2000.0  4000.0  10000.0\n    3.0     5.0     12.0\n    4.0     8.0     20.0\n    1.0     2.0      5.0\n\njulia> infos        = info(mod);\n\njulia> infos.nImputedValues\n1\n\n\n\n\n\n","category":"module"},{"location":"Imputation.html#Module-Index","page":"Imputation","title":"Module Index","text":"","category":"section"},{"location":"Imputation.html","page":"Imputation","title":"Imputation","text":"Modules = [Imputation]\nOrder   = [:constant, :type, :function, :macro]","category":"page"},{"location":"Imputation.html#Detailed-API","page":"Imputation","title":"Detailed API","text":"","category":"section"},{"location":"Imputation.html","page":"Imputation","title":"Imputation","text":"Modules = [Imputation]","category":"page"},{"location":"Imputation.html#BetaML.Imputation.GMMImputer","page":"Imputation","title":"BetaML.Imputation.GMMImputer","text":"GMMImputer\n\nMissing data imputer that uses a Generated (Gaussian) Mixture Model.\n\nFor the parameters (nClasses,mixtures,..) see  GMMImputerLearnableParameters.\n\nLimitations:\n\ndata must be numerical\nthe resulted matrix is a Matrix{Float64}\ncurrently the Mixtures available do not support random initialisation for missing imputation, and the rest of the algorithm (we use the Expectation-Maximisation) is deterministic, so there is no random component involved (i.e. no multiple imputations)    \n\n\n\n\n\n","category":"type"},{"location":"Imputation.html#BetaML.Imputation.MeanImputer","page":"Imputation","title":"BetaML.Imputation.MeanImputer","text":"MeanImputer\n\nSimple imputer using the feature (column) mean, optionally normalised by l-norms of the records (rows)\n\nParameters:\n\nnorm: Normalise the feature mean by l-norm norm of the records [default: nothing]. Use it (e.g. norm=1 to use the l-1 norm) if the records are highly heterogeneus (e.g. quantity exports of different countries).  \n\nLimitations:\n\ndata must be numerical\n\n\n\n\n\n","category":"type"},{"location":"Imputation.html#BetaML.Imputation.RFImputer","page":"Imputation","title":"BetaML.Imputation.RFImputer","text":"RFImputer\n\nImpute missing data using Random Forests, with optional replicable multiple imputations. \n\nFor the underlying random forest algorithm parameters (nTrees,maxDepth,minGain,minRecords,maxFeatures:,splittingCriterion,β,initStrategy, oob and rng) see buildTree and buildForest.\n\nSpecific parameters:\n\nforcedCategoricalCols: specify the positions of the integer columns to treat as categorical instead of cardinal. [Default: empty vector (all numerical cols are treated as cardinal by default and the others as categorical)]\nrecursivePassages: Define the times to go trough the various columns to impute their data. Useful when there are data to impute on multiple columns. The order of the first passage is given by the decreasing number of missing values per column, the other passages are random [default: 1].\nmultipleImputations: Determine the number of independent imputation of the whole dataset to make. Note that while independent, the imputations share the same random number generator (RNG).\n\nNotes:\n\nGiven a certain RNG and its status (e.g. RFImputer(...,rng=StableRNG(FIXEDSEED))), the algorithm is completely deterministic, i.e. replicable. \nThe algorithm accepts virtually any kind of data, sortable or not\n\n\n\n\n\n","category":"type"},{"location":"Imputation.html#BetaML.Api.fit!-Tuple{GMMImputer, Any}","page":"Imputation","title":"BetaML.Api.fit!","text":"fit!(imputer::GMMImputer,X)\n\nFit a matrix with missing data using GMMImputer\n\n\n\n\n\n","category":"method"},{"location":"Imputation.html#BetaML.Api.fit!-Tuple{MeanImputer, Any}","page":"Imputation","title":"BetaML.Api.fit!","text":"fit!(imputer::MeanImputer,X)\n\nFit a matrix with missing data using MeanImputer\n\n\n\n\n\n","category":"method"},{"location":"Imputation.html#BetaML.Api.fit!-Tuple{RFImputer, Any}","page":"Imputation","title":"BetaML.Api.fit!","text":"fit!(imputer::RFImputer,X)\n\nFit a matrix with missing data using RFImputer\n\n\n\n\n\n","category":"method"},{"location":"Imputation.html#BetaML.Api.predict-Tuple{GMMImputer}","page":"Imputation","title":"BetaML.Api.predict","text":"predict(m::GMMImputer)\n\nReturn the data with the missing values replaced with the imputed ones using GMMImputer.\n\n\n\n\n\n","category":"method"},{"location":"Imputation.html#BetaML.Api.predict-Tuple{MeanImputer, Any}","page":"Imputation","title":"BetaML.Api.predict","text":"predict(m::MeanImputer)\n\nReturn the data with the missing values replaced with the imputed ones using MeanImputer.\n\n\n\n\n\n","category":"method"},{"location":"Imputation.html#BetaML.Api.predict-Tuple{MeanImputer}","page":"Imputation","title":"BetaML.Api.predict","text":"predict(m::MeanImputer)\n\nReturn the data with the missing values replaced with the imputed ones using MeanImputer.\n\n\n\n\n\n","category":"method"},{"location":"Imputation.html#BetaML.Api.predict-Tuple{RFImputer}","page":"Imputation","title":"BetaML.Api.predict","text":"predict(m::RFImputer)\n\nReturn the data with the missing values replaced with the imputed ones using RFImputer. If multipleImputations was set >1 this is a vector of matrices (the individual imputations) instead of a single matrix.\n\n\n\n\n\n","category":"method"},{"location":"Imputation.html#BetaML.Imputation.predictMissing","page":"Imputation","title":"BetaML.Imputation.predictMissing","text":"predictMissing(X,K;p₀,mixtures,tol,verbosity,minVariance,minCovariance)\n\nOLD API. Use GMMClusterer instead.\n\nFill missing entries in a sparse matrix (i.e. perform a \"matrix completion\") assuming an underlying Gaussian Mixture probabilistic Model (GMM) and implementing an Expectation-Maximisation algorithm.\n\nWhile the name of the function is predictMissing, the function can be also used for system reccomendation / collaborative filtering and GMM-based regressions. The advantage over traditional algorithms as k-nearest neighbors (KNN) is that GMM can \"detect\" the hidden structure of the observed data, where some observation can be similar to a certain pool of other observvations for a certain characteristic, but similar to an other pool of observations for other characteristics.\n\nImplemented in the log-domain for better numerical accuracy with many dimensions.\n\nParameters:\n\nX  :           A (N x D) sparse matrix of data to fill according to a GMM model\nK  :           Number of mixtures (latent classes) to consider [def: 3]\np₀ :           Initial probabilities of the categorical distribution (K x 1) [default: []]\nmixtures:      An array (of length K) of the mixture to employ (see notes) [def: [DiagonalGaussian() for i in 1:K]]\ntol:           Tolerance to stop the algorithm [default: 10^(-6)]\nverbosity:     A verbosity parameter regulating the information messages frequency [def: STD]\nminVariance:   Minimum variance for the mixtures [default: 0.05]\nminCovariance: Minimum covariance for the mixtures with full covariance matrix [default: 0]. This should be set different than minVariance (see notes).\ninitStrategy:  Mixture initialisation algorithm [def: grid]\nmaxIter:       Maximum number of iterations [def: typemax(Int64), i.e. ∞]\nrng:           Random Number Generator (see FIXEDSEED) [deafult: Random.GLOBAL_RNG]\n\nReturns:\n\nA named touple of:\n̂X̂    : The Filled Matrix of size (N x D)\nnFill: The number of items filled\nlL   : The log-likelihood (without considering the last mixture optimisation)\nBIC :  The Bayesian Information Criterion (lower is better)\nAIC :  The Akaike Information Criterion (lower is better)\n\nNotes:\n\nThe mixtures currently implemented are SphericalGaussian(μ,σ²),DiagonalGaussian(μ,σ²) and FullGaussian(μ,σ²)\nFor initStrategy, look at the documentation of initMixtures! for the mixture you want. The provided gaussian mixtures support grid, kmeans or given. grid is faster, but kmeans often provides better results.\nThe algorithm requires to specify a number of \"latent classes\" (mlixtures) to divide the dataset into. If there isn't any prior domain specific knowledge on this point one can test sevaral k and verify which one minimise the BIC or AIC criteria.\n\nExample:\n\njulia>  cFOut = predictMissing([1 10.5;1.5 missing; 1.8 8; 1.7 15; 3.2 40; missing missing; 3.3 38; missing -2.3; 5.2 -2.4],3)\n\n\n\n\n\n","category":"function"},{"location":"Imputation.html#MLJModelInterface.transform-Tuple{MissingImputator, Any, Any}","page":"Imputation","title":"MLJModelInterface.transform","text":"transform(m::MissingImputator, fitResults, X) - Given a trained imputator model fill the missing data of some new observations\n\n\n\n\n\n","category":"method"},{"location":"Examples.html#Examples","page":"Examples","title":"Examples","text":"","category":"section"},{"location":"Examples.html#Supervised-learning","page":"Examples","title":"Supervised learning","text":"","category":"section"},{"location":"Examples.html#Regression","page":"Examples","title":"Regression","text":"","category":"section"},{"location":"Examples.html#Estimating-the-bike-sharing-demand","page":"Examples","title":"Estimating the bike sharing demand","text":"","category":"section"},{"location":"Examples.html","page":"Examples","title":"Examples","text":"The task is to estimate the influence of several variables (like the weather, the season, the day of the week..) on the demand of shared bicycles, so that the authority in charge of the service can organise the service in the best way.","category":"page"},{"location":"Examples.html","page":"Examples","title":"Examples","text":"Data origin:","category":"page"},{"location":"Examples.html","page":"Examples","title":"Examples","text":"original full dataset (by hour, not used here): https://archive.ics.uci.edu/ml/datasets/Bike+Sharing+Dataset\nsimplified dataset (by day, with some simple scaling): https://www.hds.utc.fr/~tdenoeux/dokuwiki/en/aec\ndescription: https://www.hds.utc.fr/~tdenoeux/dokuwiki/media/en/exam2019ace.pdf\ndata: https://www.hds.utc.fr/~tdenoeux/dokuwiki/media/en/bikesharing_day.csv.zip","category":"page"},{"location":"Examples.html","page":"Examples","title":"Examples","text":"Note that even if we are estimating a time serie, we are not using here a recurrent neural network as we assume the temporal dependence to be negligible (i.e. Y_t = f(X_t) alone).","category":"page"},{"location":"Examples.html#Classification","page":"Examples","title":"Classification","text":"","category":"section"},{"location":"Examples.html#Unsupervised-lerarning","page":"Examples","title":"Unsupervised lerarning","text":"","category":"section"},{"location":"Examples.html#Notebooks","page":"Examples","title":"Notebooks","text":"","category":"section"},{"location":"Examples.html","page":"Examples","title":"Examples","text":"The following notebooks provide runnable examples of the package functionality:","category":"page"},{"location":"Examples.html","page":"Examples","title":"Examples","text":"Pegasus classifiers: [Static notebook] - [myBinder]\nDecision Trees and Random Forest regression on Bike sharing demand forecast (daily data): [Static notebook] - [myBinder]\nNeural Networks: [Static notebook] - [myBinder]\nBike sharing demand forecast (daily data): [Static notebook] - [myBinder]\nClustering: [Static notebook] - [myBinder]","category":"page"},{"location":"Examples.html","page":"Examples","title":"Examples","text":"Note: the live, runnable computational environment is a temporary new copy made at each connection. The first time after a commit is done on this repository a new environment has to be set (instead of just being copied), and the server may take several minutes.","category":"page"},{"location":"Examples.html","page":"Examples","title":"Examples","text":"This is only if you are the unlucky user triggering the rebuild of the environment after the commit.","category":"page"},{"location":"Nn.html#nn_module","page":"Nn","title":"The BetaML.Nn Module","text":"","category":"section"},{"location":"Nn.html","page":"Nn","title":"Nn","text":"Nn","category":"page"},{"location":"Nn.html#BetaML.Nn","page":"Nn","title":"BetaML.Nn","text":"BetaML.Nn module\n\nImplement the functionality required to define an artificial Neural Network, train it with data, forecast data and assess its performances.\n\nCommon type of layers and optimisation algorithms are already provided, but you can define your own ones subclassing respectively the AbstractLayer and OptimisationAlgorithm abstract types.\n\nThe module provide the following type or functions. Use ?[type or function] to access their full signature and detailed documentation:\n\nModel definition:\n\nDenseLayer: Classical feed-forward layer with user-defined activation function\nDenseNoBiasLayer: Classical layer without the bias parameter\nVectorFunctionLayer: Parameterless layer whose activation function run over the ensable of its nodes rather than on each one individually\nbuildNetwork: Build the chained network and define a cost function\ngetParams(nn): Retrieve current weigthts\ngetGradient(nn): Retrieve the current gradient of the weights\nsetParams!(nn): Update the weigths of the network\nshow(nn): Print a representation of the Neural Network\n\nEach layer can use a default activation function, one of the functions provided in the Utils module (relu, tanh, softmax,...) or you can specify your own function. The derivative of the activation function can be optionally be provided, in such case training will be quicker, altought this difference tends to vanish with bigger datasets. You can alternativly implement your own layer defining a new type as subtype of the abstract type AbstractLayer. Each user-implemented layer must define the following methods:\n\nA suitable constructor\nforward(layer,x)\nbackward(layer,x,nextGradient)\ngetParams(layer)\ngetGradient(layer,x,nextGradient)\nsetParams!(layer,w)\nsize(layer)\n\nModel training:\n\ntrainingInfo(nn): Default callback function during training\ntrain!(nn):  Training function\nsingleUpdate!(θ,▽;optAlg): The parameter update made by the specific optimisation algorithm\nSGD: The default optimisation algorithm\nADAM: A faster moment-based optimisation algorithm (added in v0.2.2)\n\nTo define your own optimisation algorithm define a subtype of OptimisationAlgorithm and implement the function singleUpdate!(θ,▽;optAlg) and eventually initOptAlg(⋅) specific for it.\n\nModel predictions and assessment:\n\npredict(nn): Return the output given the data\nloss(nn): Compute avg. network loss on a test set\nUtils.accuracy(ŷ,y): Categorical output accuracy\n\nWhile high-level functions operating on the dataset expect it to be in the standard format (nRecords × nDimensions matrices) it is custom to represent the chain of a neural network as a flow of column vectors, so all low-level operations (operating on a single datapoint) expect both the input and the output as a column vector.\n\n\n\n\n\n","category":"module"},{"location":"Nn.html#Module-Index","page":"Nn","title":"Module Index","text":"","category":"section"},{"location":"Nn.html","page":"Nn","title":"Nn","text":"Modules = [Nn]\nOrder   = [:constant, :type, :function, :macro]","category":"page"},{"location":"Nn.html#Detailed-API","page":"Nn","title":"Detailed API","text":"","category":"section"},{"location":"Nn.html","page":"Nn","title":"Nn","text":"Modules = [Nn]","category":"page"},{"location":"Nn.html#BetaML.Nn.ADAM","page":"Nn","title":"BetaML.Nn.ADAM","text":"ADAM(;η, λ, β₁, β₂, ϵ)\n\nThe ADAM algorithm, an adaptive moment estimation optimiser.\n\nFields:\n\nη:  Learning rate (stepsize, α in the paper), as a function of the current epoch [def: t -> 0.001 (i.e. fixed)]\nλ:  Multiplicative constant to the learning rate [def: 1]\nβ₁: Exponential decay rate for the first moment estimate [range: ∈ [0,1], def: 0.9]\nβ₂: Exponential decay rate for the second moment estimate [range: ∈ [0,1], def: 0.999]\nϵ:  Epsilon value to avoid division by zero [def: 10^-8]\n\n\n\n\n\n","category":"type"},{"location":"Nn.html#BetaML.Nn.DenseLayer","page":"Nn","title":"BetaML.Nn.DenseLayer","text":"DenseLayer\n\nRepresentation of a layer in the network\n\nFields:\n\nw:  Weigths matrix with respect to the input from previous layer or data (n x n pr. layer)\nwb: Biases (n)\nf:  Activation function\ndf: Derivative of the activation function\n\n\n\n\n\n","category":"type"},{"location":"Nn.html#BetaML.Nn.DenseNoBiasLayer","page":"Nn","title":"BetaML.Nn.DenseNoBiasLayer","text":"DenseNoBiasLayer\n\nRepresentation of a layer without bias in the network\n\nFields:\n\nw:  Weigths matrix with respect to the input from previous layer or data (n x n pr. layer)\nf:  Activation function\ndf: Derivative of the activation function\n\n\n\n\n\n","category":"type"},{"location":"Nn.html#BetaML.Nn.Learnable","page":"Nn","title":"BetaML.Nn.Learnable","text":"Learnable(data)\n\nStructure representing the learnable parameters of a layer or its gradient.\n\nThe learnable parameters of a layers are given in the form of a N-tuple of Array{Float64,N2} where N2 can change (e.g. we can have a layer with the first parameter being a matrix, and the second one being a scalar). We wrap the tuple on its own structure a bit for some efficiency gain, but above all to define standard mathematic operations on the gradients without doing \"type pyracy\" with respect to Base tuples.\n\n\n\n\n\n","category":"type"},{"location":"Nn.html#BetaML.Nn.NN","page":"Nn","title":"BetaML.Nn.NN","text":"NN\n\nRepresentation of a Neural Network\n\nFields:\n\nlayers:  Array of layers objects\ncf:      Cost function\ndcf:     Derivative of the cost function\ntrained: Control flag for trained networks\n\n\n\n\n\n","category":"type"},{"location":"Nn.html#BetaML.Nn.OptimisationAlgorithm","page":"Nn","title":"BetaML.Nn.OptimisationAlgorithm","text":"OptimisationAlgorithm\n\nAbstract type representing an Optimisation algorithm.\n\nCurrently supported algorithms:\n\nSGD (Stochastic) Gradient Descent\nADAM The ADAM algorithm, an adaptive moment estimation optimiser.\n\nSee ?[Name OF THE ALGORITHM] for their details\n\nYou can implement your own optimisation algorithm using a subtype of OptimisationAlgorithm and implementing its constructor and the update function singleUpdate(⋅) (type ?singleUpdate for details).\n\n\n\n\n\n","category":"type"},{"location":"Nn.html#BetaML.Nn.RNNLayer","page":"Nn","title":"BetaML.Nn.RNNLayer","text":"RNNLayer\n\nRepresentation of a layer in the network\n\nFields:\n\nwx: Weigths matrix with respect to the input from data (n by n_input)\nws: Weigths matrix with respect to the layer state (n x n )\nwb: Biases (n)\nf:  Activation function\ndf: Derivative of the activation function\ns : State\n\n\n\n\n\n","category":"type"},{"location":"Nn.html#BetaML.Nn.SGD","page":"Nn","title":"BetaML.Nn.SGD","text":"SGD(;η=t -> 1/(1+t), λ=2)\n\nStochastic Gradient Descent algorithm (default)\n\nFields:\n\nη: Learning rate, as a function of the current epoch [def: t -> 1/(1+t)]\nλ: Multiplicative constant to the learning rate [def: 2]\n\n\n\n\n\n","category":"type"},{"location":"Nn.html#BetaML.Nn.ScalarFunctionLayer","page":"Nn","title":"BetaML.Nn.ScalarFunctionLayer","text":"ScalarFunctionLayer\n\nRepresentation of a ScalarFunction layer in the network. ScalarFunctionLayer applies the activation function directly to the output of the previous layer (i.e., without passing for a weigth matrix), but using an  optional learnable parameter (an array) used as second argument, similarly to [VectorFunctionLayer(@ref). Differently from VectorFunctionLayer, the function is applied scalarwise to each node. \n\nThe number of nodes in input must be set to the same as in the previous layer\n\nFields:\n\nw:   Weigths (parameter) array passes as second argument to the activation        function (if not empty)\nn:   Number of nodes in output (≡ number of nodes in input )\nf:   Activation function (vector)\ndfx: Derivative of the (vector) activation function with respect to the        layer inputs (x)\ndfw: Derivative of the (vector) activation function with respect to the        optional learnable weigths (w)         \n\nNotes:\n\nThe output size of this layer is the same as those of the previous layers.\n\n\n\n\n\n","category":"type"},{"location":"Nn.html#BetaML.Nn.VectorFunctionLayer","page":"Nn","title":"BetaML.Nn.VectorFunctionLayer","text":"VectorFunctionLayer\n\nRepresentation of a VectorFunction layer in the network. Vector function layer expects a vector activation function, i.e. a function taking the whole output of the previous layer an input rather than working on a single node as \"normal\" activation functions would do. Useful for example with the SoftMax function in classification or with the pool1D function to implement a \"pool\" layer in 1 dimensions. By default it is weightless, i.e. it doesn't apply any transformation to the output coming from the previous layer except the activation function. However, by passing the parameter wsize (a touple or array - tested only 1D) you can pass the learnable parameter to the activation function too. It is your responsability to be sure the activation function accept only X or also this  learnable array (as second argument).    The number of nodes in input must be set to the same as in the previous layer (and if you are using this for classification, to the number of classes, i.e. the previous layer must be set equal to the number of classes in the predictions).\n\nFields:\n\nw:   Weigths (parameter) array passes as second argument to the activation        function (if not empty)\nnₗ:  Number of nodes in input (i.e. length of previous layer)\nn:   Number of nodes in output (automatically inferred in the constructor)\nf:   Activation function (vector)\ndfx: Derivative of the (vector) activation function with respect to the        layer inputs (x)\ndfw: Derivative of the (vector) activation function with respect to the        optional learnable weigths (w)         \n\nNotes:\n\nThe output size of this layer is given by the size of the output function,\n\nthat not necessarily is the same as the previous layers.\n\n\n\n\n\n","category":"type"},{"location":"Nn.html#Base.size-Tuple{AbstractLayer}","page":"Nn","title":"Base.size","text":"size(layer)\n\nGet the dimensions of the layers in terms of (dimensions in input , dimensions in output)\n\nNotes:\n\nYou need to use import Base.size before defining this function for your layer\n\n\n\n\n\n","category":"method"},{"location":"Nn.html#BetaML.Nn.backward-Tuple{AbstractLayer, Any, Any}","page":"Nn","title":"BetaML.Nn.backward","text":"backward(layer,x,nextGradient)\n\nCompute backpropagation for this layer\n\nParameters:\n\nlayer:        Worker layer\nx:            Input to the layer\nnextGradient: Derivative of the overal loss with respect to the input of the next layer (output of this layer)\n\nReturn:\n\nThe evaluated gradient of the loss with respect to this layer inputs\n\n\n\n\n\n","category":"method"},{"location":"Nn.html#BetaML.Nn.buildNetwork-Tuple{Any, Any}","page":"Nn","title":"BetaML.Nn.buildNetwork","text":"buildNetwork(layers,cf;dcf,name)\n\nInstantiate a new Feedforward Neural Network\n\nParameters:\n\nlayers: Array of layers objects\ncf:     Cost function\ndcf:    Derivative of the cost function [def: nothing]\nname:   Name of the network [def: \"Neural Network\"]\n\nNotes:\n\nEven if the network ends with a single output note, the cost function and its derivative should always expect y and ŷ as column vectors.\n\n\n\n\n\n","category":"method"},{"location":"Nn.html#BetaML.Nn.forward-Tuple{AbstractLayer, Any}","page":"Nn","title":"BetaML.Nn.forward","text":"forward(layer,x)\n\nPredict the output of the layer given the input\n\nParameters:\n\nlayer:  Worker layer\nx:      Input to the layer\n\nReturn:\n\nAn Array{T,1} of the prediction (even for a scalar)\n\n\n\n\n\n","category":"method"},{"location":"Nn.html#BetaML.Nn.getGradient-Tuple{AbstractLayer, Any, Any}","page":"Nn","title":"BetaML.Nn.getGradient","text":"getGradient(layer,x,nextGradient)\n\nCompute backpropagation for this layer\n\nParameters:\n\nlayer:        Worker layer\nx:            Input to the layer\nnextGradient: Derivative of the overaall loss with respect to the input of the next layer (output of this layer)\n\nReturn:\n\nThe evaluated gradient of the loss with respect to this layer's trainable parameters as tuple of matrices. It is up to you to decide how to organise this tuple, as long you are consistent with the getParams() and setParams() functions. Note that starting from BetaML 0.2.2 this tuple needs to be wrapped in its Learnable type.\n\n\n\n\n\n","category":"method"},{"location":"Nn.html#BetaML.Nn.getGradient-Union{Tuple{T2}, Tuple{T}, Tuple{Any, AbstractMatrix{T}, AbstractMatrix{T2}}} where {T<:Number, T2<:Number}","page":"Nn","title":"BetaML.Nn.getGradient","text":"getGradient(nn,xbatch,ybatch)\n\nRetrieve the current gradient of the weigthts (i.e. derivative of the cost with respect to the weigths)\n\nParameters:\n\nnn:      Worker network\nxbatch:  Input to the network (n,d)\nybatch:  Label input (n,d)\n\n#Notes:\n\nThe output is a vector of tuples of each layer's input weigths and bias weigths\n\n\n\n\n\n","category":"method"},{"location":"Nn.html#BetaML.Nn.getGradient-Union{Tuple{T2}, Tuple{T}, Tuple{NN, Union{AbstractVector{T}, T}, Union{AbstractVector{T2}, T2}}} where {T<:Number, T2<:Number}","page":"Nn","title":"BetaML.Nn.getGradient","text":"getGradient(nn,x,y)\n\nRetrieve the current gradient of the weigthts (i.e. derivative of the cost with respect to the weigths)\n\nParameters:\n\nnn: Worker network\nx:   Input to the network (d,1)\ny:   Label input (d,1)\n\n#Notes:\n\nThe output is a vector of tuples of each layer's input weigths and bias weigths\n\n\n\n\n\n","category":"method"},{"location":"Nn.html#BetaML.Nn.getNParams-Tuple{AbstractLayer}","page":"Nn","title":"BetaML.Nn.getNParams","text":"getNParams(layer)\n\nReturn the number of parameters of a layer.\n\nIt doesn't need to be implemented by each layer type, as it uses getParams().\n\n\n\n\n\n","category":"method"},{"location":"Nn.html#BetaML.Nn.getNParams-Tuple{NN}","page":"Nn","title":"BetaML.Nn.getNParams","text":"getNParams(nn) - Return the number of trainable parameters of the neural network.\n\n\n\n\n\n","category":"method"},{"location":"Nn.html#BetaML.Nn.getParams-Tuple{AbstractLayer}","page":"Nn","title":"BetaML.Nn.getParams","text":"getParams(layer)\n\nGet the layers current value of its trainable parameters\n\nParameters:\n\nlayer:  Worker layer\n\nReturn:\n\nThe current value of the layer's trainable parameters as tuple of matrices. It is up to you to decide how to organise this tuple, as long you are consistent with the getGradient() and setParams() functions. Note that starting from BetaML 0.2.2 this tuple needs to be wrapped in its Learnable type.\n\n\n\n\n\n","category":"method"},{"location":"Nn.html#BetaML.Nn.getParams-Tuple{NN}","page":"Nn","title":"BetaML.Nn.getParams","text":"getParams(nn)\n\nRetrieve current weigthts\n\nParameters:\n\nnn: Worker network\n\nNotes:\n\nThe output is a vector of tuples of each layer's input weigths and bias weigths\n\n\n\n\n\n","category":"method"},{"location":"Nn.html#BetaML.Nn.initOptAlg!-Tuple{ADAM}","page":"Nn","title":"BetaML.Nn.initOptAlg!","text":"initOptAlg!(optAlg::ADAM;θ,batchSize,x,y,rng)\n\nInitialize the ADAM algorithm with the parameters m and v as zeros and check parameter bounds\n\n\n\n\n\n","category":"method"},{"location":"Nn.html#BetaML.Nn.initOptAlg!-Tuple{BetaML.Nn.OptimisationAlgorithm}","page":"Nn","title":"BetaML.Nn.initOptAlg!","text":"initOptAlg!(optAlg;θ,batchSize,x,y)\n\nInitialize the optimisation algorithm\n\nParameters:\n\noptAlg:    The Optimisation algorithm to use\nθ:         Current parameters\nbatchSize:    The size of the batch\nx:   The training (input) data\ny:   The training \"labels\" to match\nrng: Random Number Generator (see FIXEDSEED) [deafult: Random.GLOBAL_RNG]\n\nNotes:\n\nOnly a few optimizers need this function and consequently ovverride it. By default it does nothing, so if you want write your own optimizer and don't need to initialise it, you don't have to override this method\n\n\n\n\n\n","category":"method"},{"location":"Nn.html#BetaML.Nn.loss-Tuple{NN, Any, Any}","page":"Nn","title":"BetaML.Nn.loss","text":"loss(fnn,x,y)\n\nCompute avg. network loss on a test set (or a single (1 × d) data point)\n\nParameters:\n\nfnn: Worker network\nx:   Input to the network (n) or (n x d)\ny:   Label input (n) or (n x d)\n\n\n\n\n\n","category":"method"},{"location":"Nn.html#BetaML.Nn.setParams!-Tuple{AbstractLayer, Any}","page":"Nn","title":"BetaML.Nn.setParams!","text":"setParams!(layer,w)\n\nSet the trainable parameters of the layer with the given values\n\nParameters:\n\nlayer: Worker layer\nw:     The new parameters to set (Learnable)\n\nNotes:\n\nThe format of the tuple wrapped by Learnable must be consistent with those of the getParams() and getGradient() functions.\n\n\n\n\n\n","category":"method"},{"location":"Nn.html#BetaML.Nn.setParams!-Tuple{NN, Any}","page":"Nn","title":"BetaML.Nn.setParams!","text":"setParams!(nn,w)\n\nUpdate weigths of the network\n\nParameters:\n\nnn: Worker network\nw:  The new weights to set\n\n\n\n\n\n","category":"method"},{"location":"Nn.html#BetaML.Nn.show-Tuple{NN}","page":"Nn","title":"BetaML.Nn.show","text":"show(nn)\n\nPrint a representation of the Neural Network (layers, dimensions..)\n\nParameters:\n\nnn: Worker network\n\n\n\n\n\n","category":"method"},{"location":"Nn.html#BetaML.Nn.singleUpdate!-Tuple{Any, Any}","page":"Nn","title":"BetaML.Nn.singleUpdate!","text":"singleUpdate!(θ,▽;nEpoch,nBatch,batchSize,xbatch,ybatch,optAlg)\n\nPerform the parameters update based on the average batch gradient.\n\nParameters:\n\nθ:         Current parameters\n▽:         Average gradient of the batch\nnEpoch:    Count of current epoch\nnBatch:    Count of current batch\nnBatches:  Number of batches per epoch\nxbatch:    Data associated to the current batch\nybatch:    Labels associated to the current batch\noptAlg:    The Optimisation algorithm to use for the update\n\nNotes:\n\nThis function is overridden so that each optimisation algorithm implement their\n\nown version\n\nMost parameters are not used by any optimisation algorithm. They are provided\n\nto support the largest possible class of optimisation algorithms\n\nSome optimisation algorithms may change their internal structure in this function\n\n\n\n\n\n","category":"method"},{"location":"Nn.html#BetaML.Nn.train!-Tuple{NN, Any, Any}","page":"Nn","title":"BetaML.Nn.train!","text":"train!(nn,x,y;epochs,batchSize,sequential,optAlg,verbosity,cb)\n\nTrain a neural network with the given x,y data\n\nParameters:\n\nnn:         Worker network\nx:          Training input to the network (records x dimensions)\ny:          Label input (records x dimensions)\nepochs:     Number of passages over the training set [def: 100]\nbatchSize:  Size of each individual batch [def: min(size(x,1),32)]\nsequential: Wether to run all data sequentially instead of random [def: false]\noptAlg:     The optimisation algorithm to update the gradient at each batch [def: ADAM()]\nverbosity:  A verbosity parameter for the trade off information / efficiency [def: STD]\ncb:         A callback to provide information. [def: trainingInfo]\nrng:        Random Number Generator (see FIXEDSEED) [deafult: Random.GLOBAL_RNG]\n\nReturn:\n\nA named tuple with the following information\nepochs: Number of epochs actually ran\nϵ_epochs: The average error on each epoch (if verbosity > LOW)\nθ_epochs: The parameters at each epoch (if verbosity > STD)\n\nNotes:\n\nCurrently supported algorithms:\nSGD, the classical (Stochastic) Gradient Descent optimiser\nADAM,  an adaptive moment estimation optimiser\nLook at the individual optimisation algorithm (?[Name OF THE ALGORITHM]) for info on its parameter, e.g. ?SGD for the Stochastic Gradient Descent.\nYou can implement your own optimisation algorithm using a subtype of OptimisationAlgorithm and implementing its constructor and the update function singleUpdate!(⋅) (type ?singleUpdate! for details).\nYou can implement your own callback function, altought the one provided by default is already pretty generic (its output depends on the verbosity parameter). See trainingInfo for informations on the cb parameters.\nBoth the callback function and the singleUpdate! function of the optimisation algorithm can be used to stop the training algorithm, respectively returning true or stop=true.\nThe verbosity can be set to any of NONE,LOW,STD,HIGH,FULL.\nThe update is done computing the average gradient for each batch and then calling singleUpdate! to let the optimisation algorithm perform the parameters update\n\n\n\n\n\n","category":"method"},{"location":"Nn.html#BetaML.Nn.trainingInfo-Tuple{Any, Any, Any}","page":"Nn","title":"BetaML.Nn.trainingInfo","text":"trainingInfo(nn,x,y;n,batchSize,epochs,verbosity,nEpoch,nBatch)\n\nDefault callback funtion to display information during training, depending on the verbosity level\n\nParameters:\n\nnn: Worker network\nx:  Batch input to the network (batchSize,d)\ny:  Batch label input (batchSize,d)\nn: Size of the full training set\nnBatches : Number of baches per epoch\nepochs: Number of epochs defined for the training\nverbosity: Verbosity level defined for the training (NONE,LOW,STD,HIGH,FULL)\nnEpoch: Counter of the current epoch\nnBatch: Counter of the current batch\n\n#Notes:\n\nReporting of the error (loss of the network) is expensive. Use verbosity=NONE for better performances\n\n\n\n\n\n","category":"method"},{"location":"Utils.html#utils_module","page":"Utils","title":"The BetaML.Utils Module","text":"","category":"section"},{"location":"Utils.html","page":"Utils","title":"Utils","text":"Utils\n","category":"page"},{"location":"Utils.html#BetaML.Utils","page":"Utils","title":"BetaML.Utils","text":"Utils module\n\nProvide shared utility functions for various machine learning algorithms.\n\nFor the complete list of functions provided see below. The main ones are:\n\nHelper functions for logging\n\nMost BetAML functions accept a parameter verbosity that expect one of the element in the Verbosity enoum (NONE, LOW, STD, HIGH or FULL)\nWriting complex code and need to find where something is executed ? Use the macro @codeLocation\n\nStochasticity management\n\nUtils provide [FIXEDSEED], [FIXEDRNG] and generateParallelRngs. All stochastic functions accept a rng paraemter. See the \"Getting started\" section in the tutorial for details.\n\nData processing\n\nVarious small and large utilities for helping processing the data, expecially before running a ML algorithm\nIncludes getPermutations, oneHotEncoder, integerEncoder (and integerDecoder), partition, scale (and getScaleFactors), pca, crossValidation\n\nSamplers\n\nUtilities to sample from data (e.g. for neural network training or for cross-validation)\nInclude the \"generic\" type SamplerWithData, together with the sampler implementation KFold and the function batch\n\nTransformers\n\nFuntions that \"transform\" a single input (that can be also a vector or a matrix)\nIncludes varios NN \"activation\" functions (relu, celu, sigmoid, softmax, pool1d) and their derivatives (d[FunctionName]), but also gini, entropy, variance, BIC, AIC\n\nMeasures\n\nSeveral functions of a pair of parameters (often y and ŷ) to measure the goodness of ŷ, the distance between the two elements of the pair, ...\nIncludes \"classical\" distance functions (l1_distance, l2_distance, l2²_distance cosine_distance), \"cost\" functions for continuous variables (squaredCost, meanRelError) and comparision functions for multui-class variables (crossEntropy, accuracy, ConfusionMatrix).\n\nImputers\n\nImputers of missing values\n\n\n\n\n\n","category":"module"},{"location":"Utils.html#Module-Index","page":"Utils","title":"Module Index","text":"","category":"section"},{"location":"Utils.html","page":"Utils","title":"Utils","text":"Modules = [Utils]\nOrder   = [:constant, :type, :function, :macro]","category":"page"},{"location":"Utils.html#Detailed-API","page":"Utils","title":"Detailed API","text":"","category":"section"},{"location":"Utils.html","page":"Utils","title":"Utils","text":"Modules = [Utils]","category":"page"},{"location":"Utils.html#BetaML.Utils.ConfusionMatrix","page":"Utils","title":"BetaML.Utils.ConfusionMatrix","text":"ConfusionMatrix\n\nScores and measures resulting from a comparation between true and predicted categorical variables\n\nUse the function ConfusionMatrix(ŷ,y;classes,labels,rng) to build it and report(cm::ConfusionMatrix;what) to visualise it, or use the individual parts of interest, e.g. display(cm.scores).\n\nFields:\n\nlabels: Array of categorical labels\naccuracy: Overall accuracy rate\nmisclassification: Overall misclassification rate\nactualCount: Array of counts per lebel in the actual data\npredictedCount: Array of counts per label in the predicted data\nscores: Matrix actual (rows) vs predicted (columns)\nnormalisedScores: Normalised scores\ntp: True positive (by class)\ntn: True negative (by class)\nfp: False positive (by class), aka \"type I error\" or \"false allarm\"\nfn: False negative (by class), aka \"type II error\" or \"miss\"\nprecision: True class i over predicted class i (by class)\nrecall: Predicted class i over true class i (by class), aka \"True Positive Rate (TPR)\", \"Sensitivity\" or \"Probability of detection\"\nspecificity: Predicted not class i over true not class i (by class), aka \"True Negative Rate (TNR)\"\nf1Score: Harmonic mean of precision and recall\nmeanPrecision: Mean by class, respectively unweighted and weighted by actualCount\nmeanRecall: Mean by class, respectively unweighted and weighted by actualCount\nmeanSpecificity: Mean by class, respectively unweighted and weighted by actualCount\nmeanF1Score: Mean by class, respectively unweighted and weighted by actualCount\n\n\n\n\n\n","category":"type"},{"location":"Utils.html#BetaML.Utils.ConfusionMatrix-Union{Tuple{T}, Tuple{Any, AbstractArray{T, N} where N}} where T","page":"Utils","title":"BetaML.Utils.ConfusionMatrix","text":"ConfusionMatrix(ŷ,y;classes,labels,rng)\n\nBuild a \"confusion matrix\" between predicted (columns) vs actual (rows) categorical values\n\nParameters:\n\nŷ: Vector of predicted categorical data\ny: Vector of actual categorical data\nclasses: The full set of possible classes (useful to give a specicif order or if not al lclasses are represented in y) [def: unique(y) ]\nlabels: String representation of the classes [def: string.(classes)]\nrng: Random number generator. Used only if ŷ is given in terms of a PMF and there are multi-modal values, as these are assigned randomply [def: Random.GLOBAL_RNG]\n\nReturn:\n\na ConfusionMatrix object\n\n\n\n\n\n","category":"method"},{"location":"Utils.html#BetaML.Utils.KFold","page":"Utils","title":"BetaML.Utils.KFold","text":"KFold(nSplits=5,nRepeats=1,shuffle=true,rng=Random.GLOBAL_RNG)\n\nIterator for k-fold crossValidation strategy.\n\n\n\n\n\n","category":"type"},{"location":"Utils.html#BetaML.Utils.SamplerWithData","page":"Utils","title":"BetaML.Utils.SamplerWithData","text":"SamplerWithData{Tsampler}\n\nAssociate an instance of an AbstractDataSampler with the actual data to sample.\n\n\n\n\n\n","category":"type"},{"location":"Utils.html#Base.error-Union{Tuple{T}, Tuple{AbstractVector{T}, AbstractVector{T}}} where T","page":"Utils","title":"Base.error","text":"error(ŷ,y;ignoreLabels=false) - Categorical error (T vs T)\n\n\n\n\n\n","category":"method"},{"location":"Utils.html#Base.error-Union{Tuple{T}, Tuple{Array{Dict{T, Float64}, 1}, Vector{T}}} where T","page":"Utils","title":"Base.error","text":"error(ŷ,y) - Categorical error with with probabilistic predictions of a dataset given in terms of a dictionary of probabilities (Dict{T,Float64} vs T). \n\n\n\n\n\n","category":"method"},{"location":"Utils.html#Base.error-Union{Tuple{T}, Tuple{Matrix{T}, Vector{Int64}}} where T<:Number","page":"Utils","title":"Base.error","text":"error(ŷ,y) - Categorical error with probabilistic predictions of a dataset (PMF vs Int). \n\n\n\n\n\n","category":"method"},{"location":"Utils.html#Base.error-Union{Tuple{T}, Tuple{Vector{T}, Int64}} where T<:Number","page":"Utils","title":"Base.error","text":"error(ŷ,y) - Categorical error with probabilistic prediction of a single datapoint (PMF vs Int). \n\n\n\n\n\n","category":"method"},{"location":"Utils.html#Base.print-Union{Tuple{T}, Tuple{IO, ConfusionMatrix{T}}, Tuple{IO, ConfusionMatrix{T}, Any}} where T","page":"Utils","title":"Base.print","text":"print(cm,what)\n\nPrint a ConfusionMatrix object\n\nThe what parameter is a string vector that can include \"all\", \"scores\", \"normalisedScores\" or \"report\" [def: [\"all\"]]\n\n\n\n\n\n","category":"method"},{"location":"Utils.html#Base.reshape-Union{Tuple{T}, Tuple{T, Vararg{Any, N} where N}} where T<:Number","page":"Utils","title":"Base.reshape","text":"reshape(myNumber, dims..) - Reshape a number as a n dimensional Array \n\n\n\n\n\n","category":"method"},{"location":"Utils.html#BetaML.Api.partition-Union{Tuple{T}, Tuple{AbstractVector{T}, AbstractVector{Float64}}} where T<:AbstractArray","page":"Utils","title":"BetaML.Api.partition","text":"partition(data,parts;shuffle,dims,rng)\n\nPartition (by rows) one or more matrices according to the shares in parts.\n\nParameters\n\ndata: A matrix/vector or a vector of matrices/vectors\nparts: A vector of the required shares (must sum to 1)\nshufle: Whether to randomly shuffle the matrices (preserving the relative order between matrices)\ndims: The dimension for which to partition [def: 1]\ncopy: Wheter to copy the actual data or only create a reference [def: true]\nrng: Random Number Generator (see FIXEDSEED) [deafult: Random.GLOBAL_RNG]\n\nNotes:\n\nThe sum of parts must be equal to 1\nThe number of elements in the specified dimension must be the same for all the arrays in data\n\nExample:\n\njulia julia> x = [1:10 11:20] julia> y = collect(31:40) julia> ((xtrain,xtest),(ytrain,ytest)) = partition([x,y],[0.7,0.3])\n\n\n\n\n\n","category":"method"},{"location":"Utils.html#BetaML.Utils.accuracy-Union{Tuple{T}, Tuple{AbstractVector{T}, AbstractVector{T}}} where T","page":"Utils","title":"BetaML.Utils.accuracy","text":"accuracy(ŷ,y;ignoreLabels=false) - Categorical accuracy between two vectors (T vs T). \n\n\n\n\n\n","category":"method"},{"location":"Utils.html#BetaML.Utils.accuracy-Union{Tuple{T}, Tuple{Array{Dict{T, Float64}, 1}, Vector{T}}} where T","page":"Utils","title":"BetaML.Utils.accuracy","text":"accuracy(ŷ,y;tol)\n\nCategorical accuracy with probabilistic predictions of a dataset given in terms of a dictionary of probabilities (Dict{T,Float64} vs T).\n\nParameters:\n\nŷ: An array where each item is the estimated probability mass function in terms of a Dictionary(Item1 => Prob1, Item2 => Prob2, ...)\ny: The N array with the correct category for each point n.\ntol: The tollerance to the prediction, i.e. if considering \"correct\" only a prediction where the value with highest probability is the true value (tol = 1), or consider instead the set of tol maximum values [def: 1].\n\n\n\n\n\n","category":"method"},{"location":"Utils.html#BetaML.Utils.accuracy-Union{Tuple{T}, Tuple{Dict{T, Float64}, T}} where T","page":"Utils","title":"BetaML.Utils.accuracy","text":"accuracy(ŷ,y;tol)\n\nCategorical accuracy with probabilistic prediction of a single datapoint given in terms of a dictionary of probabilities (Dict{T,Float64} vs T).\n\nParameters:\n\nŷ: The returned probability mass function in terms of a Dictionary(Item1 => Prob1, Item2 => Prob2, ...)\ntol: The tollerance to the prediction, i.e. if considering \"correct\" only a prediction where the value with highest probability is the true value (tol = 1), or consider instead the set of tol maximum values [def: 1].\n\n\n\n\n\n","category":"method"},{"location":"Utils.html#BetaML.Utils.accuracy-Union{Tuple{T}, Tuple{Matrix{T}, Vector{Int64}}} where T<:Number","page":"Utils","title":"BetaML.Utils.accuracy","text":"accuracy(ŷ,y;tol,ignoreLabels)\n\nCategorical accuracy with probabilistic predictions of a dataset (PMF vs Int).\n\nParameters:\n\nŷ: An (N,K) matrix of probabilities that each hat y_n record with n in 1N  being of category k with k in 1K.\ny: The N array with the correct category for each point n.\ntol: The tollerance to the prediction, i.e. if considering \"correct\" only a prediction where the value with highest probability is the true value (tol = 1), or consider instead the set of tol maximum values [def: 1].\nignoreLabels: Whether to ignore the specific label order in y. Useful for unsupervised learning algorithms where the specific label order don't make sense [def: false]\n\n\n\n\n\n","category":"method"},{"location":"Utils.html#BetaML.Utils.accuracy-Union{Tuple{T}, Tuple{Vector{T}, Int64}} where T<:Number","page":"Utils","title":"BetaML.Utils.accuracy","text":"accuracy(ŷ,y;tol)\n\nCategorical accuracy with probabilistic prediction of a single datapoint (PMF vs Int).\n\nUse the parameter tol [def: 1] to determine the tollerance of the prediction, i.e. if considering \"correct\" only a prediction where the value with highest probability is the true value (tol = 1), or consider instead the set of tol maximum values.\n\n\n\n\n\n","category":"method"},{"location":"Utils.html#BetaML.Utils.aic-Tuple{Any, Any}","page":"Utils","title":"BetaML.Utils.aic","text":"aic(lL,k) -  Akaike information criterion (lower is better)\n\n\n\n\n\n","category":"method"},{"location":"Utils.html#BetaML.Utils.autoJacobian-Tuple{Any, Any}","page":"Utils","title":"BetaML.Utils.autoJacobian","text":"autoJacobian(f,x;nY)\n\nEvaluate the Jacobian using AD in the form of a (nY,nX) matrix of first derivatives\n\nParameters:\n\nf: The function to compute the Jacobian\nx: The input to the function where the jacobian has to be computed\nnY: The number of outputs of the function f [def: length(f(x))]\n\nReturn values:\n\nAn Array{Float64,2} of the locally evaluated Jacobian\n\nNotes:\n\nThe nY parameter is optional. If provided it avoids having to compute f(x)\n\n\n\n\n\n","category":"method"},{"location":"Utils.html#BetaML.Utils.batch-Tuple{Integer, Integer}","page":"Utils","title":"BetaML.Utils.batch","text":"batch(n,bSize;sequential=false,rng)\n\nReturn a vector of bSize vectors of indeces from 1 to n. Randomly unless the optional parameter sequential is used.\n\nExample:\n\njulia julia> Utils.batch(6,2,sequential=true) 3-element Array{Array{Int64,1},1}:  [1, 2]  [3, 4]  [5, 6]\n\n\n\n\n\n","category":"method"},{"location":"Utils.html#BetaML.Utils.bic-Tuple{Any, Any, Any}","page":"Utils","title":"BetaML.Utils.bic","text":"bic(lL,k,n) -  Bayesian information criterion (lower is better)\n\n\n\n\n\n","category":"method"},{"location":"Utils.html#BetaML.Utils.celu-Tuple{Any}","page":"Utils","title":"BetaML.Utils.celu","text":"celu(x; α=1) \n\nhttps://arxiv.org/pdf/1704.07483.pdf\n\n\n\n\n\n","category":"method"},{"location":"Utils.html#BetaML.Utils.classCounts-Tuple{Any}","page":"Utils","title":"BetaML.Utils.classCounts","text":"classCounts(x;classes=nothing)\n\nReturn a (unsorted) vector with the counts of each unique item (element or rows) in a dataset.\n\nIf order is important or not all classes are present in the data, a preset vectors of classes can be given in the parameter classes\n\n\n\n\n\n","category":"method"},{"location":"Utils.html#BetaML.Utils.classCountsWithLabels-Tuple{Any}","page":"Utils","title":"BetaML.Utils.classCountsWithLabels","text":"classCountsWithLabels(x)\n\nReturn a dictionary that counts the number of each unique item (rows) in a dataset.\n\n\n\n\n\n","category":"method"},{"location":"Utils.html#BetaML.Utils.colsWithMissing-Tuple{Any}","page":"Utils","title":"BetaML.Utils.colsWithMissing","text":"colsWithMissing(x)\n\nRetuyrn an array with the ids of the columns where there is at least a missing value.\n\n\n\n\n\n","category":"method"},{"location":"Utils.html#BetaML.Utils.cosine_distance-Tuple{Any, Any}","page":"Utils","title":"BetaML.Utils.cosine_distance","text":"Cosine distance\n\n\n\n\n\n","category":"method"},{"location":"Utils.html#BetaML.Utils.crossEntropy-Tuple{Any, Any}","page":"Utils","title":"BetaML.Utils.crossEntropy","text":"crossEntropy(ŷ, y; weight)\n\nCompute the (weighted) cross-entropy between the predicted and the sampled probability distributions.\n\nTo be used in classification problems.\n\n\n\n\n\n","category":"method"},{"location":"Utils.html#BetaML.Utils.crossValidation","page":"Utils","title":"BetaML.Utils.crossValidation","text":"crossValidation(f,data,sampler;dims,verbosity,returnStatistics)\n\nPerform crossValidation according to sampler rule by calling the function f and collecting its output\n\nParameters\n\nf: The user-defined function that consume the specific train and validation data and return somehting (often the associated validation error). See later\ndata: A single n-dimenasional array or a vector of them (e.g. X,Y), depending on the tasks required by f.\nsampler: An istance of a AbstractDataSampler, defining the \"rules\" for sampling at each iteration. [def: KFold(nSplits=5,nRepeats=1,shuffle=true,rng=Random.GLOBAL_RNG) ]\ndims: The dimension over performing the crossValidation i.e. the dimension containing the observations [def: 1]\nverbosity: The verbosity to print information during each iteration (this can also be printed in the f function) [def: STD]\nreturnStatistics: Wheter crossValidation should return the statistics of the output of f (mean and standard deviation) or the whole outputs [def: true].\n\nNotes\n\ncrossValidation works by calling the function f, defined by the user, passing to it the tuple trainData, valData and rng and collecting the result of the function f. The specific method for which trainData, and valData are selected at each iteration depends on the specific sampler, whith a single 5 k-fold rule being the default.\n\nThis approach is very flexible because the specific model to employ or the metric to use is left within the user-provided function. The only thing that crossValidation does is provide the model defined in the function f with the opportune data (and the random number generator).\n\nInput of the user-provided function trainData and valData are both themselves tuples. In supervised models, crossValidations data should be a tuple of (X,Y) and trainData and valData will be equivalent to (xtrain, ytrain) and (xval, yval). In unsupervised models data is a single array, but the training and validation data should still need to be accessed as  trainData[1] and valData[1]. Output of the user-provided function The user-defined function can return whatever. However, if returnStatistics is left on its default true value the user-defined function must return a single scalar (e.g. some error measure) so that the mean and the standard deviation are returned.\n\nNote that crossValidation can beconveniently be employed using the do syntax, as Julia automatically rewrite crossValidation(data,...) trainData,valData,rng  ...user defined body... end as crossValidation(f(trainData,valData,rng ), data,...)\n\nExample\n\njulia> X = [11:19 21:29 31:39 41:49 51:59 61:69];\njulia> Y = [1:9;];\njulia> sampler = KFold(nSplits=3);\njulia> (μ,σ) = crossValidation([X,Y],sampler) do trainData,valData,rng\n                 (xtrain,ytrain) = trainData; (xval,yval) = valData\n                 trainedModel    = buildForest(xtrain,ytrain,30)\n                 predictions     = predict(trainedModel,xval)\n                 ϵ               = meanRelError(predictions,yval,normRec=false)\n                 return ϵ\n               end\n(0.3202242202242202, 0.04307662219315022)\n\n\n\n\n\n","category":"function"},{"location":"Utils.html#BetaML.Utils.dcelu-Tuple{Any}","page":"Utils","title":"BetaML.Utils.dcelu","text":"dcelu(x; α=1) \n\nhttps://arxiv.org/pdf/1704.07483.pdf\n\n\n\n\n\n","category":"method"},{"location":"Utils.html#BetaML.Utils.delu-Tuple{Any}","page":"Utils","title":"BetaML.Utils.delu","text":"delu(x; α=1) with α > 0 \n\nhttps://arxiv.org/pdf/1511.07289.pdf\n\n\n\n\n\n","category":"method"},{"location":"Utils.html#BetaML.Utils.dmish-Tuple{Any}","page":"Utils","title":"BetaML.Utils.dmish","text":"dmish(x) \n\nhttps://arxiv.org/pdf/1908.08681v1.pdf\n\n\n\n\n\n","category":"method"},{"location":"Utils.html#BetaML.Utils.dplu-Tuple{Any}","page":"Utils","title":"BetaML.Utils.dplu","text":"dplu(x;α=0.1,c=1) \n\nPiecewise Linear Unit derivative \n\nhttps://arxiv.org/pdf/1809.09534.pdf\n\n\n\n\n\n","category":"method"},{"location":"Utils.html#BetaML.Utils.drelu-Tuple{Any}","page":"Utils","title":"BetaML.Utils.drelu","text":"drelu(x) \n\nRectified Linear Unit \n\nhttps://www.cs.toronto.edu/~hinton/absps/reluICML.pdf\n\n\n\n\n\n","category":"method"},{"location":"Utils.html#BetaML.Utils.dsigmoid-Tuple{Any}","page":"Utils","title":"BetaML.Utils.dsigmoid","text":"dsigmoid(x)\n\n\n\n\n\n","category":"method"},{"location":"Utils.html#BetaML.Utils.dsoftmax-Tuple{Any}","page":"Utils","title":"BetaML.Utils.dsoftmax","text":"dsoftmax(x; β=1) \n\nDerivative of the softmax function \n\nhttps://eli.thegreenplace.net/2016/the-softmax-function-and-its-derivative/\n\n\n\n\n\n","category":"method"},{"location":"Utils.html#BetaML.Utils.dsoftplus-Tuple{Any}","page":"Utils","title":"BetaML.Utils.dsoftplus","text":"dsoftplus(x) \n\nhttps://en.wikipedia.org/wiki/Rectifier(neuralnetworks)#Softplus\n\n\n\n\n\n","category":"method"},{"location":"Utils.html#BetaML.Utils.dtanh-Tuple{Any}","page":"Utils","title":"BetaML.Utils.dtanh","text":"dtanh(x)\n\n\n\n\n\n","category":"method"},{"location":"Utils.html#BetaML.Utils.elu-Tuple{Any}","page":"Utils","title":"BetaML.Utils.elu","text":"elu(x; α=1) with α > 0 \n\nhttps://arxiv.org/pdf/1511.07289.pdf\n\n\n\n\n\n","category":"method"},{"location":"Utils.html#BetaML.Utils.entropy-Tuple{Any}","page":"Utils","title":"BetaML.Utils.entropy","text":"entropy(x)\n\nCalculate the entropy for a list of items (or rows).\n\nSee: https://en.wikipedia.org/wiki/Decisiontreelearning#Gini_impurity\n\n\n\n\n\n","category":"method"},{"location":"Utils.html#BetaML.Utils.generateParallelRngs-Tuple{Random.AbstractRNG, Integer}","page":"Utils","title":"BetaML.Utils.generateParallelRngs","text":"generateParallelRngs(rng::AbstractRNG, n::Integer;reSeed=false)\n\nFor multi-threaded models, return n independent random number generators (one per thread) to be used in threaded computations.\n\nNote that each ring is a copy of the original random ring. This means that code that use these RNGs will not change the original RNG state.\n\nUse it with rngs = generateParallelRngs(rng,Threads.nthreads()) to have a separate rng per thread. By default the function doesn't re-seed the RNG, as you may want to have a loop index based re-seeding strategy rather than a threadid-based one (to guarantee the same result independently of the number of threads). If you prefer, you can instead re-seed the RNG here (using the parameter reSeed=true), such that each thread has a different seed. Be aware however that the stream  of number generated will depend from the number of threads at run time.\n\n\n\n\n\n","category":"method"},{"location":"Utils.html#BetaML.Utils.getPermutations-Union{Tuple{AbstractVector{T}}, Tuple{T}} where T","page":"Utils","title":"BetaML.Utils.getPermutations","text":"getPermutations(v::AbstractArray{T,1};keepStructure=false)\n\nReturn a vector of either (a) all possible permutations (uncollected) or (b) just those based on the unique values of the vector\n\nUseful to measure accuracy where you don't care about the actual name of the labels, like in unsupervised classifications (e.g. clustering)\n\n\n\n\n\n","category":"method"},{"location":"Utils.html#BetaML.Utils.getScaleFactors-Tuple{Any}","page":"Utils","title":"BetaML.Utils.getScaleFactors","text":"getScaleFactors(x;skip)\n\nReturn the scale factors (for each dimensions) in order to scale a matrix X (n,d) such that each dimension has mean 0 and variance 1. Note that missing values are skipped.\n\nParameters\n\nx: the (n × d) dimension matrix to scale on each dimension d\nskip: an array of dimension index to skip the scaling [def: []]\n\nReturn\n\nA touple whose first elmement is the shift and the second the multiplicative\n\nterm to make the scale.\n\n\n\n\n\n","category":"method"},{"location":"Utils.html#BetaML.Utils.gini-Tuple{Any}","page":"Utils","title":"BetaML.Utils.gini","text":"gini(x)\n\nCalculate the Gini Impurity for a list of items (or rows).\n\nSee: https://en.wikipedia.org/wiki/Decisiontreelearning#Information_gain\n\n\n\n\n\n","category":"method"},{"location":"Utils.html#BetaML.Utils.integerDecoder-Union{Tuple{T}, Tuple{Any, AbstractVector{T}}} where T","page":"Utils","title":"BetaML.Utils.integerDecoder","text":"integerDecoder(x,factors::AbstractVector{T};unique)\n\nDecode an array of integers to an array of T corresponding to the elements of factors\n\nParameters:\n\nx: The vector to decode\nfactors: The vector of elements to use for the encoding\nunique: Wether factors is already made of unique elements [def: true]\n\nReturn:\n\nA vector of length(x) elements corresponding to the (unique) factors elements at the position x\n\nExample:\n\njulia> integerDecoder([1, 2, 2, 3, 2, 1],[\"aa\",\"cc\",\"bb\"]) # out: [\"aa\",\"cc\",\"cc\",\"bb\",\"cc\",\"aa\"]\n\n\n\n\n\n","category":"method"},{"location":"Utils.html#BetaML.Utils.integerEncoder-Tuple{AbstractVector{T} where T}","page":"Utils","title":"BetaML.Utils.integerEncoder","text":"integerEncoder(x;factors=unique(x))\n\nEncode an array of T to an array of integers using the their position in factor vector (default to the unique vector of the input array)\n\nParameters:\n\nx: The vector to encode\nfactors: The vector of factors whose position is the result of the encoding [def: unique(x)]\n\nReturn:\n\nA vector of [1,length(x)] integers corresponding to the position of each element in the factors vector`\n\nNote:\n\nAttention that while this function creates a ordered (and sortable) set, it is up to the user to be sure that this \"property\" is not indeed used in his code if the unencoded data is indeed unordered.\n\nExample:\n\njulia> integerEncoder([\"a\",\"e\",\"b\",\"e\"],factors=[\"a\",\"b\",\"c\",\"d\",\"e\"]) # out: [1,5,2,5]\n\n\n\n\n\n","category":"method"},{"location":"Utils.html#BetaML.Utils.issortable-Union{Tuple{AbstractArray{T, N}}, Tuple{N}, Tuple{T}} where {T, N}","page":"Utils","title":"BetaML.Utils.issortable","text":"Return wheather an array is sortable, i.e. has methos issort defined\n\n\n\n\n\n","category":"method"},{"location":"Utils.html#BetaML.Utils.l1_distance-Tuple{Any, Any}","page":"Utils","title":"BetaML.Utils.l1_distance","text":"L1 norm distance (aka Manhattan Distance)\n\n\n\n\n\n","category":"method"},{"location":"Utils.html#BetaML.Utils.l2_distance-Tuple{Any, Any}","page":"Utils","title":"BetaML.Utils.l2_distance","text":"Euclidean (L2) distance\n\n\n\n\n\n","category":"method"},{"location":"Utils.html#BetaML.Utils.l2²_distance-Tuple{Any, Any}","page":"Utils","title":"BetaML.Utils.l2²_distance","text":"Squared Euclidean (L2) distance\n\n\n\n\n\n","category":"method"},{"location":"Utils.html#BetaML.Utils.lse-Tuple{Any}","page":"Utils","title":"BetaML.Utils.lse","text":"LogSumExp for efficiently computing log(sum(exp.(x))) \n\n\n\n\n\n","category":"method"},{"location":"Utils.html#BetaML.Utils.makeMatrix-Tuple{AbstractArray}","page":"Utils","title":"BetaML.Utils.makeMatrix","text":"Transform an Array{T,1} in an Array{T,2} and leave unchanged Array{T,2}.\n\n\n\n\n\n","category":"method"},{"location":"Utils.html#BetaML.Utils.meanDicts-Tuple{Any}","page":"Utils","title":"BetaML.Utils.meanDicts","text":"meanDicts(dicts)\n\nCompute the mean of the values of an array of dictionaries.\n\nGiven dicts an array of dictionaries, meanDicts first compute the union of the keys and then average the values. If the original valueas are probabilities (non-negative items summing to 1), the result is also a probability distribution.\n\n\n\n\n\n","category":"method"},{"location":"Utils.html#BetaML.Utils.meanRelError-Tuple{Any, Any}","page":"Utils","title":"BetaML.Utils.meanRelError","text":"meanRelError(ŷ,y;normDim=true,normRec=true,p=1)\n\nCompute the mean relative error (l-1 based by default) between ŷ and y.\n\nThere are many ways to compute a mean relative error. In particular, if normRec (normDim) is set to true, the records (dimensions) are normalised, in the sense that it doesn't matter if a record (dimension) is bigger or smaller than the others, the relative error is first computed for each record (dimension) and then it is averaged. With both normDim and normRec set to false the function returns the relative mean error; with both set to true (default) it returns the mean relative error (i.e. with p=1 the \"mean absolute percentage error (MAPE)\") The parameter p [def: 1] controls the p-norm used to define the error.\n\nThe mean relative error enfatises the relativeness of the error, i.e. all observations and dimensions weigth the same, wether large or small. Conversly, in the relative mean error the same relative error on larger observations (or dimensions) weights more.\n\nFor example, given y = [1,44,3] and ŷ = [2,45,2], the mean relative error meanRelError(ŷ,y) is 0.452, while the relative mean error meanRelError(ŷ,y, normRec=false) is \"only\" 0.0625.\n\n\n\n\n\n","category":"method"},{"location":"Utils.html#BetaML.Utils.mish-Tuple{Any}","page":"Utils","title":"BetaML.Utils.mish","text":"mish(x) \n\nhttps://arxiv.org/pdf/1908.08681v1.pdf\n\n\n\n\n\n","category":"method"},{"location":"Utils.html#BetaML.Utils.mode-Union{Tuple{AbstractArray{Dict{T, Float64}, N} where N}, Tuple{T}} where T","page":"Utils","title":"BetaML.Utils.mode","text":"mode(elements,rng)\n\nGiven a vector of dictionaries whose key is numerical (e.g. probabilities), a vector of vectors or a matrix, it returns the mode of each element (dictionary, vector or row) in terms of the key or the position.\n\nUse it to return a unique value from a multiclass classifier returning probabilities.\n\nNote:\n\nIf multiple classes have the highest mode, one is returned at random (use the parameter rng to fix the stochasticity)\n\n\n\n\n\n","category":"method"},{"location":"Utils.html#BetaML.Utils.mode-Union{Tuple{AbstractVector{T}}, Tuple{T}} where T<:Number","page":"Utils","title":"BetaML.Utils.mode","text":"mode(v::AbstractVector{T};rng)\n\nReturn the position with the highest value in an array, interpreted as mode (using rand in case of multimodal values)\n\n\n\n\n\n","category":"method"},{"location":"Utils.html#BetaML.Utils.mode-Union{Tuple{Dict{T, Float64}}, Tuple{T}} where T","page":"Utils","title":"BetaML.Utils.mode","text":"mode(dict::Dict{T,Float64};rng)\n\nReturn the key with highest mode (using rand in case of multimodal values)\n\n\n\n\n\n","category":"method"},{"location":"Utils.html#BetaML.Utils.mse-Tuple{Any, Any}","page":"Utils","title":"BetaML.Utils.mse","text":"mse(ŷ,y)\n\nCompute the mean squared error (MSE) (aka mean squared deviation - MSD) between two vectors ŷ and y. Note that while the deviation is averaged by the length of y is is not scaled to give it a relative meaning.\n\n\n\n\n\n","category":"method"},{"location":"Utils.html#BetaML.Utils.oneHotDecoder-Tuple{Any}","page":"Utils","title":"BetaML.Utils.oneHotDecoder","text":"oneHotDecoder(x)\n\nGiven a matrix of one-hot encoded values (e.g. [0 1 0; 1 0 0]) returns a vector of the integer positions (e.g. [2,1]).\n\n\n\n\n\n","category":"method"},{"location":"Utils.html#BetaML.Utils.oneHotEncoder-Union{Tuple{Union{AbstractVector{T}, T}}, Tuple{T}} where T","page":"Utils","title":"BetaML.Utils.oneHotEncoder","text":"oneHotEncoder(x;d,factors,count)\n\nEncode arrays (or arrays of arrays) of categorical data as matrices of one column per factor.\n\nThe case of arrays of arrays is for when at each record you have more than one categorical output. You can then decide to encode just the presence of the factors or their counting\n\nParameters:\n\nx: The data to convert (array or array of arrays)\nd: The number of dimensions in the output matrix [def: maximum(x) for integers and length(factors) otherwise]\nfactors: The factors from which to encode [def: 1:d for integer x or unique(x) otherwise]\ncount: Wether to count multiple instances on the same dimension/record (true) or indicate just presence. [def: false]\n\nExamples\n\njulia> oneHotEncoder([\"a\",\"c\",\"c\"],factors=[\"a\",\"b\",\"c\",\"d\"])\n3×4 Matrix{Int64}:\n 1  0  0  0\n 0  0  1  0\n 0  0  1  0\njulia> oneHotEncoder([2,4,4])\n3×4 Matrix{Int64}:\n 0  1  0  0\n 0  0  0  1\n 0  0  0  1\n julia> oneHotEncoder([[2,2,1],[2,4,4]],count=true)\n2×4 Matrix{Int64}:\n 1  2  0  0\n 0  1  0  2\n\n\n\n\n\n","category":"method"},{"location":"Utils.html#BetaML.Utils.pca-Tuple{Any}","page":"Utils","title":"BetaML.Utils.pca","text":"pca(X;K,error)\n\nPerform Principal Component Analysis returning the matrix reprojected among the dimensions of maximum variance.\n\nParameters:\n\nX : The (N,D) data to reproject\nK : The number of dimensions to maintain (with K<=D) [def: nothing]\nerror: The maximum approximation error that we are willing to accept [def: 0.05]\n\nReturn:\n\nA named tuple with:\nX: The reprojected (NxK) matrix with the column dimensions organized in descending order of of the proportion of explained variance\nK: The number of dimensions retieved\nerror: The actual proportion of variance not explained in the reprojected dimensions\nP: The (D,K) matrix of the eigenvectors associated to the K-largest eigenvalues used to reproject the data matrix\nexplVarByDim: An array of dimensions D with the share of the cumulative variance explained by dimensions (the last element being always 1.0)\n\nNotes:\n\nIf K is provided, the parameter error has no effect.\nIf one doesn't know a priori the error that she/he is willling to accept, nor the wished number of dimensions, he/she can run this pca function with out = pca(X,K=size(X,2)) (i.e. with K=D), analise the proportions of explained cumulative variance by dimensions in out.explVarByDim, choose the number of dimensions K according to his/her needs and finally pick from the reprojected matrix only the number of dimensions needed, i.e. out.X[:,1:K].\n\nExample:\n\njulia> X = [1 10 100; 1.1 15 120; 0.95 23 90; 0.99 17 120; 1.05 8 90; 1.1 12 95]\n6×3 Matrix{Float64}:\n 1.0   10.0  100.0\n 1.1   15.0  120.0\n 0.95  23.0   90.0\n 0.99  17.0  120.0\n 1.05   8.0   90.0\n 1.1   12.0   95.0\njulia> X = pca(X,error=0.05).X\n6×2 Matrix{Float64}:\n 100.449    3.1783\n 120.743    6.80764\n  91.3551  16.8275\n 120.878    8.80372\n  90.3363   1.86179\n  95.5965   5.51254\n\n\n\n\n\n","category":"method"},{"location":"Utils.html#BetaML.Utils.plu-Tuple{Any}","page":"Utils","title":"BetaML.Utils.plu","text":"plu(x;α=0.1,c=1) \n\nPiecewise Linear Unit \n\nhttps://arxiv.org/pdf/1809.09534.pdf\n\n\n\n\n\n","category":"method"},{"location":"Utils.html#BetaML.Utils.polynomialKernel-Tuple{Any, Any}","page":"Utils","title":"BetaML.Utils.polynomialKernel","text":"Polynomial kernel parametrised with c=0 and d=2 (i.e. a quadratic kernel). For other cᵢ and dᵢ use K = (x,y) -> polynomialKernel(x,y,c=cᵢ,d=dᵢ) as kernel function in the supporting algorithms\n\n\n\n\n\n","category":"method"},{"location":"Utils.html#BetaML.Utils.pool1d","page":"Utils","title":"BetaML.Utils.pool1d","text":"pool1d(x,poolSize=2;f=mean)\n\nApply funtion f to a rolling poolSize contiguous (in 1d) neurons.\n\nApplicable to VectorFunctionLayer, e.g. layer2  = VectorFunctionLayer(nₗ,f=(x->pool1d(x,4,f=mean)) Attention: to apply this funciton as activation function in a neural network you will need Julia version >= 1.6, otherwise you may experience a segmentation fault (see this bug report)\n\n\n\n\n\n","category":"function"},{"location":"Utils.html#BetaML.Utils.radialKernel-Tuple{Any, Any}","page":"Utils","title":"BetaML.Utils.radialKernel","text":"Radial Kernel (aka RBF kernel) parametrised with γ=1/2. For other gammas γᵢ use K = (x,y) -> radialKernel(x,y,γ=γᵢ) as kernel function in the supporting algorithms\n\n\n\n\n\n","category":"method"},{"location":"Utils.html#BetaML.Utils.relu-Tuple{Any}","page":"Utils","title":"BetaML.Utils.relu","text":"relu(x) \n\nRectified Linear Unit \n\nhttps://www.cs.toronto.edu/~hinton/absps/reluICML.pdf\n\n\n\n\n\n","category":"method"},{"location":"Utils.html#BetaML.Utils.scale","page":"Utils","title":"BetaML.Utils.scale","text":"scale(x,scaleFactors;rev)\n\nPerform a linear scaling of x using scaling factors scaleFactors.\n\nParameters\n\nx: The (n × d) dimension matrix to scale on each dimension d\nscalingFactors: A tuple of the constant and multiplicative scaling factor\n\nrespectively [def: the scaling factors needed to scale x to mean 0 and variance 1]\n\nrev: Whether to invert the scaling [def: false]\n\nReturn\n\nThe scaled matrix\n\nNotes:\n\nAlso available scale!(x,scaleFactors) for in-place scaling\nRetrieve the scale factors with the getScaleFactors() function\nNote that missing values are skipped\n\n\n\n\n\n","category":"function"},{"location":"Utils.html#BetaML.Utils.sigmoid-Tuple{Any}","page":"Utils","title":"BetaML.Utils.sigmoid","text":"sigmoid(x)\n\n\n\n\n\n","category":"method"},{"location":"Utils.html#BetaML.Utils.singleUnique-Union{Tuple{Union{AbstractArray{T, N} where N, T}}, Tuple{T}} where T","page":"Utils","title":"BetaML.Utils.singleUnique","text":"singleUnique(x) Return the unique values of x whether x is an array of arrays, an array or a scalar\n\n\n\n\n\n","category":"method"},{"location":"Utils.html#BetaML.Utils.softmax-Tuple{Any}","page":"Utils","title":"BetaML.Utils.softmax","text":"softmax (x; β=1) \n\nThe input x is a vector. Return a PMF\n\n\n\n\n\n","category":"method"},{"location":"Utils.html#BetaML.Utils.softplus-Tuple{Any}","page":"Utils","title":"BetaML.Utils.softplus","text":"softplus(x) \n\nhttps://en.wikipedia.org/wiki/Rectifier(neuralnetworks)#Softplus\n\n\n\n\n\n","category":"method"},{"location":"Utils.html#BetaML.Utils.squaredCost-Tuple{Any, Any}","page":"Utils","title":"BetaML.Utils.squaredCost","text":"squaredCost(ŷ,y)\n\nCompute the squared costs between a vector of prediction and one of observations as (1/2)*norm(y - ŷ)^2.\n\nAside the 1/2 term, it correspond to the squared l-2 norm distance and when it is averaged on multiple datapoints corresponds to the Mean Squared Error (MSE). It is mostly used for regression problems.\n\n\n\n\n\n","category":"method"},{"location":"Utils.html#BetaML.Utils.sterling-Tuple{BigInt, BigInt}","page":"Utils","title":"BetaML.Utils.sterling","text":"Sterling number: number of partitions of a set of n elements in k sets \n\n\n\n\n\n","category":"method"},{"location":"Utils.html#BetaML.Utils.variance-Tuple{Any}","page":"Utils","title":"BetaML.Utils.variance","text":"variance(x) - population variance\n\n\n\n\n\n","category":"method"},{"location":"Utils.html#Random.shuffle-Union{Tuple{AbstractVector{T}}, Tuple{T}} where T<:AbstractArray","page":"Utils","title":"Random.shuffle","text":"shuffle(data;dims,rng)\n\nShuffle a vector of n-dimensional arrays across dimension dims keeping the same order between the arrays\n\nParameters\n\ndata: The vector of arrays to shuffle\ndims: The dimension over to apply the shuffle [def: 1]\nrng:  An AbstractRNG to apply for the shuffle\n\nNotes\n\nAll the arrays must have the same size for the dimension to shuffle\n\nExample\n\njulia> a = [1 2 30; 10 20 30]; b = [100 200 300]; julia> (aShuffled, bShuffled) = shuffle([a,b],dims=2) 2-element Vector{Matrix{Int64}}:  [1 30 2; 10 30 20]  [100 300 200]\n\n\n\n\n\n","category":"method"},{"location":"Utils.html#BetaML.Utils.@codeLocation-Tuple{}","page":"Utils","title":"BetaML.Utils.@codeLocation","text":"@codeLocation()\n\nHelper macro to print during runtime an info message concerning the code being executed position\n\n\n\n\n\n","category":"macro"},{"location":"Trees.html#trees_module","page":"Trees","title":"The BetaML.Trees Module","text":"","category":"section"},{"location":"Trees.html","page":"Trees","title":"Trees","text":"Trees","category":"page"},{"location":"Trees.html#BetaML.Trees","page":"Trees","title":"BetaML.Trees","text":"BetaML.Trees module\n\nImplement the functionality required to build a Decision Tree or a whole Random Forest, predict data and assess its performances.\n\nBoth Decision Trees and Random Forests can be used for regression or classification problems, based on the type of the labels (numerical or not). You can override the automatic selection with the parameter forceClassification=true, typically if your labels are integer representing some categories rather than numbers. For classification problems the output of predictSingle is a dictionary with the key being the labels with non-zero probabilitity and the corresponding value its proobability; for regression it is a numerical value.\n\nPlease be aware that, differently from most other implementations, the Random Forest algorithm collects and averages the probabilities from the trees, rather than just repording the mode, i.e. no information is lost and the output of the forest classifier is still a PMF.\n\nMissing data on features are supported, both on training and on prediction.\n\nThe module provide the following functions. Use ?[type or function] to access their full signature and detailed documentation:\n\nModel definition and training:\n\nbuildTree(xtrain,ytrain): Build a single Decision Tree\nbuildForest(xtrain,ytrain): Build a \"forest\" of Decision Trees\n\nModel predictions and assessment:\n\npredict(tree or forest, x): Return the prediction given the feature matrix\noobError(forest,x,y): Return the out-of-bag error estimate\nUtils.accuracy(ŷ,y)): Categorical output accuracy\nUtils.meanRelError(ŷ,y,p): L-p norm based error\n\nFeatures are expected to be in the standard format (nRecords × nDimensions matrices) and the labels (either categorical or numerical) as a nRecords column vector.\n\nAcknowlegdments: originally based on the Josh Gordon's code\n\n\n\n\n\n","category":"module"},{"location":"Trees.html#Module-Index","page":"Trees","title":"Module Index","text":"","category":"section"},{"location":"Trees.html","page":"Trees","title":"Trees","text":"Modules = [Trees]\nOrder   = [:constant, :type, :function, :macro]","category":"page"},{"location":"Trees.html#Detailed-API","page":"Trees","title":"Detailed API","text":"","category":"section"},{"location":"Trees.html","page":"Trees","title":"Trees","text":"Modules = [Trees]","category":"page"},{"location":"Trees.html#BetaML.Trees.AbstractQuestion","page":"Trees","title":"BetaML.Trees.AbstractQuestion","text":"Question\n\nA question used to partition a dataset.\n\nThis struct just records a 'column number' and a 'column value' (e.g., Green).\n\n\n\n\n\n","category":"type"},{"location":"Trees.html#BetaML.Trees.DecisionNode","page":"Trees","title":"BetaML.Trees.DecisionNode","text":"DecisionNode(question,trueBranch,falseBranch, depth)\n\nA tree's non-terminal node.\n\nConstructor's arguments and struct members:\n\nquestion: The question asked in this node\ntrueBranch: A reference to the \"true\" branch of the trees\nfalseBranch: A reference to the \"false\" branch of the trees\ndepth: The nodes's depth in the tree\n\n\n\n\n\n","category":"type"},{"location":"Trees.html#BetaML.Trees.Forest","page":"Trees","title":"BetaML.Trees.Forest","text":"Forest{Ty}\n\nType representing a Random Forest.\n\nIndividual trees are stored in the array trees. The \"type\" of the forest is given by the type of the labels on which it has been trained.\n\nStruct members:\n\ntrees:        The individual Decision Trees\nisRegression: Whether the forest is to be used for regression jobs or classification\noobData:      For each tree, the rows number if the data that have not being used to train the specific tree\noobError:     The out of bag error (if it has been computed)\nweights:      A weight for each tree depending on the tree's score on the oobData (see buildForest)\n\n\n\n\n\n","category":"type"},{"location":"Trees.html#BetaML.Trees.Leaf","page":"Trees","title":"BetaML.Trees.Leaf","text":"Leaf(y,depth)\n\nA tree's leaf (terminal) node.\n\nConstructor's arguments:\n\ny: The labels assorciated to each record (either numerical or categorical)\ndepth: The nodes's depth in the tree\n\nStruct members:\n\npredictions: Either the relative label's count (i.e. a PMF) or the mean\ndepth: The nodes's depth in the tree\n\n\n\n\n\n","category":"type"},{"location":"Trees.html#BetaML.Api.partition-Union{Tuple{Tx}, Tuple{BetaML.Trees.Question{Tx}, Any, Any}} where Tx","page":"Trees","title":"BetaML.Api.partition","text":"partition(question,x)\n\nDicotomically partitions a dataset x given a question.\n\nFor each row in the dataset, check if it matches the question. If so, add it to 'true rows', otherwise, add it to 'false rows'. Rows with missing values on the question column are assigned randomly proportionally to the assignment of the non-missing rows.\n\n\n\n\n\n","category":"method"},{"location":"Trees.html#BetaML.Api.predict-Union{Tuple{Ty}, Tuple{Forest{Ty}, Any}} where Ty","page":"Trees","title":"BetaML.Api.predict","text":"predict(forest,x)\n\nPredict the labels of a feature dataset.\n\nFor each record of the dataset and each tree of the \"forest\", recursivelly traverse the tree to find the prediction most opportune for the given record. If the labels the tree has been trained with are numeric, the prediction is also numeric (the mean of the different trees predictions, in turn the mean of the labels of the training records ended in that leaf node). If the labels were categorical, the prediction is a dictionary with the probabilities of each item and in such case the probabilities of the different trees are averaged to compose the forest predictions. This is a bit different than most other implementations where the mode instead is reported.\n\nIn the first case (numerical predictions) use meanRelError(ŷ,y) to assess the mean relative error, in the second case you can use accuracy(ŷ,y).\n\n\n\n\n\n","category":"method"},{"location":"Trees.html#BetaML.Api.predict-Union{Tuple{Ty}, Tuple{Tx}, Tuple{Union{DecisionNode{Tx}, Leaf{Ty}}, Any}} where {Tx, Ty}","page":"Trees","title":"BetaML.Api.predict","text":"predict(tree,x)\n\nPredict the labels of a feature dataset.\n\nFor each record of the dataset, recursivelly traverse the tree to find the prediction most opportune for the given record. If the labels the tree has been fitted with are numeric, the prediction is also numeric. If the labels were categorical, the prediction is a dictionary with the probabilities of each item.\n\nIn the first case (numerical predictions) use meanRelError(ŷ,y) to assess the mean relative error, in the second case you can use accuracy(ŷ,y).\n\n\n\n\n\n","category":"method"},{"location":"Trees.html#BetaML.Trees._printNode","page":"Trees","title":"BetaML.Trees._printNode","text":"print(node)\n\nPrint a Decision Tree (textual)\n\n\n\n\n\n","category":"function"},{"location":"Trees.html#BetaML.Trees.buildForest-Union{Tuple{Ty}, Tuple{Any, AbstractVector{Ty}}, Tuple{Any, AbstractVector{Ty}, Any}} where Ty","page":"Trees","title":"BetaML.Trees.buildForest","text":"buildForest(x, y, nTrees; maxDepth, minGain, minRecords, maxFeatures, splittingCriterion, forceClassification)\n\nBuilds (define and train) a \"forest\" of Decision Trees.\n\nParameters:\n\nSee buildTree. The function has all the parameters of bildTree (with the maxFeatures defaulting to √D instead of D) plus the following parameters:\n\nnTrees: Number of trees in the forest [def: 30]\nβ: Parameter that regulate the weights of the scoring of each tree, to be (optionally) used in prediction (see later) [def: 0, i.e. uniform weigths]\noob: Whether to coompute the out-of-bag error, an estimation of the generalization accuracy [def: false]\nrng: Random Number Generator (see FIXEDSEED) [deafult: Random.GLOBAL_RNG]\n\nOutput:\n\nThe function returns a Forest object (see Forest).\nThe forest weights default to array of ones if β ≤ 0 and the oob error to +Inf if oob == false.\n\nNotes :\n\nEach individual decision tree is built using bootstrap over the data, i.e. \"sampling N records with replacement\" (hence, some records appear multiple times and some records do not appear in the specific tree training). The maxFeature injects further variability and reduces the correlation between the forest trees.\nThe predictions of the \"forest\" (using the function predict()) are then the aggregated predictions of the individual trees (from which the name \"bagging\": boostrap aggregating).\nThis function optionally reports a weight distribution of the performances of eanch individual trees, as measured using the records he has not being trained with. These weights can then be (optionally) used in the predict function. The parameter β ≥ 0 regulate the distribution of these weights: larger is β, the greater the importance (hence the weights) attached to the best-performing trees compared to the low-performing ones. Using these weights can significantly improve the forest performances (especially using small forests), however the correct value of β depends on the problem under exam (and the chosen caratteristics of the random forest estimator) and should be cross-validated to avoid over-fitting.\nNote that this function uses multiple threads if these are available. You can check the number of threads available with Threads.nthreads(). To set the number of threads in Julia either set the environmental variable JULIA_NUM_THREADS (before starting Julia) or start Julia with the command line option --threads (most integrated development editors for Julia already set the number of threads to 4).\n\n\n\n\n\n","category":"method"},{"location":"Trees.html#BetaML.Trees.buildTree-Union{Tuple{Ty}, Tuple{Any, AbstractVector{Ty}}} where Ty","page":"Trees","title":"BetaML.Trees.buildTree","text":"buildTree(x, y, depth; maxDepth, minGain, minRecords, maxFeatures, splittingCriterion, forceClassification)\n\nBuilds (define and train) a Decision Tree.\n\nGiven a dataset of features x and the corresponding dataset of labels y, recursivelly build a decision tree by finding at each node the best question to split the data untill either all the dataset is separated or a terminal condition is reached. The given tree is then returned.\n\nParameters:\n\nx: The dataset's features (N × D)\ny: The dataset's labels (N × 1)\nmaxDepth: The maximum depth the tree is allowed to reach. When this is reached the node is forced to become a leaf [def: N, i.e. no limits]\nminGain: The minimum information gain to allow for a node's partition [def: 0]\nminRecords:  The minimum number of records a node must holds to consider for a partition of it [def: 2]\nmaxFeatures: The maximum number of (random) features to consider at each partitioning [def: D, i.e. look at all features]\nsplittingCriterion: Either gini, entropy or variance (see infoGain ) [def: gini for categorical labels (classification task) and variance for numerical labels(regression task)]\nforceClassification: Weather to force a classification task even if the labels are numerical (typically when labels are integers encoding some feature rather than representing a real cardinal measure) [def: false]\nrng: Random Number Generator ((see FIXEDSEED)) [deafult: Random.GLOBAL_RNG]\n\nNotes:\n\nMissing data (in the feature dataset) are supported.\n\n\n\n\n\n","category":"method"},{"location":"Trees.html#BetaML.Trees.findBestSplit-Union{Tuple{Ty}, Tuple{Any, AbstractVector{Ty}, Any}} where Ty","page":"Trees","title":"BetaML.Trees.findBestSplit","text":"findBestSplit(x,y;maxFeatures,splittingCriterion)\n\nFind the best possible split of the database.\n\nFind the best question to ask by iterating over every feature / value and calculating the information gain.\n\nParameters:\n\nx: The feature dataset\ny: The labels dataset\nmaxFeatures: Maximum number of (random) features to look up for the \"best split\"\nsplittingCriterion: The metric to define the \"impurity\" of the labels\nrng: Random Number Generator (see FIXEDSEED) [deafult: Random.GLOBAL_RNG]\n\n\n\n\n\n","category":"method"},{"location":"Trees.html#BetaML.Trees.infoGain-Tuple{Any, Any, Any}","page":"Trees","title":"BetaML.Trees.infoGain","text":"infoGain(left, right, parentUncertainty; splittingCriterion)\n\nCompute the information gain of a specific partition.\n\nCompare the \"information gain\" my measuring the difference betwwen the \"impurity\" of the labels of the parent node with those of the two child nodes, weighted by the respective number of items.\n\nParameters:\n\nleftY:  Child #1 labels\nrightY: Child #2 labels\nparentUncertainty: \"Impurity\" of the labels of the parent node\nsplittingCriterion: Metric to adopt to determine the \"impurity\" (see below)\n\nYou can use your own function as the metric. We provide the following built-in metrics:\n\ngini (categorical)\nentropy (categorical)\nvariance (numerical)\n\n\n\n\n\n","category":"method"},{"location":"Trees.html#BetaML.Trees.match-Union{Tuple{Tx}, Tuple{BetaML.Trees.Question{Tx}, Any}} where Tx","page":"Trees","title":"BetaML.Trees.match","text":"match(question, x)\n\nReturn a dicotomic answer of a question when applied to a given feature record.\n\nIt compares the feature value in the given record to the value stored in the question. Numerical features are compared in terms of disequality (\">=\"), while categorical features are compared in terms of equality (\"==\").\n\n\n\n\n\n","category":"method"},{"location":"Trees.html#BetaML.Trees.oobError-Union{Tuple{Ty}, Tuple{Forest{Ty}, Any, Any}} where Ty","page":"Trees","title":"BetaML.Trees.oobError","text":"oobError(forest,x,y;rng)\n\nComute the Out-Of-Bag error, an estimation of the validation error.\n\nThis function is called at time of train the forest if the parameter oob is true, or can be used later to get the oob error on an already trained forest. The oob error reported is the mismatching error for classification and the relative mean error for regression. \n\n\n\n\n\n","category":"method"},{"location":"Trees.html#BetaML.Trees.predictSingle-Union{Tuple{Ty}, Tuple{Forest{Ty}, Any}} where Ty","page":"Trees","title":"BetaML.Trees.predictSingle","text":"predictSingle(forest,x)\n\nPredict the label of a single feature record. See predict.\n\n\n\n\n\n","category":"method"},{"location":"Trees.html#BetaML.Trees.predictSingle-Union{Tuple{Ty}, Tuple{Tx}, Tuple{Union{DecisionNode{Tx}, Leaf{Ty}}, Any}} where {Tx, Ty}","page":"Trees","title":"BetaML.Trees.predictSingle","text":"predictSingle(tree,x)\n\nPredict the label of a single feature record. See predict.\n\n\n\n\n\n","category":"method"},{"location":"Trees.html#BetaML.Trees.updateTreesWeights!-Union{Tuple{Ty}, Tuple{Forest{Ty}, Any, Any}} where Ty","page":"Trees","title":"BetaML.Trees.updateTreesWeights!","text":"updateTreesWeights!(forest,x,y;β)\n\nUpdate the weights of each tree (to use in the prediction of the forest) based on the error of the individual tree computed on the records on which it has not been trained. As training a forest is expensive, this function can be used to \"just\" upgrade the trees weights using different betas, without retraining the model.\n\n\n\n\n\n","category":"method"},{"location":"tutorials/Clustering - Iris/betaml_tutorial_cluster_iris.html","page":"A clustering task: the prediction of  plant species from floreal measures (the iris dataset)","title":"A clustering task: the prediction of  plant species from floreal measures (the iris dataset)","text":"EditURL = \"https://github.com/sylvaticus/BetaML.jl/blob/master/docs/src/tutorials/Clustering - Iris/betaml_tutorial_cluster_iris.jl\"","category":"page"},{"location":"tutorials/Clustering - Iris/betaml_tutorial_cluster_iris.html#clustering_tutorial","page":"A clustering task: the prediction of  plant species from floreal measures (the iris dataset)","title":"A clustering task: the prediction of  plant species from floreal measures (the iris dataset)","text":"","category":"section"},{"location":"tutorials/Clustering - Iris/betaml_tutorial_cluster_iris.html","page":"A clustering task: the prediction of  plant species from floreal measures (the iris dataset)","title":"A clustering task: the prediction of  plant species from floreal measures (the iris dataset)","text":"The task is to estimate the species of a plant given some floreal measurements. It use the classical \"Iris\" dataset. Note that in this example we are using clustering approaches, so we try to understand the \"structure\" of our data, without relying to actually knowing the true labels (\"classes\" or \"factors\"). However we have chosen a dataset for which the true labels are actually known, so to compare the accuracy of the algorithms we use, but these labels will not be used during the algorithms training.","category":"page"},{"location":"tutorials/Clustering - Iris/betaml_tutorial_cluster_iris.html","page":"A clustering task: the prediction of  plant species from floreal measures (the iris dataset)","title":"A clustering task: the prediction of  plant species from floreal measures (the iris dataset)","text":"Data origin:","category":"page"},{"location":"tutorials/Clustering - Iris/betaml_tutorial_cluster_iris.html","page":"A clustering task: the prediction of  plant species from floreal measures (the iris dataset)","title":"A clustering task: the prediction of  plant species from floreal measures (the iris dataset)","text":"dataset description: https://en.wikipedia.org/wiki/Irisflowerdata_set\ndata source we use here: https://github.com/JuliaStats/RDatasets.jl","category":"page"},{"location":"tutorials/Clustering - Iris/betaml_tutorial_cluster_iris.html","page":"A clustering task: the prediction of  plant species from floreal measures (the iris dataset)","title":"A clustering task: the prediction of  plant species from floreal measures (the iris dataset)","text":"!!! warning As the above example is automatically executed by GitHub on every code update, it uses parameters (epoch numbers, parameter space of hyperparameter validation, number of trees,...) that minimise the computation. In real case you will want to use better but more computationally intensive ones. For the same reason benchmarks codes are commented and the pre-run output reported rather than actually being executed.","category":"page"},{"location":"tutorials/Clustering - Iris/betaml_tutorial_cluster_iris.html#Library-and-data-loading","page":"A clustering task: the prediction of  plant species from floreal measures (the iris dataset)","title":"Library and data loading","text":"","category":"section"},{"location":"tutorials/Clustering - Iris/betaml_tutorial_cluster_iris.html","page":"A clustering task: the prediction of  plant species from floreal measures (the iris dataset)","title":"A clustering task: the prediction of  plant species from floreal measures (the iris dataset)","text":"We load the Beta Machine Learning Toolkit as well as some other packages that we use in this tutorial","category":"page"},{"location":"tutorials/Clustering - Iris/betaml_tutorial_cluster_iris.html","page":"A clustering task: the prediction of  plant species from floreal measures (the iris dataset)","title":"A clustering task: the prediction of  plant species from floreal measures (the iris dataset)","text":"using BetaML\nusing Random, Statistics, Logging, BenchmarkTools, RDatasets, Plots, DataFrames","category":"page"},{"location":"tutorials/Clustering - Iris/betaml_tutorial_cluster_iris.html","page":"A clustering task: the prediction of  plant species from floreal measures (the iris dataset)","title":"A clustering task: the prediction of  plant species from floreal measures (the iris dataset)","text":"We are also going to compare our results with two other leading packages in Julia for clustering analysis, Clustering.jl that provides (inter alia) kmeans and kmedoids algorithms and GaussianMixtures.jl that provides, as the name says, Gaussian Mixture Models. So we import them (we \"import\" them, rather than \"use\", not to bound their full names into namespace as some would collide with BetaML).","category":"page"},{"location":"tutorials/Clustering - Iris/betaml_tutorial_cluster_iris.html","page":"A clustering task: the prediction of  plant species from floreal measures (the iris dataset)","title":"A clustering task: the prediction of  plant species from floreal measures (the iris dataset)","text":"import Clustering, GaussianMixtures","category":"page"},{"location":"tutorials/Clustering - Iris/betaml_tutorial_cluster_iris.html","page":"A clustering task: the prediction of  plant species from floreal measures (the iris dataset)","title":"A clustering task: the prediction of  plant species from floreal measures (the iris dataset)","text":"We do a few tweeks for the Clustering and GaussianMixtures packages. Note that in BetaML we can also control both the random seed and the verbosity in the algorithm call, not only globally","category":"page"},{"location":"tutorials/Clustering - Iris/betaml_tutorial_cluster_iris.html","page":"A clustering task: the prediction of  plant species from floreal measures (the iris dataset)","title":"A clustering task: the prediction of  plant species from floreal measures (the iris dataset)","text":"Random.seed!(123)\n#logger  = Logging.SimpleLogger(stdout, Logging.Error); global_logger(logger); ## For suppressing GaussianMixtures output\nnothing #hide","category":"page"},{"location":"tutorials/Clustering - Iris/betaml_tutorial_cluster_iris.html","page":"A clustering task: the prediction of  plant species from floreal measures (the iris dataset)","title":"A clustering task: the prediction of  plant species from floreal measures (the iris dataset)","text":"Differently from the regression tutorial, we load the data here from [RDatasets](https://github.com/JuliaStats/RDatasets.jl](https://github.com/JuliaStats/RDatasets.jl), a package providing standard datasets.","category":"page"},{"location":"tutorials/Clustering - Iris/betaml_tutorial_cluster_iris.html","page":"A clustering task: the prediction of  plant species from floreal measures (the iris dataset)","title":"A clustering task: the prediction of  plant species from floreal measures (the iris dataset)","text":"iris = dataset(\"datasets\", \"iris\")\ndescribe(iris)","category":"page"},{"location":"tutorials/Clustering - Iris/betaml_tutorial_cluster_iris.html","page":"A clustering task: the prediction of  plant species from floreal measures (the iris dataset)","title":"A clustering task: the prediction of  plant species from floreal measures (the iris dataset)","text":"The iris dataset  provides floreal measures in columns 1 to 4 and the assigned species name in column 5. There are no missing values","category":"page"},{"location":"tutorials/Clustering - Iris/betaml_tutorial_cluster_iris.html#Data-preparation","page":"A clustering task: the prediction of  plant species from floreal measures (the iris dataset)","title":"Data preparation","text":"","category":"section"},{"location":"tutorials/Clustering - Iris/betaml_tutorial_cluster_iris.html","page":"A clustering task: the prediction of  plant species from floreal measures (the iris dataset)","title":"A clustering task: the prediction of  plant species from floreal measures (the iris dataset)","text":"The first step is to prepare the data for the analysis. We collect the first 4 columns as our feature x matrix and the last one as our y label vector. As we are using clustering algorithms, we are not actually using the labels to train the algorithms, we'll behave like we do not know them, we'll just let the algorithm \"learn\" fro mthe structure of the data itself. We'll however use it to judge the accuracy that they did reach.","category":"page"},{"location":"tutorials/Clustering - Iris/betaml_tutorial_cluster_iris.html","page":"A clustering task: the prediction of  plant species from floreal measures (the iris dataset)","title":"A clustering task: the prediction of  plant species from floreal measures (the iris dataset)","text":"x       = Matrix{Float64}(iris[:,1:4]);\nyLabels = unique(iris[:,5]);\nnothing #hide","category":"page"},{"location":"tutorials/Clustering - Iris/betaml_tutorial_cluster_iris.html","page":"A clustering task: the prediction of  plant species from floreal measures (the iris dataset)","title":"A clustering task: the prediction of  plant species from floreal measures (the iris dataset)","text":"As the labels are expressed as strings, the first thing we do is encode them as integers for our analysis using the function integerEncoder.","category":"page"},{"location":"tutorials/Clustering - Iris/betaml_tutorial_cluster_iris.html","page":"A clustering task: the prediction of  plant species from floreal measures (the iris dataset)","title":"A clustering task: the prediction of  plant species from floreal measures (the iris dataset)","text":"y       = integerEncoder(iris[:,5],factors=yLabels);\nnothing #hide","category":"page"},{"location":"tutorials/Clustering - Iris/betaml_tutorial_cluster_iris.html","page":"A clustering task: the prediction of  plant species from floreal measures (the iris dataset)","title":"A clustering task: the prediction of  plant species from floreal measures (the iris dataset)","text":"The dataset from RDatasets is ordered by species, so we need to shuffle it to avoid biases. Shuffling happens by default in crossValidation, but we are keeping here a copy of the shuffled version for later. Note that the version of shuffle that is included in BetaML accepts several n-dimensional arrays and shuffle them (by default on rows, by we can specify the dimension) keeping the association  between the various arrays in the shuffled output.","category":"page"},{"location":"tutorials/Clustering - Iris/betaml_tutorial_cluster_iris.html","page":"A clustering task: the prediction of  plant species from floreal measures (the iris dataset)","title":"A clustering task: the prediction of  plant species from floreal measures (the iris dataset)","text":"(xs,ys) = shuffle([x,y]);\nnothing #hide","category":"page"},{"location":"tutorials/Clustering - Iris/betaml_tutorial_cluster_iris.html#Main-analysis","page":"A clustering task: the prediction of  plant species from floreal measures (the iris dataset)","title":"Main analysis","text":"","category":"section"},{"location":"tutorials/Clustering - Iris/betaml_tutorial_cluster_iris.html","page":"A clustering task: the prediction of  plant species from floreal measures (the iris dataset)","title":"A clustering task: the prediction of  plant species from floreal measures (the iris dataset)","text":"We will try 3 BetaML models (kmeans, kmedoids and gmm) and we compare them with kmeans from Clusterings.jl and GMM from GaussianMixtures.jl Kmeans and kmedoids works by first initialising the centers of the k-clusters (the \"representative\" (step a ) . For kmeans they must be selected within one of the data, for kmeans they are the geometrical center) n a nutshell. Then ( b ) iterate for each point to assign the point to the cluster of the closest representative (according with a user defined distance metric, default to Euclidean), and ( c ) move each representative at the center of its newly acquired cluster (where \"center\" depends again from the metric). Steps ( b ) and ( c ) are reiterated until the algorithm converge, i.e. the tentative k representative points (and their relative clusters) don't move any more. The result (output of the algorithm) is that each point is assigned to one of the clusters (classes). The gmm algorithm is similar in that it employs an iterative approach (the ExpectationMinimisation algorithm, \"em\") but here we make the hipothesis that the data points are the observed outcomes of some _mixture probabilistic models where we have first a k-categorical variables whose outcomes are the (unobservble) parameters of a probabilistic distribution from which the data is finally drawn. Because the parameters of each of the k-possible distributions is unobservable this is also called a model with latent variables. Most gmm models use the Gaussain distribution as the family of the mixture components, so we can tought the gmm acronym to indicate Gaussian Mixture Model. In BetaML we do implemented only Gaussain components, but any distribution could be used by just subclassing AbstractMixture and implementing a couple of methids (you are invited to contribute or just ask for a distribution family you are interested), so I prefer to think \"gmm\" as an acronym for Generative Mixture Model. The algorithm try to find the mixture that maximises the likelihood that the data has been generated indeed from such mixture, where the \"E\" step refers to computing the probability that each point belongs to each of the k-composants (somehow similar to the step b in the kmeans/kmedoids algorithm), and the \"M\" step estimates, giving the association probabilities in step \"M\", the parameters of the mixture and of the individual components (similar to step c). The result here is that each point has a categorical distribution (PMF) representing the probabilities that it belongs to any of the k-components (our classes or clusters). This is interesting, as gmm can be used for many other things that clustering. It forms the backbone of the predictMissing function to impute missing values (on some or all dimensions) based to how close the record seems to its pears. For the same reasons, predictMissing can also be used to predict user's behaviours (or users' appreciation) according to the behaviour/ranking made by pears (\"collaborative filtering\"). While the result of gmm is a vector of PMFs (one for each record), error measures and reports with the true values (if known) can be directly applied, as in BetaML they internally call mode() to retrieve the class with the highest probability for each record.","category":"page"},{"location":"tutorials/Clustering - Iris/betaml_tutorial_cluster_iris.html","page":"A clustering task: the prediction of  plant species from floreal measures (the iris dataset)","title":"A clustering task: the prediction of  plant species from floreal measures (the iris dataset)","text":"As we are here, we also try different versions of the BetaML models, even if the default \"versions\" should be fine. For kmeans and kmedoids we will try different initialisation strategies (\"gird\", the default one, \"random\" and \"shuffle\"), while for the gmm model we'll choose different distributions of the Gaussain family (SphericalGaussian - where the variance is a scalar, DiagonalGaussian - with a vector variance, and FullGaussian, where the covariance is a matrix).","category":"page"},{"location":"tutorials/Clustering - Iris/betaml_tutorial_cluster_iris.html","page":"A clustering task: the prediction of  plant species from floreal measures (the iris dataset)","title":"A clustering task: the prediction of  plant species from floreal measures (the iris dataset)","text":"As the result would depend on stochasticity both in the data selected and in the random initialisation, we use a cross-validation approach to run our models several times (with different data) and then we average their results. Cross-Validation in BetaML is very flexible and it is done using the crossValidation function. crossValidation works by calling the function f, defined by the user, passing to it the tuple trainData, valData and rng and collecting the result of the function f. The specific method for which trainData, and valData are selected at each iteration depends on the specific sampler. We start by selectign a k-fold sampler that split our data in 5 different parts, it uses 4 for training and 1 part (not used here) for validation. We run the simulations twice and, to be sure to have replicable results, we fix the random seed (at the whole crossValidaiton level, not on each iteration).","category":"page"},{"location":"tutorials/Clustering - Iris/betaml_tutorial_cluster_iris.html","page":"A clustering task: the prediction of  plant species from floreal measures (the iris dataset)","title":"A clustering task: the prediction of  plant species from floreal measures (the iris dataset)","text":"sampler = KFold(nSplits=5,nRepeats=3,shuffle=true, rng=copy(FIXEDRNG))","category":"page"},{"location":"tutorials/Clustering - Iris/betaml_tutorial_cluster_iris.html","page":"A clustering task: the prediction of  plant species from floreal measures (the iris dataset)","title":"A clustering task: the prediction of  plant species from floreal measures (the iris dataset)","text":"We can now run the cross-validation with our models. Note that instead of defining the function f and then calling crossValidation[f(trainData,testData,rng),[x,y],...) we use the Julia do block syntax and we write directly the content of the f function in the do block. Also, by default crossValidation already returns the mean and the standard deviation of the output of the user-provided f function (or the do block). However this requires that the f function return a single scalar. Here we are returning a vector of the accuracies of the different models (so we can run the cross-validation only once), and hence we indicate with returnStatistics=false to crossValidation not to attempt to generate statistics but rather report the whole output. We'll compute the statistics ex-post.","category":"page"},{"location":"tutorials/Clustering - Iris/betaml_tutorial_cluster_iris.html","page":"A clustering task: the prediction of  plant species from floreal measures (the iris dataset)","title":"A clustering task: the prediction of  plant species from floreal measures (the iris dataset)","text":"Inside the do block we do 4 things:","category":"page"},{"location":"tutorials/Clustering - Iris/betaml_tutorial_cluster_iris.html","page":"A clustering task: the prediction of  plant species from floreal measures (the iris dataset)","title":"A clustering task: the prediction of  plant species from floreal measures (the iris dataset)","text":"we recover from trainData (a tuple, as we passed a tuple to crossValidation too) the xtrain features and ytrain labels;\nwe run the various clustering algorithms\nwe use the real labels to compute the model accuracy. Note that the clustering algorithm know nothing about the specific label name or even their order. This is why accuracy has the parameter ignoreLabels to compute the accuracy oven any possible permutation of the classes found.\nwe return the various models' accuracies","category":"page"},{"location":"tutorials/Clustering - Iris/betaml_tutorial_cluster_iris.html","page":"A clustering task: the prediction of  plant species from floreal measures (the iris dataset)","title":"A clustering task: the prediction of  plant species from floreal measures (the iris dataset)","text":"cOut = crossValidation([x,y],sampler,returnStatistics=false) do trainData,testData,rng\n          # For unsupervised learning we use only the train data.\n          # Also, we use the associated labels only to measure the performances\n         (xtrain,ytrain)  = trainData;\n         # We run the clustering algorithm...\n         clusteringOut     = kmeans(xtrain,3,rng=rng) ## init is grid by default\n         # ... and we compute the accuracy using the real labels\n         kMeansAccuracy    = accuracy(clusteringOut[1],ytrain,ignoreLabels=true)\n         clusteringOut     = kmeans(xtrain,3,rng=rng,initStrategy=\"random\")\n         kMeansRAccuracy   = accuracy(clusteringOut[1],ytrain,ignoreLabels=true)\n         clusteringOut     = kmeans(xtrain,3,rng=rng,initStrategy=\"shuffle\")\n         kMeansSAccuracy   = accuracy(clusteringOut[1],ytrain,ignoreLabels=true)\n         clusteringOut     = kmedoids(xtrain,3,rng=rng)   ## init is grid by default\n         kMedoidsAccuracy  = accuracy(clusteringOut[1],ytrain,ignoreLabels=true)\n         clusteringOut     = kmedoids(xtrain,3,rng=rng,initStrategy=\"random\")\n         kMedoidsRAccuracy = accuracy(clusteringOut[1],ytrain,ignoreLabels=true)\n         clusteringOut     = kmedoids(xtrain,3,rng=rng,initStrategy=\"shuffle\")\n         kMedoidsSAccuracy = accuracy(clusteringOut[1],ytrain,ignoreLabels=true)\n         clusteringOut     = gmm(xtrain,3,mixtures=[SphericalGaussian() for i in 1:3], verbosity=NONE, rng=rng)\n         gmmSpherAccuracy  = accuracy(clusteringOut.pₙₖ,ytrain,ignoreLabels=true, rng=rng)\n         clusteringOut     = gmm(xtrain,3,mixtures=[DiagonalGaussian() for i in 1:3], verbosity=NONE, rng=rng)\n         gmmDiagAccuracy   = accuracy(clusteringOut.pₙₖ,ytrain,ignoreLabels=true, rng=rng)\n         clusteringOut     = gmm(xtrain,3,mixtures=[FullGaussian() for i in 1:3], verbosity=NONE, rng=rng)\n         gmmFullAccuracy   = accuracy(clusteringOut.pₙₖ,ytrain,ignoreLabels=true, rng=rng)\n         # For comparision with Clustering.jl\n         clusteringOut     = Clustering.kmeans(xtrain', 3)\n         kMeans2Accuracy   = accuracy(clusteringOut.assignments,ytrain,ignoreLabels=true)\n         # For comparision with GaussianMistures.jl - sometimes GaussianMistures.jl em! fails with a PosDefException\n         dGMM              = GaussianMixtures.GMM(3, xtrain; method=:kmeans, kind=:diag)\n         GaussianMixtures.em!(dGMM, xtrain)\n         gmmDiag2Accuracy  = accuracy(GaussianMixtures.gmmposterior(dGMM, xtrain)[1],ytrain,ignoreLabels=true)\n         fGMM              = GaussianMixtures.GMM(3, xtrain; method=:kmeans, kind=:full)\n         GaussianMixtures.em!(fGMM, xtrain)\n         gmmFull2Accuracy  = accuracy(GaussianMixtures.gmmposterior(fGMM, xtrain)[1],ytrain,ignoreLabels=true)\n         # Returning the accuracies\n         return kMeansAccuracy,kMeansRAccuracy,kMeansSAccuracy,kMedoidsAccuracy,kMedoidsRAccuracy,kMedoidsSAccuracy,gmmSpherAccuracy,gmmDiagAccuracy,gmmFullAccuracy,kMeans2Accuracy,gmmDiag2Accuracy,gmmFull2Accuracy\n end\n\n# We transform the output in matrix for easier analysis\naccuracies = fill(0.0,(length(cOut),length(cOut[1])))\n[accuracies[r,c] = cOut[r][c] for r in 1:length(cOut),c in 1:length(cOut[1])]\nμs = mean(accuracies,dims=1)\nσs = std(accuracies,dims=1)\n\n\nmodelLabels=[\"kMeansG\",\"kMeansR\",\"kMeansS\",\"kMedoidsG\",\"kMedoidsR\",\"kMedoidsS\",\"gmmSpher\",\"gmmDiag\",\"gmmFull\",\"kMeans (Clustering.jl)\",\"gmmDiag (GaussianMixtures.jl)\",\"gmmFull (GaussianMixtures.jl)\"]\nreport = DataFrame(mName = modelLabels, avgAccuracy = dropdims(round.(μs',digits=3),dims=2), stdAccuracy = dropdims(round.(σs',digits=3),dims=2))","category":"page"},{"location":"tutorials/Clustering - Iris/betaml_tutorial_cluster_iris.html#BetaML-model-accuracies","page":"A clustering task: the prediction of  plant species from floreal measures (the iris dataset)","title":"BetaML model accuracies","text":"","category":"section"},{"location":"tutorials/Clustering - Iris/betaml_tutorial_cluster_iris.html","page":"A clustering task: the prediction of  plant species from floreal measures (the iris dataset)","title":"A clustering task: the prediction of  plant species from floreal measures (the iris dataset)","text":"From the output We see that the gmm models perform for this dataset generally better than kmeans or kmedoids algorithms, also with very low variances. In detail, it is the (default) grid initialisation that leads to the better results for kmeans and kmedoids, while for the gmm models it is the FullGaussian to perform better.","category":"page"},{"location":"tutorials/Clustering - Iris/betaml_tutorial_cluster_iris.html#Comparisions-with-Clustering.jl-and-GaussianMixtures.jl","page":"A clustering task: the prediction of  plant species from floreal measures (the iris dataset)","title":"Comparisions with Clustering.jl and GaussianMixtures.jl","text":"","category":"section"},{"location":"tutorials/Clustering - Iris/betaml_tutorial_cluster_iris.html","page":"A clustering task: the prediction of  plant species from floreal measures (the iris dataset)","title":"A clustering task: the prediction of  plant species from floreal measures (the iris dataset)","text":"For this specific case, both Clustering.jl and GaussianMixtures.jl report substantially worst accuracies, and with very high variances. But we maintain the ranking that Full Gaussian gmm > Diagonal Gaussian > Kmeans accuracy. I suspect the reason that BetaML gmm works so weel is in relation to the usage of kmeans algorithm with itself the grid initialisation. The grid initialisation \"guarantee\" indeed that the initial means of the mixture components are well spread across the multidimensional space defined by the data, and it helps avoiding the EM algoritm to converge to a bad local optimus.","category":"page"},{"location":"tutorials/Clustering - Iris/betaml_tutorial_cluster_iris.html#Working-without-the-labels","page":"A clustering task: the prediction of  plant species from floreal measures (the iris dataset)","title":"Working without the labels","text":"","category":"section"},{"location":"tutorials/Clustering - Iris/betaml_tutorial_cluster_iris.html","page":"A clustering task: the prediction of  plant species from floreal measures (the iris dataset)","title":"A clustering task: the prediction of  plant species from floreal measures (the iris dataset)","text":"Up to now we used the real labels to compare the model accuracies. But in real clustering examples we don't have the true classes, or we wouln't need to do clustering in the first instance, so we don't know the number of classes to use. There are several methods to judge clusters algorithms goodness, perhaps the simplest one, at least for the expectation-maximisation algorithm employed in gmm to fit the data to the unknown mixture, is to use a information criteria that trade the goodness of the lickelyhood with the parameters used to do the fit. BetaML provide by default in the gmm clustering outputs both the Bayesian information criterion  (BIC) and the Akaike information criterion  (AIC), where for both a lower value is better.","category":"page"},{"location":"tutorials/Clustering - Iris/betaml_tutorial_cluster_iris.html","page":"A clustering task: the prediction of  plant species from floreal measures (the iris dataset)","title":"A clustering task: the prediction of  plant species from floreal measures (the iris dataset)","text":"We can then run the model with different number of classes and see which one leads to the lower BIC or AIC. We run hence crossValidation again with the FullGaussian gmm model Note that we use the BIC/AIC criteria here for establishing the \"best\" number of classes but we could have used it also to select the kind of Gaussain distribution to use. This is one example of hyper-parameter tuning that we developed more in detail (but without using cross-validation) in the regression tutorial.","category":"page"},{"location":"tutorials/Clustering - Iris/betaml_tutorial_cluster_iris.html","page":"A clustering task: the prediction of  plant species from floreal measures (the iris dataset)","title":"A clustering task: the prediction of  plant species from floreal measures (the iris dataset)","text":"Let's try up to 4 possible classes:","category":"page"},{"location":"tutorials/Clustering - Iris/betaml_tutorial_cluster_iris.html","page":"A clustering task: the prediction of  plant species from floreal measures (the iris dataset)","title":"A clustering task: the prediction of  plant species from floreal measures (the iris dataset)","text":"K = 4\nsampler = KFold(nSplits=5,nRepeats=2,shuffle=true, rng=copy(FIXEDRNG))\ncOut = crossValidation([x,y],sampler,returnStatistics=false) do trainData,testData,rng\n    (xtrain,ytrain)  = trainData;\n    clusteringOut  = [gmm(xtrain,k,mixtures=[FullGaussian() for i in 1:k], verbosity=NONE, rng=rng) for k in 1:K]\n    BICS           = [clusteringOut[i].BIC for i in 1:K]\n    AICS           = [clusteringOut[i].AIC for i in 1:K]\n    return (BICS,AICS)\nend\n\n# Transforming the output in matrices for easier analysis\nNit = length(cOut)\n\nBICS = fill(0.0,(Nit,K))\nAICS = fill(0.0,(Nit,K))\n[BICS[r,c] = cOut[r][1][c] for r in 1:Nit,c in 1:K]\n[AICS[r,c] = cOut[r][2][c] for r in 1:Nit,c in 1:K]\n\nμsBICS = mean(BICS,dims=1)","category":"page"},{"location":"tutorials/Clustering - Iris/betaml_tutorial_cluster_iris.html","page":"A clustering task: the prediction of  plant species from floreal measures (the iris dataset)","title":"A clustering task: the prediction of  plant species from floreal measures (the iris dataset)","text":"σsBICS = std(BICS,dims=1)","category":"page"},{"location":"tutorials/Clustering - Iris/betaml_tutorial_cluster_iris.html","page":"A clustering task: the prediction of  plant species from floreal measures (the iris dataset)","title":"A clustering task: the prediction of  plant species from floreal measures (the iris dataset)","text":"μsAICS = mean(AICS,dims=1)","category":"page"},{"location":"tutorials/Clustering - Iris/betaml_tutorial_cluster_iris.html","page":"A clustering task: the prediction of  plant species from floreal measures (the iris dataset)","title":"A clustering task: the prediction of  plant species from floreal measures (the iris dataset)","text":"σsAICS = std(AICS,dims=1)","category":"page"},{"location":"tutorials/Clustering - Iris/betaml_tutorial_cluster_iris.html","page":"A clustering task: the prediction of  plant species from floreal measures (the iris dataset)","title":"A clustering task: the prediction of  plant species from floreal measures (the iris dataset)","text":"plot(1:K,[μsBICS' μsAICS'], labels=[\"BIC\" \"AIC\"], title=\"Information criteria by number of classes\", xlabel=\"number of classes\", ylabel=\"lower is better\")","category":"page"},{"location":"tutorials/Clustering - Iris/betaml_tutorial_cluster_iris.html","page":"A clustering task: the prediction of  plant species from floreal measures (the iris dataset)","title":"A clustering task: the prediction of  plant species from floreal measures (the iris dataset)","text":"We see that following the \"lowest AIC\" rule we would indeed choose three classes, while following the \"best AIC\" criteria we would have choosen only two classes. This means that there is two classes that, concerning the floreal measures used in the database, are very similar, and opur models are unsure about them. Perhaps the biologists will end up one day with the conclusion that it is indeed only one specie :-).","category":"page"},{"location":"tutorials/Clustering - Iris/betaml_tutorial_cluster_iris.html","page":"A clustering task: the prediction of  plant species from floreal measures (the iris dataset)","title":"A clustering task: the prediction of  plant species from floreal measures (the iris dataset)","text":"We could study this issue more in detail by analysing the ConfusionMatrix, but the one used in BetaML does not account for the ignoreLabels option (yet).","category":"page"},{"location":"tutorials/Clustering - Iris/betaml_tutorial_cluster_iris.html#Benchmarking-computational-efficiency","page":"A clustering task: the prediction of  plant species from floreal measures (the iris dataset)","title":"Benchmarking computational efficiency","text":"","category":"section"},{"location":"tutorials/Clustering - Iris/betaml_tutorial_cluster_iris.html","page":"A clustering task: the prediction of  plant species from floreal measures (the iris dataset)","title":"A clustering task: the prediction of  plant species from floreal measures (the iris dataset)","text":"We now benchmark the time and memory required by the various models by using the @btime macro of the BenchmarkTools package:","category":"page"},{"location":"tutorials/Clustering - Iris/betaml_tutorial_cluster_iris.html","page":"A clustering task: the prediction of  plant species from floreal measures (the iris dataset)","title":"A clustering task: the prediction of  plant species from floreal measures (the iris dataset)","text":"@btime kmeans($xs,3);\n# 261.540 μs (3777 allocations: 442.53 KiB)\n@btime kmedoids($xs,3);\n4.576 ms (97356 allocations: 10.42 MiB)\n@btime gmm($xs,3,mixtures=[SphericalGaussian() for i in 1:3], verbosity=NONE);\n# 5.498 ms (133365 allocations: 8.42 MiB)\n@btime gmm($xs,3,mixtures=[DiagonalGaussian() for i in 1:3], verbosity=NONE);\n# 18.901 ms (404333 allocations: 25.65 MiB)\n@btime gmm($xs,3,mixtures=[FullGaussian() for i in 1:3], verbosity=NONE);\n# 49.257 ms (351500 allocations: 61.95 MiB)\n@btime Clustering.kmeans($xs', 3);\n# 17.071 μs (23 allocations: 14.31 KiB)\n@btime begin dGMM = GaussianMixtures.GMM(3, $xs; method=:kmeans, kind=:diag); GaussianMixtures.em!(dGMM, $xs) end;\n# 530.528 μs (2088 allocations: 488.05 KiB)\n@btime begin fGMM = GaussianMixtures.GMM(3, $xs; method=:kmeans, kind=:full); GaussianMixtures.em!(fGMM, $xs) end;\n# 4.166 ms (58910 allocations: 3.59 MiB)","category":"page"},{"location":"tutorials/Clustering - Iris/betaml_tutorial_cluster_iris.html","page":"A clustering task: the prediction of  plant species from floreal measures (the iris dataset)","title":"A clustering task: the prediction of  plant species from floreal measures (the iris dataset)","text":"(note: the values reported here are of a local pc, not of the GitHub CI server, as sometimes - depending on data and random initialisation - GaussainMixtures.em!fails with aPosDefException`. This in turn would lead the whole documentation to fail to compile)","category":"page"},{"location":"tutorials/Clustering - Iris/betaml_tutorial_cluster_iris.html","page":"A clustering task: the prediction of  plant species from floreal measures (the iris dataset)","title":"A clustering task: the prediction of  plant species from floreal measures (the iris dataset)","text":"Like for supervised models, dedicated models are much better optimized than BetaML models, and are order of magnitude more efficient. However even the slowest BetaML clusering model (gmm using full gaussians) is realtively fast and can handle mid-size datasets (tens to hundreds of thousand records) without significant slow downs.","category":"page"},{"location":"tutorials/Clustering - Iris/betaml_tutorial_cluster_iris.html#Conclusions","page":"A clustering task: the prediction of  plant species from floreal measures (the iris dataset)","title":"Conclusions","text":"","category":"section"},{"location":"tutorials/Clustering - Iris/betaml_tutorial_cluster_iris.html","page":"A clustering task: the prediction of  plant species from floreal measures (the iris dataset)","title":"A clustering task: the prediction of  plant species from floreal measures (the iris dataset)","text":"We have shown in this tutorial how we can easily run clustering almgorithms in BetaML with just one line of code choosenModel(x,k), but also how can we use cross-validation in order to help the model or parameter selection, with or whithout knowing the real classes. We retrieve here what we observed with supervised models. Globally the accuracy of BetaML models are comparable to those of leading specialised packages (in this case they are even better), but there is a significant gap in computational efficiency that restricts the pratical usage of BetaML to mid-size datasets. However we trade this relative inefficiency with very flexible model definition and utility functions (for example the BetaML gmm works with missing data, allowing it to be used as the backbone of the predictMissing missing imputation function, or for collaborative reccomendation systems).","category":"page"},{"location":"tutorials/Clustering - Iris/betaml_tutorial_cluster_iris.html","page":"A clustering task: the prediction of  plant species from floreal measures (the iris dataset)","title":"A clustering task: the prediction of  plant species from floreal measures (the iris dataset)","text":"View this file on Github.","category":"page"},{"location":"tutorials/Clustering - Iris/betaml_tutorial_cluster_iris.html","page":"A clustering task: the prediction of  plant species from floreal measures (the iris dataset)","title":"A clustering task: the prediction of  plant species from floreal measures (the iris dataset)","text":"","category":"page"},{"location":"tutorials/Clustering - Iris/betaml_tutorial_cluster_iris.html","page":"A clustering task: the prediction of  plant species from floreal measures (the iris dataset)","title":"A clustering task: the prediction of  plant species from floreal measures (the iris dataset)","text":"This page was generated using Literate.jl.","category":"page"},{"location":"tutorials/Regression - bike sharing/betaml_tutorial_regression_sharingBikes.html","page":"A regression task: the prediction of  bike  sharing demand","title":"A regression task: the prediction of  bike  sharing demand","text":"EditURL = \"https://github.com/sylvaticus/BetaML.jl/blob/master/docs/src/tutorials/Regression - bike sharing/betaml_tutorial_regression_sharingBikes.jl\"","category":"page"},{"location":"tutorials/Regression - bike sharing/betaml_tutorial_regression_sharingBikes.html#regression_tutorial","page":"A regression task: the prediction of  bike  sharing demand","title":"A regression task: the prediction of  bike  sharing demand","text":"","category":"section"},{"location":"tutorials/Regression - bike sharing/betaml_tutorial_regression_sharingBikes.html","page":"A regression task: the prediction of  bike  sharing demand","title":"A regression task: the prediction of  bike  sharing demand","text":"The task is to estimate the influence of several variables (like the weather, the season, the day of the week..) on the demand of shared bicycles, so that the authority in charge of the service can organise the service in the best way.","category":"page"},{"location":"tutorials/Regression - bike sharing/betaml_tutorial_regression_sharingBikes.html","page":"A regression task: the prediction of  bike  sharing demand","title":"A regression task: the prediction of  bike  sharing demand","text":"Data origin:","category":"page"},{"location":"tutorials/Regression - bike sharing/betaml_tutorial_regression_sharingBikes.html","page":"A regression task: the prediction of  bike  sharing demand","title":"A regression task: the prediction of  bike  sharing demand","text":"original full dataset (by hour, not used here): https://archive.ics.uci.edu/ml/datasets/Bike+Sharing+Dataset\nsimplified dataset (by day, with some simple scaling): https://www.hds.utc.fr/~tdenoeux/dokuwiki/en/aec\ndescription: https://www.hds.utc.fr/~tdenoeux/dokuwiki/media/en/exam2019ace.pdf\ndata: https://www.hds.utc.fr/~tdenoeux/dokuwiki/media/en/bikesharing_day.csv.zip","category":"page"},{"location":"tutorials/Regression - bike sharing/betaml_tutorial_regression_sharingBikes.html","page":"A regression task: the prediction of  bike  sharing demand","title":"A regression task: the prediction of  bike  sharing demand","text":"Note that even if we are estimating a time serie, we are not using here a recurrent neural network as we assume the temporal dependence to be negligible (i.e. Y_t = f(X_t) alone).","category":"page"},{"location":"tutorials/Regression - bike sharing/betaml_tutorial_regression_sharingBikes.html","page":"A regression task: the prediction of  bike  sharing demand","title":"A regression task: the prediction of  bike  sharing demand","text":"!!! warning As the above example is automatically executed by GitHub on every code update, it uses parameters (epoch numbers, parameter space of hyperparameter validation, number of trees,...) that minimise the computation. In real case you will want to use better but more computationally intensive ones. For the same reason benchmarks codes are commented and the pre-run output reported rather than actually being executed.","category":"page"},{"location":"tutorials/Regression - bike sharing/betaml_tutorial_regression_sharingBikes.html#Library-and-data-loading","page":"A regression task: the prediction of  bike  sharing demand","title":"Library and data loading","text":"","category":"section"},{"location":"tutorials/Regression - bike sharing/betaml_tutorial_regression_sharingBikes.html","page":"A regression task: the prediction of  bike  sharing demand","title":"A regression task: the prediction of  bike  sharing demand","text":"We first load all the packages we are going to use","category":"page"},{"location":"tutorials/Regression - bike sharing/betaml_tutorial_regression_sharingBikes.html","page":"A regression task: the prediction of  bike  sharing demand","title":"A regression task: the prediction of  bike  sharing demand","text":"using  LinearAlgebra, Random, Statistics, DataFrames, CSV, Plots, Pipe, BenchmarkTools, BetaML\nimport Distributions: Uniform\nimport DecisionTree, Flux ## For comparisions","category":"page"},{"location":"tutorials/Regression - bike sharing/betaml_tutorial_regression_sharingBikes.html","page":"A regression task: the prediction of  bike  sharing demand","title":"A regression task: the prediction of  bike  sharing demand","text":"Here we load the data from a csv provided by the BataML package","category":"page"},{"location":"tutorials/Regression - bike sharing/betaml_tutorial_regression_sharingBikes.html","page":"A regression task: the prediction of  bike  sharing demand","title":"A regression task: the prediction of  bike  sharing demand","text":"baseDir = joinpath(dirname(pathof(BetaML)),\"..\",\"docs\",\"src\",\"tutorials\",\"Regression - bike sharing\")\ndata    = CSV.File(joinpath(baseDir,\"data\",\"bike_sharing_day.csv\"),delim=',') |> DataFrame\ndescribe(data)","category":"page"},{"location":"tutorials/Regression - bike sharing/betaml_tutorial_regression_sharingBikes.html","page":"A regression task: the prediction of  bike  sharing demand","title":"A regression task: the prediction of  bike  sharing demand","text":"The variable we want to learn to predict is cnt, the total demand of bikes for a given day. Even if it is indeed an integer, we treat it as a continuous variable, so each single prediction will be a scalar Y in mathbbR.","category":"page"},{"location":"tutorials/Regression - bike sharing/betaml_tutorial_regression_sharingBikes.html","page":"A regression task: the prediction of  bike  sharing demand","title":"A regression task: the prediction of  bike  sharing demand","text":"plot(data.cnt, title=\"Daily bike sharing rents (2Y)\", label=nothing)","category":"page"},{"location":"tutorials/Regression - bike sharing/betaml_tutorial_regression_sharingBikes.html#Decision-Trees","page":"A regression task: the prediction of  bike  sharing demand","title":"Decision Trees","text":"","category":"section"},{"location":"tutorials/Regression - bike sharing/betaml_tutorial_regression_sharingBikes.html","page":"A regression task: the prediction of  bike  sharing demand","title":"A regression task: the prediction of  bike  sharing demand","text":"We start our regression task with Decision Trees.","category":"page"},{"location":"tutorials/Regression - bike sharing/betaml_tutorial_regression_sharingBikes.html","page":"A regression task: the prediction of  bike  sharing demand","title":"A regression task: the prediction of  bike  sharing demand","text":"Decision trees training consist in choosing the set of questions (in a hierarcical way, so to form indeed a \"decision tree\") that \"best\" split the dataset given for training, in the sense that the split generate the sub-samples (always 2 subsamples in the BetaML implementation) that are, for the characteristic we want to predict, the most homogeneous possible. Decision trees are one of the few ML algorithms that has an intuitive interpretation and can be used for both regression or classification tasks.","category":"page"},{"location":"tutorials/Regression - bike sharing/betaml_tutorial_regression_sharingBikes.html#Data-preparation","page":"A regression task: the prediction of  bike  sharing demand","title":"Data preparation","text":"","category":"section"},{"location":"tutorials/Regression - bike sharing/betaml_tutorial_regression_sharingBikes.html","page":"A regression task: the prediction of  bike  sharing demand","title":"A regression task: the prediction of  bike  sharing demand","text":"The first step is to prepare the data for the analysis. This indeed depends already on the model we want to employ, as some models \"accept\" almost everything as input, no matter if the data is numerical or categorical, if it has missing values or not... while other models are instead much more exigents, and require more work to \"clean up\" our dataset.","category":"page"},{"location":"tutorials/Regression - bike sharing/betaml_tutorial_regression_sharingBikes.html","page":"A regression task: the prediction of  bike  sharing demand","title":"A regression task: the prediction of  bike  sharing demand","text":"Here we start using  Decision Tree and Random Forest models that definitly belong to the first group, so the only thing we have to do is to select the variables in input (the \"feature matrix\", that we will indicate with \"X\") and the variable representing our output (the information we want to learn to predict, we call it \"y\"):","category":"page"},{"location":"tutorials/Regression - bike sharing/betaml_tutorial_regression_sharingBikes.html","page":"A regression task: the prediction of  bike  sharing demand","title":"A regression task: the prediction of  bike  sharing demand","text":"x    = Matrix{Float64}(data[:,[:instant,:season,:yr,:mnth,:holiday,:weekday,:workingday,:weathersit,:temp,:atemp,:hum,:windspeed]])\ny    = data[:,16];\nnothing #hide","category":"page"},{"location":"tutorials/Regression - bike sharing/betaml_tutorial_regression_sharingBikes.html","page":"A regression task: the prediction of  bike  sharing demand","title":"A regression task: the prediction of  bike  sharing demand","text":"We can now split the dataset between the data we will use for training the algorithm (xtrain/ytrain), those for selecting the hyperparameters (xval/yval) and finally those for testing the quality of the algoritm with the optimal hyperparameters (xtest/ytest). We use the partition function specifying the share we want to use for these three different subsets, here 75%, 12.5% and 12.5 respectively. As our data represents indeed a time serie, we want our model to be able to predict future demand of bike sharing from past, observed rented bikes, so we do not shuffle the datasets as it would be the default.","category":"page"},{"location":"tutorials/Regression - bike sharing/betaml_tutorial_regression_sharingBikes.html","page":"A regression task: the prediction of  bike  sharing demand","title":"A regression task: the prediction of  bike  sharing demand","text":"((xtrain,xval,xtest),(ytrain,yval,ytest)) = partition([x,y],[0.75,0.125,1-0.75-0.125],shuffle=false)\n(ntrain, nval, ntest) = size.([ytrain,yval,ytest],1)","category":"page"},{"location":"tutorials/Regression - bike sharing/betaml_tutorial_regression_sharingBikes.html","page":"A regression task: the prediction of  bike  sharing demand","title":"A regression task: the prediction of  bike  sharing demand","text":"We can now \"tune\" our model so-called hyper-parameters, i.e. choose the best exogenous parameters of our algorithm, where \"best\" refers to some minimisation of a \"loss\" function between the true and the predicted values. We compute this loss function on a specific subset of data, that we call the \"validation\" subset (xval and yval).","category":"page"},{"location":"tutorials/Regression - bike sharing/betaml_tutorial_regression_sharingBikes.html","page":"A regression task: the prediction of  bike  sharing demand","title":"A regression task: the prediction of  bike  sharing demand","text":"BetaML doesn't have a dedicated function for hyper-parameters optimisation, but it is easy to write some custom julia code, at least for a simple grid-based \"search\". Indeed one of the main reasons that a dedicated function exists in other Machine Learning libraries is that loops in other languages are slow, but this is not a problem in julia, so we can retain the flexibility to write the kind of hyper-parameter tuning that best fits our needs.","category":"page"},{"location":"tutorials/Regression - bike sharing/betaml_tutorial_regression_sharingBikes.html","page":"A regression task: the prediction of  bike  sharing demand","title":"A regression task: the prediction of  bike  sharing demand","text":"Below is an example of a possible such function. Note there are more \"elegant\" ways to code it, but this one does the job. In particular, for simplicity, this hyper-paramerter tuning function just run multiple repetitions. In real world it is better to use cross-validation in the hyper-parameter tuning, expecially when the observations are small. The Clustering tutorial shows an example on how to use crossValidation.","category":"page"},{"location":"tutorials/Regression - bike sharing/betaml_tutorial_regression_sharingBikes.html","page":"A regression task: the prediction of  bike  sharing demand","title":"A regression task: the prediction of  bike  sharing demand","text":"We will see the various functions inside tuneHyperParameters() in a moment. For now let's going just to observe that tuneHyperParameters just loops over all the possible hyper-parameters and selects the ones where the error between xval and yval is minimised. For the meaning of the various hyper-parameter, consult the documentation of the buildTree and buildForest functions. The function uses multiple threads, so we calls generateParallelRngs() (in the BetaML.Utils submodule) to generate thread-safe random number generators and locks the comparision step.","category":"page"},{"location":"tutorials/Regression - bike sharing/betaml_tutorial_regression_sharingBikes.html","page":"A regression task: the prediction of  bike  sharing demand","title":"A regression task: the prediction of  bike  sharing demand","text":"function tuneHyperParameters(model,xtrain,ytrain,xval,yval;maxDepthRange=15:15,maxFeaturesRange=size(xtrain,2):size(xtrain,2),nTreesRange=20:20,βRange=0:0,minRecordsRange=2:2,repetitions=5,rng=Random.GLOBAL_RNG)\n    # We start with an infinitely high error\n    bestRme         = +Inf\n    bestMaxDepth    = 1\n    bestMaxFeatures = 1\n    bestMinRecords  = 2\n    bestNTrees      = 1\n    bestβ           = 0\n    compLock        = ReentrantLock()\n\n    # Generate one random number generator per thread\n    masterSeed = rand(rng,100:9999999999999) ## Some RNG have problems with very small seed. Also, the master seed has to be computed _before_ generateParallelRngs\n    rngs = generateParallelRngs(rng,Threads.nthreads())\n\n    # We loop over all possible hyperparameter combinations...\n    parLengths = (length(maxDepthRange),length(maxFeaturesRange),length(minRecordsRange),length(nTreesRange),length(βRange))\n    Threads.@threads for ij in CartesianIndices(parLengths) ## This to avoid many nested for loops\n           (maxDepth,maxFeatures,minRecords,nTrees,β)   = (maxDepthRange[Tuple(ij)[1]], maxFeaturesRange[Tuple(ij)[2]], minRecordsRange[Tuple(ij)[3]], nTreesRange[Tuple(ij)[4]], βRange[Tuple(ij)[5]]) ## The specific hyperparameters of this nested loop\n           tsrng = rngs[Threads.threadid()] ## The random number generator is specific for each thread..\n           joinedIndx = LinearIndices(parLengths)[ij]\n           # And here we make the seeding depending on the id of the loop, not the thread: hence we get the same results indipendently of the number of threads\n           Random.seed!(tsrng,masterSeed+joinedIndx*10)\n           totAttemptError = 0.0\n           # We run several repetitions with the same hyperparameter combination to account for stochasticity...\n           for r in 1:repetitions\n              if model == \"DecisionTree\"\n                 # Here we train the Decition Tree model\n                 myTrainedModel = buildTree(xtrain,ytrain, maxDepth=maxDepth,maxFeatures=maxFeatures,minRecords=minRecords,rng=tsrng)\n              else\n                 # Here we train the Random Forest model\n                 myTrainedModel = buildForest(xtrain,ytrain,nTrees,maxDepth=maxDepth,maxFeatures=maxFeatures,minRecords=minRecords,β=β,rng=tsrng)\n              end\n              # Here we make prediciton with this trained model and we compute its error\n              ŷval   = predict(myTrainedModel, xval,rng=tsrng)\n              rmeVal = meanRelError(ŷval,yval,normRec=false)\n              totAttemptError += rmeVal\n           end\n           avgAttemptedDepthError = totAttemptError / repetitions\n           begin\n               lock(compLock) ## This step can't be run in parallel...\n               try\n                   # Select this specific combination of hyperparameters if the error is the lowest\n                   if avgAttemptedDepthError < bestRme\n                     bestRme         = avgAttemptedDepthError\n                     bestMaxDepth    = maxDepth\n                     bestMaxFeatures = maxFeatures\n                     bestNTrees      = nTrees\n                     bestβ           = β\n                     bestMinRecords  = minRecords\n                   end\n               finally\n                   unlock(compLock)\n               end\n           end\n    end\n    return (bestRme,bestMaxDepth,bestMaxFeatures,bestMinRecords,bestNTrees,bestβ)\nend","category":"page"},{"location":"tutorials/Regression - bike sharing/betaml_tutorial_regression_sharingBikes.html","page":"A regression task: the prediction of  bike  sharing demand","title":"A regression task: the prediction of  bike  sharing demand","text":"We can now run the hyperparameter optimisation function with some \"reasonable\" ranges. To obtain replicable results we call tuneHyperParameters with rng=copy(FIXEDRNG), where FIXEDRNG is a fixed-seeded random number generator guaranteed to maintain the same stream of random numbers even between different julia versions. That's also what we use for our unit tests (see the Getting started for more details).","category":"page"},{"location":"tutorials/Regression - bike sharing/betaml_tutorial_regression_sharingBikes.html","page":"A regression task: the prediction of  bike  sharing demand","title":"A regression task: the prediction of  bike  sharing demand","text":"(bestRme,bestMaxDepth,bestMaxFeatures,bestMinRecords) = tuneHyperParameters(\"DecisionTree\",xtrain,ytrain,xval,yval,\n           maxDepthRange=4:5,maxFeaturesRange=11:12,minRecordsRange=5:5,repetitions=3,rng=copy(FIXEDRNG))","category":"page"},{"location":"tutorials/Regression - bike sharing/betaml_tutorial_regression_sharingBikes.html","page":"A regression task: the prediction of  bike  sharing demand","title":"A regression task: the prediction of  bike  sharing demand","text":"Now that we have found the \"optimal\" hyperparameters we can build (\"train\") our model using them:","category":"page"},{"location":"tutorials/Regression - bike sharing/betaml_tutorial_regression_sharingBikes.html","page":"A regression task: the prediction of  bike  sharing demand","title":"A regression task: the prediction of  bike  sharing demand","text":"myTree = buildTree(xtrain,ytrain, maxDepth=bestMaxDepth, maxFeatures=bestMaxFeatures,minRecords=bestMinRecords,rng=copy(FIXEDRNG));\nnothing #hide","category":"page"},{"location":"tutorials/Regression - bike sharing/betaml_tutorial_regression_sharingBikes.html","page":"A regression task: the prediction of  bike  sharing demand","title":"A regression task: the prediction of  bike  sharing demand","text":"Let's benchmark the time and memory usage of the training step of a decision tree:","category":"page"},{"location":"tutorials/Regression - bike sharing/betaml_tutorial_regression_sharingBikes.html","page":"A regression task: the prediction of  bike  sharing demand","title":"A regression task: the prediction of  bike  sharing demand","text":"@btime  buildTree(xtrain,ytrain, maxDepth=bestMaxDepth, maxFeatures=bestMaxFeatures,minRecords=bestMinRecords,rng=copy(FIXEDRNG));\n26.538 ms (55753 allocations: 58.57 MiB)","category":"page"},{"location":"tutorials/Regression - bike sharing/betaml_tutorial_regression_sharingBikes.html","page":"A regression task: the prediction of  bike  sharing demand","title":"A regression task: the prediction of  bike  sharing demand","text":"Individual decision trees are blazing fast, among the fastest algorithms we could use.","category":"page"},{"location":"tutorials/Regression - bike sharing/betaml_tutorial_regression_sharingBikes.html","page":"A regression task: the prediction of  bike  sharing demand","title":"A regression task: the prediction of  bike  sharing demand","text":"The above buildTree function produces a DecisionTree object that can be used to make predictions given some new features, i.e. given some X matrix of (number of observations x dimensions), predict the corresponding Y vector of scalers in R.","category":"page"},{"location":"tutorials/Regression - bike sharing/betaml_tutorial_regression_sharingBikes.html","page":"A regression task: the prediction of  bike  sharing demand","title":"A regression task: the prediction of  bike  sharing demand","text":"(ŷtrain,ŷval,ŷtest) = predict.([myTree], [xtrain,xval,xtest])","category":"page"},{"location":"tutorials/Regression - bike sharing/betaml_tutorial_regression_sharingBikes.html","page":"A regression task: the prediction of  bike  sharing demand","title":"A regression task: the prediction of  bike  sharing demand","text":"Note that the above code uses the \"dot syntax\" to \"broadcast\" predict() over an array of label matrices. It is exactly equivalent to:","category":"page"},{"location":"tutorials/Regression - bike sharing/betaml_tutorial_regression_sharingBikes.html","page":"A regression task: the prediction of  bike  sharing demand","title":"A regression task: the prediction of  bike  sharing demand","text":"ŷtrain = predict(myTree, xtrain);\nŷval   = predict(myTree, xval);\nŷtest  = predict(myTree, xtest);\nnothing #hide","category":"page"},{"location":"tutorials/Regression - bike sharing/betaml_tutorial_regression_sharingBikes.html","page":"A regression task: the prediction of  bike  sharing demand","title":"A regression task: the prediction of  bike  sharing demand","text":"We now compute the relative mean error for the training, the validation and the test set. The meanRelError is a very flexible error function. Without additional parameter, it computes, as the name says, the mean relative error, also known as the \"mean absolute percentage error\" (MAPE) between an estimated and a true vector. However it can also compute the relative mean error (as we do here), or use a p-norm higher than 1. The mean relative error enfatises the relativeness of the error, i.e. all observations and dimensions weigth the same, wether large or small. Conversly, in the relative mean error the same relative error on larger observations (or dimensions) weights more. In this exercise we use the later, as our data has clearly some outlier days with very small rents, and we care more of avoiding our customers finding empty bike racks than having unrented bikes on the rack. Targeting a low mean average error would push all our predicitons down to try accomodate the low-level predicitons (to avoid a large relative error), and that's not what we want.","category":"page"},{"location":"tutorials/Regression - bike sharing/betaml_tutorial_regression_sharingBikes.html","page":"A regression task: the prediction of  bike  sharing demand","title":"A regression task: the prediction of  bike  sharing demand","text":"For example let's consider the following example:","category":"page"},{"location":"tutorials/Regression - bike sharing/betaml_tutorial_regression_sharingBikes.html","page":"A regression task: the prediction of  bike  sharing demand","title":"A regression task: the prediction of  bike  sharing demand","text":"y     = [30,28,27,3,32,38];\nŷpref = [32,30,28,10,31,40];\nŷbad  = [29,25,24,5,28,35];\nnothing #hide","category":"page"},{"location":"tutorials/Regression - bike sharing/betaml_tutorial_regression_sharingBikes.html","page":"A regression task: the prediction of  bike  sharing demand","title":"A regression task: the prediction of  bike  sharing demand","text":"Here ŷpref is an ipotetical output of a model that minimises the relative mean error, while ŷbad minimises the mean realative error.","category":"page"},{"location":"tutorials/Regression - bike sharing/betaml_tutorial_regression_sharingBikes.html","page":"A regression task: the prediction of  bike  sharing demand","title":"A regression task: the prediction of  bike  sharing demand","text":"meanRelError.([ŷbad, ŷpref],[y,y],normRec=true) ## Mean relative error","category":"page"},{"location":"tutorials/Regression - bike sharing/betaml_tutorial_regression_sharingBikes.html","page":"A regression task: the prediction of  bike  sharing demand","title":"A regression task: the prediction of  bike  sharing demand","text":"meanRelError.([ŷbad, ŷpref],[y,y],normRec=false) ## Relative mean error","category":"page"},{"location":"tutorials/Regression - bike sharing/betaml_tutorial_regression_sharingBikes.html","page":"A regression task: the prediction of  bike  sharing demand","title":"A regression task: the prediction of  bike  sharing demand","text":"plot([y ŷbad ŷpref], colour=[:black :red :green], label=[\"obs\" \"bad est\" \"good est\"])","category":"page"},{"location":"tutorials/Regression - bike sharing/betaml_tutorial_regression_sharingBikes.html","page":"A regression task: the prediction of  bike  sharing demand","title":"A regression task: the prediction of  bike  sharing demand","text":"We can then compute the relative mean error for the decision tree","category":"page"},{"location":"tutorials/Regression - bike sharing/betaml_tutorial_regression_sharingBikes.html","page":"A regression task: the prediction of  bike  sharing demand","title":"A regression task: the prediction of  bike  sharing demand","text":"(rmeTrain, rmeVal, rmeTest) = meanRelError.([ŷtrain,ŷval,ŷtest],[ytrain,yval,ytest],normRec=false)","category":"page"},{"location":"tutorials/Regression - bike sharing/betaml_tutorial_regression_sharingBikes.html","page":"A regression task: the prediction of  bike  sharing demand","title":"A regression task: the prediction of  bike  sharing demand","text":"We can plot the true labels vs the estimated one for the three subsets...","category":"page"},{"location":"tutorials/Regression - bike sharing/betaml_tutorial_regression_sharingBikes.html","page":"A regression task: the prediction of  bike  sharing demand","title":"A regression task: the prediction of  bike  sharing demand","text":"scatter(ytrain,ŷtrain,xlabel=\"daily rides\",ylabel=\"est. daily rides\",label=nothing,title=\"Est vs. obs in training period (DT)\")","category":"page"},{"location":"tutorials/Regression - bike sharing/betaml_tutorial_regression_sharingBikes.html","page":"A regression task: the prediction of  bike  sharing demand","title":"A regression task: the prediction of  bike  sharing demand","text":"scatter(yval,ŷval,xlabel=\"daily rides\",ylabel=\"est. daily rides\",label=nothing,title=\"Est vs. obs in validation period (DT)\")","category":"page"},{"location":"tutorials/Regression - bike sharing/betaml_tutorial_regression_sharingBikes.html","page":"A regression task: the prediction of  bike  sharing demand","title":"A regression task: the prediction of  bike  sharing demand","text":"scatter(ytest,ŷtest,xlabel=\"daily rides\",ylabel=\"est. daily rides\",label=nothing,title=\"Est vs. obs in testing period (DT)\")","category":"page"},{"location":"tutorials/Regression - bike sharing/betaml_tutorial_regression_sharingBikes.html","page":"A regression task: the prediction of  bike  sharing demand","title":"A regression task: the prediction of  bike  sharing demand","text":"Or we can visualise the true vs estimated bike shared on a temporal base. First on the full period (2 years) ...","category":"page"},{"location":"tutorials/Regression - bike sharing/betaml_tutorial_regression_sharingBikes.html","page":"A regression task: the prediction of  bike  sharing demand","title":"A regression task: the prediction of  bike  sharing demand","text":"ŷtrainfull = vcat(ŷtrain,fill(missing,nval+ntest))\nŷvalfull   = vcat(fill(missing,ntrain), ŷval, fill(missing,ntest))\nŷtestfull  = vcat(fill(missing,ntrain+nval), ŷtest)\nplot(data[:,:dteday],[data[:,:cnt] ŷtrainfull ŷvalfull ŷtestfull], label=[\"obs\" \"train\" \"val\" \"test\"], legend=:topleft, ylabel=\"daily rides\", title=\"Daily bike sharing demand observed/estimated across the\\n whole 2-years period (DT)\")","category":"page"},{"location":"tutorials/Regression - bike sharing/betaml_tutorial_regression_sharingBikes.html","page":"A regression task: the prediction of  bike  sharing demand","title":"A regression task: the prediction of  bike  sharing demand","text":"..and then focusing on the testing period","category":"page"},{"location":"tutorials/Regression - bike sharing/betaml_tutorial_regression_sharingBikes.html","page":"A regression task: the prediction of  bike  sharing demand","title":"A regression task: the prediction of  bike  sharing demand","text":"stc = 620\nendc = size(x,1)\nplot(data[stc:endc,:dteday],[data[stc:endc,:cnt] ŷvalfull[stc:endc] ŷtestfull[stc:endc]], label=[\"obs\" \"val\" \"test\"], legend=:bottomleft, ylabel=\"Daily rides\", title=\"Focus on the testing period (DT)\")","category":"page"},{"location":"tutorials/Regression - bike sharing/betaml_tutorial_regression_sharingBikes.html","page":"A regression task: the prediction of  bike  sharing demand","title":"A regression task: the prediction of  bike  sharing demand","text":"The predictions aren't so bad in this case, however decision trees are highly instable, and the output could have depended just from the specific initial random seed.","category":"page"},{"location":"tutorials/Regression - bike sharing/betaml_tutorial_regression_sharingBikes.html#Random-Forests","page":"A regression task: the prediction of  bike  sharing demand","title":"Random Forests","text":"","category":"section"},{"location":"tutorials/Regression - bike sharing/betaml_tutorial_regression_sharingBikes.html","page":"A regression task: the prediction of  bike  sharing demand","title":"A regression task: the prediction of  bike  sharing demand","text":"Rather than trying to solve this problem using a single Decision Tree model, let's not try to use a Random Forest model. Random forests average the results of many different decision trees and provide a more \"stable\" result. Being made of many decision trees, random forests are hovever more computationally expensive to train, but luckily they tend to self-tune (or self-regularise). In particular the parameters maxDepth and maxFeatures shouldn't need tuning.","category":"page"},{"location":"tutorials/Regression - bike sharing/betaml_tutorial_regression_sharingBikes.html","page":"A regression task: the prediction of  bike  sharing demand","title":"A regression task: the prediction of  bike  sharing demand","text":"We still tune however the model for other parameters, and in particular the β parameter, a prerogative of BetaML Random Forests that allows to assign more weigth to the best performing trees in the forest. It may be particularly important if there are many outliers in the data we don't want to \"learn\" from.","category":"page"},{"location":"tutorials/Regression - bike sharing/betaml_tutorial_regression_sharingBikes.html","page":"A regression task: the prediction of  bike  sharing demand","title":"A regression task: the prediction of  bike  sharing demand","text":"minRecordsRange=[5]; nTreesRange=[60]; βRange=100:100:300\n(bestRme,bestMaxDepth,bestMaxFeatures,bestMinRecords,bestNTrees,bestβ) = tuneHyperParameters(\"RandomForest\",xtrain,ytrain,xval,yval,\n        maxDepthRange=size(xtrain,1):size(xtrain,1),maxFeaturesRange=Int(round(sqrt(size(xtrain,2)))):Int(round(sqrt(size(xtrain,2)))),\n        minRecordsRange=minRecordsRange,nTreesRange=nTreesRange,βRange=βRange,repetitions=5,rng=copy(FIXEDRNG))","category":"page"},{"location":"tutorials/Regression - bike sharing/betaml_tutorial_regression_sharingBikes.html","page":"A regression task: the prediction of  bike  sharing demand","title":"A regression task: the prediction of  bike  sharing demand","text":"As for decision trees, once the hyper-parameters of the model are tuned we wan train again the model using the optimal parameters.","category":"page"},{"location":"tutorials/Regression - bike sharing/betaml_tutorial_regression_sharingBikes.html","page":"A regression task: the prediction of  bike  sharing demand","title":"A regression task: the prediction of  bike  sharing demand","text":"myForest = buildForest(xtrain,ytrain, bestNTrees, maxDepth=bestMaxDepth,maxFeatures=bestMaxFeatures,minRecords=bestMinRecords,β=bestβ,oob=true,rng=copy(FIXEDRNG));\nnothing #hide","category":"page"},{"location":"tutorials/Regression - bike sharing/betaml_tutorial_regression_sharingBikes.html","page":"A regression task: the prediction of  bike  sharing demand","title":"A regression task: the prediction of  bike  sharing demand","text":"Let's now benchmark the training of the BetaML Random Forest model @btime buildForest(xtrain,ytrain, bestNTrees, maxDepth=bestMaxDepth,maxFeatures=bestMaxFeatures,minRecords=bestMinRecords,β=bestβ,oob=true,rng=copy(FIXEDRNG)); 863.842 ms (2451894 allocations: 971.33 MiB) Random forests are evidently slower than individual decision trees but are still relativly fast. We should also consider that they are by default efficiently parallelised, so their speed increases with the number of available cores (in building this documentation page, GitHub CI servers allow for a single core, so all the bechmark you see in this tutorial are run with a single core available).","category":"page"},{"location":"tutorials/Regression - bike sharing/betaml_tutorial_regression_sharingBikes.html","page":"A regression task: the prediction of  bike  sharing demand","title":"A regression task: the prediction of  bike  sharing demand","text":"Random forests support the so-called \"out-of-bag\" error, an estimation of the error that we would have when the model is applied on a testing sample. However in this case the oob reported is much smaller than the testing error we will actually find. This is due to the fact that the division between training/validation and testing in this exercise is not random, but has a temporal basis. It seems that in this example the data in validation/testing follows a different pattern/variance than those in training (in probabilistic terms, the daily observations are not i.i.d.).","category":"page"},{"location":"tutorials/Regression - bike sharing/betaml_tutorial_regression_sharingBikes.html","page":"A regression task: the prediction of  bike  sharing demand","title":"A regression task: the prediction of  bike  sharing demand","text":"oobError, trueTestMeanRelativeError  = myForest.oobError,meanRelError(ŷtest,ytest,normRec=true)","category":"page"},{"location":"tutorials/Regression - bike sharing/betaml_tutorial_regression_sharingBikes.html","page":"A regression task: the prediction of  bike  sharing demand","title":"A regression task: the prediction of  bike  sharing demand","text":"(ŷtrain,ŷval,ŷtest)         = predict.([myForest], [xtrain,xval,xtest])\n(rmeTrain, rmeVal, rmeTest) = meanRelError.([ŷtrain,ŷval,ŷtest],[ytrain,yval,ytest],normRec=false)","category":"page"},{"location":"tutorials/Regression - bike sharing/betaml_tutorial_regression_sharingBikes.html","page":"A regression task: the prediction of  bike  sharing demand","title":"A regression task: the prediction of  bike  sharing demand","text":"In this case we found an error very similar to the one employing a single decision tree. Let's print the observed data vs the estimated one using the random forest and then along the temporal axis:","category":"page"},{"location":"tutorials/Regression - bike sharing/betaml_tutorial_regression_sharingBikes.html","page":"A regression task: the prediction of  bike  sharing demand","title":"A regression task: the prediction of  bike  sharing demand","text":"scatter(ytrain,ŷtrain,xlabel=\"daily rides\",ylabel=\"est. daily rides\",label=nothing,title=\"Est vs. obs in training period (RF)\")","category":"page"},{"location":"tutorials/Regression - bike sharing/betaml_tutorial_regression_sharingBikes.html","page":"A regression task: the prediction of  bike  sharing demand","title":"A regression task: the prediction of  bike  sharing demand","text":"scatter(yval,ŷval,xlabel=\"daily rides\",ylabel=\"est. daily rides\",label=nothing,title=\"Est vs. obs in validation period (RF)\")","category":"page"},{"location":"tutorials/Regression - bike sharing/betaml_tutorial_regression_sharingBikes.html","page":"A regression task: the prediction of  bike  sharing demand","title":"A regression task: the prediction of  bike  sharing demand","text":"scatter(ytest,ŷtest,xlabel=\"daily rides\",ylabel=\"est. daily rides\",label=nothing,title=\"Est vs. obs in testing period (RF)\")","category":"page"},{"location":"tutorials/Regression - bike sharing/betaml_tutorial_regression_sharingBikes.html","page":"A regression task: the prediction of  bike  sharing demand","title":"A regression task: the prediction of  bike  sharing demand","text":"Full period plot (2 years):","category":"page"},{"location":"tutorials/Regression - bike sharing/betaml_tutorial_regression_sharingBikes.html","page":"A regression task: the prediction of  bike  sharing demand","title":"A regression task: the prediction of  bike  sharing demand","text":"ŷtrainfull = vcat(ŷtrain,fill(missing,nval+ntest))\nŷvalfull   = vcat(fill(missing,ntrain), ŷval, fill(missing,ntest))\nŷtestfull  = vcat(fill(missing,ntrain+nval), ŷtest)\nplot(data[:,:dteday],[data[:,:cnt] ŷtrainfull ŷvalfull ŷtestfull], label=[\"obs\" \"train\" \"val\" \"test\"], legend=:topleft, ylabel=\"daily rides\", title=\"Daily bike sharing demand observed/estimated across the\\n whole 2-years period (RF)\")","category":"page"},{"location":"tutorials/Regression - bike sharing/betaml_tutorial_regression_sharingBikes.html","page":"A regression task: the prediction of  bike  sharing demand","title":"A regression task: the prediction of  bike  sharing demand","text":"Focus on the testing period:","category":"page"},{"location":"tutorials/Regression - bike sharing/betaml_tutorial_regression_sharingBikes.html","page":"A regression task: the prediction of  bike  sharing demand","title":"A regression task: the prediction of  bike  sharing demand","text":"stc = 620\nendc = size(x,1)\nplot(data[stc:endc,:dteday],[data[stc:endc,:cnt] ŷvalfull[stc:endc] ŷtestfull[stc:endc]], label=[\"obs\" \"val\" \"test\"], legend=:bottomleft, ylabel=\"Daily rides\", title=\"Focus on the testing period (RF)\")","category":"page"},{"location":"tutorials/Regression - bike sharing/betaml_tutorial_regression_sharingBikes.html#Comparison-with-DecisionTree.jl-random-forest","page":"A regression task: the prediction of  bike  sharing demand","title":"Comparison with DecisionTree.jl random forest","text":"","category":"section"},{"location":"tutorials/Regression - bike sharing/betaml_tutorial_regression_sharingBikes.html","page":"A regression task: the prediction of  bike  sharing demand","title":"A regression task: the prediction of  bike  sharing demand","text":"We now compare our results with those obtained employing the same model in the DecisionTree package, using the default suggested hyperparameters:","category":"page"},{"location":"tutorials/Regression - bike sharing/betaml_tutorial_regression_sharingBikes.html","page":"A regression task: the prediction of  bike  sharing demand","title":"A regression task: the prediction of  bike  sharing demand","text":"Hyperparameters of the DecisionTree.jl random forest model","category":"page"},{"location":"tutorials/Regression - bike sharing/betaml_tutorial_regression_sharingBikes.html","page":"A regression task: the prediction of  bike  sharing demand","title":"A regression task: the prediction of  bike  sharing demand","text":"n_subfeatures=-1; n_trees=bestNTrees; partial_sampling=1; max_depth=26\nmin_samples_leaf=bestMinRecords; min_samples_split=bestMinRecords; min_purity_increase=0.0; seed=3","category":"page"},{"location":"tutorials/Regression - bike sharing/betaml_tutorial_regression_sharingBikes.html","page":"A regression task: the prediction of  bike  sharing demand","title":"A regression task: the prediction of  bike  sharing demand","text":"We train the model..","category":"page"},{"location":"tutorials/Regression - bike sharing/betaml_tutorial_regression_sharingBikes.html","page":"A regression task: the prediction of  bike  sharing demand","title":"A regression task: the prediction of  bike  sharing demand","text":"model = DecisionTree.build_forest(ytrain, convert(Matrix,xtrain),\n                     n_subfeatures,\n                     n_trees,\n                     partial_sampling,\n                     max_depth,\n                     min_samples_leaf,\n                     min_samples_split,\n                     min_purity_increase;\n                     rng = seed)","category":"page"},{"location":"tutorials/Regression - bike sharing/betaml_tutorial_regression_sharingBikes.html","page":"A regression task: the prediction of  bike  sharing demand","title":"A regression task: the prediction of  bike  sharing demand","text":"And we generate predictions and measure their error","category":"page"},{"location":"tutorials/Regression - bike sharing/betaml_tutorial_regression_sharingBikes.html","page":"A regression task: the prediction of  bike  sharing demand","title":"A regression task: the prediction of  bike  sharing demand","text":"(ŷtrain,ŷval,ŷtest) = DecisionTree.apply_forest.([model],[xtrain,xval,xtest]);\nnothing #hide","category":"page"},{"location":"tutorials/Regression - bike sharing/betaml_tutorial_regression_sharingBikes.html","page":"A regression task: the prediction of  bike  sharing demand","title":"A regression task: the prediction of  bike  sharing demand","text":"Let's benchmark the DecisionTrees.jl Random Forest training","category":"page"},{"location":"tutorials/Regression - bike sharing/betaml_tutorial_regression_sharingBikes.html","page":"A regression task: the prediction of  bike  sharing demand","title":"A regression task: the prediction of  bike  sharing demand","text":"@btime  DecisionTree.build_forest(ytrain, convert(Matrix,xtrain),\n                     n_subfeatures,\n                     n_trees,\n                     partial_sampling,\n                     max_depth,\n                     min_samples_leaf,\n                     min_samples_split,\n                     min_purity_increase;\n                     rng = seed);\n144.026 ms (41085 allocations: 11.01 MiB)","category":"page"},{"location":"tutorials/Regression - bike sharing/betaml_tutorial_regression_sharingBikes.html","page":"A regression task: the prediction of  bike  sharing demand","title":"A regression task: the prediction of  bike  sharing demand","text":"DecisionTrees.jl makes a good job in optimising the Random Forest algorithm, as it is over 3 times faster that BetaML.","category":"page"},{"location":"tutorials/Regression - bike sharing/betaml_tutorial_regression_sharingBikes.html","page":"A regression task: the prediction of  bike  sharing demand","title":"A regression task: the prediction of  bike  sharing demand","text":"(rmeTrain, rmeVal, rmeTest) = meanRelError.([ŷtrain,ŷval,ŷtest],[ytrain,yval,ytest],normRec=false)","category":"page"},{"location":"tutorials/Regression - bike sharing/betaml_tutorial_regression_sharingBikes.html","page":"A regression task: the prediction of  bike  sharing demand","title":"A regression task: the prediction of  bike  sharing demand","text":"However the error on the test set remains relativly high. The very low error level on the training set is a sign that it overspecialised on the training set, and we should have better ran a dedicated hyper-parameter tuning function for the model.","category":"page"},{"location":"tutorials/Regression - bike sharing/betaml_tutorial_regression_sharingBikes.html","page":"A regression task: the prediction of  bike  sharing demand","title":"A regression task: the prediction of  bike  sharing demand","text":"Finally we plot the DecisionTree.jl predictions alongside the observed value:","category":"page"},{"location":"tutorials/Regression - bike sharing/betaml_tutorial_regression_sharingBikes.html","page":"A regression task: the prediction of  bike  sharing demand","title":"A regression task: the prediction of  bike  sharing demand","text":"ŷtrainfull = vcat(ŷtrain,fill(missing,nval+ntest))\nŷvalfull   = vcat(fill(missing,ntrain), ŷval, fill(missing,ntest))\nŷtestfull  = vcat(fill(missing,ntrain+nval), ŷtest)\nplot(data[:,:dteday],[data[:,:cnt] ŷtrainfull ŷvalfull ŷtestfull], label=[\"obs\" \"train\" \"val\" \"test\"], legend=:topleft, ylabel=\"daily rides\", title=\"Daily bike sharing demand observed/estimated across the\\n whole 2-years period (DT.jl RF)\")","category":"page"},{"location":"tutorials/Regression - bike sharing/betaml_tutorial_regression_sharingBikes.html","page":"A regression task: the prediction of  bike  sharing demand","title":"A regression task: the prediction of  bike  sharing demand","text":"Again, focusing on the testing data:","category":"page"},{"location":"tutorials/Regression - bike sharing/betaml_tutorial_regression_sharingBikes.html","page":"A regression task: the prediction of  bike  sharing demand","title":"A regression task: the prediction of  bike  sharing demand","text":"stc  = 620\nendc = size(x,1)\nplot(data[stc:endc,:dteday],[data[stc:endc,:cnt] ŷvalfull[stc:endc] ŷtestfull[stc:endc]], label=[\"obs\" \"val\" \"test\"], legend=:bottomleft, ylabel=\"Daily rides\", title=\"Focus on the testing period (DT.jl RF)\")","category":"page"},{"location":"tutorials/Regression - bike sharing/betaml_tutorial_regression_sharingBikes.html#Conclusions-of-Decision-Trees-/-Random-Forests-methods","page":"A regression task: the prediction of  bike  sharing demand","title":"Conclusions of Decision Trees / Random Forests methods","text":"","category":"section"},{"location":"tutorials/Regression - bike sharing/betaml_tutorial_regression_sharingBikes.html","page":"A regression task: the prediction of  bike  sharing demand","title":"A regression task: the prediction of  bike  sharing demand","text":"The error obtained employing DecisionTree.jl is significantly larger than those obtained using a BetaML random forest model, altought to be fair with DecisionTrees.jl we didn't tuned its hyper-parameters. Also, the DecisionTree.jl random forest model is much faster. This is partially due by the fact that, internally, DecisionTree.jl models optimise the algorithm by sorting the observations. BetaML trees/forests don't employ this optimisation and hence they can work with true categorical data for which ordering is not defined. An other explanation of this difference in speed is that BetaML Random Forest models accept missing values within the feature matrix. To sum up, BetaML random forests are ideal algorithms when we want to obtain good predictions in the most simpler way, even without tuning the hyper-parameters, and without spending time in cleaning (\"munging\") the feature matrix, as they accept almost \"any kind\" of data as it is.","category":"page"},{"location":"tutorials/Regression - bike sharing/betaml_tutorial_regression_sharingBikes.html#Neural-Networks","page":"A regression task: the prediction of  bike  sharing demand","title":"Neural Networks","text":"","category":"section"},{"location":"tutorials/Regression - bike sharing/betaml_tutorial_regression_sharingBikes.html","page":"A regression task: the prediction of  bike  sharing demand","title":"A regression task: the prediction of  bike  sharing demand","text":"BetaML provides only deep forward neural networks, artificial neural network units where the individual \"nodes\" are arranged in layers, from the input layer, where each unit holds the input coordinate, through various hidden layer transformations, until the actual output of the model:","category":"page"},{"location":"tutorials/Regression - bike sharing/betaml_tutorial_regression_sharingBikes.html","page":"A regression task: the prediction of  bike  sharing demand","title":"A regression task: the prediction of  bike  sharing demand","text":"(Image: Neural Networks)","category":"page"},{"location":"tutorials/Regression - bike sharing/betaml_tutorial_regression_sharingBikes.html","page":"A regression task: the prediction of  bike  sharing demand","title":"A regression task: the prediction of  bike  sharing demand","text":"In this layerwise computation, each unit in a particular layer takes input from all the preceding layer units and it has its own parameters that are adjusted to perform the overall computation. The training of the network consists in retrieving the coefficients that minimise a loss function between the output of the model and the known data. In particular, a deep (feedforward) neural network refers to a neural network that contains not only the input and output layers, but also hidden layers in between.","category":"page"},{"location":"tutorials/Regression - bike sharing/betaml_tutorial_regression_sharingBikes.html","page":"A regression task: the prediction of  bike  sharing demand","title":"A regression task: the prediction of  bike  sharing demand","text":"Neural networks accept only numerical inputs. We hence need to convert all categorical data in numerical units. A common approach is to use the so-called \"one-hot-encoding\" where the catagorical values are converted into indicator variables (0/1), one for each possible value. This can be done in BetaML using the oneHotEncoder function:","category":"page"},{"location":"tutorials/Regression - bike sharing/betaml_tutorial_regression_sharingBikes.html","page":"A regression task: the prediction of  bike  sharing demand","title":"A regression task: the prediction of  bike  sharing demand","text":"seasonDummies  = convert(Array{Float64,2},oneHotEncoder(data[:,:season]))\nweatherDummies = convert(Array{Float64,2},oneHotEncoder(data[:,:weathersit]))\nwdayDummies    = convert(Array{Float64,2},oneHotEncoder(data[:,:weekday] .+ 1 ))\n\n# We compose the feature matrix with the new dimensions obtained from the oneHotEncoder functions\nx = hcat(Matrix{Float64}(data[:,[:instant,:yr,:mnth,:holiday,:workingday,:temp,:atemp,:hum,:windspeed]]),\n         seasonDummies,\n         weatherDummies,\n         wdayDummies)\ny = data[:,16];\nnothing #hide","category":"page"},{"location":"tutorials/Regression - bike sharing/betaml_tutorial_regression_sharingBikes.html","page":"A regression task: the prediction of  bike  sharing demand","title":"A regression task: the prediction of  bike  sharing demand","text":"As usual, we split the data in training, validation and testing sets","category":"page"},{"location":"tutorials/Regression - bike sharing/betaml_tutorial_regression_sharingBikes.html","page":"A regression task: the prediction of  bike  sharing demand","title":"A regression task: the prediction of  bike  sharing demand","text":"((xtrain,xval,xtest),(ytrain,yval,ytest)) = partition([x,y],[0.75,0.125,1-0.75-0.125],shuffle=false)\n(ntrain, nval, ntest) = size.([ytrain,yval,ytest],1)","category":"page"},{"location":"tutorials/Regression - bike sharing/betaml_tutorial_regression_sharingBikes.html","page":"A regression task: the prediction of  bike  sharing demand","title":"A regression task: the prediction of  bike  sharing demand","text":"An other common operation with neural networks is to scale the feature vectors (X) and the labels (Y). The BetaML scale function, by default, scales the data such that each dimension has mean 0 and variance 1.","category":"page"},{"location":"tutorials/Regression - bike sharing/betaml_tutorial_regression_sharingBikes.html","page":"A regression task: the prediction of  bike  sharing demand","title":"A regression task: the prediction of  bike  sharing demand","text":"Note that we can provide the function with different scale factors or specify the columns that shoudn't be scaled (e.g. those resulting from the one-hot encoding). Finally we can reverse the scaling (this is useful to retrieve the unscaled features from a model trained with scaled ones).","category":"page"},{"location":"tutorials/Regression - bike sharing/betaml_tutorial_regression_sharingBikes.html","page":"A regression task: the prediction of  bike  sharing demand","title":"A regression task: the prediction of  bike  sharing demand","text":"colsNotToScale = [2;4;5;10:23]\nxScaleFactors   = getScaleFactors(xtrain,skip=colsNotToScale)\nyScaleFactors   = ([0],[0.001]) # getScaleFactors(ytrain) # This just divide by 1000. Using full scaling of Y we may get negative demand.\nxtrainScaled    = scale(xtrain,xScaleFactors)\nxvalScaled      = scale(xval,xScaleFactors)\nxtestScaled     = scale(xtest,xScaleFactors)\nytrainScaled    = scale(ytrain,yScaleFactors)\nyvalScaled      = scale(yval,yScaleFactors)\nytestScaled     = scale(ytest,yScaleFactors)\nD               = size(xtrain,2)","category":"page"},{"location":"tutorials/Regression - bike sharing/betaml_tutorial_regression_sharingBikes.html","page":"A regression task: the prediction of  bike  sharing demand","title":"A regression task: the prediction of  bike  sharing demand","text":"As we did above for decision trees and random forests, we select the best hyper-parameters by using the validation set. We consider here two hyper-parameters, the first one (hiddenLayerSizeRange) concerns the structure of the neural network, and in particular the size (in nodes) of the hidden layer, the second one (epochRange) concerns the training itself, and in particular the number of so-called \"epochs\" (number of iterations trough the whole dataset) to train the model.","category":"page"},{"location":"tutorials/Regression - bike sharing/betaml_tutorial_regression_sharingBikes.html","page":"A regression task: the prediction of  bike  sharing demand","title":"A regression task: the prediction of  bike  sharing demand","text":"Again, we use here repetitions for simplicity, but a cross-validation approach would have bee nmore appropriate:","category":"page"},{"location":"tutorials/Regression - bike sharing/betaml_tutorial_regression_sharingBikes.html","page":"A regression task: the prediction of  bike  sharing demand","title":"A regression task: the prediction of  bike  sharing demand","text":"function tuneHyperParameters(xtrain,ytrain,xval,yval;epochRange=50:50,hiddenLayerSizeRange=12:12,repetitions=5,rng=Random.GLOBAL_RNG)\n    # We start with an infinititly high error\n    bestRme         = +Inf\n    bestEpoch       = 0\n    bestSize        = 0\n    compLock        = ReentrantLock()\n\n    # Generate one random number generator per thread\n    masterSeed = rand(rng,100:9999999999999) ## Some RNG have problems with very small seed. Also, the master seed has to be computed _before_ generateParallelRngs\n    rngs       = generateParallelRngs(rng,Threads.nthreads())\n\n    # We loop over all possible hyperparameter combinations...\n    parLengths = (length(epochRange),length(hiddenLayerSizeRange))\n    Threads.@threads for ij in CartesianIndices(parLengths)\n       (epoch,hiddenLayerSize)   = (epochRange[Tuple(ij)[1]], hiddenLayerSizeRange[Tuple(ij)[2]])\n       tsrng = rngs[Threads.threadid()]\n       joinedIndx = LinearIndices(parLengths)[ij]\n       # And here we make the seeding depending on the i of the loop, not the thread: hence we get the same results indipendently of the number of threads\n       Random.seed!(tsrng,masterSeed+joinedIndx*10)\n       totAttemptError = 0.0\n       println(\"Testing epochs $epoch, layer size $hiddenLayerSize ...\")\n       # We run several repetitions with the same hyperparameter combination to account for stochasticity...\n       for r in 1:repetitions\n           l1   = DenseLayer(D,hiddenLayerSize,f=relu,rng=tsrng) # Activation function is ReLU\n           l2   = DenseLayer(hiddenLayerSize,hiddenLayerSize,f=identity,rng=tsrng)\n           l3   = DenseLayer(hiddenLayerSize,1,f=relu,rng=tsrng)\n           mynn = buildNetwork([l1,l2,l3],squaredCost,name=\"Bike sharing regression model\") # Build the NN and use the squared cost (aka MSE) as error function\n           # Training it (default to ADAM)\n           res  = train!(mynn,xtrain,ytrain,epochs=epoch,batchSize=8,optAlg=ADAM(),verbosity=NONE, rng=tsrng) # Use optAlg=SGD() to use Stochastic Gradient Descent\n           ŷval = predict(mynn,xval)\n           rmeVal  = meanRelError(ŷval,yval,normRec=false)\n           totAttemptError += rmeVal\n       end\n       avgRme = totAttemptError / repetitions\n       begin\n           lock(compLock) ## This step can't be run in parallel...\n           try\n               # Select this specific combination of hyperparameters if the error is the lowest\n               if avgRme < bestRme\n                 bestRme    = avgRme\n                 bestEpoch  = epoch\n                 bestSize   = hiddenLayerSize\n               end\n           finally\n               unlock(compLock)\n           end\n       end\n    end\n    return (bestRme=bestRme,bestEpoch=bestEpoch,bestSize=bestSize)\nend","category":"page"},{"location":"tutorials/Regression - bike sharing/betaml_tutorial_regression_sharingBikes.html","page":"A regression task: the prediction of  bike  sharing demand","title":"A regression task: the prediction of  bike  sharing demand","text":"# Note: the following code block may take a bit to run...\nepochsToTest     = [100]\nhiddenLayerSizes = [5,10]\n(bestRme,bestEpoch,bestSize) = tuneHyperParameters(xtrainScaled,ytrainScaled,xvalScaled,yvalScaled;epochRange=epochsToTest,hiddenLayerSizeRange=hiddenLayerSizes,repetitions=3,rng=copy(FIXEDRNG))","category":"page"},{"location":"tutorials/Regression - bike sharing/betaml_tutorial_regression_sharingBikes.html","page":"A regression task: the prediction of  bike  sharing demand","title":"A regression task: the prediction of  bike  sharing demand","text":"We can now build our feed-forward neaural network. We create three layers, the first layers will always have a input size equal to the dimensions of our data (the number of columns), and the output layer, for a simple regression where the predictions are scalars, it will always be one. The middle layer has the size we obtained from tuneHyperParameters.","category":"page"},{"location":"tutorials/Regression - bike sharing/betaml_tutorial_regression_sharingBikes.html","page":"A regression task: the prediction of  bike  sharing demand","title":"A regression task: the prediction of  bike  sharing demand","text":"There are already several kind of layers available (and you can build your own kind by defining a new struct and implementing a few functions. See the Nn module documentation for details). Here we use only dense layers, those found in typycal feed-fordward neural networks.","category":"page"},{"location":"tutorials/Regression - bike sharing/betaml_tutorial_regression_sharingBikes.html","page":"A regression task: the prediction of  bike  sharing demand","title":"A regression task: the prediction of  bike  sharing demand","text":"For each layer, on top of its size (in \"neurons\") we can specify an activation function. Here we use the relu for the terminal layer (this will guarantee that our predictions are always positive) and identity for the hidden layer. Again, consult the Nn module documentation for other activation layers already defined, or use any function of your choice.","category":"page"},{"location":"tutorials/Regression - bike sharing/betaml_tutorial_regression_sharingBikes.html","page":"A regression task: the prediction of  bike  sharing demand","title":"A regression task: the prediction of  bike  sharing demand","text":"Initial weight parameters can also be specified if needed. By default DenseLayer use the so-called Xavier initialisation.","category":"page"},{"location":"tutorials/Regression - bike sharing/betaml_tutorial_regression_sharingBikes.html","page":"A regression task: the prediction of  bike  sharing demand","title":"A regression task: the prediction of  bike  sharing demand","text":"l1   = DenseLayer(D,bestSize,f=relu,rng=copy(FIXEDRNG)) # Activation function is ReLU\nl2   = DenseLayer(bestSize,bestSize,f=identity,rng=copy(FIXEDRNG))\nl3   = DenseLayer(bestSize,1,f=relu,rng=copy(FIXEDRNG))","category":"page"},{"location":"tutorials/Regression - bike sharing/betaml_tutorial_regression_sharingBikes.html","page":"A regression task: the prediction of  bike  sharing demand","title":"A regression task: the prediction of  bike  sharing demand","text":"Finally we \"chain\" the layers together and we assign a final loss function (agian, you can provide your own loss function, if those available in BetaML don't suit your needs) in order to create the \"neural network\" NN object with the [buildNetwork][@ref] function:","category":"page"},{"location":"tutorials/Regression - bike sharing/betaml_tutorial_regression_sharingBikes.html","page":"A regression task: the prediction of  bike  sharing demand","title":"A regression task: the prediction of  bike  sharing demand","text":"mynn = buildNetwork([l1,l2,l3],squaredCost,name=\"Bike sharing regression model\") ## Build the NN and use the squared cost (aka MSE) as error function","category":"page"},{"location":"tutorials/Regression - bike sharing/betaml_tutorial_regression_sharingBikes.html","page":"A regression task: the prediction of  bike  sharing demand","title":"A regression task: the prediction of  bike  sharing demand","text":"The above neural network will use automatic differentiation (using the Zygote package) to compute the gradient used in the loss minimisation during the training step. It is also possible, for the layers that support it, to use manual differentiation. The network below is exactly equivalent to the one above, except it avoids automatic differentiation as we tell BetaML the functions to use as derivatives of the activation functions and of the overall network loss function:","category":"page"},{"location":"tutorials/Regression - bike sharing/betaml_tutorial_regression_sharingBikes.html","page":"A regression task: the prediction of  bike  sharing demand","title":"A regression task: the prediction of  bike  sharing demand","text":"mynnManual = buildNetwork([\n        DenseLayer(D,bestSize,f=relu,df=drelu,rng=copy(FIXEDRNG)),\n        DenseLayer(bestSize,bestSize,f=identity,df=didentity,rng=copy(FIXEDRNG)),\n        DenseLayer(bestSize,1,f=relu,df=drelu,rng=copy(FIXEDRNG))\n    ], squaredCost, name=\"Bike sharing regression model\", dcf=dSquaredCost)","category":"page"},{"location":"tutorials/Regression - bike sharing/betaml_tutorial_regression_sharingBikes.html","page":"A regression task: the prediction of  bike  sharing demand","title":"A regression task: the prediction of  bike  sharing demand","text":"We can now train the neural network with the best hyper-parameters using the function train!. Note the esclamation point. By convention in julia, functions that end with an exclamation mark modify some of their inputs, normally the first one.","category":"page"},{"location":"tutorials/Regression - bike sharing/betaml_tutorial_regression_sharingBikes.html","page":"A regression task: the prediction of  bike  sharing demand","title":"A regression task: the prediction of  bike  sharing demand","text":"Here train! has a question mark as indeed it modifies the neural network object, by updating its weights. If you want to keep a copy of the network before training (for example, if you want to train it in different independent ways), make a deepcopy of the NN object or first save its parameters with getParams and then re-apply the saved parameters with setParams!.","category":"page"},{"location":"tutorials/Regression - bike sharing/betaml_tutorial_regression_sharingBikes.html","page":"A regression task: the prediction of  bike  sharing demand","title":"A regression task: the prediction of  bike  sharing demand","text":"Several optimisation algorithms are available, and each accepts different parameters, like the learning rate for the Stochastic Gradient Descent algorithm (SGD or the exponential decay rates for the  moments estimates for the ADAM algorithm (that we use here, with the default parameters).","category":"page"},{"location":"tutorials/Regression - bike sharing/betaml_tutorial_regression_sharingBikes.html","page":"A regression task: the prediction of  bike  sharing demand","title":"A regression task: the prediction of  bike  sharing demand","text":"println(\"Final training of $bestEpoch epochs, with layer size $bestSize ...\")\nres  = train!(mynn,xtrainScaled,ytrainScaled,epochs=bestEpoch,batchSize=8,optAlg=ADAM(),rng=copy(FIXEDRNG)) ## Use optAlg=SGD() to use Stochastic Gradient Descent","category":"page"},{"location":"tutorials/Regression - bike sharing/betaml_tutorial_regression_sharingBikes.html","page":"A regression task: the prediction of  bike  sharing demand","title":"A regression task: the prediction of  bike  sharing demand","text":"Let's benchmark the BetaML neural network training","category":"page"},{"location":"tutorials/Regression - bike sharing/betaml_tutorial_regression_sharingBikes.html","page":"A regression task: the prediction of  bike  sharing demand","title":"A regression task: the prediction of  bike  sharing demand","text":"@btime train!(mynnManual,xtrainScaled,ytrainScaled,epochs=bestEpoch,batchSize=8,optAlg=ADAM(),rng=copy(FIXEDRNG), verbosity=NONE);\n1.951 s (8716955 allocations: 702.71 MiB)","category":"page"},{"location":"tutorials/Regression - bike sharing/betaml_tutorial_regression_sharingBikes.html","page":"A regression task: the prediction of  bike  sharing demand","title":"A regression task: the prediction of  bike  sharing demand","text":"As we can see the model training is one order of magnitude slower than random forests, altought the memory requirement is approximatly the same.","category":"page"},{"location":"tutorials/Regression - bike sharing/betaml_tutorial_regression_sharingBikes.html","page":"A regression task: the prediction of  bike  sharing demand","title":"A regression task: the prediction of  bike  sharing demand","text":"To obtain the neural network predictions we apply the function predict to the feature matrix X for which we want to generate previsions, and then, in order to obtain the unscaled estimates we use the scale function applied to the scaled values with the original scaling factors and the parameter rev set to true. Note the usage of the pipe operator to avoid ugly function1(function2(function3(...))) nested calls:","category":"page"},{"location":"tutorials/Regression - bike sharing/betaml_tutorial_regression_sharingBikes.html","page":"A regression task: the prediction of  bike  sharing demand","title":"A regression task: the prediction of  bike  sharing demand","text":"ŷtrain = @pipe predict(mynn,xtrainScaled) |> scale(_, yScaleFactors,rev=true);\nŷval   = @pipe predict(mynn,xvalScaled)   |> scale(_, yScaleFactors,rev=true);\nŷtest  = @pipe predict(mynn,xtestScaled)  |> scale(_ ,yScaleFactors,rev=true);\nnothing #hide","category":"page"},{"location":"tutorials/Regression - bike sharing/betaml_tutorial_regression_sharingBikes.html","page":"A regression task: the prediction of  bike  sharing demand","title":"A regression task: the prediction of  bike  sharing demand","text":"(mreTrain, mreVal, mreTest) = meanRelError.([ŷtrain,ŷval,ŷtest],[ytrain,yval,ytest],normRec=false)","category":"page"},{"location":"tutorials/Regression - bike sharing/betaml_tutorial_regression_sharingBikes.html","page":"A regression task: the prediction of  bike  sharing demand","title":"A regression task: the prediction of  bike  sharing demand","text":"The error is much lower. Let's plot our predictions:","category":"page"},{"location":"tutorials/Regression - bike sharing/betaml_tutorial_regression_sharingBikes.html","page":"A regression task: the prediction of  bike  sharing demand","title":"A regression task: the prediction of  bike  sharing demand","text":"Again, we can start by plotting the estimated vs the observed value:","category":"page"},{"location":"tutorials/Regression - bike sharing/betaml_tutorial_regression_sharingBikes.html","page":"A regression task: the prediction of  bike  sharing demand","title":"A regression task: the prediction of  bike  sharing demand","text":"scatter(ytrain,ŷtrain,xlabel=\"daily rides\",ylabel=\"est. daily rides\",label=nothing,title=\"Est vs. obs in training period (NN)\")","category":"page"},{"location":"tutorials/Regression - bike sharing/betaml_tutorial_regression_sharingBikes.html","page":"A regression task: the prediction of  bike  sharing demand","title":"A regression task: the prediction of  bike  sharing demand","text":"scatter(yval,ŷval,xlabel=\"daily rides\",ylabel=\"est. daily rides\",label=nothing,title=\"Est vs. obs in validation period (NN)\")","category":"page"},{"location":"tutorials/Regression - bike sharing/betaml_tutorial_regression_sharingBikes.html","page":"A regression task: the prediction of  bike  sharing demand","title":"A regression task: the prediction of  bike  sharing demand","text":"scatter(ytest,ŷtest,xlabel=\"daily rides\",ylabel=\"est. daily rides\",label=nothing,title=\"Est vs. obs in testing period (NN)\")","category":"page"},{"location":"tutorials/Regression - bike sharing/betaml_tutorial_regression_sharingBikes.html","page":"A regression task: the prediction of  bike  sharing demand","title":"A regression task: the prediction of  bike  sharing demand","text":"We now plot across the time dimension, first plotting the whole period (2 years):","category":"page"},{"location":"tutorials/Regression - bike sharing/betaml_tutorial_regression_sharingBikes.html","page":"A regression task: the prediction of  bike  sharing demand","title":"A regression task: the prediction of  bike  sharing demand","text":"ŷtrainfull = vcat(ŷtrain,fill(missing,nval+ntest))\nŷvalfull   = vcat(fill(missing,ntrain), ŷval, fill(missing,ntest))\nŷtestfull  = vcat(fill(missing,ntrain+nval), ŷtest)\nplot(data[:,:dteday],[data[:,:cnt] ŷtrainfull ŷvalfull ŷtestfull], label=[\"obs\" \"train\" \"val\" \"test\"], legend=:topleft, ylabel=\"daily rides\", title=\"Daily bike sharing demand observed/estimated across the\\n whole 2-years period  (NN)\")","category":"page"},{"location":"tutorials/Regression - bike sharing/betaml_tutorial_regression_sharingBikes.html","page":"A regression task: the prediction of  bike  sharing demand","title":"A regression task: the prediction of  bike  sharing demand","text":"...and then focusing on the testing data","category":"page"},{"location":"tutorials/Regression - bike sharing/betaml_tutorial_regression_sharingBikes.html","page":"A regression task: the prediction of  bike  sharing demand","title":"A regression task: the prediction of  bike  sharing demand","text":"stc  = 620\nendc = size(x,1)\nplot(data[stc:endc,:dteday],[data[stc:endc,:cnt] ŷvalfull[stc:endc] ŷtestfull[stc:endc]], label=[\"obs\" \"val\" \"test\"], legend=:bottomleft, ylabel=\"Daily rides\", title=\"Focus on the testing period (NN)\")","category":"page"},{"location":"tutorials/Regression - bike sharing/betaml_tutorial_regression_sharingBikes.html#Comparison-with-Flux","page":"A regression task: the prediction of  bike  sharing demand","title":"Comparison with Flux","text":"","category":"section"},{"location":"tutorials/Regression - bike sharing/betaml_tutorial_regression_sharingBikes.html","page":"A regression task: the prediction of  bike  sharing demand","title":"A regression task: the prediction of  bike  sharing demand","text":"We now apply the same Neural Network model using the Flux framework, a dedicated neural network library, reusing the optimal parameters that we did learn in tuneHyperParameters","category":"page"},{"location":"tutorials/Regression - bike sharing/betaml_tutorial_regression_sharingBikes.html","page":"A regression task: the prediction of  bike  sharing demand","title":"A regression task: the prediction of  bike  sharing demand","text":"We fix the default random number generator so that the Flux example gives a reproducible output","category":"page"},{"location":"tutorials/Regression - bike sharing/betaml_tutorial_regression_sharingBikes.html","page":"A regression task: the prediction of  bike  sharing demand","title":"A regression task: the prediction of  bike  sharing demand","text":"Random.seed!(123)","category":"page"},{"location":"tutorials/Regression - bike sharing/betaml_tutorial_regression_sharingBikes.html","page":"A regression task: the prediction of  bike  sharing demand","title":"A regression task: the prediction of  bike  sharing demand","text":"We define the Flux neural network model and load it with data...","category":"page"},{"location":"tutorials/Regression - bike sharing/betaml_tutorial_regression_sharingBikes.html","page":"A regression task: the prediction of  bike  sharing demand","title":"A regression task: the prediction of  bike  sharing demand","text":"l1         = Flux.Dense(D,bestSize,Flux.relu)\nl2         = Flux.Dense(bestSize,bestSize,identity)\nl3         = Flux.Dense(bestSize,1,Flux.relu)\nFlux_nn    = Flux.Chain(l1,l2,l3)\nloss(x, y) = Flux.mse(Flux_nn(x), y)\nps         = Flux.params(Flux_nn)\nnndata     = Flux.Data.DataLoader((xtrainScaled', ytrainScaled'), batchsize=8,shuffle=true)\n\nFlux_nn2   = deepcopy(Flux_nn)      ## A copy for the time benchmarking\nps2        = Flux.params(Flux_nn2)  ## A copy for the time benchmarking","category":"page"},{"location":"tutorials/Regression - bike sharing/betaml_tutorial_regression_sharingBikes.html","page":"A regression task: the prediction of  bike  sharing demand","title":"A regression task: the prediction of  bike  sharing demand","text":"We do the training of the Flux model...","category":"page"},{"location":"tutorials/Regression - bike sharing/betaml_tutorial_regression_sharingBikes.html","page":"A regression task: the prediction of  bike  sharing demand","title":"A regression task: the prediction of  bike  sharing demand","text":"Flux.@epochs bestEpoch Flux.train!(loss, ps, nndata, Flux.ADAM(0.001, (0.9, 0.8)));\nnothing #hide","category":"page"},{"location":"tutorials/Regression - bike sharing/betaml_tutorial_regression_sharingBikes.html","page":"A regression task: the prediction of  bike  sharing demand","title":"A regression task: the prediction of  bike  sharing demand","text":"..and we benchmark it..","category":"page"},{"location":"tutorials/Regression - bike sharing/betaml_tutorial_regression_sharingBikes.html","page":"A regression task: the prediction of  bike  sharing demand","title":"A regression task: the prediction of  bike  sharing demand","text":"@btime begin for i in 1:bestEpoch Flux.train!(loss, ps2, nndata, Flux.ADAM(0.001, (0.9, 0.8))) end end\n690.231 ms (3349901 allocations: 266.76 MiB)","category":"page"},{"location":"tutorials/Regression - bike sharing/betaml_tutorial_regression_sharingBikes.html","page":"A regression task: the prediction of  bike  sharing demand","title":"A regression task: the prediction of  bike  sharing demand","text":"On this small example the speed of Flux is on the same order than BetaML (the actual difference seems to depend on the specific RNG seed and hardware), however I suspect that Flux scales much better with larger networks and/or data.","category":"page"},{"location":"tutorials/Regression - bike sharing/betaml_tutorial_regression_sharingBikes.html","page":"A regression task: the prediction of  bike  sharing demand","title":"A regression task: the prediction of  bike  sharing demand","text":"We obtain the estimates...","category":"page"},{"location":"tutorials/Regression - bike sharing/betaml_tutorial_regression_sharingBikes.html","page":"A regression task: the prediction of  bike  sharing demand","title":"A regression task: the prediction of  bike  sharing demand","text":"ŷtrainf = @pipe Flux_nn(xtrainScaled')' |> scale(_,yScaleFactors,rev=true);\nŷvalf   = @pipe Flux_nn(xvalScaled')'   |> scale(_,yScaleFactors,rev=true);\nŷtestf  = @pipe Flux_nn(xtestScaled')'  |> scale(_,yScaleFactors,rev=true);\nnothing #hide","category":"page"},{"location":"tutorials/Regression - bike sharing/betaml_tutorial_regression_sharingBikes.html","page":"A regression task: the prediction of  bike  sharing demand","title":"A regression task: the prediction of  bike  sharing demand","text":"..and we compute the mean relative errors..","category":"page"},{"location":"tutorials/Regression - bike sharing/betaml_tutorial_regression_sharingBikes.html","page":"A regression task: the prediction of  bike  sharing demand","title":"A regression task: the prediction of  bike  sharing demand","text":"(mreTrain, mreVal, mreTest) = meanRelError.([ŷtrainf,ŷvalf,ŷtestf],[ytrain,yval,ytest],normRec=false)","category":"page"},{"location":"tutorials/Regression - bike sharing/betaml_tutorial_regression_sharingBikes.html","page":"A regression task: the prediction of  bike  sharing demand","title":"A regression task: the prediction of  bike  sharing demand","text":".. finding an error not significantly different than the one obtained from BetaML.Nn.","category":"page"},{"location":"tutorials/Regression - bike sharing/betaml_tutorial_regression_sharingBikes.html","page":"A regression task: the prediction of  bike  sharing demand","title":"A regression task: the prediction of  bike  sharing demand","text":"Plots:","category":"page"},{"location":"tutorials/Regression - bike sharing/betaml_tutorial_regression_sharingBikes.html","page":"A regression task: the prediction of  bike  sharing demand","title":"A regression task: the prediction of  bike  sharing demand","text":"scatter(ytrain,ŷtrainf,xlabel=\"daily rides\",ylabel=\"est. daily rides\",label=nothing,title=\"Est vs. obs in training period (Flux.NN)\")","category":"page"},{"location":"tutorials/Regression - bike sharing/betaml_tutorial_regression_sharingBikes.html","page":"A regression task: the prediction of  bike  sharing demand","title":"A regression task: the prediction of  bike  sharing demand","text":"scatter(yval,ŷvalf,xlabel=\"daily rides\",ylabel=\"est. daily rides\",label=nothing,title=\"Est vs. obs in validation period (Flux.NN)\")","category":"page"},{"location":"tutorials/Regression - bike sharing/betaml_tutorial_regression_sharingBikes.html","page":"A regression task: the prediction of  bike  sharing demand","title":"A regression task: the prediction of  bike  sharing demand","text":"scatter(ytest,ŷtestf,xlabel=\"daily rides\",ylabel=\"est. daily rides\",label=nothing,title=\"Est vs. obs in testing period (Flux.NN)\")","category":"page"},{"location":"tutorials/Regression - bike sharing/betaml_tutorial_regression_sharingBikes.html","page":"A regression task: the prediction of  bike  sharing demand","title":"A regression task: the prediction of  bike  sharing demand","text":"ŷtrainfullf = vcat(ŷtrainf,fill(missing,nval+ntest))\nŷvalfullf   = vcat(fill(missing,ntrain), ŷvalf, fill(missing,ntest))\nŷtestfullf  = vcat(fill(missing,ntrain+nval), ŷtestf)\nplot(data[:,:dteday],[data[:,:cnt] ŷtrainfullf ŷvalfullf ŷtestfullf], label=[\"obs\" \"train\" \"val\" \"test\"], legend=:topleft, ylabel=\"daily rides\", title=\"Daily bike sharing demand observed/estimated across the\\n whole 2-years period (Flux.NN)\")","category":"page"},{"location":"tutorials/Regression - bike sharing/betaml_tutorial_regression_sharingBikes.html","page":"A regression task: the prediction of  bike  sharing demand","title":"A regression task: the prediction of  bike  sharing demand","text":"stc = 620\nendc = size(x,1)\nplot(data[stc:endc,:dteday],[data[stc:endc,:cnt] ŷvalfullf[stc:endc] ŷtestfullf[stc:endc]], label=[\"obs\" \"val\" \"test\"], legend=:bottomleft, ylabel=\"Daily rides\", title=\"Focus on the testing period (Flux.NN)\")","category":"page"},{"location":"tutorials/Regression - bike sharing/betaml_tutorial_regression_sharingBikes.html#Conclusions-of-Neural-Network-models","page":"A regression task: the prediction of  bike  sharing demand","title":"Conclusions of Neural Network models","text":"","category":"section"},{"location":"tutorials/Regression - bike sharing/betaml_tutorial_regression_sharingBikes.html","page":"A regression task: the prediction of  bike  sharing demand","title":"A regression task: the prediction of  bike  sharing demand","text":"If we strive for the most accurate predictions, deep neural networks are usually the best choice. However they are computationally expensive, so with limited resourses we may get better results by fine tuning and running many repetitions of \"simpler\" decision trees or even random forest models than a large naural network with insufficient hyper-parameter tuning. Also, we shoudl consider that decision trees/random forests are much simpler to work with.","category":"page"},{"location":"tutorials/Regression - bike sharing/betaml_tutorial_regression_sharingBikes.html","page":"A regression task: the prediction of  bike  sharing demand","title":"A regression task: the prediction of  bike  sharing demand","text":"That said, specialised neural network libraries, like Flux, allow to use GPU and specialised hardware letting neural networks to scale with very large datasets.","category":"page"},{"location":"tutorials/Regression - bike sharing/betaml_tutorial_regression_sharingBikes.html","page":"A regression task: the prediction of  bike  sharing demand","title":"A regression task: the prediction of  bike  sharing demand","text":"Still, for small and medium datasets, BetaML provides simpler yet customisable solutions that are accurate and fast.","category":"page"},{"location":"tutorials/Regression - bike sharing/betaml_tutorial_regression_sharingBikes.html#Using-GMM-based-regressors","page":"A regression task: the prediction of  bike  sharing demand","title":"Using GMM-based regressors","text":"","category":"section"},{"location":"tutorials/Regression - bike sharing/betaml_tutorial_regression_sharingBikes.html","page":"A regression task: the prediction of  bike  sharing demand","title":"A regression task: the prediction of  bike  sharing demand","text":"These are newly addded regression algorithms based on Gaussian Mixture Model. There are two variants available, GMMRegressor1 and GMMRegressor2 This example uses the V2 API","category":"page"},{"location":"tutorials/Regression - bike sharing/betaml_tutorial_regression_sharingBikes.html","page":"A regression task: the prediction of  bike  sharing demand","title":"A regression task: the prediction of  bike  sharing demand","text":"First we define the model with its hyperparameters and the options (random seed, verbosity level..)","category":"page"},{"location":"tutorials/Regression - bike sharing/betaml_tutorial_regression_sharingBikes.html","page":"A regression task: the prediction of  bike  sharing demand","title":"A regression task: the prediction of  bike  sharing demand","text":"m = GMMRegressor1(rng=copy(FIXEDRNG), verbosity=NONE)","category":"page"},{"location":"tutorials/Regression - bike sharing/betaml_tutorial_regression_sharingBikes.html","page":"A regression task: the prediction of  bike  sharing demand","title":"A regression task: the prediction of  bike  sharing demand","text":"@btime begin fit!(m,xtrainScaled,ytrainScaled); reset!(m) end 13.584 ms (103690 allocations: 25.08 MiB)","category":"page"},{"location":"tutorials/Regression - bike sharing/betaml_tutorial_regression_sharingBikes.html","page":"A regression task: the prediction of  bike  sharing demand","title":"A regression task: the prediction of  bike  sharing demand","text":"Here we fit the model to the training data..","category":"page"},{"location":"tutorials/Regression - bike sharing/betaml_tutorial_regression_sharingBikes.html","page":"A regression task: the prediction of  bike  sharing demand","title":"A regression task: the prediction of  bike  sharing demand","text":"fit!(m,xtrainScaled,ytrainScaled)","category":"page"},{"location":"tutorials/Regression - bike sharing/betaml_tutorial_regression_sharingBikes.html","page":"A regression task: the prediction of  bike  sharing demand","title":"A regression task: the prediction of  bike  sharing demand","text":"And here we predict...","category":"page"},{"location":"tutorials/Regression - bike sharing/betaml_tutorial_regression_sharingBikes.html","page":"A regression task: the prediction of  bike  sharing demand","title":"A regression task: the prediction of  bike  sharing demand","text":"ŷtrainGMM = @pipe predict(m,xtrainScaled) |> scale(_, yScaleFactors,rev=true);\nŷvalGMM   = @pipe predict(m,xvalScaled)   |> scale(_, yScaleFactors,rev=true);\nŷtestGMM  = @pipe predict(m,xtestScaled)  |> scale(_ ,yScaleFactors,rev=true);\n\n(mreTrainGMM, mreValGMM, mreTestGMM) = meanRelError.([ŷtrainGMM,ŷvalGMM,ŷtestGMM],[ytrain,yval,ytest],normRec=false)","category":"page"},{"location":"tutorials/Regression - bike sharing/betaml_tutorial_regression_sharingBikes.html","page":"A regression task: the prediction of  bike  sharing demand","title":"A regression task: the prediction of  bike  sharing demand","text":"Better (test MRE ≈ 0.25) can be obtained by using more mixtures, but at a much larger computational costs","category":"page"},{"location":"tutorials/Regression - bike sharing/betaml_tutorial_regression_sharingBikes.html","page":"A regression task: the prediction of  bike  sharing demand","title":"A regression task: the prediction of  bike  sharing demand","text":"m = GMMRegressor2(rng=copy(FIXEDRNG), verbosity=NONE)\n@btime begin fit!(m,xtrainScaled,ytrainScaled); reset!(m) end","category":"page"},{"location":"tutorials/Regression - bike sharing/betaml_tutorial_regression_sharingBikes.html","page":"A regression task: the prediction of  bike  sharing demand","title":"A regression task: the prediction of  bike  sharing demand","text":"10.704 ms (84754 allocations: 19.54 MiB)","category":"page"},{"location":"tutorials/Regression - bike sharing/betaml_tutorial_regression_sharingBikes.html","page":"A regression task: the prediction of  bike  sharing demand","title":"A regression task: the prediction of  bike  sharing demand","text":"fit!(m,xtrainScaled,ytrainScaled)\nŷtrainGMM = @pipe predict(m,xtrainScaled) |> scale(_, yScaleFactors,rev=true);\nŷvalGMM   = @pipe predict(m,xvalScaled)   |> scale(_, yScaleFactors,rev=true);\nŷtestGMM  = @pipe predict(m,xtestScaled)  |> scale(_ ,yScaleFactors,rev=true);\n\n(mreTrainGMM, mreValGMM, mreTestGMM) = meanRelError.([ŷtrainGMM,ŷvalGMM,ŷtestGMM],[ytrain,yval,ytest],normRec=false)","category":"page"},{"location":"tutorials/Regression - bike sharing/betaml_tutorial_regression_sharingBikes.html#Summary","page":"A regression task: the prediction of  bike  sharing demand","title":"Summary","text":"","category":"section"},{"location":"tutorials/Regression - bike sharing/betaml_tutorial_regression_sharingBikes.html","page":"A regression task: the prediction of  bike  sharing demand","title":"A regression task: the prediction of  bike  sharing demand","text":"This is the summary of the results we had trying to predict the daily bike sharing demand, given weather and calendar information of the day","category":"page"},{"location":"tutorials/Regression - bike sharing/betaml_tutorial_regression_sharingBikes.html","page":"A regression task: the prediction of  bike  sharing demand","title":"A regression task: the prediction of  bike  sharing demand","text":"Model Train rme Test rme Training time (ms)* Training mem (MB)*\nDT 0.1266 0.2223 26.5 58\nRF 0.0651 0.2223 362 971\nRF (DecisionTree.jl) 0.0312 0.3142 36 11\nNN 0.0884 0.1761 1768 758\nNN (Flux.jl) 0.0981 0.1618 1708 282\nGMMRegressor1 0.2388 0.4394 13.5 25.1\nGMMRegressor2 0.2588 0.2763 10.7 19.5","category":"page"},{"location":"tutorials/Regression - bike sharing/betaml_tutorial_regression_sharingBikes.html","page":"A regression task: the prediction of  bike  sharing demand","title":"A regression task: the prediction of  bike  sharing demand","text":"on a Intel Core i5-8350U laptop","category":"page"},{"location":"tutorials/Regression - bike sharing/betaml_tutorial_regression_sharingBikes.html","page":"A regression task: the prediction of  bike  sharing demand","title":"A regression task: the prediction of  bike  sharing demand","text":"Neural networks can be more precise than random forests models, but are more computationally expensive (and tricky to set up). When we compare BetaML with the algorithm-specific leading packages, we found similar results in terms of accuracy, but often the leading packages are better optimised and run more efficiently (but sometimes at the cost of being less versatile). GMM regressors are very computationally cheap and a good choice if accuracy can be traded off for performances.","category":"page"},{"location":"tutorials/Regression - bike sharing/betaml_tutorial_regression_sharingBikes.html","page":"A regression task: the prediction of  bike  sharing demand","title":"A regression task: the prediction of  bike  sharing demand","text":"View this file on Github.","category":"page"},{"location":"tutorials/Regression - bike sharing/betaml_tutorial_regression_sharingBikes.html","page":"A regression task: the prediction of  bike  sharing demand","title":"A regression task: the prediction of  bike  sharing demand","text":"","category":"page"},{"location":"tutorials/Regression - bike sharing/betaml_tutorial_regression_sharingBikes.html","page":"A regression task: the prediction of  bike  sharing demand","title":"A regression task: the prediction of  bike  sharing demand","text":"This page was generated using Literate.jl.","category":"page"},{"location":"Clustering.html#clustering_module","page":"Clustering","title":"The BetaML.Clustering Module","text":"","category":"section"},{"location":"Clustering.html","page":"Clustering","title":"Clustering","text":"Clustering","category":"page"},{"location":"Clustering.html#BetaML.Clustering","page":"Clustering","title":"BetaML.Clustering","text":"Clustering module (WIP)\n\n(Hard) Clustering algorithms\n\nProvide hard clustering methods using K-means and k-medoids. Please see also the GMM module for GMM-mased soft clustering, missing values imputation / collaborative filtering / reccomendation systems using clustering methods as backend.\n\nThe module provides the following functions. Use ?[function] to access their full signature and detailed documentation:\n\ninitRepresentatives(X,K;initStrategy,Z₀): Initialisation strategies for Kmean and Kmedoids\nkmeans(X,K;dist,initStrategy,Z₀): Classical KMean algorithm\nkmedoids(X,K;dist,initStrategy,Z₀): Kmedoids algorithm\n\n\n\n\n\n","category":"module"},{"location":"Clustering.html#Module-Index","page":"Clustering","title":"Module Index","text":"","category":"section"},{"location":"Clustering.html","page":"Clustering","title":"Clustering","text":"Modules = [Clustering]\nOrder   = [:constant, :type, :function, :macro]","category":"page"},{"location":"Clustering.html#Detailed-API","page":"Clustering","title":"Detailed API","text":"","category":"section"},{"location":"Clustering.html","page":"Clustering","title":"Clustering","text":"Modules = [Clustering]","category":"page"},{"location":"Clustering.html#BetaML.Api.fit!-Tuple{KMeansModel, Any}","page":"Clustering","title":"BetaML.Api.fit!","text":"fit!(m::KMeansModel,x)\n\n\n\n\n\n","category":"method"},{"location":"Clustering.html#BetaML.Api.fit!-Tuple{KMedoidsModel, Any}","page":"Clustering","title":"BetaML.Api.fit!","text":"fit!(m::KMeansModel,x)\n\n\n\n\n\n","category":"method"},{"location":"Clustering.html#BetaML.Clustering.initRepresentatives-Tuple{Any, Any}","page":"Clustering","title":"BetaML.Clustering.initRepresentatives","text":"initRepresentatives(X,K;initStrategy,Z₀)\n\nInitialisate the representatives for a K-Mean or K-Medoids algorithm\n\nParameters:\n\nX: a (N x D) data to clusterise\nK: Number of cluster wonted\ninitStrategy: Whether to select the initial representative vectors:\nrandom: randomly in the X space\ngrid: using a grid approach [default]\nshuffle: selecting randomly within the available points\ngiven: using a provided set of initial representatives provided in the Z₀ parameter\nZ₀: Provided (K x D) matrix of initial representatives (used only together with the given initStrategy) [default: nothing]\nrng: Random Number Generator (see FIXEDSEED) [deafult: Random.GLOBAL_RNG]\n\nReturns:\n\nA (K x D) matrix of initial representatives\n\nExample:\n\njulia> Z₀ = initRepresentatives([1 10.5;1.5 10.8; 1.8 8; 1.7 15; 3.2 40; 3.6 32; 3.6 38],2,initStrategy=\"given\",Z₀=[1.7 15; 3.6 40])\n\n\n\n\n\n","category":"method"},{"location":"Clustering.html#BetaML.Clustering.kmeans-Tuple{Any, Any}","page":"Clustering","title":"BetaML.Clustering.kmeans","text":"kmeans(X,K;dist,initStrategy,Z₀)\n\nCompute K-Mean algorithm to identify K clusters of X using Euclidean distance\n\nParameters:\n\nX: a (N x D) data to clusterise\nK: Number of cluster wonted\ndist: Function to employ as distance (see notes). Default to Euclidean distance.\ninitStrategy: Whether to select the initial representative vectors:\nrandom: randomly in the X space\ngrid: using a grid approach [default]\nshuffle: selecting randomly within the available points\ngiven: using a provided set of initial representatives provided in the Z₀ parameter\nZ₀: Provided (K x D) matrix of initial representatives (used only together with the given initStrategy) [default: nothing]\nrng: Random Number Generator (see FIXEDSEED) [deafult: Random.GLOBAL_RNG]\n\nReturns:\n\nA tuple of two items, the first one being a vector of size N of ids of the clusters associated to each point and the second one the (K x D) matrix of representatives\n\nNotes:\n\nSome returned clusters could be empty\nThe dist parameter can be:\nAny user defined function accepting two vectors and returning a scalar\nAn anonymous function with the same characteristics (e.g. dist = (x,y) -> norm(x-y)^2)\nOne of the above predefined distances: l1_distance, l2_distance, l2²_distance, cosine_distance\n\nExample:\n\njulia> (clIdx,Z) = kmeans([1 10.5;1.5 10.8; 1.8 8; 1.7 15; 3.2 40; 3.6 32; 3.3 38; 5.1 -2.3; 5.2 -2.4],3)\n\n\n\n\n\n","category":"method"},{"location":"Clustering.html#BetaML.Clustering.kmedoids-Tuple{Any, Any}","page":"Clustering","title":"BetaML.Clustering.kmedoids","text":"kmedoids(X,K;dist,initStrategy,Z₀)\n\nCompute K-Medoids algorithm to identify K clusters of X using distance definition dist\n\nParameters:\n\nX: a (n x d) data to clusterise\nK: Number of cluster wonted\ndist: Function to employ as distance (see notes). Default to Euclidean distance.\ninitStrategy: Whether to select the initial representative vectors:\nrandom: randomly in the X space\ngrid: using a grid approach\nshuffle: selecting randomly within the available points [default]\ngiven: using a provided set of initial representatives provided in the Z₀ parameter\nZ₀: Provided (K x D) matrix of initial representatives (used only together with the given initStrategy) [default: nothing]\nrng: Random Number Generator (see FIXEDSEED) [deafult: Random.GLOBAL_RNG]\n\nReturns:\n\nA tuple of two items, the first one being a vector of size N of ids of the clusters associated to each point and the second one the (K x D) matrix of representatives\n\nNotes:\n\nSome returned clusters could be empty\nThe dist parameter can be:\nAny user defined function accepting two vectors and returning a scalar\nAn anonymous function with the same characteristics (e.g. dist = (x,y) -> norm(x-y)^2)\nOne of the above predefined distances: l1_distance, l2_distance, l2²_distance, cosine_distance\n\nExample:\n\njulia> (clIdx,Z) = kmedoids([1 10.5;1.5 10.8; 1.8 8; 1.7 15; 3.2 40; 3.6 32; 3.3 38; 5.1 -2.3; 5.2 -2.4],3,initStrategy=\"grid\")\n\n\n\n\n\n","category":"method"},{"location":"Clustering.html#MLJModelInterface.predict-Tuple{Union{KMeans, KMedoids}, Any, Any}","page":"Clustering","title":"MLJModelInterface.predict","text":"predict(m::KMeans, fitResults, X) - Given a fitted clustering model and some observations, predict the class of the observation\n\n\n\n\n\n","category":"method"},{"location":"Clustering.html#MLJModelInterface.transform-Tuple{Union{KMeans, KMedoids}, Any, Any}","page":"Clustering","title":"MLJModelInterface.transform","text":"fit(m::KMeans, fitResults, X) - Given a fitted clustering model and some observations, return the distances to each centroids \n\n\n\n\n\n","category":"method"},{"location":"Perceptron.html#perceptron_module","page":"Perceptron","title":"The BetaML.Perceptron Module","text":"","category":"section"},{"location":"Perceptron.html","page":"Perceptron","title":"Perceptron","text":"Perceptron","category":"page"},{"location":"Perceptron.html#BetaML.Perceptron","page":"Perceptron","title":"BetaML.Perceptron","text":"Perceptron module\n\nProvide linear and kernel classifiers.\n\nSee a runnable example on myBinder\n\nperceptron: Train data using the classical perceptron\nkernelPerceptron: Train data using the kernel perceptron\npegasos: Train data using the pegasos algorithm\npredict: Predict data using parameters from one of the above algorithms\n\nAll algorithms are multiclass, with perceptron and pegasos employing a one-vs-all strategy, while kernelPerceptron employs a one-vs-one approach, and return a \"probability\" for each class in term of a dictionary for each record. Use mode(ŷ) to return a single class prediction per record.\n\nThe binary equivalent algorithms, accepting only {-1,+1} labels, are available as peceptronBinary, kernelPerceptronBinary and pegasosBinary. They are slighly faster as they don't need to be wrapped in the multi-class equivalent and return a more informative output.\n\nThe multi-class versions are available in the MLJ framework as PerceptronClassifier,KernelPerceptronClassifier and PegasosClassifier respectivly.\n\n\n\n\n\n","category":"module"},{"location":"Perceptron.html#Module-Index","page":"Perceptron","title":"Module Index","text":"","category":"section"},{"location":"Perceptron.html","page":"Perceptron","title":"Perceptron","text":"Modules = [Perceptron]\nOrder   = [:constant, :type, :function, :macro]","category":"page"},{"location":"Perceptron.html#Detailed-API","page":"Perceptron","title":"Detailed API","text":"","category":"section"},{"location":"Perceptron.html","page":"Perceptron","title":"Perceptron","text":"Modules = [Perceptron]","category":"page"},{"location":"Perceptron.html#BetaML.Api.predict","page":"Perceptron","title":"BetaML.Api.predict","text":"predict(x,θ,θ₀)\n\nPredict a binary label {-1,1} given the feature vector and the linear coefficients\n\nParameters:\n\nx:        Feature matrix of the training data (n × d)\nθ:        The trained parameters\nθ₀:       The trained bias barameter [def: 0]\n\nReturn :\n\ny: Vector of the predicted labels\n\nExample:\n\njulia> predict([1.1 2.1; 5.3 4.2; 1.8 1.7], [3.2,1.2])\n\n\n\n\n\n","category":"function"},{"location":"Perceptron.html#BetaML.Api.predict-NTuple{4, Any}","page":"Perceptron","title":"BetaML.Api.predict","text":"predict(x,xtrain,ytrain,α;K)\n\nPredict a binary label {-1,1} given the feature vector and the training data together with their errors (as trained by a kernel perceptron algorithm)\n\nParameters:\n\nx:      Feature matrix of the training data (n × d)\nxtrain: The feature vectors used for the training\nytrain: The labels of the training set\nα:      The errors associated to each record\nK:      The kernel function used for the training and to be used for the prediction [def: radialKernel]\n\nReturn :\n\ny: Vector of the predicted labels\n\nExample:\n\njulia> predict([1.1 2.1; 5.3 4.2; 1.8 1.7], [3.2,1.2])\n\n\n\n\n\n","category":"method"},{"location":"Perceptron.html#BetaML.Api.predict-Union{Tuple{Tcl}, Tuple{Any, Any, Any, Any, AbstractVector{Tcl}}} where Tcl","page":"Perceptron","title":"BetaML.Api.predict","text":"predict(x,xtrain,ytrain,α,classes;K)\n\nPredict a multiclass label given the new feature vector and a trained kernel perceptron model.\n\nParameters:\n\nx:      Feature matrix of the training data (n × d)\nxtrain: A vector of the feature matrix used for training each of the one-vs-one class matches (i.e. model.x)\nytrain: A vector of the label vector used for training each of the one-vs-one class matches (i.e. model.y)\nα:      A vector of the errors associated to each record (i.e. model.α)\nclasses: The overal classes encountered in training (i.e. model.classes)\nK:      The kernel function used for the training and to be used for the prediction [def: radialKernel]\n\nReturn :\n\nŷ: Vector of dictionaries label=>probability (warning: it isn't really a probability, it is just the standardized number of matches \"won\" by this class compared with the other classes)\n\nNotes:\n\nUse mode(ŷ) if you want a single predicted label per record\n\nExample:\n\njulia> model  = kernelPerceptron([1.1 2.1; 5.3 4.2; 1.8 1.7], [-1,1,-1])\njulia> ŷtrain = Perceptron.predict([10 10; 2.2 2.5],model.x,model.y,model.α, model.classes,K=model.K)\n\n\n\n\n\n","category":"method"},{"location":"Perceptron.html#BetaML.Api.predict-Union{Tuple{Tcl}, Tuple{T}, Tuple{Any, AbstractVector{T}, AbstractVector{Float64}, Vector{Tcl}}} where {T<:AbstractVector{Float64}, Tcl}","page":"Perceptron","title":"BetaML.Api.predict","text":"predict(x,θ,θ₀,classes)\n\nPredict a multiclass label given the feature vector, the linear coefficients and the classes vector\n\nParameters:\n\nx:       Feature matrix of the training data (n × d)\nθ:       Vector of the trained parameters for each one-vs-all model (i.e. model.θ)\nθ₀:      Vector of the trained bias barameter for each one-vs-all model (i.e. model.θ₀)\nclasses: The overal classes encountered in training (i.e. model.classes)\n\nReturn :\n\nŷ: Vector of dictionaries label=>probability\n\nNotes:\n\nUse mode(ŷ) if you want a single predicted label per record\n\nExample:\n\n```julia julia> model  = perceptron([1.1 2.1; 5.3 4.2; 1.8 1.7], [-1,1,-1]) julia> ŷtrain = predict([10 10; 2.5 2.5],model.θ,model.θ₀, model.classes)\n\n\n\n\n\n","category":"method"},{"location":"Perceptron.html#BetaML.Perceptron.kernelPerceptron-Tuple{Any, Any}","page":"Perceptron","title":"BetaML.Perceptron.kernelPerceptron","text":"kernelPerceptron(x,y;K,T,α,nMsgs,shuffle)\n\nTrain a multiclass kernel classifier \"perceptron\" algorithm based on x and y.\n\nkernelPerceptron is a (potentially) non-linear perceptron-style classifier employing user-defined kernel funcions. Multiclass is supported using a one-vs-one approach.\n\nParameters:\n\nx:        Feature matrix of the training data (n × d)\ny:        Associated labels of the training data\nK:        Kernel function to employ. See ?radialKernel or ?polynomialKernelfor details or check ?BetaML.Utils to verify if other kernels are defined (you can alsways define your own kernel) [def: radialKernel]\nT:        Maximum number of iterations (aka \"epochs\") across the whole set (if the set is not fully classified earlier) [def: 100]\nα:        Initial distribution of the errors [def: zeros(length(y))]\nnMsg:     Maximum number of messages to show if all iterations are done [def: 0]\nshuffle:  Whether to randomly shuffle the data at each iteration [def: false]\nrng:      Random Number Generator (see FIXEDSEED) [deafult: Random.GLOBAL_RNG]\n\nReturn a named tuple with:\n\nx: The x data (eventually shuffled if shuffle=true)\ny: The label\nα: The errors associated to each record\nclasses: The labels classes encountered in the training\n\nNotes:\n\nThe trained model can then be used to make predictions using the function predict().\nThis model is available in the MLJ framework as the KernelPerceptronClassifier\n\nExample:\n\njulia> model = kernelPerceptron([1.1 1.1; 5.3 4.2; 1.8 1.7; 7.5 5.2;], [\"a\",\"c\",\"b\",\"c\"])\njulia> ŷtest = Perceptron.predict([10 10; 2.2 2.5; 1 1],model.x,model.y,model.α, model.classes,K=model.K)\n\n\n\n\n\n","category":"method"},{"location":"Perceptron.html#BetaML.Perceptron.kernelPerceptronBinary-Tuple{Any, Any}","page":"Perceptron","title":"BetaML.Perceptron.kernelPerceptronBinary","text":"kernelPerceptronBinary(x,y;K,T,α,nMsgs,shuffle)\n\nTrain a binary kernel classifier \"perceptron\" algorithm based on x and y\n\nParameters:\n\nx:        Feature matrix of the training data (n × d)\ny:        Associated labels of the training data, in the format of ⨦ 1\nK:        Kernel function to employ. See ?radialKernel or ?polynomialKernelfor details or check ?BetaML.Utils to verify if other kernels are defined (you can alsways define your own kernel) [def: radialKernel]\nT:        Maximum number of iterations across the whole set (if the set is not fully classified earlier) [def: 1000]\nα:        Initial distribution of the errors [def: zeros(length(y))]\nnMsg:     Maximum number of messages to show if all iterations are done\nshuffle:  Whether to randomly shuffle the data at each iteration [def: false]\nrng:      Random Number Generator (see FIXEDSEED) [deafult: Random.GLOBAL_RNG]\n\nReturn a named tuple with:\n\nx: the x data (eventually shuffled if shuffle=true)\ny: the label\nα: the errors associated to each record\nerrors: the number of errors in the last iteration\nbesterrors: the minimum number of errors in classifying the data ever reached\niterations: the actual number of iterations performed\nseparated: a flag if the data has been successfully separated\n\nNotes:\n\nThe trained data can then be used to make predictions using the function predict(). If the option shuffle has been used, it is important to use there the returned (x,y,α) as these would have been shuffle compared with the original (x,y).\nPlease see @kernelPerceptron for a multi-class version\n\nExample:\n\njulia> model = kernelPerceptronBinary([1.1 2.1; 5.3 4.2; 1.8 1.7], [-1,1,-1])\n\n\n\n\n\n","category":"method"},{"location":"Perceptron.html#BetaML.Perceptron.pegasos-Tuple{Any, Any}","page":"Perceptron","title":"BetaML.Perceptron.pegasos","text":"pegasos(x,y;θ,θ₀,λ,η,T,nMsgs,shuffle,forceOrigin,returnMeanHyperplane)\n\nTrain the multiclass classifier \"pegasos\" algorithm according to x (features) and y (labels)\n\nPegasos is a linear, gradient-based classifier. Multiclass is supported using a one-vs-all approach.\n\nParameters:\n\nx:           Feature matrix of the training data (n × d)\ny:           Associated labels of the training data, can be in any format (string, integers..)\nθ:           Initial value of the weights (parameter) [def: zeros(d)]\nθ₀:          Initial value of the weight (parameter) associated to the constant term [def: 0]\nλ:           Multiplicative term of the learning rate\nη:           Learning rate [def: (t -> 1/sqrt(t))]\nT:           Maximum number of iterations across the whole set (if the set is not fully classified earlier) [def: 1000]\nnMsg:        Maximum number of messages to show if all iterations are done\nshuffle:     Whether to randomly shuffle the data at each iteration [def: false]\nforceOrigin: Whehter to force θ₀ to remain zero [def: false]\nreturnMeanHyperplane: Whether to return the average hyperplane coefficients instead of the average ones  [def: false]\nrng:         Random Number Generator (see FIXEDSEED) [deafult: Random.GLOBAL_RNG]\n\nReturn a named tuple with:\n\nθ:          The weights of the classifier\nθ₀:         The weight of the classifier associated to the constant term\nclasses:    The classes (unique values) of y\n\nNotes:\n\nThe trained parameters can then be used to make predictions using the function predict().\nThis model is available in the MLJ framework as the PegasosClassifier\n\nExample:\n\njulia> model = pegasos([1.1 2.1; 5.3 4.2; 1.8 1.7], [-1,1,-1])\njulia> ŷ     = predict([2.1 3.1; 7.3 5.2], model.θ, model.θ₀, model.classes)\n\n\n\n\n\n","category":"method"},{"location":"Perceptron.html#BetaML.Perceptron.pegasosBinary-Tuple{Any, Any}","page":"Perceptron","title":"BetaML.Perceptron.pegasosBinary","text":"pegasosBinary(x,y;θ,θ₀,λ,η,T,nMsgs,shuffle,forceOrigin)\n\nTrain the peagasos algorithm based on x and y (labels)\n\nParameters:\n\nx:           Feature matrix of the training data (n × d)\ny:           Associated labels of the training data, in the format of ⨦ 1\nθ:           Initial value of the weights (parameter) [def: zeros(d)]\nθ₀:          Initial value of the weight (parameter) associated to the constant term [def: 0]\nλ:           Multiplicative term of the learning rate\nη:           Learning rate [def: (t -> 1/sqrt(t))]\nT:           Maximum number of iterations across the whole set (if the set is not fully classified earlier) [def: 1000]\nnMsg:        Maximum number of messages to show if all iterations are done\nshuffle:    Whether to randomly shuffle the data at each iteration [def: false]\nforceOrigin: Whether to force θ₀ to remain zero [def: false]\n\nReturn a named tuple with:\n\nθ:          The final weights of the classifier\nθ₀:         The final weight of the classifier associated to the constant term\navgθ:       The average weights of the classifier\navgθ₀:      The average weight of the classifier associated to the constant term\nerrors:     The number of errors in the last iteration\nbesterrors: The minimum number of errors in classifying the data ever reached\niterations: The actual number of iterations performed\nseparated:  Weather the data has been successfully separated\n\nNotes:\n\nThe trained parameters can then be used to make predictions using the function predict().\n\nExample:\n\njulia> pegasos([1.1 2.1; 5.3 4.2; 1.8 1.7], [-1,1,-1])\n\n\n\n\n\n","category":"method"},{"location":"Perceptron.html#BetaML.Perceptron.perceptron-Tuple{AbstractMatrix{T} where T, AbstractVector{T} where T}","page":"Perceptron","title":"BetaML.Perceptron.perceptron","text":"perceptron(x,y;θ,θ₀,T,nMsgs,shuffle,forceOrigin,returnMeanHyperplane)\n\nTrain the multiclass classifier \"perceptron\" algorithm  based on x and y (labels).\n\nThe perceptron is a linear classifier. Multiclass is supported using a one-vs-all approach.\n\nParameters:\n\nx:           Feature matrix of the training data (n × d)\ny:           Associated labels of the training data, can be in any format (string, integers..)\nθ:           Initial value of the weights (parameter) [def: zeros(d)]\nθ₀:          Initial value of the weight (parameter) associated to the constant                term [def: 0]\nT:           Maximum number of iterations across the whole set (if the set                is not fully classified earlier) [def: 1000]\nnMsg:        Maximum number of messages to show if all iterations are done [def: 0]\nshuffle:     Whether to randomly shuffle the data at each iteration [def: false]\nforceOrigin: Whether to force θ₀ to remain zero [def: false]\nreturnMeanHyperplane: Whether to return the average hyperplane coefficients instead of the final ones  [def: false]\nrng:         Random Number Generator (see FIXEDSEED) [deafult: Random.GLOBAL_RNG]\n\nReturn a named tuple with:\n\nθ:          The weights of the classifier\nθ₀:         The weight of the classifier associated to the constant term\nclasses:    The classes (unique values) of y\n\nNotes:\n\nThe trained parameters can then be used to make predictions using the function predict().\nThis model is available in the MLJ framework as the PerceptronClassifier\n\nExample:\n\njulia> model = perceptron([1.1 2.1; 5.3 4.2; 1.8 1.7], [-1,1,-1])\njulia> ŷ     = predict([2.1 3.1; 7.3 5.2], model.θ, model.θ₀, model.classes)\n\n\n\n\n\n","category":"method"},{"location":"Perceptron.html#BetaML.Perceptron.perceptronBinary-Tuple{Any, Any}","page":"Perceptron","title":"BetaML.Perceptron.perceptronBinary","text":"perceptronBinary(x,y;θ,θ₀,T,nMsgs,shuffle,forceOrigin)\n\nTrain the binary classifier \"perceptron\" algorithm based on x and y (labels)\n\nParameters:\n\nx:           Feature matrix of the training data (n × d)\ny:           Associated labels of the training data, in the format of ⨦ 1\nθ:           Initial value of the weights (parameter) [def: zeros(d)]\nθ₀:          Initial value of the weight (parameter) associated to the constant                term [def: 0]\nT:           Maximum number of iterations across the whole set (if the set                is not fully classified earlier) [def: 1000]\nnMsg:        Maximum number of messages to show if all iterations are done\nshuffle:     Whether to randomly shuffle the data at each iteration [def: false]\nforceOrigin: Whether to force θ₀ to remain zero [def: false]\nrng:         Random Number Generator (see FIXEDSEED) [deafult: Random.GLOBAL_RNG]\n\nReturn a named tuple with:\n\nθ:          The final weights of the classifier\nθ₀:         The final weight of the classifier associated to the constant term\navgθ:       The average weights of the classifier\navgθ₀:      The average weight of the classifier associated to the constant term\nerrors:     The number of errors in the last iteration\nbesterrors: The minimum number of errors in classifying the data ever reached\niterations: The actual number of iterations performed\nseparated:  Weather the data has been successfully separated\n\nNotes:\n\nThe trained parameters can then be used to make predictions using the function predict().\n\nExample:\n\njulia> model = perceptronBinary([1.1 2.1; 5.3 4.2; 1.8 1.7], [-1,1,-1])\n\n\n\n\n\n","category":"method"},{"location":"tutorials/Classification - cars/betaml_tutorial_classification_cars.html","page":"A classification task when labels are known - determining the country of origin of cars given the cars characteristics","title":"A classification task when labels are known - determining the country of origin of cars given the cars characteristics","text":"EditURL = \"https://github.com/sylvaticus/BetaML.jl/blob/master/docs/src/tutorials/Classification - cars/betaml_tutorial_classification_cars.jl\"","category":"page"},{"location":"tutorials/Classification - cars/betaml_tutorial_classification_cars.html#classification_tutorial","page":"A classification task when labels are known - determining the country of origin of cars given the cars characteristics","title":"A classification task when labels are known - determining the country of origin of cars given the cars characteristics","text":"","category":"section"},{"location":"tutorials/Classification - cars/betaml_tutorial_classification_cars.html","page":"A classification task when labels are known - determining the country of origin of cars given the cars characteristics","title":"A classification task when labels are known - determining the country of origin of cars given the cars characteristics","text":"In this exercise we have some car technical characteristics (mpg, horsepower,weight, model year...) and the country of origin and we would like to create a model such that the country of origin can be accurately predicted given the technical characteristics. As the information to predict is a multi-class one, this is a [classification](https://en.wikipedia.org/wiki/Statistical_classification) task. It is a challenging exercise due to the simultaneous presence of three factors: (1) presence of missing data; (2) unbalanced data - 254 out of 406 cars are US made; (3) small dataset.","category":"page"},{"location":"tutorials/Classification - cars/betaml_tutorial_classification_cars.html","page":"A classification task when labels are known - determining the country of origin of cars given the cars characteristics","title":"A classification task when labels are known - determining the country of origin of cars given the cars characteristics","text":"Data origin:","category":"page"},{"location":"tutorials/Classification - cars/betaml_tutorial_classification_cars.html","page":"A classification task when labels are known - determining the country of origin of cars given the cars characteristics","title":"A classification task when labels are known - determining the country of origin of cars given the cars characteristics","text":"dataset description: https://archive.ics.uci.edu/ml/datasets/auto+mpg\ndata source we use here: https://archive.ics.uci.edu/ml/machine-learning-databases/auto-mpg/auto-mpg.data","category":"page"},{"location":"tutorials/Classification - cars/betaml_tutorial_classification_cars.html","page":"A classification task when labels are known - determining the country of origin of cars given the cars characteristics","title":"A classification task when labels are known - determining the country of origin of cars given the cars characteristics","text":"Field description:","category":"page"},{"location":"tutorials/Classification - cars/betaml_tutorial_classification_cars.html","page":"A classification task when labels are known - determining the country of origin of cars given the cars characteristics","title":"A classification task when labels are known - determining the country of origin of cars given the cars characteristics","text":"mpg:           continuous\ncylinders:     multi-valued discrete\ndisplacement:  continuous\nhorsepower:    continuous\nweight:        continuous\nacceleration:  continuous\nmodel year:    multi-valued discrete\norigin:        multi-valued discrete\ncar name:      string (unique for each instance) - not used here","category":"page"},{"location":"tutorials/Classification - cars/betaml_tutorial_classification_cars.html","page":"A classification task when labels are known - determining the country of origin of cars given the cars characteristics","title":"A classification task when labels are known - determining the country of origin of cars given the cars characteristics","text":"!!! warning As the above example is automatically executed by GitHub on every code update, it uses parameters (epoch numbers, parameter space of hyperparameter validation, number of trees,...) that minimise the computation. In real case you will want to use better but more computationally intensive ones. For the same reason benchmarks codes are commented and the pre-run output reported rather than actually being executed.","category":"page"},{"location":"tutorials/Classification - cars/betaml_tutorial_classification_cars.html#Library-and-data-loading","page":"A classification task when labels are known - determining the country of origin of cars given the cars characteristics","title":"Library and data loading","text":"","category":"section"},{"location":"tutorials/Classification - cars/betaml_tutorial_classification_cars.html","page":"A classification task when labels are known - determining the country of origin of cars given the cars characteristics","title":"A classification task when labels are known - determining the country of origin of cars given the cars characteristics","text":"We load a buch of packages that we'll use during this tutorial..","category":"page"},{"location":"tutorials/Classification - cars/betaml_tutorial_classification_cars.html","page":"A classification task when labels are known - determining the country of origin of cars given the cars characteristics","title":"A classification task when labels are known - determining the country of origin of cars given the cars characteristics","text":"using Random, HTTP, CSV, DataFrames, BenchmarkTools, BetaML\nimport DecisionTree, Flux\nimport Pipe: @pipe","category":"page"},{"location":"tutorials/Classification - cars/betaml_tutorial_classification_cars.html","page":"A classification task when labels are known - determining the country of origin of cars given the cars characteristics","title":"A classification task when labels are known - determining the country of origin of cars given the cars characteristics","text":"To load the data from the internet our workflow is (1) Retrieve the data –> (2) Clean it –> (3) Load it –> (4) Output it as a DataFrame.","category":"page"},{"location":"tutorials/Classification - cars/betaml_tutorial_classification_cars.html","page":"A classification task when labels are known - determining the country of origin of cars given the cars characteristics","title":"A classification task when labels are known - determining the country of origin of cars given the cars characteristics","text":"For step (1) we use HTTP.get(), for step (2) we use replace!, for steps (3) and (4) we uses the CSV package, and we use the \"pip\" |> operator to chain these operations:","category":"page"},{"location":"tutorials/Classification - cars/betaml_tutorial_classification_cars.html","page":"A classification task when labels are known - determining the country of origin of cars given the cars characteristics","title":"A classification task when labels are known - determining the country of origin of cars given the cars characteristics","text":"urlDataOriginal = \"https://archive.ics.uci.edu/ml/machine-learning-databases/auto-mpg/auto-mpg.data-original\"\ndata = @pipe HTTP.get(urlDataOriginal).body                                                |>\n             replace!(_, UInt8('\\t') => UInt8(' '))                                        |>\n             CSV.File(_, delim=' ', missingstring=\"NA\", ignorerepeated=true, header=false) |>\n             DataFrame;\nnothing #hide","category":"page"},{"location":"tutorials/Classification - cars/betaml_tutorial_classification_cars.html","page":"A classification task when labels are known - determining the country of origin of cars given the cars characteristics","title":"A classification task when labels are known - determining the country of origin of cars given the cars characteristics","text":"This results in a table where the rows are the observations (the various cars) and the column the fields. All BetaML models expect this layout.","category":"page"},{"location":"tutorials/Classification - cars/betaml_tutorial_classification_cars.html","page":"A classification task when labels are known - determining the country of origin of cars given the cars characteristics","title":"A classification task when labels are known - determining the country of origin of cars given the cars characteristics","text":"As the dataset is ordered, we randomly shuffle the data. Note that we pass to shuffle copy(FIXEDRNG) as the random nuber generator in order to obtain reproducible output ( FIXEDRNG is nothing else than an istance of StableRNG(123) defined in the BetaML.Utils sub-module, but you can choose of course your own \"fixed\" RNG). See the Dealing with stochasticity section in the Getting started tutorial for details.","category":"page"},{"location":"tutorials/Classification - cars/betaml_tutorial_classification_cars.html","page":"A classification task when labels are known - determining the country of origin of cars given the cars characteristics","title":"A classification task when labels are known - determining the country of origin of cars given the cars characteristics","text":"data[shuffle(copy(FIXEDRNG),axes(data, 1)), :]\ndescribe(data)","category":"page"},{"location":"tutorials/Classification - cars/betaml_tutorial_classification_cars.html","page":"A classification task when labels are known - determining the country of origin of cars given the cars characteristics","title":"A classification task when labels are known - determining the country of origin of cars given the cars characteristics","text":"Columns 1 to 7 contain  characteristics of the car, while column 8 encodes the country or origin (\"1\" -> US, \"2\" -> EU, \"3\" -> Japan). That's the variable we want to be able to predict.","category":"page"},{"location":"tutorials/Classification - cars/betaml_tutorial_classification_cars.html","page":"A classification task when labels are known - determining the country of origin of cars given the cars characteristics","title":"A classification task when labels are known - determining the country of origin of cars given the cars characteristics","text":"Columns 9 contains the car name, but we are not going to use this information in this tutorial. Note also that some fields have missing data.","category":"page"},{"location":"tutorials/Classification - cars/betaml_tutorial_classification_cars.html","page":"A classification task when labels are known - determining the country of origin of cars given the cars characteristics","title":"A classification task when labels are known - determining the country of origin of cars given the cars characteristics","text":"Our first step is hence to divide the dataset in features (the x) and the labels (the y) we want to predict. The x is then a Julia standard Matrix of 406 rows by 7 columns and the y is a vector of the 406 observations:","category":"page"},{"location":"tutorials/Classification - cars/betaml_tutorial_classification_cars.html","page":"A classification task when labels are known - determining the country of origin of cars given the cars characteristics","title":"A classification task when labels are known - determining the country of origin of cars given the cars characteristics","text":"x     = Matrix{Union{Missing,Float64}}(data[:,1:7]);\ny     = Vector{Int64}(data[:,8]);\nnothing #hide","category":"page"},{"location":"tutorials/Classification - cars/betaml_tutorial_classification_cars.html","page":"A classification task when labels are known - determining the country of origin of cars given the cars characteristics","title":"A classification task when labels are known - determining the country of origin of cars given the cars characteristics","text":"Some algorithms that we will use today don't work with missing data, so we need to impute them. We use the predictMissing function provided by the BetaML.Clustering sub-module. Internally the function uses a Gaussian Mixture Model to assign to the missing walue of a given record an average of the values of the non-missing records weighted for how close they are to our specific record. Note that the same function (predictMissing) can be used for Collaborative Filtering / recomendation systems. Using GMM has the advantage over traditional algorithms as k-nearest neighbors (KNN) that GMM can \"detect\" the hidden structure of the observed data, where some observation can be similar to a certain pool of other observvations for a certain characteristic, but similar to an other pool of observations for other characteristics.","category":"page"},{"location":"tutorials/Classification - cars/betaml_tutorial_classification_cars.html","page":"A classification task when labels are known - determining the country of origin of cars given the cars characteristics","title":"A classification task when labels are known - determining the country of origin of cars given the cars characteristics","text":"xFull = predictMissing(x,rng=copy(FIXEDRNG)).X̂;\nnothing #hide","category":"page"},{"location":"tutorials/Classification - cars/betaml_tutorial_classification_cars.html","page":"A classification task when labels are known - determining the country of origin of cars given the cars characteristics","title":"A classification task when labels are known - determining the country of origin of cars given the cars characteristics","text":"Further, some models don't work with categorical data as such, so we need to represent our y as a matrix with a separate column for each possible categorical value (the so called \"one-hot\" representation). For example, within a three classes field, the individual value 2 (or \"Europe\" for what it matters) would be represented as the vector [0 1 0], while 3 (or \"Japan\") would become the vector [0 0 1]. To encode as one-hot we use the function oneHotEncoder in BetaML.Utils","category":"page"},{"location":"tutorials/Classification - cars/betaml_tutorial_classification_cars.html","page":"A classification task when labels are known - determining the country of origin of cars given the cars characteristics","title":"A classification task when labels are known - determining the country of origin of cars given the cars characteristics","text":"y_oh  = oneHotEncoder(y);\nnothing #hide","category":"page"},{"location":"tutorials/Classification - cars/betaml_tutorial_classification_cars.html","page":"A classification task when labels are known - determining the country of origin of cars given the cars characteristics","title":"A classification task when labels are known - determining the country of origin of cars given the cars characteristics","text":"In supervised machine learning it is good practice to partition the available data in a training, validation, and test subsets, where the first one is used to train the ML algorithm, the second one to train any eventual \"hyper-parameters\" of the algorithm and the test subset is finally used to evaluate the quality of the algorithm. Here, for brevity, we use only the train and the test subsets, implicitly assuming we already know the best hyper-parameters. Please refer to the regression tutorial for examples of how to use the validation subset to train the hyper-parameters, or even better the clustering tutorial for an example of using the crossValidation function.","category":"page"},{"location":"tutorials/Classification - cars/betaml_tutorial_classification_cars.html","page":"A classification task when labels are known - determining the country of origin of cars given the cars characteristics","title":"A classification task when labels are known - determining the country of origin of cars given the cars characteristics","text":"We use then the partition function in BetaML.Utils, where we can specify the different data to partition (each matrix or vector to partition must have the same number of observations) and the shares of observation that we want in each subset. Here we keep 80% of observations for training (xtrain, xTrainFull and ytrain) and we use 20% of them for testing (xtest, xTestFull and ytest):","category":"page"},{"location":"tutorials/Classification - cars/betaml_tutorial_classification_cars.html","page":"A classification task when labels are known - determining the country of origin of cars given the cars characteristics","title":"A classification task when labels are known - determining the country of origin of cars given the cars characteristics","text":"((xtrain,xtest),(xtrainFull,xtestFull),(ytrain,ytest),(ytrain_oh,ytest_oh)) = partition([x,xFull,y,y_oh],[0.8,1-0.8],rng=copy(FIXEDRNG));\nnothing #hide","category":"page"},{"location":"tutorials/Classification - cars/betaml_tutorial_classification_cars.html#Random-Forests","page":"A classification task when labels are known - determining the country of origin of cars given the cars characteristics","title":"Random Forests","text":"","category":"section"},{"location":"tutorials/Classification - cars/betaml_tutorial_classification_cars.html","page":"A classification task when labels are known - determining the country of origin of cars given the cars characteristics","title":"A classification task when labels are known - determining the country of origin of cars given the cars characteristics","text":"We are now ready to use our first model, the Random Forests (in the BetaML.Trees sub-module). Random Forests build a \"forest\" of decision trees models and then average their predictions in order to make an overall prediction out of a feature vector.","category":"page"},{"location":"tutorials/Classification - cars/betaml_tutorial_classification_cars.html","page":"A classification task when labels are known - determining the country of origin of cars given the cars characteristics","title":"A classification task when labels are known - determining the country of origin of cars given the cars characteristics","text":"To \"build\" the forest model (i.e. to \"train\" it) we need to give the model the training feature matrix and the associated \"true\" training labels, and we need to specify the number of trees to employ (this is an example of hyper-parameters). Here we use 30 individual decision trees.","category":"page"},{"location":"tutorials/Classification - cars/betaml_tutorial_classification_cars.html","page":"A classification task when labels are known - determining the country of origin of cars given the cars characteristics","title":"A classification task when labels are known - determining the country of origin of cars given the cars characteristics","text":"As the labels are encoded using integers,  we need also to specify the parameter forceClassification=true, otherwise the model would undergo a regression job instead.","category":"page"},{"location":"tutorials/Classification - cars/betaml_tutorial_classification_cars.html","page":"A classification task when labels are known - determining the country of origin of cars given the cars characteristics","title":"A classification task when labels are known - determining the country of origin of cars given the cars characteristics","text":"myForest       = buildForest(xtrain,ytrain,30, rng=copy(FIXEDRNG),forceClassification=true);\nnothing #hide","category":"page"},{"location":"tutorials/Classification - cars/betaml_tutorial_classification_cars.html","page":"A classification task when labels are known - determining the country of origin of cars given the cars characteristics","title":"A classification task when labels are known - determining the country of origin of cars given the cars characteristics","text":"To obtain the predicted values, we can simply use the function BetaML.Trees.predict with our myForest model and either the training or the testing data.","category":"page"},{"location":"tutorials/Classification - cars/betaml_tutorial_classification_cars.html","page":"A classification task when labels are known - determining the country of origin of cars given the cars characteristics","title":"A classification task when labels are known - determining the country of origin of cars given the cars characteristics","text":"ŷtrain,ŷtest   = predict.(Ref(myForest), [xtrain,xtest],rng=copy(FIXEDRNG));\nnothing #hide","category":"page"},{"location":"tutorials/Classification - cars/betaml_tutorial_classification_cars.html","page":"A classification task when labels are known - determining the country of origin of cars given the cars characteristics","title":"A classification task when labels are known - determining the country of origin of cars given the cars characteristics","text":"Finally we can measure the accuracy of our predictions with the accuracy function, with the sidenote that we need first to \"parse\" the ŷs as forcing the classification job transformed automatically them to strings (they originally were integers):","category":"page"},{"location":"tutorials/Classification - cars/betaml_tutorial_classification_cars.html","page":"A classification task when labels are known - determining the country of origin of cars given the cars characteristics","title":"A classification task when labels are known - determining the country of origin of cars given the cars characteristics","text":"trainAccuracy,testAccuracy  = accuracy.([parse.(Int64,mode(ŷtrain,rng=copy(FIXEDRNG))),parse.(Int64,mode(ŷtest,rng=copy(FIXEDRNG)))],[ytrain,ytest])","category":"page"},{"location":"tutorials/Classification - cars/betaml_tutorial_classification_cars.html","page":"A classification task when labels are known - determining the country of origin of cars given the cars characteristics","title":"A classification task when labels are known - determining the country of origin of cars given the cars characteristics","text":"The predictions are quite good, for the training set the algoritm predicted almost all cars' origins correctly, while for the testing set (i.e. those records that has not been used to train the algorithm), the correct prediction level is still quite high, at 80%","category":"page"},{"location":"tutorials/Classification - cars/betaml_tutorial_classification_cars.html","page":"A classification task when labels are known - determining the country of origin of cars given the cars characteristics","title":"A classification task when labels are known - determining the country of origin of cars given the cars characteristics","text":"While accuracy can sometimes suffice, we may often want to better understand which categories our model has trouble to predict correctly. We can investigate the output of a multi-class classifier more in-deep with a ConfusionMatrix where the true values (y) are given in rows and the predicted ones (ŷ) in columns, together to some per-class metrics like the precision (true class i over predicted in class i), the recall (predicted class i over the true class i) and others.","category":"page"},{"location":"tutorials/Classification - cars/betaml_tutorial_classification_cars.html","page":"A classification task when labels are known - determining the country of origin of cars given the cars characteristics","title":"A classification task when labels are known - determining the country of origin of cars given the cars characteristics","text":"We fist build the ConfusionMatrix object between ŷ and y and then we print it (we do it here for the test subset):","category":"page"},{"location":"tutorials/Classification - cars/betaml_tutorial_classification_cars.html","page":"A classification task when labels are known - determining the country of origin of cars given the cars characteristics","title":"A classification task when labels are known - determining the country of origin of cars given the cars characteristics","text":"cm = ConfusionMatrix(parse.(Int64,mode(ŷtest,rng=copy(FIXEDRNG))),ytest,classes=[1,2,3],labels=[\"US\",\"EU\",\"Japan\"])\nprint(cm,\"all\")","category":"page"},{"location":"tutorials/Classification - cars/betaml_tutorial_classification_cars.html","page":"A classification task when labels are known - determining the country of origin of cars given the cars characteristics","title":"A classification task when labels are known - determining the country of origin of cars given the cars characteristics","text":"From the report we can see that Japanese cars have more trouble in being correctly classified, and in particular many Japanease cars are classified as US ones. This is likely a result of the class imbalance of the data set, and could be solved by balancing the dataset with various sampling tecniques before training the model.","category":"page"},{"location":"tutorials/Classification - cars/betaml_tutorial_classification_cars.html","page":"A classification task when labels are known - determining the country of origin of cars given the cars characteristics","title":"A classification task when labels are known - determining the country of origin of cars given the cars characteristics","text":"When we benchmark the resourse used (time and memory) we find that Random Forests remain pretty fast, expecially when we compare them with neural networks (see later) @btime buildForest(xtrain,ytrain,30, rng=copy(FIXEDRNG),forceClassification=true); 134.096 ms (781027 allocations: 196.30 MiB)","category":"page"},{"location":"tutorials/Classification - cars/betaml_tutorial_classification_cars.html#Comparision-with-DecisionTree.jl","page":"A classification task when labels are known - determining the country of origin of cars given the cars characteristics","title":"Comparision with DecisionTree.jl","text":"","category":"section"},{"location":"tutorials/Classification - cars/betaml_tutorial_classification_cars.html","page":"A classification task when labels are known - determining the country of origin of cars given the cars characteristics","title":"A classification task when labels are known - determining the country of origin of cars given the cars characteristics","text":"DecisionTrees.jl random forests are similar in usage: we first \"build\" (train) the forest and we then make predictions out of the trained model. The main difference is that the model requires data with nonmissing values, so we are going to use the xtrainFull and xtestFull feature labels we created earlier:","category":"page"},{"location":"tutorials/Classification - cars/betaml_tutorial_classification_cars.html","page":"A classification task when labels are known - determining the country of origin of cars given the cars characteristics","title":"A classification task when labels are known - determining the country of origin of cars given the cars characteristics","text":"# We train the model...\nmodel = DecisionTree.build_forest(ytrain, xtrainFull,-1,30,rng=123)\n# ..and we generate predictions and measure their error\n(ŷtrain,ŷtest) = DecisionTree.apply_forest.([model],[xtrainFull,xtestFull]);\n(trainAccuracy,testAccuracy) = accuracy.([ŷtrain,ŷtest],[ytrain,ytest])","category":"page"},{"location":"tutorials/Classification - cars/betaml_tutorial_classification_cars.html","page":"A classification task when labels are known - determining the country of origin of cars given the cars characteristics","title":"A classification task when labels are known - determining the country of origin of cars given the cars characteristics","text":"While the accuracy on the training set is exactly the same as for BetaML random forets, DecisionTree.jl random forests are slighly less accurate in the testing sample. Where however DecisionTrees.jl excell is in the efficiency: they are extremelly fast and memory thrifty, even if to this benchmark we should add the resources needed to impute the missing values.","category":"page"},{"location":"tutorials/Classification - cars/betaml_tutorial_classification_cars.html","page":"A classification task when labels are known - determining the country of origin of cars given the cars characteristics","title":"A classification task when labels are known - determining the country of origin of cars given the cars characteristics","text":"Also, one of the reasons DecisionTrees are such efficient is that internally they sort the data to avoid repeated comparision, but in this way they work only with features that are sortable, while BetaML random forests accept virtually any kind of input without the need of adapt it. @btime  DecisionTree.build_forest(ytrain, xtrainFull,-1,30,rng=123); 1.431 ms (10875 allocations: 1.52 MiB)","category":"page"},{"location":"tutorials/Classification - cars/betaml_tutorial_classification_cars.html#Neural-network","page":"A classification task when labels are known - determining the country of origin of cars given the cars characteristics","title":"Neural network","text":"","category":"section"},{"location":"tutorials/Classification - cars/betaml_tutorial_classification_cars.html","page":"A classification task when labels are known - determining the country of origin of cars given the cars characteristics","title":"A classification task when labels are known - determining the country of origin of cars given the cars characteristics","text":"Neural networks (NN) can be very powerfull, but have two \"inconvenients\" compared with random forests: first, are a bit \"picky\". We need to do a bit of work to provide data in specific format. Note that this is not feature engineering. One of the advantages on neural network is that for the most this is not needed for neural networks. However we still need to \"clean\" the data. One issue is that NN don't like missing data. So we need to provide them with the feature matrix \"clean\" of missing data. Secondly, they work only with numerical data. So we need to use the one-hot encoding we saw earlier. Further, they work best if the features are scaled such that each feature has mean zero and standard deviation 1. We can achieve it with the function scale or, as in this case, getScaleFactors.","category":"page"},{"location":"tutorials/Classification - cars/betaml_tutorial_classification_cars.html","page":"A classification task when labels are known - determining the country of origin of cars given the cars characteristics","title":"A classification task when labels are known - determining the country of origin of cars given the cars characteristics","text":"xScaleFactors   = getScaleFactors(xtrainFull)\nD               = size(xtrainFull,2)\nclasses         = unique(y)\nnCl             = length(classes)","category":"page"},{"location":"tutorials/Classification - cars/betaml_tutorial_classification_cars.html","page":"A classification task when labels are known - determining the country of origin of cars given the cars characteristics","title":"A classification task when labels are known - determining the country of origin of cars given the cars characteristics","text":"The second \"inconvenient\" of NN is that, while not requiring feature engineering, they stil lneed a bit of practice on the way to build the network. It's not as simple as train(model,x,y). We need here to specify how we want our layers, chain the layers together and then decide a loss overall function. Only when we done these steps, we have the model ready for training. Here we define 2 DenseLayer where, for each of them, we specify the number of neurons in input (the first layer being equal to the dimensions of the data), the output layer (for a classification task, the last layer output size beying equal to the number of classes) and an activation function for each layer (default the identity function).","category":"page"},{"location":"tutorials/Classification - cars/betaml_tutorial_classification_cars.html","page":"A classification task when labels are known - determining the country of origin of cars given the cars characteristics","title":"A classification task when labels are known - determining the country of origin of cars given the cars characteristics","text":"ls   = 50\nl1   = DenseLayer(D,ls,f=relu,rng=copy(FIXEDRNG))\nl2   = DenseLayer(ls,nCl,f=relu,rng=copy(FIXEDRNG))","category":"page"},{"location":"tutorials/Classification - cars/betaml_tutorial_classification_cars.html","page":"A classification task when labels are known - determining the country of origin of cars given the cars characteristics","title":"A classification task when labels are known - determining the country of origin of cars given the cars characteristics","text":"For a classification the last layer is a VectorFunctionLayer that has no learnable parameters but whose activation function is applied to the ensemble of the neurons, rather than individually on each neuron. In particular, for classification we pass the BetaML.Utils.softmax function whose output has the same size as the input (and the number of classes to predict), but we can use the VectorFunctionLayer with any function, including the pool1d function to create a \"pooling\" layer (using maximum, mean or whatever other subfunction we pass to pool1d)","category":"page"},{"location":"tutorials/Classification - cars/betaml_tutorial_classification_cars.html","page":"A classification task when labels are known - determining the country of origin of cars given the cars characteristics","title":"A classification task when labels are known - determining the country of origin of cars given the cars characteristics","text":"l3   = VectorFunctionLayer(nCl,f=softmax) ## Add a (parameterless) layer whose activation function (softMax in this case) is defined to all its nodes at once","category":"page"},{"location":"tutorials/Classification - cars/betaml_tutorial_classification_cars.html","page":"A classification task when labels are known - determining the country of origin of cars given the cars characteristics","title":"A classification task when labels are known - determining the country of origin of cars given the cars characteristics","text":"Finally we chain the layers and assign a loss function with buildNetwork:","category":"page"},{"location":"tutorials/Classification - cars/betaml_tutorial_classification_cars.html","page":"A classification task when labels are known - determining the country of origin of cars given the cars characteristics","title":"A classification task when labels are known - determining the country of origin of cars given the cars characteristics","text":"mynn = buildNetwork([l1,l2,l3],squaredCost,name=\"Multinomial logistic regression Model Cars\") ## Build the NN and use the squared cost (aka MSE) as error function (crossEntropy could also be used)","category":"page"},{"location":"tutorials/Classification - cars/betaml_tutorial_classification_cars.html","page":"A classification task when labels are known - determining the country of origin of cars given the cars characteristics","title":"A classification task when labels are known - determining the country of origin of cars given the cars characteristics","text":"Now we can train our network using the function train!. It has many options, have a look at the documentation for all the possible arguments. Note that we train the network based on the scaled feature matrix.","category":"page"},{"location":"tutorials/Classification - cars/betaml_tutorial_classification_cars.html","page":"A classification task when labels are known - determining the country of origin of cars given the cars characteristics","title":"A classification task when labels are known - determining the country of origin of cars given the cars characteristics","text":"res  = train!(mynn,scale(xtrainFull,xScaleFactors),ytrain_oh,epochs=500,batchSize=8,optAlg=ADAM(),rng=copy(FIXEDRNG)) ## Use optAlg=SGD() to use Stochastic Gradient Descent instead","category":"page"},{"location":"tutorials/Classification - cars/betaml_tutorial_classification_cars.html","page":"A classification task when labels are known - determining the country of origin of cars given the cars characteristics","title":"A classification task when labels are known - determining the country of origin of cars given the cars characteristics","text":"Once trained, we can predict the label. As the trained was based on the scaled feature matrix, so must be for the predictions","category":"page"},{"location":"tutorials/Classification - cars/betaml_tutorial_classification_cars.html","page":"A classification task when labels are known - determining the country of origin of cars given the cars characteristics","title":"A classification task when labels are known - determining the country of origin of cars given the cars characteristics","text":"(ŷtrain,ŷtest)  = predict.(Ref(mynn),[scale(xtrainFull,xScaleFactors),scale(xtestFull,xScaleFactors)])\ntrainAccuracy   = accuracy(ŷtrain,ytrain,rng=copy(FIXEDRNG))","category":"page"},{"location":"tutorials/Classification - cars/betaml_tutorial_classification_cars.html","page":"A classification task when labels are known - determining the country of origin of cars given the cars characteristics","title":"A classification task when labels are known - determining the country of origin of cars given the cars characteristics","text":"testAccuracy    = accuracy(ŷtest,ytest,rng=copy(FIXEDRNG))\n\n\n\ncm = ConfusionMatrix(ŷtest,ytest,classes=[1,2,3],labels=[\"US\",\"EU\",\"Japan\"],rng=copy(FIXEDRNG))","category":"page"},{"location":"tutorials/Classification - cars/betaml_tutorial_classification_cars.html","page":"A classification task when labels are known - determining the country of origin of cars given the cars characteristics","title":"A classification task when labels are known - determining the country of origin of cars given the cars characteristics","text":"print(cm)","category":"page"},{"location":"tutorials/Classification - cars/betaml_tutorial_classification_cars.html","page":"A classification task when labels are known - determining the country of origin of cars given the cars characteristics","title":"A classification task when labels are known - determining the country of origin of cars given the cars characteristics","text":"4×4 Matrix{Any}:  \"Labels\"    \"US\"    \"EU\"   \"Japan\"  \"US\"      44       0      5  \"EU\"       3      10      3  \"Japan\"    6       2      8 4×4 Matrix{Any}:  \"Labels\"   \"US\"      \"EU\"   \"Japan\"  \"US\"      0.897959  0.0    0.102041  \"EU\"      0.1875    0.625  0.1875  \"Japan\"   0.375     0.125  0.5","category":"page"},{"location":"tutorials/Classification - cars/betaml_tutorial_classification_cars.html","page":"A classification task when labels are known - determining the country of origin of cars given the cars characteristics","title":"A classification task when labels are known - determining the country of origin of cars given the cars characteristics","text":"We see a bit the limits of neural networks in this example. While NN can be extremelly performant in many domains, they also require lot of data and computational power, expecially considering the many possible hyper-parameters and hence its large space in the hyper-parameter tuning. In this example we arrive short to the performance of random forests, yet with a significant numberof neurons.","category":"page"},{"location":"tutorials/Classification - cars/betaml_tutorial_classification_cars.html","page":"A classification task when labels are known - determining the country of origin of cars given the cars characteristics","title":"A classification task when labels are known - determining the country of origin of cars given the cars characteristics","text":"@btime train!(mynn,scale(xtrainFull),ytrain_oh,epochs=300,batchSize=8,rng=copy(FIXEDRNG),verbosity=NONE); 11.841 s (62860672 allocations: 4.21 GiB)","category":"page"},{"location":"tutorials/Classification - cars/betaml_tutorial_classification_cars.html#Comparisons-with-Flux","page":"A classification task when labels are known - determining the country of origin of cars given the cars characteristics","title":"Comparisons with Flux","text":"","category":"section"},{"location":"tutorials/Classification - cars/betaml_tutorial_classification_cars.html","page":"A classification task when labels are known - determining the country of origin of cars given the cars characteristics","title":"A classification task when labels are known - determining the country of origin of cars given the cars characteristics","text":"In Flux the input must be in the form (fields, observations), so we transpose our original matrices","category":"page"},{"location":"tutorials/Classification - cars/betaml_tutorial_classification_cars.html","page":"A classification task when labels are known - determining the country of origin of cars given the cars characteristics","title":"A classification task when labels are known - determining the country of origin of cars given the cars characteristics","text":"xtrainT, ytrain_ohT = transpose.([scale(xtrainFull,xScaleFactors), ytrain_oh])\nxtestT, ytest_ohT   = transpose.([scale(xtestFull,xScaleFactors), ytest_oh])","category":"page"},{"location":"tutorials/Classification - cars/betaml_tutorial_classification_cars.html","page":"A classification task when labels are known - determining the country of origin of cars given the cars characteristics","title":"A classification task when labels are known - determining the country of origin of cars given the cars characteristics","text":"We define the Flux neural network model in a similar way than BetaML and load it with data, we train it, predict and measure the accuracies on the training and the test sets:","category":"page"},{"location":"tutorials/Classification - cars/betaml_tutorial_classification_cars.html","page":"A classification task when labels are known - determining the country of origin of cars given the cars characteristics","title":"A classification task when labels are known - determining the country of origin of cars given the cars characteristics","text":"Random.seed!(123)\n\nl1         = Flux.Dense(D,ls,Flux.relu)\n#l2         = Flux.Dense(ls,ls,Flux.relu)\nl3         = Flux.Dense(ls,nCl,Flux.relu)\nFlux_nn    = Flux.Chain(l1,l3)\nloss(x, y) = Flux.logitcrossentropy(Flux_nn(x), y)\nps         = Flux.params(Flux_nn)\nnndata     = Flux.Data.DataLoader((xtrainT, ytrain_ohT), batchsize=8,shuffle=true)\nbegin for i in 1:500  Flux.train!(loss, ps, nndata, Flux.ADAM()) end end\nŷtrain     = Flux.onecold(Flux_nn(xtrainT),1:3)\nŷtest      = Flux.onecold(Flux_nn(xtestT),1:3)\ntrainAccuracy =  accuracy(ŷtrain,ytrain)","category":"page"},{"location":"tutorials/Classification - cars/betaml_tutorial_classification_cars.html","page":"A classification task when labels are known - determining the country of origin of cars given the cars characteristics","title":"A classification task when labels are known - determining the country of origin of cars given the cars characteristics","text":"testAccuracy  = accuracy(ŷtest,ytest)","category":"page"},{"location":"tutorials/Classification - cars/betaml_tutorial_classification_cars.html","page":"A classification task when labels are known - determining the country of origin of cars given the cars characteristics","title":"A classification task when labels are known - determining the country of origin of cars given the cars characteristics","text":"While the train accuracy is little bit higher that BetaML, the test accuracy remains comparable","category":"page"},{"location":"tutorials/Classification - cars/betaml_tutorial_classification_cars.html","page":"A classification task when labels are known - determining the country of origin of cars given the cars characteristics","title":"A classification task when labels are known - determining the country of origin of cars given the cars characteristics","text":"However the time is again lower than BetaML, even if here for \"just\" a factor 2 @btime begin for i in 1:500 Flux.train!(loss, ps, nndata, Flux.ADAM()) end end; 5.665 s (8943640 allocations: 1.07 GiB)","category":"page"},{"location":"tutorials/Classification - cars/betaml_tutorial_classification_cars.html#Summary","page":"A classification task when labels are known - determining the country of origin of cars given the cars characteristics","title":"Summary","text":"","category":"section"},{"location":"tutorials/Classification - cars/betaml_tutorial_classification_cars.html","page":"A classification task when labels are known - determining the country of origin of cars given the cars characteristics","title":"A classification task when labels are known - determining the country of origin of cars given the cars characteristics","text":"This is the summary of the results we had trying to predict the country of origin of the cars, based on their technical characteristics:","category":"page"},{"location":"tutorials/Classification - cars/betaml_tutorial_classification_cars.html","page":"A classification task when labels are known - determining the country of origin of cars given the cars characteristics","title":"A classification task when labels are known - determining the country of origin of cars given the cars characteristics","text":"Model Train acc Test Acc Training time (ms)* Training mem (MB) *\nRF 0.9969 0.8025 134 196\nRF (DecisionTree.jl) 0.9969 0.7531 1.43 1.5\nNN 0.895 0.765 11841 4311\nNN (Flux.jl) 0.938 0.741 5665 1096","category":"page"},{"location":"tutorials/Classification - cars/betaml_tutorial_classification_cars.html","page":"A classification task when labels are known - determining the country of origin of cars given the cars characteristics","title":"A classification task when labels are known - determining the country of origin of cars given the cars characteristics","text":"on a Intel Core i5-8350U laptop","category":"page"},{"location":"tutorials/Classification - cars/betaml_tutorial_classification_cars.html","page":"A classification task when labels are known - determining the country of origin of cars given the cars characteristics","title":"A classification task when labels are known - determining the country of origin of cars given the cars characteristics","text":"We warn that this table just provides a rought idea of the various algorithms performances. Indeed there is a large amount of stochasticity both in the sampling of the data used for training/testing and in the initial settings of the parameters of the algorithm. For a statistically significant comparision we would have to repeat the analysis with multiple sampling (e.g. by cross-validation, see the clustering tutorial for an example) and initial random parameters.","category":"page"},{"location":"tutorials/Classification - cars/betaml_tutorial_classification_cars.html","page":"A classification task when labels are known - determining the country of origin of cars given the cars characteristics","title":"A classification task when labels are known - determining the country of origin of cars given the cars characteristics","text":"Neverthless the table above shows that, when we compare BetaML with the algorithm-specific leading packages, we found similar results in terms of accuracy, but often the leading packages are better optimised and run more efficiently (but sometimes at the cost of being less verstatile). Also, for this dataset, Random Forests seems to remain marginally more accurate than Neural Network, altought of course this depends on the hyper-parameters and, with a single run of the models, we don't know if this difference is significant.","category":"page"},{"location":"tutorials/Classification - cars/betaml_tutorial_classification_cars.html","page":"A classification task when labels are known - determining the country of origin of cars given the cars characteristics","title":"A classification task when labels are known - determining the country of origin of cars given the cars characteristics","text":"View this file on Github.","category":"page"},{"location":"tutorials/Classification - cars/betaml_tutorial_classification_cars.html","page":"A classification task when labels are known - determining the country of origin of cars given the cars characteristics","title":"A classification task when labels are known - determining the country of origin of cars given the cars characteristics","text":"","category":"page"},{"location":"tutorials/Classification - cars/betaml_tutorial_classification_cars.html","page":"A classification task when labels are known - determining the country of origin of cars given the cars characteristics","title":"A classification task when labels are known - determining the country of origin of cars given the cars characteristics","text":"This page was generated using Literate.jl.","category":"page"},{"location":"tutorials/Betaml_tutorial_getting_started.html#getting_started","page":"Getting started","title":"Getting started","text":"","category":"section"},{"location":"tutorials/Betaml_tutorial_getting_started.html#Introduction","page":"Getting started","title":"Introduction","text":"","category":"section"},{"location":"tutorials/Betaml_tutorial_getting_started.html","page":"Getting started","title":"Getting started","text":"This \"tutorial\" part of the documentation presents a step-by-step guide to the main algorithms and utility functions provided by BetaML and comparisons with the leading packages in each field. Aside this page, the tutorial is divided in the following sections:","category":"page"},{"location":"tutorials/Betaml_tutorial_getting_started.html","page":"Getting started","title":"Getting started","text":"Regression tutorial - Arguments: Decision trees, Random forests, neural networks, hyper-parameter tuning, continuous error measures\nClassification tutorial - Arguments: Decision trees and random forests, neural networks (softmax), pre-processing workflow, confusion matrix\nClustering tutorial - Arguments: k-means, kmedoids, generative (gaussian) mixture models (gmm), cross-validation","category":"page"},{"location":"tutorials/Betaml_tutorial_getting_started.html","page":"Getting started","title":"Getting started","text":"Detailed information on the algorithms can be instead found in the API (Reference manual) of the individual modules. The following modules are currently implemented: Perceptron (linear and kernel-based classifiers), Trees (Decision Trees and Random Forests), Nn (Neural Networks), Clustering (Kmean, Kmenoids, Expectation-Maximisation, Missing value imputation, ...) and Utils.","category":"page"},{"location":"tutorials/Betaml_tutorial_getting_started.html","page":"Getting started","title":"Getting started","text":"Finally, theoretical notes describing most of these algorithms can be found at the companion repository https://github.com/sylvaticus/MITx_6.86x.","category":"page"},{"location":"tutorials/Betaml_tutorial_getting_started.html","page":"Getting started","title":"Getting started","text":"The overall \"philosophy\" of BetaML is to support simple machine learning tasks easily and make complex tasks possible. An the most basic level, the majority of  algorithms have default parameters suitable for a basic analysis. A great level of flexibility can be already achieved by just employing the full set of model parameters, for example changing the distance function in kmedoids to l1_distance (aka \"Manhattan distance\"). Finally, the greatest flexibility can be obtained by customising BetaML and writing, for example, its own neural network layer type (by subclassing AbstractLayer), its own sampler (by subclassing AbstractDataSampler) or its own mixture component (by subclassing AbstractMixture), In such a case, while not required by any means, please consider to give it back to the community and open a pull request to integrate your types in BetaML.","category":"page"},{"location":"tutorials/Betaml_tutorial_getting_started.html","page":"Getting started","title":"Getting started","text":"If you are looking for an introductory book on Julia, you could have a look on \"Julia Quick Syntax Reference\" (Apress,2019).","category":"page"},{"location":"tutorials/Betaml_tutorial_getting_started.html","page":"Getting started","title":"Getting started","text":"A few miscellaneous notes:","category":"page"},{"location":"tutorials/Betaml_tutorial_getting_started.html","page":"Getting started","title":"Getting started","text":"Functions and type names use the so-called \"CamelCase\" convention, where the words are separated by a capital letter rather than _;\nWhile some functions provide a dims parameter, most BetaML algorithms expect the input data layout with observations organised by rows and fields/features by columns. Almost everywhere we call N the number of observations/records, and D the number of dimensions;\nWhile some algorithms accept as input DataFrames, the usage of standard arrays is encourages (if the data is passed to the function as dataframe, it may be converted to standard arrays somewhere inside inner loops, leading to great inefficiencies).","category":"page"},{"location":"tutorials/Betaml_tutorial_getting_started.html#using_betaml_from_other_languages","page":"Getting started","title":"Using BetaML from other programming languages","text":"","category":"section"},{"location":"tutorials/Betaml_tutorial_getting_started.html","page":"Getting started","title":"Getting started","text":"Thanks to respectively PyJulia and JuliaCall, using BetaML in Python or R is almost as simple as using a native library. In both cases we need first to download and install the Julia binaries for our operating system from JuliaLang.org. Be sure that Julia is working by opening the Julia terminal and e.g. typing println(\"hello world\") (JuliaCall has an option to install a private-to-R version of Julia from within R). Also, in both case we do not need to think to converting Python/R objects to Julia objects when calling a Julia function and converting back the result from the Julia object to a Pytoh or R object, as this is handled automatically by PyJulia and JuliaCall, at least for simple types (arrays, strings,...)","category":"page"},{"location":"tutorials/Betaml_tutorial_getting_started.html#Use-BetaML-in-Python","page":"Getting started","title":"Use BetaML in Python","text":"","category":"section"},{"location":"tutorials/Betaml_tutorial_getting_started.html","page":"Getting started","title":"Getting started","text":"$ python3 -m pip install --user julia   # the name of the package in `pip` is `julia`, not `PyJulia`","category":"page"},{"location":"tutorials/Betaml_tutorial_getting_started.html","page":"Getting started","title":"Getting started","text":"For the sake of this tutorial, let's also install in Python a package that contains the dataset that we will use:","category":"page"},{"location":"tutorials/Betaml_tutorial_getting_started.html","page":"Getting started","title":"Getting started","text":"$ python3 -m pip install --user sklearn # only for retrieving the dataset in the python way","category":"page"},{"location":"tutorials/Betaml_tutorial_getting_started.html","page":"Getting started","title":"Getting started","text":"We can now open a Python terminal and, to obtain an interface to Julia, just run:","category":"page"},{"location":"tutorials/Betaml_tutorial_getting_started.html","page":"Getting started","title":"Getting started","text":">>> import julia\n>>> julia.install() # Only once to set-up in julia the julia packages required by PyJulia\n>>> jl = julia.Julia(compiled_modules=False)","category":"page"},{"location":"tutorials/Betaml_tutorial_getting_started.html","page":"Getting started","title":"Getting started","text":"If we have multiple Julia versions, we can specify the one to use in Python passing julia=\"/path/to/julia/binary/executable\" (e.g. julia = \"/home/myUser/lib/julia-1.1.0/bin/julia\") to the install() function.","category":"page"},{"location":"tutorials/Betaml_tutorial_getting_started.html","page":"Getting started","title":"Getting started","text":"The compiled_module=False in the Julia constructor is a workaround to the common situation when the Python interpreter is statically linked to libpython, but it will slow down the interactive experience, as it will disable Julia packages pre-compilation, and every time we will use a module for the first time, this will need to be compiled first. Other, more efficient but also more complicate, workarounds are given in the package documentation, under the https://pyjulia.readthedocs.io/en/stable/troubleshooting.html[Troubleshooting section].","category":"page"},{"location":"tutorials/Betaml_tutorial_getting_started.html","page":"Getting started","title":"Getting started","text":"Let's now add to Julia the BetaML package. We can surely do it from within Julia, but we can also do it while remaining in Python:","category":"page"},{"location":"tutorials/Betaml_tutorial_getting_started.html","page":"Getting started","title":"Getting started","text":">>> jl.eval('using Pkg; Pkg.add(\"BetaML\")') # Only once to install BetaML","category":"page"},{"location":"tutorials/Betaml_tutorial_getting_started.html","page":"Getting started","title":"Getting started","text":"While jl.eval('some Julia code') evaluates any arbitrary Julia code (see below), most of the time we can use Julia in a more direct way. Let's start by importing the BetaML Julia package as a submodule of the Python Julia module:","category":"page"},{"location":"tutorials/Betaml_tutorial_getting_started.html","page":"Getting started","title":"Getting started","text":">>> from julia import BetaML","category":"page"},{"location":"tutorials/Betaml_tutorial_getting_started.html","page":"Getting started","title":"Getting started","text":"As you can see, it is no different than importing any other Python module.","category":"page"},{"location":"tutorials/Betaml_tutorial_getting_started.html","page":"Getting started","title":"Getting started","text":"For the data, let's load it \"Python side\":","category":"page"},{"location":"tutorials/Betaml_tutorial_getting_started.html","page":"Getting started","title":"Getting started","text":">>> from sklearn import datasets\n>>> iris = datasets.load_iris()\n>>> X = iris.data[:, :4]\n>>> y = iris.target + 1 # Julia arrays start from 1 not 0","category":"page"},{"location":"tutorials/Betaml_tutorial_getting_started.html","page":"Getting started","title":"Getting started","text":"Note that X and y are Numpy arrays.","category":"page"},{"location":"tutorials/Betaml_tutorial_getting_started.html","page":"Getting started","title":"Getting started","text":"We can now call BetaML functions as we would do for any other Python library functions. In particular, we can pass to the functions (and retrieve) complex data types without worrying too much about the conversion between Python and Julia types, as these are converted automatically:","category":"page"},{"location":"tutorials/Betaml_tutorial_getting_started.html","page":"Getting started","title":"Getting started","text":" >>> (Xs,ys) = BetaML.shuffle([X,y]) # X and y are first converted to julia arrays and then the returned julia arrays are converted back to python Numpy arrays\n >>> cOut    = BetaML.kmeans(Xs,3)\n >>> y_hat   = cOut[0]\n >>> acc     = BetaML.accuracy(y_hat,ys)\n >>> acc\n 0.8933333333333333","category":"page"},{"location":"tutorials/Betaml_tutorial_getting_started.html","page":"Getting started","title":"Getting started","text":"Note: If we are using the jl.eval() interface, the objects we use must be already known to julia. To pass objects from Python to Julia, import the julia Main module (the root module in julia) and assign the needed variables, e.g.","category":"page"},{"location":"tutorials/Betaml_tutorial_getting_started.html","page":"Getting started","title":"Getting started","text":">>> X_python = [1,2,3,2,4]\n>>> from julia import Main\n>>> Main.X_julia = X_python\n>>> jl.eval('BetaML.gini(X_julia)')\n0.7199999999999999","category":"page"},{"location":"tutorials/Betaml_tutorial_getting_started.html#Use-BetaML-in-R","page":"Getting started","title":"Use BetaML in R","text":"","category":"section"},{"location":"tutorials/Betaml_tutorial_getting_started.html","page":"Getting started","title":"Getting started","text":"We start by installing the JuliaCall R package:","category":"page"},{"location":"tutorials/Betaml_tutorial_getting_started.html","page":"Getting started","title":"Getting started","text":"> install.packages(\"JuliaCall\")\n> library(JuliaCall)\n> julia_setup(installJulia = FALSE) # use installJulia = FALSE to let R download and install a private copy of julia","category":"page"},{"location":"tutorials/Betaml_tutorial_getting_started.html","page":"Getting started","title":"Getting started","text":"Note that, differently than PyJulia, the \"setup\" function needs to be called every time we start a new R section, not just when we install the JuliaCall package. If we don't have julia in the path of our system, or if we have multiple versions and we want to specify the one to work with, we can pass the JULIA_HOME = \"/path/to/julia/binary/executable/directory\" (e.g. JULIA_HOME = \"/home/myUser/lib/julia-1.1.0/bin\") parameter to the julia_setup call. Or just let JuliaCall automatically download and install a private copy of julia.","category":"page"},{"location":"tutorials/Betaml_tutorial_getting_started.html","page":"Getting started","title":"Getting started","text":"JuliaCall depends for some things (like object conversion between Julia and R) from the Julia RCall package. If we don't already have it installed in Julia, it will try to install it automatically.","category":"page"},{"location":"tutorials/Betaml_tutorial_getting_started.html","page":"Getting started","title":"Getting started","text":"As in Python, let's start from the data loaded from R and do some work with them in Julia:","category":"page"},{"location":"tutorials/Betaml_tutorial_getting_started.html","page":"Getting started","title":"Getting started","text":"> library(datasets)\n> X <- as.matrix(sapply(iris[,1:4], as.numeric))\n> y <- sapply(iris[,5], as.integer)","category":"page"},{"location":"tutorials/Betaml_tutorial_getting_started.html","page":"Getting started","title":"Getting started","text":"Let's install BetaML. As we did in Python, we can install a Julia package from Julia itself or from within R:","category":"page"},{"location":"tutorials/Betaml_tutorial_getting_started.html","page":"Getting started","title":"Getting started","text":"> julia_eval('using Pkg; Pkg.add(\"BetaML\")')","category":"page"},{"location":"tutorials/Betaml_tutorial_getting_started.html","page":"Getting started","title":"Getting started","text":"We can now \"import\" the BetaML julia package (in julia a \"Package\" is basically a module plus some metadata that facilitate its discovery and integration with other packages, like the reuired set) and call its functions with the julia_call(\"juliaFunction\",args) R function:","category":"page"},{"location":"tutorials/Betaml_tutorial_getting_started.html","page":"Getting started","title":"Getting started","text":"> julia_eval(\"using BetaML\")\n> yencoded <- julia_call(\"integerEncoder\",y)\n> ids      <- julia_call(\"shuffle\",1:length(y))\n> Xs       <- X[ids,]\n> ys       <- yencoded[ids]\n> cOut     <- julia_call(\"kmeans\",Xs,3L)    # kmeans expects K to be an integer\n> y_hat    <- sapply(cOut[1],as.integer)[,] # We need a vector, not a matrix\n> acc      <- julia_call(\"accuracy\",y_hat,ys)\n> acc\n[1] 0.8933333","category":"page"},{"location":"tutorials/Betaml_tutorial_getting_started.html","page":"Getting started","title":"Getting started","text":"As alternative, we can embed Julia code directly in R using the julia_eval() function:","category":"page"},{"location":"tutorials/Betaml_tutorial_getting_started.html","page":"Getting started","title":"Getting started","text":"kMeansR  <- julia_eval('\n    function accFromKmeans(x,k,y_true)\n      cOut = kmeans(x,Int(k))\n      acc = accuracy(cOut[1],y_true)\n      return acc\n    end\n')","category":"page"},{"location":"tutorials/Betaml_tutorial_getting_started.html","page":"Getting started","title":"Getting started","text":"We can then call the above function in R in one of the following three ways:","category":"page"},{"location":"tutorials/Betaml_tutorial_getting_started.html","page":"Getting started","title":"Getting started","text":"kMeansR(Xs,3,ys)\njulia_assign(\"Xs_julia\", Xs); julia_assign(\"ys_julia\", ys); julia_eval(\"accFromKmeans(Xs_julia,3,ys_julia)\")\njulia_call(\"accFromKmeans\",Xs,3,ys).","category":"page"},{"location":"tutorials/Betaml_tutorial_getting_started.html","page":"Getting started","title":"Getting started","text":"While other \"convenience\" functions are provided by the package, using  julia_call or julia_assign followed by julia_eval should suffix to accomplish any task we may need with BetaML.","category":"page"},{"location":"tutorials/Betaml_tutorial_getting_started.html#dealing_with_stochasticity","page":"Getting started","title":"Dealing with stochasticity","text":"","category":"section"},{"location":"tutorials/Betaml_tutorial_getting_started.html","page":"Getting started","title":"Getting started","text":"All BetaML models with a stochastic components support a rng parameter, standing for Random Number Generator. A RNG is a \"machine\" that streams a flow of random numbers. The flow itself however is deterministically determined for each \"seed\" (an integer number) that the RNG has been told to use. Normally this seed changes at each running of the script/model, so that stochastic models are indeed stochastic and their output differs at each run.","category":"page"},{"location":"tutorials/Betaml_tutorial_getting_started.html","page":"Getting started","title":"Getting started","text":"If we want to obtain reproductible results we can fix the seed at the very beginning of our model with Random.seed!([AnInteger]). Now our model or script will pick up a specific flow of random numbers, but this flow will always be the same, so that its results will always be the same.","category":"page"},{"location":"tutorials/Betaml_tutorial_getting_started.html","page":"Getting started","title":"Getting started","text":"However the default Julia RNG guarantee to provide the same flow of random numbers, conditional to the seed, only within minor versions of Julia. If we want to \"guarantee\" reproducibility of the results with different versions of Julia, or \"fix\" only some parts of our script, we can call the individual functions passing FIXEDRNG, an instance of StableRNG(FIXEDSEED) provided by BetaML, to the rng parameter. Use it with:","category":"page"},{"location":"tutorials/Betaml_tutorial_getting_started.html","page":"Getting started","title":"Getting started","text":"myAlgorithm(;rng=FIXEDRNG)               : always produce the same sequence of results on each run of the script (\"pulling\" from the same rng object on different calls)\nmyAlgorithm(;rng=StableRNG(SOMEINTEGER)) : always produce the same result (new rng object on each call)","category":"page"},{"location":"tutorials/Betaml_tutorial_getting_started.html","page":"Getting started","title":"Getting started","text":"In particular, use rng=StableRNG(FIXEDSEED) or rng=copy(FIXEDRNG) with FIXEDSEED  to retrieve the exact output as in the documentation or in the unit tests.","category":"page"},{"location":"tutorials/Betaml_tutorial_getting_started.html","page":"Getting started","title":"Getting started","text":"Most of the stochasticity appears in training a model. However in few cases (e.g. decision trees with missing values) some stochasticity appears also in predicting new data using a trained model. In such cases the model doesn't restrict the random seed, so that you can choose at predict time to use a fixed or a variable random seed.","category":"page"},{"location":"GMM.html#gmm_module","page":"GMM","title":"The BetaML.GMM Module","text":"","category":"section"},{"location":"GMM.html","page":"GMM","title":"GMM","text":"GMM","category":"page"},{"location":"GMM.html#BetaML.GMM","page":"GMM","title":"BetaML.GMM","text":"GMM module\n\nGenerative (Gaussian) Mixed Model learners (supervised/unsupervised)\n\nProvides clustering/collaborative filtering (via clustering) / missing values imputation / collaborative filtering / reccomendation systems, regressor and fitter using Generative Gaussiam Model (probabilistic). \n\nThe module provides the following functions. Use ?[function] to access their full signature and detailed documentation:\n\ngmm(X,K;p₀,mixtures,tol,verbosity,minVariance,minCovariance,initStrategy): gmm algorithm over GMM\npredictMissing(X,K;p₀,mixtures,tol,verbosity,minVariance,minCovariance): Impute mixing values (\"matrix completion\") using gmm as backbone. Note that this can be used for collaborative filtering / reccomendation systems often with better results than traditional algorithms as k-nearest neighbors (KNN)\n\n{Spherical|Diagonal|Full} Gaussian mixtures are already provided. User defined mixtures can be used defining a struct as subtype of AbstractMixture and implementing for that mixture the following functions:\n\ninitMixtures!(mixtures, X; minVariance, minCovariance, initStrategy)\nlpdf(m,x,mask) (for the e-step)\nupdateParameters!(mixtures, X, pₙₖ; minVariance, minCovariance) (the m-step)\n\n\n\n\n\n","category":"module"},{"location":"GMM.html#Module-Index","page":"GMM","title":"Module Index","text":"","category":"section"},{"location":"GMM.html","page":"GMM","title":"GMM","text":"Modules = [GMM]\nOrder   = [:constant, :type, :function, :macro]","category":"page"},{"location":"GMM.html#Detailed-API","page":"GMM","title":"Detailed API","text":"","category":"section"},{"location":"GMM.html","page":"GMM","title":"GMM","text":"Modules = [GMM]","category":"page"},{"location":"GMM.html#BetaML.GMM.GMMClusterHyperParametersSet","page":"GMM","title":"BetaML.GMM.GMMClusterHyperParametersSet","text":"mutable struct GMMClusterHyperParametersSet <: BetaMLHyperParametersSet\n\nHyperparameters for GMM clusters and other GMM-related algorithms\n\nParameters:\n\nnClasses\nNumber of mixtures (latent classes) to consider [def: 3]\nprobMixtures\nInitial probabilities of the categorical distribution (nClasses x 1) [default: []]\nmixtures\nAn array (of length K) of the mixture to employ (see notes) [def: [DiagonalGaussian() for i in 1:K]]\ntol\nTolerance to stop the algorithm [default: 10^(-6)]\nminVariance\nMinimum variance for the mixtures [default: 0.05]\nminCovariance\nMinimum covariance for the mixtures with full covariance matrix [default: 0]. This should be set different than minVariance (see notes).\ninitStrategy\nMixture initialisation algorithm [def: kmeans]\nmaxIter\nMaximum number of iterations [def: typemax(Int64), i.e. ∞]\n\n\n\n\n\n","category":"type"},{"location":"GMM.html#BetaML.GMM.GMMRegressor1","page":"GMM","title":"BetaML.GMM.GMMRegressor1","text":"GMMRegressor1\n\nA multi-dimensional, missing data friendly non-linear regressor based on Generative (Gaussian) Mixture Model (strategy \"1\").\n\nThe training data is used to fit a probabilistic model with latent mixtures (Gaussian distributions with different covariances are already implemented) and then predictions of new data is obtained by fitting the new data to the mixtures.\n\nFor hyperparameters see GMMClusterHyperParametersSet and GMMClusterOptionsSet.\n\nthis strategy (GMMRegressor1) works by training the EM algorithm on the feature matrix X. Once the data has been probabilistically assigned to the various classes, a mean value of Y is computed for each cluster (using the probabilities as weigths). At predict time, the new data is first fitted to the learned mixtures using the e-step part of the EM algorithm to obtain the probabilistic assignment of each record to the various mixtures. Then these probabilities are multiplied to the mixture averages for the Y dimensions learned at training time to obtain the predicted value(s) for each record. \n\n\n\n\n\n","category":"type"},{"location":"GMM.html#BetaML.GMM.GMMRegressor2","page":"GMM","title":"BetaML.GMM.GMMRegressor2","text":"GMMRegressor2\n\nA multi-dimensional, missing data friendly non-linear regressor based on Generative (Gaussian) Mixture Model.\n\nThe training data is used to fit a probabilistic model with latent mixtures (Gaussian distributions with different covariances are already implemented) and then predictions of new data is obtained by fitting the new data to the mixtures.\n\nFor hyperparameters see GMMClusterHyperParametersSet and GMMClusterOptionsSet.\n\nThsi strategy (GMMRegressor2) works by training the EM algorithm on a combined (hcat) matrix of X and Y. At predict time, the new data is first fitted to the learned mixtures using the e-step part of the EM algorithm (and using missing values for the dimensions belonging to Y) to obtain the probabilistic assignment of each record to the various mixtures. Thes these probabilities are multiplied to the mixture averages for the Y dimensions to obtain the predicted value(s) for each record. \n\n\n\n\n\n","category":"type"},{"location":"GMM.html#BetaML.Api.fit!-Tuple{GMMClusterModel, Any}","page":"GMM","title":"BetaML.Api.fit!","text":"fit!(m::GMMClusterModel,x)\n\nNotes:\n\nfit! caches as record probabilities only those of the last set of data used to train the model\n\n\n\n\n\n","category":"method"},{"location":"GMM.html#BetaML.Api.fit!-Tuple{GMMRegressor1, Any, Any}","page":"GMM","title":"BetaML.Api.fit!","text":"fit!(m::GMMRegressor1,x)\n\nNotes:\n\nfit! caches as record probabilities only those of the last set of data used to train the model\n\n\n\n\n\n","category":"method"},{"location":"GMM.html#BetaML.Api.fit!-Tuple{GMMRegressor2, Any, Any}","page":"GMM","title":"BetaML.Api.fit!","text":"fit!(m::GMMRegressor2,x)\n\nNotes:\n\nfit! caches as record probabilities only those of the last set of data used to train the model\n\n\n\n\n\n","category":"method"},{"location":"GMM.html#BetaML.GMM.estep-Tuple{Any, Any, Any}","page":"GMM","title":"BetaML.GMM.estep","text":"estep(X,pₖ,mixtures)\n\nE-step: assign the posterior prob p(j|xi) and computing the log-Likelihood of the parameters given the set of data(this last one for informative purposes and terminating the algorithm only)\n\n\n\n\n\n","category":"method"},{"location":"GMM.html#BetaML.GMM.gmm-Tuple{Any, Any}","page":"GMM","title":"BetaML.GMM.gmm","text":"gmm(X,K;p₀,mixtures,tol,verbosity,minVariance,minCovariance,initStrategy)\n\nCompute Expectation-Maximisation algorithm to identify K clusters of X data, i.e. employ a Generative Mixture Model as the underlying probabilistic model.\n\nX can contain missing values in some or all of its dimensions. In such case the learning is done only with the available data. Implemented in the log-domain for better numerical accuracy with many dimensions.\n\nParameters:\n\nX  :           A (n x d) data to clusterise\nK  :           Number of cluster wanted\np₀ :           Initial probabilities of the categorical distribution (K x 1) [default: []]\nmixtures:      An array (of length K) of the mixture to employ (see notes) [def: [DiagonalGaussian() for i in 1:K]]\ntol:           Tolerance to stop the algorithm [default: 10^(-6)]\nverbosity:     A verbosity parameter regulating the information messages frequency [def: STD]\nminVariance:   Minimum variance for the mixtures [default: 0.05]\nminCovariance: Minimum covariance for the mixtures with full covariance matrix [default: 0]. This should be set different than minVariance (see notes).\ninitStrategy:  Mixture initialisation algorithm [def: kmeans]\nmaxIter:       Maximum number of iterations [def: typemax(Int64), i.e. ∞]\nrng:           Random Number Generator (see FIXEDSEED) [deafult: Random.GLOBAL_RNG]\n\nReturns:\n\nA named touple of:\npₙₖ:      Matrix of size (N x K) of the probabilities of each point i to belong to cluster j\npₖ:       Probabilities of the categorical distribution (K x 1)\nmixtures: Vector (K x 1) of the estimated underlying distributions\nϵ:        Vector of the discrepancy (matrix norm) between pⱼₓ and the lagged pⱼₓ at each iteration\nlL:       The log-likelihood (without considering the last mixture optimisation)\nBIC:      The Bayesian Information Criterion (lower is better)\nAIC:      The Akaike Information Criterion (lower is better)\n\nNotes:\n\nThe mixtures currently implemented are SphericalGaussian(μ,σ²),DiagonalGaussian(μ,σ²) and FullGaussian(μ,σ²)\nReasonable choices for the minVariance/Covariance depends on the mixture. For example 0.25 seems a reasonable value for the SphericalGaussian, 0.05 seems better for the DiagonalGaussian, and FullGaussian seems to prefer either very low values of variance/covariance (e.g. (0.05,0.05) ) or very big but similar ones (e.g. (100,100) ).\nFor initStrategy, look at the documentation of initMixtures! for the mixture you want. The provided gaussian mixtures support grid, kmeans or given. grid is faster (expecially if X contains missing values), but kmeans often provides better results.\n\nResources:\n\nPaper describing gmm with missing values\nClass notes from MITx 6.86x (Sec 15.9)\nLimitations of gmm\n\nExample:\n\njulia> clusters = gmm([1 10.5;1.5 0; 1.8 8; 1.7 15; 3.2 40; 0 0; 3.3 38; 0 -2.3; 5.2 -2.4],3,verbosity=HIGH)\n\n\n\n\n\n","category":"method"},{"location":"GMM.html#BetaML.GMM.initMixtures!-Union{Tuple{T}, Tuple{Vector{T}, Any}} where T<:BetaML.GMM.AbstractGaussian","page":"GMM","title":"BetaML.GMM.initMixtures!","text":"initMixtures!(mixtures::Array{T,1}, X; minVariance=0.25, minCovariance=0.0, initStrategy=\"grid\",rng=Random.GLOBAL_RNG)\n\nThe parameter initStrategy can be grid, kmeans or given:\n\ngrid: Uniformly cover the space observed by the data\nkmeans: Use the kmeans algorithm. If the data contains missing values, a first run of predictMissing is done under init=grid to impute the missing values just to allow the kmeans algorithm. Then the em algorithm is used with the output of kmean as init values.\ngiven: Leave the provided set of initial mixtures\n\n\n\n\n\n","category":"method"},{"location":"GMM.html#BetaML.GMM.lpdf-Tuple{DiagonalGaussian, Any, Any}","page":"GMM","title":"BetaML.GMM.lpdf","text":"lpdf(m::DiagonalGaussian,x,mask) - Log PDF of the mixture given the observation x\n\n\n\n\n\n","category":"method"},{"location":"GMM.html#BetaML.GMM.lpdf-Tuple{FullGaussian, Any, Any}","page":"GMM","title":"BetaML.GMM.lpdf","text":"lpdf(m::FullGaussian,x,mask) - Log PDF of the mixture given the observation x\n\n\n\n\n\n","category":"method"},{"location":"GMM.html#BetaML.GMM.lpdf-Tuple{SphericalGaussian, Any, Any}","page":"GMM","title":"BetaML.GMM.lpdf","text":"lpdf(m::SphericalGaussian,x,mask) - Log PDF of the mixture given the observation x\n\n\n\n\n\n","category":"method"},{"location":"Api_v2_user.html#BetaML-Api-v2","page":"Introduction for user","title":"BetaML Api v2","text":"","category":"section"},{"location":"Api_v2_user.html","page":"Introduction for user","title":"Introduction for user","text":"!!! info Compatibility     The API described below is experimental in BetaML 0.7 and will be default in BetaML 0.8, when at the same time the old API will be deprecated. In 0.7 not all BetaML models may have this new API implemented.","category":"page"},{"location":"Api_v2_user.html","page":"Introduction for user","title":"Introduction for user","text":"The following API is designed to further simply the usage of the various ML models provided by BetaML introducing a common workflow. This is the user documentation. Refer to the developer documentation to read how the API is implemented. ","category":"page"},{"location":"Api_v2_user.html#Supervised-models","page":"Introduction for user","title":"Supervised models","text":"","category":"section"},{"location":"Api_v2_user.html","page":"Introduction for user","title":"Introduction for user","text":"This refer to models designed to learn a relation between some features (often noted with X) and labels (often noted with Y) in order to predict the label of new data given the observed features alone. Perceptron, decision trees or neural networks are common examples.","category":"page"},{"location":"Api_v2_user.html#Model-constructor","page":"Introduction for user","title":"Model constructor","text":"","category":"section"},{"location":"Api_v2_user.html","page":"Introduction for user","title":"Introduction for user","text":"The first step is to build the model constructor by passing (using keyword arguments) the agorithm hyperparameters and various options (debug levels, random number generators, ...)","category":"page"},{"location":"Api_v2_user.html","page":"Introduction for user","title":"Introduction for user","text":"m = ModelName(⋅)","category":"page"},{"location":"Api_v2_user.html#Training-of-the-model","page":"Introduction for user","title":"Training of the model","text":"","category":"section"},{"location":"Api_v2_user.html","page":"Introduction for user","title":"Introduction for user","text":"The second step is to fit (aka train) the model:","category":"page"},{"location":"Api_v2_user.html","page":"Introduction for user","title":"Introduction for user","text":"fit!(m,X,y)","category":"page"},{"location":"Api_v2_user.html","page":"Introduction for user","title":"Introduction for user","text":"For online algorithms, i.e. models that support updating of the learned parameters with new data, fit! can be repeated as new data arrive, altought not all algorithms guarantee that training each record at the time is equivalent to train all the records at once. In some algorithms the \"old training\" could be used as initial conditions, without consideration if these has been achieved with hundread or millions of records, and the new data we use for training become much more important than the old one for the determination of the learned parameters. As a naming convention, while we would have preferred the name \"train\" for this funtion, as it makes explicit that we are here changing the learned parameters of the model, it seems that most other ML libraries call this step \"fit\", so we stuck with it. ","category":"page"},{"location":"Api_v2_user.html#Prediction","page":"Introduction for user","title":"Prediction","text":"","category":"section"},{"location":"Api_v2_user.html","page":"Introduction for user","title":"Introduction for user","text":"Trained models can be used to predict y given new X:","category":"page"},{"location":"Api_v2_user.html","page":"Introduction for user","title":"Introduction for user","text":"ŷ = predict(m,X)","category":"page"},{"location":"Api_v2_user.html#Other-functions","page":"Introduction for user","title":"Other functions","text":"","category":"section"},{"location":"Api_v2_user.html","page":"Introduction for user","title":"Introduction for user","text":"Models can be resetted to lost the learned information with reset!(m) and training information (other than the algorithm learned parameters) can be retrieved with info(m).","category":"page"},{"location":"Api_v2_user.html#Unupervised-and-transformed-models","page":"Introduction for user","title":"Unupervised and transformed models","text":"","category":"section"},{"location":"Api_v2_user.html","page":"Introduction for user","title":"Introduction for user","text":"This relate to models that learn a \"structure\" from the data itself (without any label attached from which to learn) and report either some new information using this learned structure (e.g. a cluster class) or direclty process a transformation of the data itself, like PCA or missing imputers.","category":"page"},{"location":"Api_v2_user.html","page":"Introduction for user","title":"Introduction for user","text":"The main differences with supervised models is that the fit! function takes only the features and that the predict one take only the (trained) model as argument - models that do generalise to new data can accept also a predict(m,newX) version that uses what has been learn in fit!:","category":"page"},{"location":"Api_v2_user.html","page":"Introduction for user","title":"Introduction for user","text":"m = Model()\nfit!(m,X)\nresult = predict(m) # or result = predict(m, newX)","category":"page"},{"location":"index.html#![BLogos](assets/BetaML_logo_30x30.png)-BetaML.jl-Documentation","page":"Index","title":"(Image: BLogos) BetaML.jl Documentation","text":"","category":"section"},{"location":"index.html","page":"Index","title":"Index","text":"Welcome to the documentation of the Beta Machine Learning toolkit.","category":"page"},{"location":"index.html#About","page":"Index","title":"About","text":"","category":"section"},{"location":"index.html","page":"Index","title":"Index","text":"The BetaML toolkit provides classical algorithms written in the Julia programming language useful to \"learn\" the relationship between some inputs and some outputs, with the objective to make accurate predictions of the output given new inputs (\"supervised machine learning\") or to better understand the structure of the data, perhaps hidden because of the high dimensionality (\"unsupervised machine learning\").","category":"page"},{"location":"index.html","page":"Index","title":"Index","text":"While specific packages exist for state-of-the art implementations of these algorithms (see the section \"Alternative Packages\"), thanks to the Just-In-Time compilation nature of Julia, BetaML is reasonably fast for datasets that fit in memory while keeping both the code and the usage as simple as possible.","category":"page"},{"location":"index.html","page":"Index","title":"Index","text":"Aside the algorithms themselves, BetaML provides many \"utility\" functions. Because algorithms are all self-contained in the library itself (you are invited to explore their source code by typing @edit functionOfInterest(par1,par2,...)), the utility functions have APIs that are coordinated with the algorithms, facilitating the \"preparation\" of the data for the analysis, the evaluation of the models or the implementation of several models in chains (pipelines). While BetaML doesn't provide itself tools for hyper-parameters optimisation or complex pipeline building tools, most models have an interface for the MLJ framework that allows it.","category":"page"},{"location":"index.html","page":"Index","title":"Index","text":"Aside Julia, BetaML can be accessed in R or Python using respectively JuliaCall and PyJulia. See the tutorial for details.","category":"page"},{"location":"index.html#Installation","page":"Index","title":"Installation","text":"","category":"section"},{"location":"index.html","page":"Index","title":"Index","text":"The BetaML package is included in the standard Julia register, install it with:","category":"page"},{"location":"index.html","page":"Index","title":"Index","text":"] add BetaML","category":"page"},{"location":"index.html#Loading-the-module(s)","page":"Index","title":"Loading the module(s)","text":"","category":"section"},{"location":"index.html","page":"Index","title":"Index","text":"This package is split in several submodules, but all modules are re-exported at the root module level. This means that you can access their functionality by simply using BetaML.","category":"page"},{"location":"index.html","page":"Index","title":"Index","text":"using BetaML\nmyLayer = DenseLayer(2,3) # DenseLayer is defined in the Nn submodule\nres     = kernelPerceptron([1.1 2.1; 5.3 4.2; 1.8 1.7], [-1,1,-1]) # kernelPerceptron is defined in the Perceptron module\n@edit DenseLayer(2,3)     # Open a text editor with to the relevant source code","category":"page"},{"location":"index.html#Usage","page":"Index","title":"Usage","text":"","category":"section"},{"location":"index.html","page":"Index","title":"Index","text":"New to BetaML or even to Julia / Machine Learning altogether? Start from the tutorial!","category":"page"},{"location":"index.html","page":"Index","title":"Index","text":"Detailed documentation for most algorithms can be retrieved using the inline Julia help system (just press the question mark ? and then, on the special help prompt help?>, type the function name) or on these pages under the section \"Api (Reference Manual)\" for the individual modules:","category":"page"},{"location":"index.html","page":"Index","title":"Index","text":"BetaML.Perceptron: The Perceptron, Kernel Perceptron and Pegasos classification algorithms;\nBetaML.Trees: The Decision Trees and Random Forests algorithms for classification or regression (with missing values supported);\nBetaML.Nn: Implementation of Artificial Neural Networks;\nBetaML.Clustering: (hard) Clustering algorithms (Kmeans, Mdedoids\nBetaML.GMM: Various algorithms (Clustering, regressor, missing imputation / collaborative filtering / recommandation systems) that use a Generative (Gaussian) mixture models (probabilistic) fitter fitted using a EM algorithm;\nBetaML.Imputation: Imputation algorithms;\nBetaML.Utils`: Various utility functions (scale, one-hot, distances, kernels, pca, accuracy/error measures..).","category":"page"},{"location":"index.html#MLJ-interface","page":"Index","title":"MLJ interface","text":"","category":"section"},{"location":"index.html","page":"Index","title":"Index","text":"BetaML exports the following modules for usage with the MLJ toolkit:","category":"page"},{"location":"index.html","page":"Index","title":"Index","text":"Perceptron models: PerceptronClassifier, KernelPerceptronClassifier, PegasosClassifier\nDecision trees/Random forest models:  DecisionTreeClassifier, DecisionTreeRegressor, RandomForestClassifier, RandomForestRegressor\nClustering models and derived models: KMeans, KMedoids, GMMClusterer, MissingImputator","category":"page"},{"location":"index.html","page":"Index","title":"Index","text":"Currently BetaML neural network models are not available in MLJ.","category":"page"},{"location":"index.html#Quick-examples","page":"Index","title":"Quick examples","text":"","category":"section"},{"location":"index.html","page":"Index","title":"Index","text":"(see the tutorial for a more step-by-step guide to the examples below and to other examples)","category":"page"},{"location":"index.html","page":"Index","title":"Index","text":"A \"V2\" API that uses a more uniform fit!(model,X,[Y]), predict(model,X) workflow is currently worked on.","category":"page"},{"location":"index.html","page":"Index","title":"Index","text":"Using an Artificial Neural Network for multinomial categorisation","category":"page"},{"location":"index.html","page":"Index","title":"Index","text":"# Load Modules\nusing BetaML, DelimitedFiles, Random, StatsPlots # Load the main module and ausiliary modules\nRandom.seed!(123); # Fix the random seed (to obtain reproducible results)\n\n# Load the data\niris     = readdlm(joinpath(dirname(pathof(BetaML)),\"..\",\"test\",\"data\",\"iris.csv\"),',',skipstart=1)\niris     = iris[shuffle(axes(iris, 1)), :] # Shuffle the records, as they aren't by default\nx        = convert(Array{Float64,2}, iris[:,1:4])\ny        = map(x->Dict(\"setosa\" => 1, \"versicolor\" => 2, \"virginica\" =>3)[x],iris[:, 5]) # Convert the target column to numbers\ny_oh     = oneHotEncoder(y) # Convert to One-hot representation (e.g. 2 => [0 1 0], 3 => [0 0 1])\n\n# Split the data in training/testing sets\n((xtrain,xtest),(ytrain,ytest),(ytrain_oh,ytest_oh)) = partition([x,y,y_oh],[0.8,0.2],shuffle=false)\n(ntrain, ntest) = size.([xtrain,xtest],1)\n\n# Define the Artificial Neural Network model\nl1   = DenseLayer(4,10,f=relu) # Activation function is ReLU\nl2   = DenseLayer(10,3)        # Activation function is identity by default\nl3   = VectorFunctionLayer(3,3,f=softMax) # Add a (parameterless) layer whose activation function (softMax in this case) is defined to all its nodes at once\nmynn = buildNetwork([l1,l2,l3],squaredCost,name=\"Multinomial logistic regression Model Sepal\") # Build the NN and use the squared cost (aka MSE) as error function\n\n# Training it (default to ADAM)\nres = train!(mynn,scale(xtrain),ytrain_oh,epochs=100,batchSize=6) # Use optAlg=SGD (Stochastic Gradient Descent) by default\n\n# Test it\nŷtrain        = predict(mynn,scale(xtrain))   # Note the scaling function\nŷtest         = predict(mynn,scale(xtest))\ntrainAccuracy = accuracy(ŷtrain,ytrain,tol=1) # 0.983\ntestAccuracy  = accuracy(ŷtest,ytest,tol=1)   # 1.0\n\n# Visualise results\ntestSize    = size(ŷtest,1)\nŷtestChosen =  [argmax(ŷtest[i,:]) for i in 1:testSize]\ngroupedbar([ytest ŷtestChosen], label=[\"ytest\" \"ŷtest (est)\"], title=\"True vs estimated categories\") # All records correctly labelled !\nplot(0:res.epochs,res.ϵ_epochs, ylabel=\"epochs\",xlabel=\"error\",legend=nothing,title=\"Avg. error per epoch on the Sepal dataset\")","category":"page"},{"location":"index.html","page":"Index","title":"Index","text":"(Image: results)","category":"page"},{"location":"index.html","page":"Index","title":"Index","text":"(Image: results)","category":"page"},{"location":"index.html","page":"Index","title":"Index","text":"Using the Expectation-Maximisation algorithm for clustering","category":"page"},{"location":"index.html","page":"Index","title":"Index","text":"using BetaML, DelimitedFiles, Random, StatsPlots # Load the main module and ausiliary modules\nRandom.seed!(123); # Fix the random seed (to obtain reproducible results)\n\n# Load the data\niris     = readdlm(joinpath(dirname(pathof(BetaML)),\"..\",\"test\",\"data\",\"iris.csv\"),',',skipstart=1)\niris     = iris[shuffle(axes(iris, 1)), :] # Shuffle the records, as they aren't by default\nx        = convert(Array{Float64,2}, iris[:,1:4])\nx        = scale(x) # normalise all dimensions to (μ=0, σ=1)\ny        = map(x->Dict(\"setosa\" => 1, \"versicolor\" => 2, \"virginica\" =>3)[x],iris[:, 5]) # Convert the target column to numbers\n\n# Get some ranges of minVariance and minCovariance to test\nminVarRange   = collect(0.04:0.05:1.5)\nminCovarRange = collect(0:0.05:1.45)\n\n# Run the gmm(em) algorithm for the various cases...\nsphOut  = [gmm(x,3,mixtures=[SphericalGaussian() for i in 1:3],minVariance=v, minCovariance=cv, verbosity=NONE) for v in minVarRange, cv in minCovarRange[1:1]]\ndiagOut  = [gmm(x,3,mixtures=[DiagonalGaussian() for i in 1:3],minVariance=v, minCovariance=cv, verbosity=NONE)  for v in minVarRange, cv in minCovarRange[1:1]]\nfullOut = [gmm(x,3,mixtures=[FullGaussian() for i in 1:3],minVariance=v, minCovariance=cv, verbosity=NONE)  for v in minVarRange, cv in minCovarRange]\n\n# Get the Bayesian information criterion (AIC is also available)\nsphBIC = [sphOut[v,cv].BIC for v in 1:length(minVarRange), cv in 1:1]\ndiagBIC = [diagOut[v,cv].BIC for v in 1:length(minVarRange), cv in 1:1]\nfullBIC = [fullOut[v,cv].BIC for v in 1:length(minVarRange), cv in 1:length(minCovarRange)]\n\n# Compare the accuracy with true categories\nsphAcc  = [accuracy(sphOut[v,cv].pₙₖ,y,ignoreLabels=true) for v in 1:length(minVarRange), cv in 1:1]\ndiagAcc = [accuracy(diagOut[v,cv].pₙₖ,y,ignoreLabels=true) for v in 1:length(minVarRange), cv in 1:1]\nfullAcc = [accuracy(fullOut[v,cv].pₙₖ,y,ignoreLabels=true) for v in 1:length(minVarRange), cv in 1:length(minCovarRange)]\n\nplot(minVarRange,[sphBIC diagBIC fullBIC[:,1] fullBIC[:,15] fullBIC[:,30]], markershape=:circle, label=[\"sph\" \"diag\" \"full (cov=0)\" \"full (cov=0.7)\" \"full (cov=1.45)\"], title=\"BIC\", xlabel=\"minVariance\")\nplot(minVarRange,[sphAcc diagAcc fullAcc[:,1] fullAcc[:,15] fullAcc[:,30]], markershape=:circle, label=[\"sph\" \"diag\" \"full (cov=0)\" \"full (cov=0.7)\" \"full (cov=1.45)\"], title=\"Accuracies\", xlabel=\"minVariance\")","category":"page"},{"location":"index.html","page":"Index","title":"Index","text":"(Image: results)","category":"page"},{"location":"index.html","page":"Index","title":"Index","text":"(Image: results)","category":"page"},{"location":"index.html","page":"Index","title":"Index","text":"Further examples","category":"page"},{"location":"index.html","page":"Index","title":"Index","text":"Finally, you may want to give a look at the \"test\" folder. While the primary objective of the scripts under the \"test\" folder is to provide automatic testing of the BetaML toolkit, they can also be used to see how functions should be called, as virtually all functions provided by BetaML are tested there.","category":"page"},{"location":"index.html#Acknowledgements","page":"Index","title":"Acknowledgements","text":"","category":"section"},{"location":"index.html","page":"Index","title":"Index","text":"The development of this package at the Bureau d'Economie Théorique et Appliquée (BETA, Nancy) was supported by the French National Research Agency through the Laboratory of Excellence ARBRE, a part of the “Investissements d'Avenir” Program (ANR 11 – LABX-0002-01).","category":"page"},{"location":"index.html","page":"Index","title":"Index","text":"(Image: BLogos)","category":"page"},{"location":"Api_v2_developer.html#Api-v2-developer-documentation-(API-implementation)","page":"For developers","title":"Api v2 - developer documentation (API implementation)","text":"","category":"section"},{"location":"Api_v2_developer.html","page":"For developers","title":"For developers","text":"BetaMLOptionsSet\nBetaMLHyperParametersSet\nBetaMLLearnedParametersSet\nBetaMLModel\n\nBetaMLSuperVisedModel   <: BetaMLModel\nBetaMLUnsupervisedModel <: BetaMLModel\nRFModel                 <: BetaMLSuperVisedModel\n\n\nRFOptionsSet           <: BetaMLOptionsSet\nRFHyperParametersSet   <: BetaMLHyperParametersSet\nRFLearnedParametersSet <: BetaMLLearnedParametersSet\n\nmutable struct DTModel <: BetaMLSupervisedModel\n    hpar::DTHyperParametersSet\n    opt::DTOptionsSet\n    par::DTLearnableParameters\n    trained::Bool\n    info\nend","category":"page"}]
}
