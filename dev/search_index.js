var documenterSearchIndex = {"docs":
[{"location":"tutorials/Getting started/betaml_tutorial_getting_started.html","page":"Getting started","title":"Getting started","text":"EditURL = \"https://github.com/sylvaticus/BetaML.jl/blob/master/docs/src/tutorials/Getting started/betaml_tutorial_getting_started.jl\"","category":"page"},{"location":"tutorials/Getting started/betaml_tutorial_getting_started.html#getting_started","page":"Getting started","title":"Getting started","text":"","category":"section"},{"location":"tutorials/Getting started/betaml_tutorial_getting_started.html#Work-in-progress","page":"Getting started","title":"Work in progress","text":"","category":"section"},{"location":"tutorials/Getting started/betaml_tutorial_getting_started.html","page":"Getting started","title":"Getting started","text":"This document presents some general information concerning BetaML. For detailed information on the algorithms provided by the Toolkit refer to the individual module API or to the tutorial below:","category":"page"},{"location":"tutorials/Getting started/betaml_tutorial_getting_started.html","page":"Getting started","title":"Getting started","text":"Regression tutorial - Arguments: Decision trees, Random forests, neural networks, hyper-parameter tuning\nClassification tutorial - Arguments: Decision trees and random forests, neural networks (softmax), pre-processing workflow, confusion matrix\nClustering tutorial - Arguments: k-means, kmedoids, generative gaussain models, cross-validation","category":"page"},{"location":"tutorials/Getting started/betaml_tutorial_getting_started.html#dealing_with_stochasticity","page":"Getting started","title":"Dealing with stochasticity","text":"","category":"section"},{"location":"tutorials/Getting started/betaml_tutorial_getting_started.html","page":"Getting started","title":"Getting started","text":"Most models have some stochastic components and support a rng parameter. By default, the outputs of these models will hence not be absolutelly equal on each run. If you want to be sure that the output of a model remain constant given the same inputs you can pass a fixed Random Number Generator to the rng parameter. Use it with:","category":"page"},{"location":"tutorials/Getting started/betaml_tutorial_getting_started.html","page":"Getting started","title":"Getting started","text":"myAlgorithm(;rng=FIXEDRNG)               # always produce the same sequence of results on each run of the script (\"pulling\" from the same rng object on different calls)\nmyAlgorithm(;rng=StableRNG(SOMEINTEGER)) # always produce the same result (new rng object on each call)","category":"page"},{"location":"tutorials/Getting started/betaml_tutorial_getting_started.html","page":"Getting started","title":"Getting started","text":"In particular, use rng=StableRNG(FIXEDSEED) to retrieve the exacty output as in the documentation or in the unit tests.","category":"page"},{"location":"tutorials/Getting started/betaml_tutorial_getting_started.html","page":"Getting started","title":"Getting started","text":"Most of the stochasticity appears in training a model. However in few cases (e.g. decision trees with missing values) some stocasticity appears also in predicting new data with a trained model. In such cases the model doesn't stire the random seed, so that you can choose at predict time to use a fixed or a variable random seed.","category":"page"},{"location":"tutorials/Getting started/betaml_tutorial_getting_started.html","page":"Getting started","title":"Getting started","text":"View this file on Github.","category":"page"},{"location":"tutorials/Getting started/betaml_tutorial_getting_started.html","page":"Getting started","title":"Getting started","text":"","category":"page"},{"location":"tutorials/Getting started/betaml_tutorial_getting_started.html","page":"Getting started","title":"Getting started","text":"This page was generated using Literate.jl.","category":"page"},{"location":"Examples.html#Examples","page":"Examples","title":"Examples","text":"","category":"section"},{"location":"Examples.html#Supervised-learning","page":"Examples","title":"Supervised learning","text":"","category":"section"},{"location":"Examples.html#Regression","page":"Examples","title":"Regression","text":"","category":"section"},{"location":"Examples.html#Estimating-the-bike-sharing-demand","page":"Examples","title":"Estimating the bike sharing demand","text":"","category":"section"},{"location":"Examples.html","page":"Examples","title":"Examples","text":"The task is to estimate the influence of several variables (like the weather, the season, the day of the week..) on the demand of shared bicycles, so that the authority in charge of the service can organise the service in the best way.","category":"page"},{"location":"Examples.html","page":"Examples","title":"Examples","text":"Data origin:","category":"page"},{"location":"Examples.html","page":"Examples","title":"Examples","text":"original full dataset (by hour, not used here): https://archive.ics.uci.edu/ml/datasets/Bike+Sharing+Dataset\nsimplified dataset (by day, with some simple scaling): https://www.hds.utc.fr/~tdenoeux/dokuwiki/en/aec\ndescription: https://www.hds.utc.fr/~tdenoeux/dokuwiki/media/en/exam2019ace.pdf\ndata: https://www.hds.utc.fr/~tdenoeux/dokuwiki/media/en/bikesharing_day.csv.zip","category":"page"},{"location":"Examples.html","page":"Examples","title":"Examples","text":"Note that even if we are estimating a time serie, we are not using here a recurrent neural network as we assume the temporal dependence to be negligible (i.e. Y_t = f(X_t) alone).","category":"page"},{"location":"Examples.html#Classification","page":"Examples","title":"Classification","text":"","category":"section"},{"location":"Examples.html#Unsupervised-lerarning","page":"Examples","title":"Unsupervised lerarning","text":"","category":"section"},{"location":"Examples.html#Notebooks","page":"Examples","title":"Notebooks","text":"","category":"section"},{"location":"Examples.html","page":"Examples","title":"Examples","text":"The following notebooks provide runnable examples of the package functionality:","category":"page"},{"location":"Examples.html","page":"Examples","title":"Examples","text":"Pegasus classifiers: [Static notebook] - [myBinder]\nDecision Trees and Random Forest regression on Bike sharing demand forecast (daily data): [Static notebook] - [myBinder]\nNeural Networks: [Static notebook] - [myBinder]\nBike sharing demand forecast (daily data): [Static notebook] - [myBinder]\nClustering: [Static notebook] - [myBinder]","category":"page"},{"location":"Examples.html","page":"Examples","title":"Examples","text":"Note: the live, runnable computational environment is a temporary new copy made at each connection. The first time after a commit is done on this repository a new environment has to be set (instead of just being copied), and the server may take several minutes.","category":"page"},{"location":"Examples.html","page":"Examples","title":"Examples","text":"This is only if you are the unlucky user triggering the rebuild of the environment after the commit.","category":"page"},{"location":"Nn.html#The-BetaML.Nn-Module","page":"Nn","title":"The BetaML.Nn Module","text":"","category":"section"},{"location":"Nn.html","page":"Nn","title":"Nn","text":"Nn","category":"page"},{"location":"Nn.html#BetaML.Nn","page":"Nn","title":"BetaML.Nn","text":"BetaML.Nn module\n\nImplement the functionality required to define an artificial Neural Network, train it with data, forecast data and assess its performances.\n\nCommon type of layers and optimisation algorithms are already provided, but you can define your own ones subclassing respectively the AbstractLayer and OptimisationAlgorithm abstract types.\n\nThe module provide the following type or functions. Use ?[type or function] to access their full signature and detailed documentation:\n\nModel definition:\n\nDenseLayer: Classical feed-forward layer with user-defined activation function\nDenseNoBiasLayer: Classical layer without the bias parameter\nVectorFunctionLayer: Parameterless layer whose activation function run over the ensable of its nodes rather than on each one individually\nbuildNetwork: Build the chained network and define a cost function\ngetParams(nn): Retrieve current weigthts\ngetGradient(nn): Retrieve the current gradient of the weights\nsetParams!(nn): Update the weigths of the network\nshow(nn): Print a representation of the Neural Network\n\nEach layer can use a default activation function, one of the functions provided in the Utils module (relu, tanh, softmax,...) or you can specify your own function. The derivative of the activation function can be optionally be provided, in such case training will be quicker, altought this difference tends to vanish with bigger datasets. You can alternativly implement your own layer defining a new type as subtype of the abstract type AbstractLayer. Each user-implemented layer must define the following methods:\n\nA suitable constructor\nforward(layer,x)\nbackward(layer,x,nextGradient)\ngetParams(layer)\ngetGradient(layer,x,nextGradient)\nsetParams!(layer,w)\nsize(layer)\n\nModel training:\n\ntrainingInfo(nn): Default callback function during training\ntrain!(nn):  Training function\nsingleUpdate!(θ,▽;optAlg): The parameter update made by the specific optimisation algorithm\nSGD: The default optimisation algorithm\nADAM: A faster moment-based optimisation algorithm (added in v0.2.2)\n\nTo define your own optimisation algorithm define a subtype of OptimisationAlgorithm and implement the function singleUpdate!(θ,▽;optAlg) and eventually initOptAlg(⋅) specific for it.\n\nModel predictions and assessment:\n\npredict(nn): Return the output given the data\nloss(nn): Compute avg. network loss on a test set\nUtils.accuracy(ŷ,y): Categorical output accuracy\n\nWhile high-level functions operating on the dataset expect it to be in the standard format (nRecords × nDimensions matrices) it is custom to represent the chain of a neural network as a flow of column vectors, so all low-level operations (operating on a single datapoint) expect both the input and the output as a column vector.\n\n\n\n\n\n","category":"module"},{"location":"Nn.html#Module-Index","page":"Nn","title":"Module Index","text":"","category":"section"},{"location":"Nn.html","page":"Nn","title":"Nn","text":"Modules = [Nn]\nOrder   = [:constant, :type, :function, :macro]","category":"page"},{"location":"Nn.html#Detailed-API","page":"Nn","title":"Detailed API","text":"","category":"section"},{"location":"Nn.html","page":"Nn","title":"Nn","text":"Modules = [Nn]","category":"page"},{"location":"Nn.html#BetaML.Nn.ADAM","page":"Nn","title":"BetaML.Nn.ADAM","text":"ADAM(;η, λ, β₁, β₂, ϵ)\n\nThe ADAM algorithm, an adaptive moment estimation optimiser.\n\nFields:\n\nη:  Learning rate (stepsize, α in the paper), as a function of the current epoch [def: t -> 0.001 (i.e. fixed)]\nλ:  Multiplicative constant to the learning rate [def: 1]\nβ₁: Exponential decay rate for the first moment estimate [range: ∈ [0,1], def: 0.9]\nβ₂: Exponential decay rate for the second moment estimate [range: ∈ [0,1], def: 0.999]\nϵ:  Epsilon value to avoid division by zero [def: 10^-8]\n\n\n\n\n\n","category":"type"},{"location":"Nn.html#BetaML.Nn.DenseLayer","page":"Nn","title":"BetaML.Nn.DenseLayer","text":"DenseLayer\n\nRepresentation of a layer in the network\n\nFields:\n\nw:  Weigths matrix with respect to the input from previous layer or data (n x n pr. layer)\nwb: Biases (n)\nf:  Activation function\ndf: Derivative of the activation function\n\n\n\n\n\n","category":"type"},{"location":"Nn.html#BetaML.Nn.DenseNoBiasLayer","page":"Nn","title":"BetaML.Nn.DenseNoBiasLayer","text":"DenseNoBiasLayer\n\nRepresentation of a layer without bias in the network\n\nFields:\n\nw:  Weigths matrix with respect to the input from previous layer or data (n x n pr. layer)\nf:  Activation function\ndf: Derivative of the activation function\n\n\n\n\n\n","category":"type"},{"location":"Nn.html#BetaML.Nn.Learnable","page":"Nn","title":"BetaML.Nn.Learnable","text":"Learnable(data)\n\nStructure representing the learnable parameters of a layer or its gradient.\n\nThe learnable parameters of a layers are given in the form of a N-tuple of Array{Float64,N2} where N2 can change (e.g. we can have a layer with the first parameter being a matrix, and the second one being a scalar). We wrap the tuple on its own structure a bit for some efficiency gain, but above all to define standard mathematic operations on the gradients without doing \"type pyracy\" with respect to Base tuples.\n\n\n\n\n\n","category":"type"},{"location":"Nn.html#BetaML.Nn.NN","page":"Nn","title":"BetaML.Nn.NN","text":"NN\n\nRepresentation of a Neural Network\n\nFields:\n\nlayers:  Array of layers objects\ncf:      Cost function\ndcf:     Derivative of the cost function\ntrained: Control flag for trained networks\n\n\n\n\n\n","category":"type"},{"location":"Nn.html#BetaML.Nn.OptimisationAlgorithm","page":"Nn","title":"BetaML.Nn.OptimisationAlgorithm","text":"OptimisationAlgorithm\n\nAbstract type representing an Optimisation algorithm.\n\nCurrently supported algorithms:\n\nSGD (Stochastic) Gradient Descent\n\nSee ?[Name OF THE ALGORITHM] for their details\n\nYou can implement your own optimisation algorithm using a subtype of OptimisationAlgorithm and implementing its constructor and the update function singleUpdate(⋅) (type ?singleUpdate for details).\n\n\n\n\n\n","category":"type"},{"location":"Nn.html#BetaML.Nn.SGD","page":"Nn","title":"BetaML.Nn.SGD","text":"SGD(;η=t -> 1/(1+t), λ=2)\n\nStochastic Gradient Descent algorithm (default)\n\nFields:\n\nη: Learning rate, as a function of the current epoch [def: t -> 1/(1+t)]\nλ: Multiplicative constant to the learning rate [def: 2]\n\n\n\n\n\n","category":"type"},{"location":"Nn.html#BetaML.Nn.VectorFunctionLayer","page":"Nn","title":"BetaML.Nn.VectorFunctionLayer","text":"VectorFunctionLayer\n\nRepresentation of a (weightless) VectorFunction layer in the network. Vector function layer expects a vector activation function, i.e. a function taking the whole output of the previous layer an input rather than working on a single node as \"normal\" activation functions would do. Useful for example with the SoftMax function in classification or with the pool1D function to implement a \"pool\" layer in 1 dimensions. As it is weightless, it doesn't apply any transformation to the output coming from the previous layer. It means that the number of nodes must be set to the same as in the previous layer (and if you are using this for classification, to the number of classes, i.e. the previous layer must be set equal to the number of classes in the predictions).\n\nFields:\n\nnₗ: Number of nodes in input (i.e. length of previous layer)\nn:  Number of nodes in output (automatically inferred in the constructor)\nf:  Activation function (vector)\ndf: Derivative of the (vector) activation function\n\nNotes:\n\nThe output size of this layer is given by the size of the output function,\n\nthat not necessarily is the same as the previous layers.\n\n\n\n\n\n","category":"type"},{"location":"Nn.html#Base.size-Tuple{AbstractLayer}","page":"Nn","title":"Base.size","text":"size(layer)\n\nGet the dimensions of the layers in terms of (dimensions in input , dimensions in output)\n\nNotes:\n\nYou need to use import Base.size before defining this function for your layer\n\n\n\n\n\n","category":"method"},{"location":"Nn.html#BetaML.Nn.backward-Tuple{AbstractLayer, Any, Any}","page":"Nn","title":"BetaML.Nn.backward","text":"backward(layer,x,nextGradient)\n\nCompute backpropagation for this layer\n\nParameters:\n\nlayer:        Worker layer\nx:            Input to the layer\nnextGradient: Derivative of the overaall loss with respect to the input of the next layer (output of this layer)\n\nReturn:\n\nThe evaluated gradient of the loss with respect to this layer inputs\n\n\n\n\n\n","category":"method"},{"location":"Nn.html#BetaML.Nn.buildNetwork-Tuple{Any, Any}","page":"Nn","title":"BetaML.Nn.buildNetwork","text":"buildNetwork(layers,cf;dcf,name)\n\nInstantiate a new Feedforward Neural Network\n\nParameters:\n\nlayers: Array of layers objects\ncf:     Cost function\ndcf:    Derivative of the cost function [def: nothing]\nname:   Name of the network [def: \"Neural Network\"]\n\nNotes:\n\nEven if the network ends with a single output note, the cost function and its derivative should always expect y and ŷ as column vectors.\n\n\n\n\n\n","category":"method"},{"location":"Nn.html#BetaML.Nn.forward-Tuple{AbstractLayer, Any}","page":"Nn","title":"BetaML.Nn.forward","text":"forward(layer,x)\n\nPredict the output of the layer given the input\n\nParameters:\n\nlayer:  Worker layer\nx:      Input to the layer\n\nReturn:\n\nAn Array{T,1} of the prediction (even for a scalar)\n\n\n\n\n\n","category":"method"},{"location":"Nn.html#BetaML.Nn.getGradient-Tuple{AbstractLayer, Any, Any}","page":"Nn","title":"BetaML.Nn.getGradient","text":"getGradient(layer,x,nextGradient)\n\nCompute backpropagation for this layer\n\nParameters:\n\nlayer:        Worker layer\nx:            Input to the layer\nnextGradient: Derivative of the overaall loss with respect to the input of the next layer (output of this layer)\n\nReturn:\n\nThe evaluated gradient of the loss with respect to this layer's trainable parameters as tuple of matrices. It is up to you to decide how to organise this tuple, as long you are consistent with the getParams() and setParams() functions. Note that starting from BetaML 0.2.2 this tuple needs to be wrapped in its Learnable type.\n\n\n\n\n\n","category":"method"},{"location":"Nn.html#BetaML.Nn.getGradient-Union{Tuple{T2}, Tuple{T}, Tuple{Any, AbstractMatrix{T}, AbstractMatrix{T2}}} where {T<:Number, T2<:Number}","page":"Nn","title":"BetaML.Nn.getGradient","text":"getGradient(nn,xbatch,ybatch)\n\nRetrieve the current gradient of the weigthts (i.e. derivative of the cost with respect to the weigths)\n\nParameters:\n\nnn:      Worker network\nxbatch:  Input to the network (n,d)\nybatch:  Label input (n,d)\n\n#Notes:\n\nThe output is a vector of tuples of each layer's input weigths and bias weigths\n\n\n\n\n\n","category":"method"},{"location":"Nn.html#BetaML.Nn.getGradient-Union{Tuple{T2}, Tuple{T}, Tuple{NN, Union{AbstractVector{T}, T}, Union{AbstractVector{T2}, T2}}} where {T<:Number, T2<:Number}","page":"Nn","title":"BetaML.Nn.getGradient","text":"getGradient(nn,x,y)\n\nRetrieve the current gradient of the weigthts (i.e. derivative of the cost with respect to the weigths)\n\nParameters:\n\nnn: Worker network\nx:   Input to the network (d,1)\ny:   Label input (d,1)\n\n#Notes:\n\nThe output is a vector of tuples of each layer's input weigths and bias weigths\n\n\n\n\n\n","category":"method"},{"location":"Nn.html#BetaML.Nn.getNParams-Tuple{AbstractLayer}","page":"Nn","title":"BetaML.Nn.getNParams","text":"getNParams(layer)\n\nReturn the number of parameters of a layer.\n\nIt doesn't need to be implemented by each layer type, as it uses getParams().\n\n\n\n\n\n","category":"method"},{"location":"Nn.html#BetaML.Nn.getNParams-Tuple{NN}","page":"Nn","title":"BetaML.Nn.getNParams","text":"getNParams(nn) - Return the number of trainable parameters of the neural network.\n\n\n\n\n\n","category":"method"},{"location":"Nn.html#BetaML.Nn.getParams-Tuple{AbstractLayer}","page":"Nn","title":"BetaML.Nn.getParams","text":"getParams(layer)\n\nGet the layers current value of its trainable parameters\n\nParameters:\n\nlayer:  Worker layer\n\nReturn:\n\nThe current value of the layer's trainable parameters as tuple of matrices. It is up to you to decide how to organise this tuple, as long you are consistent with the getGradient() and setParams() functions. Note that starting from BetaML 0.2.2 this tuple needs to be wrapped in its Learnable type.\n\n\n\n\n\n","category":"method"},{"location":"Nn.html#BetaML.Nn.getParams-Tuple{NN}","page":"Nn","title":"BetaML.Nn.getParams","text":"getParams(nn)\n\nRetrieve current weigthts\n\nParameters:\n\nnn: Worker network\n\nNotes:\n\nThe output is a vector of tuples of each layer's input weigths and bias weigths\n\n\n\n\n\n","category":"method"},{"location":"Nn.html#BetaML.Nn.initOptAlg!-Tuple{ADAM}","page":"Nn","title":"BetaML.Nn.initOptAlg!","text":"initOptAlg!(optAlg::ADAM;θ,batchSize,x,y,rng)\n\nInitialize the ADAM algorithm with the parameters m and v as zeros and check parameter bounds\n\n\n\n\n\n","category":"method"},{"location":"Nn.html#BetaML.Nn.initOptAlg!-Tuple{BetaML.Nn.OptimisationAlgorithm}","page":"Nn","title":"BetaML.Nn.initOptAlg!","text":"initOptAlg!(optAlg;θ,batchSize,x,y)\n\nInitialize the optimisation algorithm\n\nParameters:\n\noptAlg:    The Optimisation algorithm to use\nθ:         Current parameters\nbatchSize:    The size of the batch\nx:   The training (input) data\ny:   The training \"labels\" to match\nrng: Random Number Generator (see FIXEDSEED) [deafult: Random.GLOBAL_RNG]\n\nNotes:\n\nOnly a few optimizers need this function and consequently ovverride it. By default it does nothing, so if you want write your own optimizer and don't need to initialise it, you don't have to override this method\n\n\n\n\n\n","category":"method"},{"location":"Nn.html#BetaML.Nn.loss-Tuple{NN, Any, Any}","page":"Nn","title":"BetaML.Nn.loss","text":"loss(fnn,x,y)\n\nCompute avg. network loss on a test set (or a single (1 × d) data point)\n\nParameters:\n\nfnn: Worker network\nx:   Input to the network (n) or (n x d)\ny:   Label input (n) or (n x d)\n\n\n\n\n\n","category":"method"},{"location":"Nn.html#BetaML.Nn.setParams!-Tuple{AbstractLayer, Any}","page":"Nn","title":"BetaML.Nn.setParams!","text":" setParams!(layer,w)\n\nSet the trainable parameters of the layer with the given values\n\nParameters:\n\nlayer: Worker layer\nw:     The new parameters to set (Learnable)\n\nNotes:\n\nThe format of the tuple wrapped by Learnable must be consistent with those of the getParams() and getGradient() functions.\n\n\n\n\n\n","category":"method"},{"location":"Nn.html#BetaML.Nn.setParams!-Tuple{NN, Any}","page":"Nn","title":"BetaML.Nn.setParams!","text":"setParams!(nn,w)\n\nUpdate weigths of the network\n\nParameters:\n\nnn: Worker network\nw:  The new weights to set\n\n\n\n\n\n","category":"method"},{"location":"Nn.html#BetaML.Nn.show-Tuple{NN}","page":"Nn","title":"BetaML.Nn.show","text":"show(nn)\n\nPrint a representation of the Neural Network (layers, dimensions..)\n\nParameters:\n\nnn: Worker network\n\n\n\n\n\n","category":"method"},{"location":"Nn.html#BetaML.Nn.singleUpdate!-Tuple{Any, Any}","page":"Nn","title":"BetaML.Nn.singleUpdate!","text":"singleUpdate!(θ,▽;nEpoch,nBatch,batchSize,xbatch,ybatch,optAlg)\n\nPerform the parameters update based on the average batch gradient.\n\nParameters:\n\nθ:         Current parameters\n▽:         Average gradient of the batch\nnEpoch:    Count of current epoch\nnBatch:    Count of current batch\nnBatches:  Number of batches per epoch\nxbatch:    Data associated to the current batch\nybatch:    Labels associated to the current batch\noptAlg:    The Optimisation algorithm to use for the update\n\nNotes:\n\nThis function is overridden so that each optimisation algorithm implement their\n\nown version\n\nMost parameters are not used by any optimisation algorithm. They are provided\n\nto support the largest possible class of optimisation algorithms\n\nSome optimisation algorithms may change their internal structure in this function\n\n\n\n\n\n","category":"method"},{"location":"Nn.html#BetaML.Nn.train!-Tuple{NN, Any, Any}","page":"Nn","title":"BetaML.Nn.train!","text":"train!(nn,x,y;epochs,batchSize,sequential,optAlg,verbosity,cb)\n\nTrain a neural network with the given x,y data\n\nParameters:\n\nnn:         Worker network\nx:          Training input to the network (records x dimensions)\ny:          Label input (records x dimensions)\nepochs:     Number of passages over the training set [def: 100]\nbatchSize:  Size of each individual batch [def: min(size(x,1),32)]\nsequential: Wether to run all data sequentially instead of random [def: false]\noptAlg:     The optimisation algorithm to update the gradient at each batch [def: ADAM()]\nverbosity:  A verbosity parameter for the trade off information / efficiency [def: STD]\ncb:         A callback to provide information. [def: trainingInfo]\nrng:        Random Number Generator (see FIXEDSEED) [deafult: Random.GLOBAL_RNG]\n\nReturn:\n\nA named tuple with the following information\nepochs: Number of epochs actually ran\nϵ_epochs: The average error on each epoch (if verbosity > LOW)\nθ_epochs: The parameters at each epoch (if verbosity > STD)\n\nNotes:\n\nCurrently supported algorithms:\nSGD, the classical (Stochastic) Gradient Descent optimiser\nADAM,  an adaptive moment estimation optimiser\nLook at the individual optimisation algorithm (?[Name OF THE ALGORITHM]) for info on its parameter, e.g. ?SGD for the Stochastic Gradient Descent.\nYou can implement your own optimisation algorithm using a subtype of OptimisationAlgorithm and implementing its constructor and the update function singleUpdate!(⋅) (type ?singleUpdate! for details).\nYou can implement your own callback function, altought the one provided by default is already pretty generic (its output depends on the verbosity parameter). See trainingInfo for informations on the cb parameters.\nBoth the callback function and the singleUpdate! function of the optimisation algorithm can be used to stop the training algorithm, respectively returning true or stop=true.\nThe verbosity can be set to any of NONE,LOW,STD,HIGH,FULL.\nThe update is done computing the average gradient for each batch and then calling singleUpdate! to let the optimisation algorithm perform the parameters update\n\n\n\n\n\n","category":"method"},{"location":"Nn.html#BetaML.Nn.trainingInfo-Tuple{Any, Any, Any}","page":"Nn","title":"BetaML.Nn.trainingInfo","text":"trainingInfo(nn,x,y;n,batchSize,epochs,verbosity,nEpoch,nBatch)\n\nDefault callback funtion to display information during training, depending on the verbosity level\n\nParameters:\n\nnn: Worker network\nx:  Batch input to the network (batchSize,d)\ny:  Batch label input (batchSize,d)\nn: Size of the full training set\nnBatches : Number of baches per epoch\nepochs: Number of epochs defined for the training\nverbosity: Verbosity level defined for the training (NONE,LOW,STD,HIGH,FULL)\nnEpoch: Counter of the current epoch\nnBatch: Counter of the current batch\n\n#Notes:\n\nReporting of the error (loss of the network) is expensive. Use verbosity=NONE for better performances\n\n\n\n\n\n","category":"method"},{"location":"Trees.html#The-BetaML.Trees-Module","page":"Trees","title":"The BetaML.Trees Module","text":"","category":"section"},{"location":"Trees.html","page":"Trees","title":"Trees","text":"Trees","category":"page"},{"location":"Trees.html#BetaML.Trees","page":"Trees","title":"BetaML.Trees","text":"BetaML.Trees module\n\nImplement the functionality required to build a Decision Tree or a whole Random Forest, predict data and assess its performances.\n\nBoth Decision Trees and Random Forests can be used for regression or classification problems, based on the type of the labels (numerical or not). You can override the automatic selection with the parameter forceClassification=true, typically if your labels are integer representing some categories rather than numbers. For classification problems the output of predictSingle is a dictionary with the key being the labels with non-zero probabilitity and the corresponding value its proobability; for regression it is a numerical value.\n\nPlease be aware that, differently from most other implementations, the Random Forest algorithm collects and averages the probabilities from the trees, rather than just repording the mode, i.e. no information is lost and the output of the forest classifier is still a PMF.\n\nMissing data on features are supported, both on training and on prediction.\n\nThe module provide the following functions. Use ?[type or function] to access their full signature and detailed documentation:\n\nModel definition and training:\n\nbuildTree(xtrain,ytrain): Build a single Decision Tree\nbuildForest(xtrain,ytrain): Build a \"forest\" of Decision Trees\n\nModel predictions and assessment:\n\npredict(tree or forest, x): Return the prediction given the feature matrix\noobError(forest,x,y): Return the out-of-bag error estimate\nUtils.accuracy(ŷ,y)): Categorical output accuracy\nUtils.meanRelError(ŷ,y,p): L-p norm based error\n\nFeatures are expected to be in the standard format (nRecords × nDimensions matrices) and the labels (either categorical or numerical) as a nRecords column vector.\n\nAcknowlegdments: originally based on the Josh Gordon's code\n\n\n\n\n\n","category":"module"},{"location":"Trees.html#Module-Index","page":"Trees","title":"Module Index","text":"","category":"section"},{"location":"Trees.html","page":"Trees","title":"Trees","text":"Modules = [Trees]\nOrder   = [:constant, :type, :function, :macro]","category":"page"},{"location":"Trees.html#Detailed-API","page":"Trees","title":"Detailed API","text":"","category":"section"},{"location":"Trees.html","page":"Trees","title":"Trees","text":"Modules = [Trees]","category":"page"},{"location":"Trees.html#BetaML.Trees.AbstractQuestion","page":"Trees","title":"BetaML.Trees.AbstractQuestion","text":"Question\n\nA question used to partition a dataset.\n\nThis struct just records a 'column number' and a 'column value' (e.g., Green).\n\n\n\n\n\n","category":"type"},{"location":"Trees.html#BetaML.Trees.DecisionNode","page":"Trees","title":"BetaML.Trees.DecisionNode","text":"DecisionNode(question,trueBranch,falseBranch, depth)\n\nA tree's non-terminal node.\n\nConstructor's arguments and struct members:\n\nquestion: The question asked in this node\ntrueBranch: A reference to the \"true\" branch of the trees\nfalseBranch: A reference to the \"false\" branch of the trees\ndepth: The nodes's depth in the tree\n\n\n\n\n\n","category":"type"},{"location":"Trees.html#BetaML.Trees.Forest","page":"Trees","title":"BetaML.Trees.Forest","text":"Forest{Ty}\n\nType representing a Random Forest.\n\nIndividual trees are stored in the array trees. The \"type\" of the forest is given by the type of the labels on which it has been trained.\n\nStruct members:\n\ntrees:        The individual Decision Trees\nisRegression: Whether the forest is to be used for regression jobs or classification\noobData:      For each tree, the rows number if the data that have not being used to train the specific tree\noobError:     The out of bag error (if it has been computed)\nweights:      A weight for each tree depending on the tree's score on the oobData (see buildForest)\n\n\n\n\n\n","category":"type"},{"location":"Trees.html#BetaML.Trees.Leaf","page":"Trees","title":"BetaML.Trees.Leaf","text":"Leaf(y,depth)\n\nA tree's leaf (terminal) node.\n\nConstructor's arguments:\n\ny: The labels assorciated to each record (either numerical or categorical)\ndepth: The nodes's depth in the tree\n\nStruct members:\n\npredictions: Either the relative label's count (i.e. a PMF) or the mean\ndepth: The nodes's depth in the tree\n\n\n\n\n\n","category":"type"},{"location":"Trees.html#Base.print","page":"Trees","title":"Base.print","text":"print(node)\n\nPrint a Decision Tree (textual)\n\n\n\n\n\n","category":"function"},{"location":"Trees.html#BetaML.Api.partition-Union{Tuple{Tx}, Tuple{BetaML.Trees.Question{Tx}, Any, Any}} where Tx","page":"Trees","title":"BetaML.Api.partition","text":"partition(question,x)\n\nDicotomically partitions a dataset x given a question.\n\nFor each row in the dataset, check if it matches the question. If so, add it to 'true rows', otherwise, add it to 'false rows'. Rows with missing values on the question column are assigned randomly proportionally to the assignment of the non-missing rows.\n\n\n\n\n\n","category":"method"},{"location":"Trees.html#BetaML.Api.predict-Union{Tuple{Ty}, Tuple{Forest{Ty}, Any}} where Ty","page":"Trees","title":"BetaML.Api.predict","text":"predict(forest,x)\n\nPredict the labels of a feature dataset.\n\nFor each record of the dataset and each tree of the \"forest\", recursivelly traverse the tree to find the prediction most opportune for the given record. If the labels the tree has been trained with are numeric, the prediction is also numeric (the mean of the different trees predictions, in turn the mean of the labels of the training records ended in that leaf node). If the labels were categorical, the prediction is a dictionary with the probabilities of each item and in such case the probabilities of the different trees are averaged to compose the forest predictions. This is a bit different than most other implementations where the mode instead is reported.\n\nIn the first case (numerical predictions) use meanRelError(ŷ,y) to assess the mean relative error, in the second case you can use accuracy(ŷ,y).\n\n\n\n\n\n","category":"method"},{"location":"Trees.html#BetaML.Api.predict-Union{Tuple{Ty}, Tuple{Tx}, Tuple{Union{DecisionNode{Tx}, Leaf{Ty}}, Any}} where {Tx, Ty}","page":"Trees","title":"BetaML.Api.predict","text":"predict(tree,x)\n\nPredict the labels of a feature dataset.\n\nFor each record of the dataset, recursivelly traverse the tree to find the prediction most opportune for the given record. If the labels the tree has been trained with are numeric, the prediction is also numeric. If the labels were categorical, the prediction is a dictionary with the probabilities of each item.\n\nIn the first case (numerical predictions) use meanRelError(ŷ,y) to assess the mean relative error, in the second case you can use accuracy(ŷ,y).\n\n\n\n\n\n","category":"method"},{"location":"Trees.html#BetaML.Trees.buildForest-Union{Tuple{Ty}, Tuple{Any, AbstractVector{Ty}}, Tuple{Any, AbstractVector{Ty}, Any}} where Ty","page":"Trees","title":"BetaML.Trees.buildForest","text":"buildForest(x, y, nTrees; maxDepth, minGain, minRecords, maxFeatures, splittingCriterion, forceClassification)\n\nBuilds (define and train) a \"forest\" of Decision Trees.\n\nParameters:\n\nSee buildTree. The function has all the parameters of bildTree (with the maxFeatures defaulting to √D instead of D) plus the following parameters:\n\nnTrees: Number of trees in the forest [def: 30]\nβ: Parameter that regulate the weights of the scoring of each tree, to be (optionally) used in prediction (see later) [def: 0, i.e. uniform weigths]\noob: Whether to coompute the out-of-bag error, an estimation of the generalization accuracy [def: false]\nrng: Random Number Generator (see FIXEDSEED) [deafult: Random.GLOBAL_RNG]\n\nOutput:\n\nThe function returns a Forest object (see Forest).\nThe forest weights default to array of ones if β ≤ 0 and the oob error to +Inf if oob == false.\n\nNotes :\n\nEach individual decision tree is built using bootstrap over the data, i.e. \"sampling N records with replacement\" (hence, some records appear multiple times and some records do not appear in the specific tree training). The maxFeature injects further variability and reduces the correlation between the forest trees.\nThe predictions of the \"forest\" (using the function predict()) are then the aggregated predictions of the individual trees (from which the name \"bagging\": boostrap aggregating).\nThis function optionally reports a weight distribution of the performances of eanch individual trees, as measured using the records he has not being trained with. These weights can then be (optionally) used in the predict function. The parameter β ≥ 0 regulate the distribution of these weights: larger is β, the greater the importance (hence the weights) attached to the best-performing trees compared to the low-performing ones. Using these weights can significantly improve the forest performances (especially using small forests), however the correct value of β depends on the problem under exam (and the chosen caratteristics of the random forest estimator) and should be cross-validated to avoid over-fitting.\nNote that this function uses multiple threads if these are available. You can check the number of threads available with Threads.nthreads(). To set the number of threads in Julia either set the environmental variable JULIA_NUM_THREADS (before starting Julia) or start Julia with the command line option --threads (most integrated development editors for Julia already set the number of threads to 4).\n\n\n\n\n\n","category":"method"},{"location":"Trees.html#BetaML.Trees.buildTree-Union{Tuple{Ty}, Tuple{Any, AbstractVector{Ty}}} where Ty","page":"Trees","title":"BetaML.Trees.buildTree","text":"buildTree(x, y, depth; maxDepth, minGain, minRecords, maxFeatures, splittingCriterion, forceClassification)\n\nBuilds (define and train) a Decision Tree.\n\nGiven a dataset of features x and the corresponding dataset of labels y, recursivelly build a decision tree by finding at each node the best question to split the data untill either all the dataset is separated or a terminal condition is reached. The given tree is then returned.\n\nParameters:\n\nx: The dataset's features (N × D)\ny: The dataset's labels (N × 1)\nmaxDepth: The maximum depth the tree is allowed to reach. When this is reached the node is forced to become a leaf [def: N, i.e. no limits]\nminGain: The minimum information gain to allow for a node's partition [def: 0]\nminRecords:  The minimum number of records a node must holds to consider for a partition of it [def: 2]\nmaxFeatures: The maximum number of (random) features to consider at each partitioning [def: D, i.e. look at all features]\nsplittingCriterion: Either gini, entropy or variance (see infoGain ) [def: gini for categorical labels (classification task) and variance for numerical labels(regression task)]\nforceClassification: Weather to force a classification task even if the labels are numerical (typically when labels are integers encoding some feature rather than representing a real cardinal measure) [def: false]\nrng: Random Number Generator ((see FIXEDSEED)) [deafult: Random.GLOBAL_RNG]\n\nNotes:\n\nMissing data (in the feature dataset) are supported.\n\n\n\n\n\n","category":"method"},{"location":"Trees.html#BetaML.Trees.findBestSplit-Union{Tuple{Ty}, Tuple{Any, AbstractVector{Ty}, Any}} where Ty","page":"Trees","title":"BetaML.Trees.findBestSplit","text":"findBestSplit(x,y;maxFeatures,splittingCriterion)\n\nFind the best possible split of the database.\n\nFind the best question to ask by iterating over every feature / value and calculating the information gain.\n\nParameters:\n\nx: The feature dataset\ny: The labels dataset\nmaxFeatures: Maximum number of (random) features to look up for the \"best split\"\nsplittingCriterion: The metric to define the \"impurity\" of the labels\nrng: Random Number Generator (see FIXEDSEED) [deafult: Random.GLOBAL_RNG]\n\n\n\n\n\n","category":"method"},{"location":"Trees.html#BetaML.Trees.infoGain-Tuple{Any, Any, Any}","page":"Trees","title":"BetaML.Trees.infoGain","text":"infoGain(left, right, parentUncertainty; splittingCriterion)\n\nCompute the information gain of a specific partition.\n\nCompare the \"information gain\" my measuring the difference betwwen the \"impurity\" of the labels of the parent node with those of the two child nodes, weighted by the respective number of items.\n\nParameters:\n\nleftY:  Child #1 labels\nrightY: Child #2 labels\nparentUncertainty: \"Impurity\" of the labels of the parent node\nsplittingCriterion: Metric to adopt to determine the \"impurity\" (see below)\n\nYou can use your own function as the metric. We provide the following built-in metrics:\n\ngini (categorical)\nentropy (categorical)\nvariance (numerical)\n\n\n\n\n\n","category":"method"},{"location":"Trees.html#BetaML.Trees.match-Union{Tuple{Tx}, Tuple{BetaML.Trees.Question{Tx}, Any}} where Tx","page":"Trees","title":"BetaML.Trees.match","text":"match(question, x)\n\nReturn a dicotomic answer of a question when applied to a given feature record.\n\nIt compares the feature value in the given record to the value stored in the question. Numerical features are compared in terms of disequality (\">=\"), while categorical features are compared in terms of equality (\"==\").\n\n\n\n\n\n","category":"method"},{"location":"Trees.html#BetaML.Trees.oobError-Union{Tuple{Ty}, Tuple{Forest{Ty}, Any, Any}} where Ty","page":"Trees","title":"BetaML.Trees.oobError","text":"oobError(forest,x,y)\n\nComute the Out-Of-Bag error, an estimation of the validation error.\n\nThis function is called at time of train the forest if the parameter oob is true, or can be used later to get the oob error on an already trained forest.\n\n\n\n\n\n","category":"method"},{"location":"Trees.html#BetaML.Trees.predictSingle-Union{Tuple{Ty}, Tuple{Forest{Ty}, Any}} where Ty","page":"Trees","title":"BetaML.Trees.predictSingle","text":"predictSingle(forest,x)\n\nPredict the label of a single feature record. See predict.\n\n\n\n\n\n","category":"method"},{"location":"Trees.html#BetaML.Trees.predictSingle-Union{Tuple{Ty}, Tuple{Tx}, Tuple{Union{DecisionNode{Tx}, Leaf{Ty}}, Any}} where {Tx, Ty}","page":"Trees","title":"BetaML.Trees.predictSingle","text":"predictSingle(tree,x)\n\nPredict the label of a single feature record. See predict.\n\n\n\n\n\n","category":"method"},{"location":"Trees.html#BetaML.Trees.updateTreesWeights!-Union{Tuple{Ty}, Tuple{Forest{Ty}, Any, Any}} where Ty","page":"Trees","title":"BetaML.Trees.updateTreesWeights!","text":"updateTreesWeights!(forest,x,y;β)\n\nUpdate the weights of each tree (to use in the prediction of the forest) based on the error of the individual tree computed on the records on which it has not been trained. As training a forest is expensive, this function can be used to \"just\" upgrade the trees weights using different betas, without retraining the model.\n\n\n\n\n\n","category":"method"},{"location":"Utils.html#The-BetaML.Utils-Module","page":"Utils","title":"The BetaML.Utils Module","text":"","category":"section"},{"location":"Utils.html","page":"Utils","title":"Utils","text":"Utils\n","category":"page"},{"location":"Utils.html#BetaML.Utils","page":"Utils","title":"BetaML.Utils","text":"Utils module\n\nProvide shared utility functions for various machine learning algorithms. You don't usually need to import from this module, as each other module (Nn, Perceptron, Clusters,...) reexport it.\n\n\n\n\n\n","category":"module"},{"location":"Utils.html#Module-Index","page":"Utils","title":"Module Index","text":"","category":"section"},{"location":"Utils.html","page":"Utils","title":"Utils","text":"Modules = [Utils]\nOrder   = [:constant, :type, :function, :macro]","category":"page"},{"location":"Utils.html#Detailed-API","page":"Utils","title":"Detailed API","text":"","category":"section"},{"location":"Utils.html","page":"Utils","title":"Utils","text":"Modules = [Utils]","category":"page"},{"location":"Utils.html#BetaML.Utils.FIXEDRNG","page":"Utils","title":"BetaML.Utils.FIXEDRNG","text":"FIXEDRNG\n\nFixed ring to allow reproducible results\n\nUse it with:\n\nmyAlgorithm(;rng=FIXEDRNG)         # always produce the same sequence of results on each run of the script (\"pulling\" from the same rng object on different calls)\nmyAlgorithm(;rng=copy(FIXEDRNG))   # always produce the same result (new rng object on each function call)\n\n\n\n\n\n","category":"constant"},{"location":"Utils.html#BetaML.Utils.FIXEDSEED","page":"Utils","title":"BetaML.Utils.FIXEDSEED","text":"FIXEDSEED\n\nFixed seed to allow reproducible results. This is the seed used to obtain the same results under unit tests.\n\nUse it with:\n\nmyAlgorithm(;rng=FIXEDRNG)             # always produce the same sequence of results on each run of the script (\"pulling\" from the same rng object on different calls)\nmyAlgorithm(;rng=copy(FIXEDRNG)        # always produce the same result (new rng object on each call)\n\n\n\n\n\n","category":"constant"},{"location":"Utils.html#BetaML.Utils.ConfusionMatrix","page":"Utils","title":"BetaML.Utils.ConfusionMatrix","text":"ConfusionMatrix\n\nScores and measures resulting from a comparation between true and predicted categorical variables\n\nUse the function ConfusionMatrix(ŷ,y;classes,labels,rng) to build it and report(cm::ConfusionMatrix;what) to visualise it, or use the individual parts of interest, e.g. display(cm.scores).\n\nFields:\n\nlabels: Array of categorical labels\naccuracy: Overall accuracy rate\nmisclassification: Overall misclassification rate\nactualCount: Array of counts per lebel in the actual data\npredictedCount: Array of counts per label in the predicted data\nscores: Matrix actual (rows) vs predicted (columns)\nnormalisedScores: Normalised scores\ntp: True positive (by class)\ntn: True negative (by class)\nfp: False positive (by class), aka \"type I error\" or \"false allarm\"\nfn: False negative (by class), aka \"type II error\" or \"miss\"\nprecision: True class i over predicted class i (by class)\nrecall: Predicted class i over true class i (by class), aka \"True Positive Rate (TPR)\", \"Sensitivity\" or \"Probability of detection\"\nspecificity: Predicted not class i over true not class i (by class), aka \"True Negative Rate (TNR)\"\nf1Score: Harmonic mean of precision and recall\nmeanPrecision: Mean by class, respectively unweighted and weighted by actualCount\nmeanRecall: Mean by class, respectively unweighted and weighted by actualCount\nmeanSpecificity: Mean by class, respectively unweighted and weighted by actualCount\nmeanF1Score: Mean by class, respectively unweighted and weighted by actualCount\n\n\n\n\n\n","category":"type"},{"location":"Utils.html#BetaML.Utils.ConfusionMatrix-Union{Tuple{T}, Tuple{Any, AbstractArray{T, N} where N}} where T","page":"Utils","title":"BetaML.Utils.ConfusionMatrix","text":"ConfusionMatrix(ŷ,y;classes,labels,rng)\n\nBuild a \"confusion matrix\" between predicted (columns) vs actual (rows) categorical values\n\nParameters:\n\nŷ: Vector of predicted categorical data\ny: Vector of actual categorical data\nclasses: The full set of possible classes (useful to give a specicif order or if not al lclasses are represented in y) [def: unique(y) ]\nlabels: String representation of the classes [def: string.(classes)]\nrng: Random number generator. Used only if ŷ is given in terms of a PMF and there are multi-modal values, as these are assigned randomply [def: Random.GLOBAL_RNG]\n\nReturn:\n\na ConfusionMatrix object\n\n\n\n\n\n","category":"method"},{"location":"Utils.html#BetaML.Utils.KFold","page":"Utils","title":"BetaML.Utils.KFold","text":"KFold(nSplits=5,nRepeats=1,shuffle=true,rng=Random.GLOBAL_RNG)\n\nIterator for k-fold crossValidation strategy.\n\n\n\n\n\n","category":"type"},{"location":"Utils.html#Base.error-Union{Tuple{T}, Tuple{AbstractVector{T}, AbstractVector{T}}} where T","page":"Utils","title":"Base.error","text":"error(ŷ,y;ignoreLabels=false) - Categorical error (T vs T)\n\n\n\n\n\n","category":"method"},{"location":"Utils.html#Base.error-Union{Tuple{T}, Tuple{Array{Dict{T, Float64}, 1}, Vector{T}}} where T","page":"Utils","title":"Base.error","text":"error(ŷ,y) - Categorical error with with probabilistic predictions of a dataset given in terms of a dictionary of probabilities (Dict{T,Float64} vs T). \n\n\n\n\n\n","category":"method"},{"location":"Utils.html#Base.error-Union{Tuple{T}, Tuple{Matrix{T}, Vector{Int64}}} where T<:Number","page":"Utils","title":"Base.error","text":"error(ŷ,y) - Categorical error with probabilistic predictions of a dataset (PMF vs Int). \n\n\n\n\n\n","category":"method"},{"location":"Utils.html#Base.error-Union{Tuple{T}, Tuple{Vector{T}, Int64}} where T<:Number","page":"Utils","title":"Base.error","text":"error(ŷ,y) - Categorical error with probabilistic prediction of a single datapoint (PMF vs Int). \n\n\n\n\n\n","category":"method"},{"location":"Utils.html#Base.println-Union{Tuple{T}, Tuple{IO, ConfusionMatrix{T}}} where T","page":"Utils","title":"Base.println","text":"print(cm;what)\n\nPrint a ConfusionMatrix object\n\nThe what parameter is a string vector that can include \"all\", \"scores\", \"normalisedScores\" or \"report\" [def: [\"all\"]]\n\n\n\n\n\n","category":"method"},{"location":"Utils.html#Base.reshape-Union{Tuple{T}, Tuple{T, Vararg{Any, N} where N}} where T<:Number","page":"Utils","title":"Base.reshape","text":"reshape(myNumber, dims..) - Reshape a number as a n dimensional Array \n\n\n\n\n\n","category":"method"},{"location":"Utils.html#BetaML.Api.partition-Union{Tuple{T}, Tuple{AbstractVector{T}, AbstractVector{Float64}}} where T<:AbstractArray","page":"Utils","title":"BetaML.Api.partition","text":"partition(data,parts;shuffle,dims,rng)\n\nPartition (by rows) one or more matrices according to the shares in parts.\n\nParameters\n\ndata: A matrix/vector or a vector of matrices/vectors\nparts: A vector of the required shares (must sum to 1)\nshufle: Whether to randomly shuffle the matrices (preserving the relative order between matrices)\ndims: The dimension for which to partition [def: 1]\ncopy: Wheter to copy the actual data or only create a reference [def: true]\nrng: Random Number Generator (see FIXEDSEED) [deafult: Random.GLOBAL_RNG]\n\nNotes:\n\nThe sum of parts must be equal to 1\nThe number of elements in the specified dimension must be the same for all the arrays in data\n\nExample:\n\njulia julia> x = [1:10 11:20] julia> y = collect(31:40) julia> ((xtrain,xtest),(ytrain,ytest)) = partition([x,y],[0.7,0.3])\n\n\n\n\n\n","category":"method"},{"location":"Utils.html#BetaML.Utils.accuracy-Union{Tuple{T}, Tuple{AbstractVector{T}, AbstractVector{T}}} where T","page":"Utils","title":"BetaML.Utils.accuracy","text":"accuracy(ŷ,y;ignoreLabels=false) - Categorical accuracy between two vectors (T vs T). \n\n\n\n\n\n","category":"method"},{"location":"Utils.html#BetaML.Utils.accuracy-Union{Tuple{T}, Tuple{Array{Dict{T, Float64}, 1}, Vector{T}}} where T","page":"Utils","title":"BetaML.Utils.accuracy","text":"accuracy(ŷ,y;tol)\n\nCategorical accuracy with probabilistic predictions of a dataset given in terms of a dictionary of probabilities (Dict{T,Float64} vs T).\n\nParameters:\n\nŷ: A narray where each item is the estimated probability mass function in terms of a Dictionary(Item1 => Prob1, Item2 => Prob2, ...)\ny: The N array with the correct category for each point n.\ntol: The tollerance to the prediction, i.e. if considering \"correct\" only a prediction where the value with highest probability is the true value (tol = 1), or consider instead the set of tol maximum values [def: 1].\n\n\n\n\n\n","category":"method"},{"location":"Utils.html#BetaML.Utils.accuracy-Union{Tuple{T}, Tuple{Dict{T, Float64}, T}} where T","page":"Utils","title":"BetaML.Utils.accuracy","text":"accuracy(ŷ,y;tol)\n\nCategorical accuracy with probabilistic prediction of a single datapoint given in terms of a dictionary of probabilities (Dict{T,Float64} vs T).\n\nParameters:\n\nŷ: The returned probability mass function in terms of a Dictionary(Item1 => Prob1, Item2 => Prob2, ...)\ntol: The tollerance to the prediction, i.e. if considering \"correct\" only a prediction where the value with highest probability is the true value (tol = 1), or consider instead the set of tol maximum values [def: 1].\n\n\n\n\n\n","category":"method"},{"location":"Utils.html#BetaML.Utils.accuracy-Union{Tuple{T}, Tuple{Matrix{T}, Vector{Int64}}} where T<:Number","page":"Utils","title":"BetaML.Utils.accuracy","text":"accuracy(ŷ,y;tol,ignoreLabels)\n\nCategorical accuracy with probabilistic predictions of a dataset (PMF vs Int).\n\nParameters:\n\nŷ: An (N,K) matrix of probabilities that each hat y_n record with n in 1N  being of category k with k in 1K.\ny: The N array with the correct category for each point n.\ntol: The tollerance to the prediction, i.e. if considering \"correct\" only a prediction where the value with highest probability is the true value (tol = 1), or consider instead the set of tol maximum values [def: 1].\nignoreLabels: Whether to ignore the specific label order in y. Useful for unsupervised learning algorithms where the specific label order don't make sense [def: false]\n\n\n\n\n\n","category":"method"},{"location":"Utils.html#BetaML.Utils.accuracy-Union{Tuple{T}, Tuple{Vector{T}, Int64}} where T<:Number","page":"Utils","title":"BetaML.Utils.accuracy","text":"accuracy(ŷ,y;tol)\n\nCategorical accuracy with probabilistic prediction of a single datapoint (PMF vs Int).\n\nUse the parameter tol [def: 1] to determine the tollerance of the prediction, i.e. if considering \"correct\" only a prediction where the value with highest probability is the true value (tol = 1), or consider instead the set of tol maximum values.\n\n\n\n\n\n","category":"method"},{"location":"Utils.html#BetaML.Utils.aic-Tuple{Any, Any}","page":"Utils","title":"BetaML.Utils.aic","text":"aic(lL,k) -  Akaike information criterion (lower is better)\n\n\n\n\n\n","category":"method"},{"location":"Utils.html#BetaML.Utils.autoJacobian-Tuple{Any, Any}","page":"Utils","title":"BetaML.Utils.autoJacobian","text":"autoJacobian(f,x;nY)\n\nEvaluate the Jacobian using AD in the form of a (nY,nX) matrix of first derivatives\n\nParameters:\n\nf: The function to compute the Jacobian\nx: The input to the function where the jacobian has to be computed\nnY: The number of outputs of the function f [def: length(f(x))]\n\nReturn values:\n\nAn Array{Float64,2} of the locally evaluated Jacobian\n\nNotes:\n\nThe nY parameter is optional. If provided it avoids having to compute f(x)\n\n\n\n\n\n","category":"method"},{"location":"Utils.html#BetaML.Utils.batch-Tuple{Integer, Integer}","page":"Utils","title":"BetaML.Utils.batch","text":"batch(n,bSize;sequential=false,rng)\n\nReturn a vector of bSize vectors of indeces from 1 to n. Randomly unless the optional parameter sequential is used.\n\nExample:\n\njulia julia> Utils.batch(6,2,sequential=true) 3-element Array{Array{Int64,1},1}:  [1, 2]  [3, 4]  [5, 6]\n\n\n\n\n\n","category":"method"},{"location":"Utils.html#BetaML.Utils.bic-Tuple{Any, Any, Any}","page":"Utils","title":"BetaML.Utils.bic","text":"bic(lL,k,n) -  Bayesian information criterion (lower is better)\n\n\n\n\n\n","category":"method"},{"location":"Utils.html#BetaML.Utils.celu-Tuple{Any}","page":"Utils","title":"BetaML.Utils.celu","text":"celu(x; α=1) \n\nhttps://arxiv.org/pdf/1704.07483.pdf\n\n\n\n\n\n","category":"method"},{"location":"Utils.html#BetaML.Utils.classCounts-Tuple{Any}","page":"Utils","title":"BetaML.Utils.classCounts","text":"classCounts(x;classes=nothing)\n\nReturn a (unsorted) vector with the counts of each unique item (element or rows) in a dataset.\n\nIf order is important or not all classes are present in the data, a preset vectors of classes can be given in the parameter classes\n\n\n\n\n\n","category":"method"},{"location":"Utils.html#BetaML.Utils.classCountsWithLabels-Tuple{Any}","page":"Utils","title":"BetaML.Utils.classCountsWithLabels","text":"classCountsWithLabels(x)\n\nReturn a dictionary that counts the number of each unique item (rows) in a dataset.\n\n\n\n\n\n","category":"method"},{"location":"Utils.html#BetaML.Utils.colsWithMissing-Tuple{Any}","page":"Utils","title":"BetaML.Utils.colsWithMissing","text":"colsWithMissing(x)\n\nRetuyrn an array with the ids of the columns where there is at least a missing value.\n\n\n\n\n\n","category":"method"},{"location":"Utils.html#BetaML.Utils.cosine_distance-Tuple{Any, Any}","page":"Utils","title":"BetaML.Utils.cosine_distance","text":"Cosine distance\n\n\n\n\n\n","category":"method"},{"location":"Utils.html#BetaML.Utils.crossEntropy-Tuple{Any, Any}","page":"Utils","title":"BetaML.Utils.crossEntropy","text":"crossEntropy(ŷ, y; weight)\n\nCompute the (weighted) cross-entropy between the predicted and the sampled probability distributions.\n\nTo be used in classification problems.\n\n\n\n\n\n","category":"method"},{"location":"Utils.html#BetaML.Utils.crossValidation","page":"Utils","title":"BetaML.Utils.crossValidation","text":"crossValidation(f,data,sampler;dims,verbosity,returnStatistics)\n\nPerform crossValidation according to sampler rule by calling the function f and collecting its output\n\nParameters\n\nf: The user-defined function that consume the specific train and validation data and return somehting (often the associated validation error). See later\ndata: A single n-dimenasional array or a vector of them (e.g. X,Y), depending on the tasks required by f.\nsampler: An istance of a AbstractDataSampler, defining the \"rules\" for sampling at each iteration. [def: KFold(nSplits=5,nRepeats=1,shuffle=true,rng=Random.GLOBAL_RNG) ]\ndims: The dimension over performing the crossValidation i.e. the dimension containing the observations [def: 1]\nverbosity: The verbosity to print information during each iteration (this can also be printed in the f function) [def: STD]\nreturnStatistics: Wheter crossValidation should return the statistics of the output of f (mean and standard deviation) or the whole outputs [def: true].\n\nNotes\n\ncrossValidation works by calling the function f, defined by the user, passing to it the tuple trainData, valData and rng and collecting the result of the function f. The specific method for which trainData, and valData are selected at each iteration depends on the specific sampler, whith a single 5 k-fold rule being the default.\n\nThis approach is very flexible because the specific model to employ or the metric to use is left within the user-provided function. The only thing that crossValidation does is provide the model defined in the function f with the opportune data (and the random number generator).\n\nInput of the user-provided function trainData and valData are both themselves tuples. In supervised models, crossValidations data should be a tuple of (X,Y) and trainData and valData will be equivalent to (xtrain, ytrain) and (xval, yval). In unsupervised models data is a single array, but the training and validation data should still need to be accessed as  trainData[1] and valData[1]. Output of the user-provided function The user-defined function can return whatever. However, if returnStatistics is left on its default true value the user-defined function must return a single scalar (e.g. some error measure) so that the mean and the standard deviation are returned.\n\nNote that crossValidation can beconveniently be employed using the do syntax, as Julia automatically rewrite crossValidation(data,...) trainData,valData,rng  ...user defined body... end as crossValidation(f(trainData,valData,rng ), data,...)\n\nExample\n\njulia> X = [11:19 21:29 31:39 41:49 51:59 61:69];\njulia> Y = [1:9;];\njulia> sampler = KFold(nSplits=3);\njulia> (μ,σ) = crossValidation([X,Y],sampler) do trainData,valData,rng\n                 (xtrain,ytrain) = trainData; (xval,yval) = valData\n                 trainedModel    = buildForest(xtrain,ytrain,30)\n                 predictions     = predict(trainedModel,xval)\n                 ϵ               = meanRelError(predictions,yval,normRec=false)\n                 return ϵ\n               end\n(0.3202242202242202, 0.04307662219315022)\n\n\n\n\n\n","category":"function"},{"location":"Utils.html#BetaML.Utils.dcelu-Tuple{Any}","page":"Utils","title":"BetaML.Utils.dcelu","text":"dcelu(x; α=1) \n\nhttps://arxiv.org/pdf/1704.07483.pdf\n\n\n\n\n\n","category":"method"},{"location":"Utils.html#BetaML.Utils.delu-Tuple{Any}","page":"Utils","title":"BetaML.Utils.delu","text":"delu(x; α=1) with α > 0 \n\nhttps://arxiv.org/pdf/1511.07289.pdf\n\n\n\n\n\n","category":"method"},{"location":"Utils.html#BetaML.Utils.dmish-Tuple{Any}","page":"Utils","title":"BetaML.Utils.dmish","text":"dmish(x) \n\nhttps://arxiv.org/pdf/1908.08681v1.pdf\n\n\n\n\n\n","category":"method"},{"location":"Utils.html#BetaML.Utils.dplu-Tuple{Any}","page":"Utils","title":"BetaML.Utils.dplu","text":"dplu(x;α=0.1,c=1) \n\nPiecewise Linear Unit derivative \n\nhttps://arxiv.org/pdf/1809.09534.pdf\n\n\n\n\n\n","category":"method"},{"location":"Utils.html#BetaML.Utils.drelu-Tuple{Any}","page":"Utils","title":"BetaML.Utils.drelu","text":"drelu(x) \n\nRectified Linear Unit \n\nhttps://www.cs.toronto.edu/~hinton/absps/reluICML.pdf\n\n\n\n\n\n","category":"method"},{"location":"Utils.html#BetaML.Utils.dsigmoid-Tuple{Any}","page":"Utils","title":"BetaML.Utils.dsigmoid","text":"dsigmoid(x)\n\n\n\n\n\n","category":"method"},{"location":"Utils.html#BetaML.Utils.dsoftmax-Tuple{Any}","page":"Utils","title":"BetaML.Utils.dsoftmax","text":"dsoftmax(x; β=1) \n\nDerivative of the softmax function \n\nhttps://eli.thegreenplace.net/2016/the-softmax-function-and-its-derivative/\n\n\n\n\n\n","category":"method"},{"location":"Utils.html#BetaML.Utils.dsoftplus-Tuple{Any}","page":"Utils","title":"BetaML.Utils.dsoftplus","text":"dsoftplus(x) \n\nhttps://en.wikipedia.org/wiki/Rectifier(neuralnetworks)#Softplus\n\n\n\n\n\n","category":"method"},{"location":"Utils.html#BetaML.Utils.dtanh-Tuple{Any}","page":"Utils","title":"BetaML.Utils.dtanh","text":"dtanh(x)\n\n\n\n\n\n","category":"method"},{"location":"Utils.html#BetaML.Utils.elu-Tuple{Any}","page":"Utils","title":"BetaML.Utils.elu","text":"elu(x; α=1) with α > 0 \n\nhttps://arxiv.org/pdf/1511.07289.pdf\n\n\n\n\n\n","category":"method"},{"location":"Utils.html#BetaML.Utils.entropy-Tuple{Any}","page":"Utils","title":"BetaML.Utils.entropy","text":"entropy(x)\n\nCalculate the entropy for a list of items (or rows).\n\nSee: https://en.wikipedia.org/wiki/Decisiontreelearning#Gini_impurity\n\n\n\n\n\n","category":"method"},{"location":"Utils.html#BetaML.Utils.generateParallelRngs-Tuple{Random.AbstractRNG, Integer}","page":"Utils","title":"BetaML.Utils.generateParallelRngs","text":"generateParallelRngs(rng::AbstractRNG, n::Integer;reSeed=false)\n\nFor multi-threaded models, return n independent random number generators (one per thread) to be used in threaded computations.\n\nNote that each ring is a copy of the original random ring. This means that code that use these RNGs will not change the original RNG state.\n\nUse it with rngs = generateParallelRngs(rng,Threads.nthreads()) to have a separate rng per thread. By default the function doesn't re-seed the RNG, as you may want to have a loop index based re-seeding strategy rather than a threadid-based one (to guarantee the same result independently of the number of threads). If you prefer, you can instead re-seed the RNG here (using the parameter reSeed=true), such that each thread has a different seed. Be aware however that the stream  of number generated will depend from the number of threads at run time.\n\n\n\n\n\n","category":"method"},{"location":"Utils.html#BetaML.Utils.getPermutations-Union{Tuple{AbstractVector{T}}, Tuple{T}} where T","page":"Utils","title":"BetaML.Utils.getPermutations","text":"getPermutations(v::AbstractArray{T,1};keepStructure=false)\n\nReturn a vector of either (a) all possible permutations (uncollected) or (b) just those based on the unique values of the vector\n\nUseful to measure accuracy where you don't care about the actual name of the labels, like in unsupervised classifications (e.g. clustering)\n\n\n\n\n\n","category":"method"},{"location":"Utils.html#BetaML.Utils.getScaleFactors-Tuple{Any}","page":"Utils","title":"BetaML.Utils.getScaleFactors","text":"getScaleFactors(x;skip)\n\nReturn the scale factors (for each dimensions) in order to scale a matrix X (n,d) such that each dimension has mean 0 and variance 1.\n\nParameters\n\nx: the (n × d) dimension matrix to scale on each dimension d\nskip: an array of dimension index to skip the scaling [def: []]\n\nReturn\n\nA touple whose first elmement is the shift and the second the multiplicative\n\nterm to make the scale.\n\n\n\n\n\n","category":"method"},{"location":"Utils.html#BetaML.Utils.gini-Tuple{Any}","page":"Utils","title":"BetaML.Utils.gini","text":"gini(x)\n\nCalculate the Gini Impurity for a list of items (or rows).\n\nSee: https://en.wikipedia.org/wiki/Decisiontreelearning#Information_gain\n\n\n\n\n\n","category":"method"},{"location":"Utils.html#BetaML.Utils.integerDecoder-Union{Tuple{T}, Tuple{Any, AbstractVector{T}}} where T","page":"Utils","title":"BetaML.Utils.integerDecoder","text":"integerDecoder(x,factors::AbstractVector{T};unique)\n\nDecode an array of integers to an array of T corresponding to the elements of factors\n\nParameters:\n\nx: The vector to decode\nfactors: The vector of elements to use for the encoding\nunique: Wether factors is already made of unique elements [def: true]\n\nReturn:\n\nA vector of length(x) elements corresponding to the (unique) factors elements at the position x\n\nExample:\n\njulia> integerDecoder([1, 2, 2, 3, 2, 1],[\"aa\",\"cc\",\"bb\"]) # out: [\"aa\",\"cc\",\"cc\",\"bb\",\"cc\",\"aa\"]\n\n\n\n\n\n","category":"method"},{"location":"Utils.html#BetaML.Utils.integerEncoder-Tuple{AbstractVector{T} where T}","page":"Utils","title":"BetaML.Utils.integerEncoder","text":"integerEncoder(x;factors=unique(x))\n\nEncode an array of T to an array of integers using the their position in factor vector (default to the unique vector of the input array)\n\nParameters:\n\nx: The vector to encode\nfactors: The vector of factors whose position is the result of the encoding [def: unique(x)]\n\nReturn:\n\nA vector of [1,length(x)] integers corresponding to the position of each element in the factors vector`\n\nNote:\n\nAttention that while this function creates a ordered (and sortable) set, it is up to the user to be sure that this \"property\" is not indeed used in his code if the unencoded data is indeed unordered.\n\nExample:\n\njulia> integerEncoder([\"a\",\"e\",\"b\",\"e\"],factors=[\"a\",\"b\",\"c\",\"d\",\"e\"]) # out: [1,5,2,5]\n\n\n\n\n\n","category":"method"},{"location":"Utils.html#BetaML.Utils.issortable-Union{Tuple{AbstractArray{T, N}}, Tuple{N}, Tuple{T}} where {T, N}","page":"Utils","title":"BetaML.Utils.issortable","text":"Return wheather an array is sortable, i.e. has methos issort defined\n\n\n\n\n\n","category":"method"},{"location":"Utils.html#BetaML.Utils.l1_distance-Tuple{Any, Any}","page":"Utils","title":"BetaML.Utils.l1_distance","text":"L1 norm distance (aka Manhattan Distance)\n\n\n\n\n\n","category":"method"},{"location":"Utils.html#BetaML.Utils.l2_distance-Tuple{Any, Any}","page":"Utils","title":"BetaML.Utils.l2_distance","text":"Euclidean (L2) distance\n\n\n\n\n\n","category":"method"},{"location":"Utils.html#BetaML.Utils.l2²_distance-Tuple{Any, Any}","page":"Utils","title":"BetaML.Utils.l2²_distance","text":"Squared Euclidean (L2) distance\n\n\n\n\n\n","category":"method"},{"location":"Utils.html#BetaML.Utils.lse-Tuple{Any}","page":"Utils","title":"BetaML.Utils.lse","text":"LogSumExp for efficiently computing log(sum(exp.(x))) \n\n\n\n\n\n","category":"method"},{"location":"Utils.html#BetaML.Utils.makeMatrix-Tuple{AbstractArray}","page":"Utils","title":"BetaML.Utils.makeMatrix","text":"Transform an Array{T,1} in an Array{T,2} and leave unchanged Array{T,2}.\n\n\n\n\n\n","category":"method"},{"location":"Utils.html#BetaML.Utils.meanDicts-Tuple{Any}","page":"Utils","title":"BetaML.Utils.meanDicts","text":"meanDicts(dicts)\n\nCompute the mean of the values of an array of dictionaries.\n\nGiven dicts an array of dictionaries, meanDicts first compute the union of the keys and then average the values. If the original valueas are probabilities (non-negative items summing to 1), the result is also a probability distribution.\n\n\n\n\n\n","category":"method"},{"location":"Utils.html#BetaML.Utils.meanRelError-Tuple{Any, Any}","page":"Utils","title":"BetaML.Utils.meanRelError","text":"meanRelError(ŷ,y;normDim=true,normRec=true,p=1)\n\nCompute the mean relative error (l-1 based by default) between ŷ and y.\n\nThere are many ways to compute a mean relative error. In particular, if normRec (normDim) is set to true, the records (dimensions) are normalised, in the sense that it doesn't matter if a record (dimension) is bigger or smaller than the others, the relative error is first computed for each record (dimension) and then it is averaged. With both normDim and normRec set to false the function returns the relative mean error; with both set to true (default) it returns the mean relative error (i.e. with p=1 the \"mean absolute percentage error (MAPE)\") The parameter p [def: 1] controls the p-norm used to define the error.\n\nThe mean relative error enfatises the relativeness of the error, i.e. all observations and dimensions weigth the same, wether large or small. Conversly, in the relative mean error the same relative error on larger observations (or dimensions) weights more.\n\nFor example, given y = [1,44,3] and ŷ = [2,45,2], the mean relative error meanRelError(ŷ,y) is 0.452, while the relative mean error meanRelError(ŷ,y, normRec=false) is \"only\" 0.0625.\n\n\n\n\n\n","category":"method"},{"location":"Utils.html#BetaML.Utils.mish-Tuple{Any}","page":"Utils","title":"BetaML.Utils.mish","text":"mish(x) \n\nhttps://arxiv.org/pdf/1908.08681v1.pdf\n\n\n\n\n\n","category":"method"},{"location":"Utils.html#BetaML.Utils.mode-Union{Tuple{AbstractArray{Dict{T, Float64}, N} where N}, Tuple{T}} where T","page":"Utils","title":"BetaML.Utils.mode","text":"mode(elements,rng)\n\nGiven a vector of dictionaries whose key is numerical (e.g. probabilities), a vector of vectors or a matrix, it returns the mode of each element (dictionary, vector or row) in terms of the key or the position.\n\nUse it to return a unique value from a multiclass classifier returning probabilities.\n\nNote:\n\nIf multiple classes have the highest mode, one is returned at random (use the parameter rng to fix the stochasticity)\n\n\n\n\n\n","category":"method"},{"location":"Utils.html#BetaML.Utils.mode-Union{Tuple{AbstractVector{T}}, Tuple{T}} where T<:Number","page":"Utils","title":"BetaML.Utils.mode","text":"mode(v::AbstractVector{T};rng)\n\nReturn the position with the highest mode (using rand in case of multimodal values)\n\n\n\n\n\n","category":"method"},{"location":"Utils.html#BetaML.Utils.mode-Union{Tuple{Dict{T, Float64}}, Tuple{T}} where T","page":"Utils","title":"BetaML.Utils.mode","text":"mode(dict::Dict{T,Float64};rng)\n\nReturn the key with highest mode (using rand in case of multimodal values)\n\n\n\n\n\n","category":"method"},{"location":"Utils.html#BetaML.Utils.oneHotEncoder-Union{Tuple{Union{AbstractVector{T}, T}}, Tuple{T}} where T","page":"Utils","title":"BetaML.Utils.oneHotEncoder","text":"oneHotEncoder(x;d,factors,count)\n\nEncode arrays (or arrays of arrays) of categorical data as matrices of one column per factor.\n\nThe case of arrays of arrays is for when at each record you have more than one categorical output. You can then decide to encode just the presence of the factors or their counting\n\nParameters:\n\nx: The data to convert (array or array of arrays)\nd: The number of dimensions in the output matrix [def: maximum(x) for integers and length(factors) otherwise]\nfactors: The factors from which to encode [def: 1:d for integer x or unique(x) otherwise]\ncount: Wether to count multiple instances on the same dimension/record (true) or indicate just presence. [def: false]\n\nExamples\n\njulia> oneHotEncoder([\"a\",\"c\",\"c\"],factors=[\"a\",\"b\",\"c\",\"d\"])\n3×4 Matrix{Int64}:\n 1  0  0  0\n 0  0  1  0\n 0  0  1  0\njulia> oneHotEncoder([2,4,4])\n3×4 Matrix{Int64}:\n 0  1  0  0\n 0  0  0  1\n 0  0  0  1\n julia> oneHotEncoder([[2,2,1],[2,4,4]],count=true)\n2×4 Matrix{Int64}:\n 1  2  0  0\n 0  1  0  2\n\n\n\n\n\n","category":"method"},{"location":"Utils.html#BetaML.Utils.pca-Tuple{Any}","page":"Utils","title":"BetaML.Utils.pca","text":"pca(X;K,error)\n\nPerform Principal Component Analysis returning the matrix reprojected among the dimensions of maximum variance.\n\nParameters:\n\nX : The (N,D) data to reproject\nK : The number of dimensions to maintain (with K<=D) [def: nothing]\nerror: The maximum approximation error that we are willing to accept [def: 0.05]\n\nReturn:\n\nA named tuple with:\nX: The reprojected (NxK) matrix with the column dimensions organized in descending order of of the proportion of explained variance\nK: The number of dimensions retieved\nerror: The actual proportion of variance not explained in the reprojected dimensions\nP: The (D,K) matrix of the eigenvectors associated to the K-largest eigenvalues used to reproject the data matrix\nexplVarByDim: An array of dimensions D with the share of the cumulative variance explained by dimensions (the last element being always 1.0)\n\nNotes:\n\nIf K is provided, the parameter error has no effect.\nIf one doesn't know a priori the error that she/he is willling to accept, nor the wished number of dimensions, he/she can run this pca function with out = pca(X,K=size(X,2)) (i.e. with K=D), analise the proportions of explained cumulative variance by dimensions in out.explVarByDim, choose the number of dimensions K according to his/her needs and finally pick from the reprojected matrix only the number of dimensions needed, i.e. out.X[:,1:K].\n\nExample:\n\njulia> X = [1 10 100; 1.1 15 120; 0.95 23 90; 0.99 17 120; 1.05 8 90; 1.1 12 95]\n6×3 Array{Float64,2}:\n 1.0   10.0  100.0\n 1.1   15.0  120.0\n 0.95  23.0   90.0\n 0.99  17.0  120.0\n 1.05   8.0   90.0\n 1.1   12.0   95.0\n julia> X = pca(X,error=0.05).X\n6×2 Array{Float64,2}:\n  3.1783   100.449\n  6.80764  120.743\n 16.8275    91.3551\n  8.80372  120.878\n  1.86179   90.3363\n  5.51254   95.5965\n\n\n\n\n\n","category":"method"},{"location":"Utils.html#BetaML.Utils.plu-Tuple{Any}","page":"Utils","title":"BetaML.Utils.plu","text":"plu(x;α=0.1,c=1) \n\nPiecewise Linear Unit \n\nhttps://arxiv.org/pdf/1809.09534.pdf\n\n\n\n\n\n","category":"method"},{"location":"Utils.html#BetaML.Utils.polynomialKernel-Tuple{Any, Any}","page":"Utils","title":"BetaML.Utils.polynomialKernel","text":"Polynomial kernel parametrised with c=0 and d=2 (i.e. a quadratic kernel). For other cᵢ and dᵢ use K = (x,y) -> polynomialKernel(x,y,c=cᵢ,d=dᵢ) as kernel function in the supporting algorithms\n\n\n\n\n\n","category":"method"},{"location":"Utils.html#BetaML.Utils.pool1d","page":"Utils","title":"BetaML.Utils.pool1d","text":"pool1d(x,poolSize=2;f=mean)\n\nApply funtion f to a rolling poolSize contiguous (in 1d) neurons.\n\nApplicable to VectorFunctionLayer, e.g. layer2  = VectorFunctionLayer(nₗ,f=(x->pool1d(x,4,f=mean)) Attention: to apply this funciton as activation function in a neural network you will need Julia version >= 1.6, otherwise you may experience a segmentation fault (see this bug report)\n\n\n\n\n\n","category":"function"},{"location":"Utils.html#BetaML.Utils.radialKernel-Tuple{Any, Any}","page":"Utils","title":"BetaML.Utils.radialKernel","text":"Radial Kernel (aka RBF kernel) parametrised with γ=1/2. For other gammas γᵢ use K = (x,y) -> radialKernel(x,y,γ=γᵢ) as kernel function in the supporting algorithms\n\n\n\n\n\n","category":"method"},{"location":"Utils.html#BetaML.Utils.relu-Tuple{Any}","page":"Utils","title":"BetaML.Utils.relu","text":"relu(x) \n\nRectified Linear Unit \n\nhttps://www.cs.toronto.edu/~hinton/absps/reluICML.pdf\n\n\n\n\n\n","category":"method"},{"location":"Utils.html#BetaML.Utils.scale","page":"Utils","title":"BetaML.Utils.scale","text":"scale(x,scaleFactors;rev)\n\nPerform a linear scaling of x using scaling factors scaleFactors.\n\nParameters\n\nx: The (n × d) dimension matrix to scale on each dimension d\nscalingFactors: A tuple of the constant and multiplicative scaling factor\n\nrespectively [def: the scaling factors needed to scale x to mean 0 and variance 1]\n\nrev: Whether to invert the scaling [def: false]\n\nReturn\n\nThe scaled matrix\n\nNotes:\n\nAlso available scale!(x,scaleFactors) for in-place scaling.\nRetrieve the scale factors with the getScaleFactors() function\n\n\n\n\n\n","category":"function"},{"location":"Utils.html#BetaML.Utils.sigmoid-Tuple{Any}","page":"Utils","title":"BetaML.Utils.sigmoid","text":"sigmoid(x)\n\n\n\n\n\n","category":"method"},{"location":"Utils.html#BetaML.Utils.singleUnique-Union{Tuple{Union{AbstractArray{T, N} where N, T}}, Tuple{T}} where T","page":"Utils","title":"BetaML.Utils.singleUnique","text":"singleUnique(x) Return the unique values of x whether x is an array of arrays, an array or a scalar\n\n\n\n\n\n","category":"method"},{"location":"Utils.html#BetaML.Utils.softmax-Tuple{Any}","page":"Utils","title":"BetaML.Utils.softmax","text":"softmax (x; β=1) \n\nThe input x is a vector. Return a PMF\n\n\n\n\n\n","category":"method"},{"location":"Utils.html#BetaML.Utils.softplus-Tuple{Any}","page":"Utils","title":"BetaML.Utils.softplus","text":"softplus(x) \n\nhttps://en.wikipedia.org/wiki/Rectifier(neuralnetworks)#Softplus\n\n\n\n\n\n","category":"method"},{"location":"Utils.html#BetaML.Utils.squaredCost-Tuple{Any, Any}","page":"Utils","title":"BetaML.Utils.squaredCost","text":"squaredCost(ŷ,y)\n\nCompute the squared costs between a vector of prediction and one of observations as (1/2)*norm(y - ŷ)^2.\n\nAside the 1/2 term, it correspond to the squared l-2 norm distance and when it is averaged on multiple datapoints corresponds to the Mean Squared Error (MSE). It is mostly used for regression problems.\n\n\n\n\n\n","category":"method"},{"location":"Utils.html#BetaML.Utils.sterling-Tuple{BigInt, BigInt}","page":"Utils","title":"BetaML.Utils.sterling","text":"Sterling number: number of partitions of a set of n elements in k sets \n\n\n\n\n\n","category":"method"},{"location":"Utils.html#BetaML.Utils.variance-Tuple{Any}","page":"Utils","title":"BetaML.Utils.variance","text":"variance(x) - population variance\n\n\n\n\n\n","category":"method"},{"location":"Utils.html#Random.shuffle-Union{Tuple{AbstractVector{T}}, Tuple{T}} where T<:AbstractArray","page":"Utils","title":"Random.shuffle","text":"shuffle(data;dims,rng)\n\nShuffle a vector of n-dimensional arrays across dimension dims keeping the same order between the arrays\n\nParameters\n\ndata: The vector of arrays to shuffle\ndims: The dimension over to apply the shuffle [def: 1]\nrng:  An AbstractRNG to apply for the shuffle\n\nNotes\n\nAll the arrays must have the same size for the dimension to shuffle\n\nExample\n\njulia> a = [1 2 30; 10 20 30]; b = [100 200 300]; julia> (aShuffled, bShuffled) = shuffle([a,b],dims=2) 2-element Vector{Matrix{Int64}}:  [1 30 2; 10 30 20]  [100 300 200]\n\n\n\n\n\n","category":"method"},{"location":"tutorials/Regression - bike sharing/betaml_tutorial_regression_sharingBikes.html","page":"A regression task: the prediction of  bike  sharing demand","title":"A regression task: the prediction of  bike  sharing demand","text":"EditURL = \"https://github.com/sylvaticus/BetaML.jl/blob/master/docs/src/tutorials/Regression - bike sharing/betaml_tutorial_regression_sharingBikes.jl\"","category":"page"},{"location":"tutorials/Regression - bike sharing/betaml_tutorial_regression_sharingBikes.html#regression_tutorial","page":"A regression task: the prediction of  bike  sharing demand","title":"A regression task: the prediction of  bike  sharing demand","text":"","category":"section"},{"location":"tutorials/Regression - bike sharing/betaml_tutorial_regression_sharingBikes.html","page":"A regression task: the prediction of  bike  sharing demand","title":"A regression task: the prediction of  bike  sharing demand","text":"The task is to estimate the influence of several variables (like the weather, the season, the day of the week..) on the demand of shared bicycles, so that the authority in charge of the service can organise the service in the best way.","category":"page"},{"location":"tutorials/Regression - bike sharing/betaml_tutorial_regression_sharingBikes.html","page":"A regression task: the prediction of  bike  sharing demand","title":"A regression task: the prediction of  bike  sharing demand","text":"Data origin:","category":"page"},{"location":"tutorials/Regression - bike sharing/betaml_tutorial_regression_sharingBikes.html","page":"A regression task: the prediction of  bike  sharing demand","title":"A regression task: the prediction of  bike  sharing demand","text":"original full dataset (by hour, not used here): https://archive.ics.uci.edu/ml/datasets/Bike+Sharing+Dataset\nsimplified dataset (by day, with some simple scaling): https://www.hds.utc.fr/~tdenoeux/dokuwiki/en/aec\ndescription: https://www.hds.utc.fr/~tdenoeux/dokuwiki/media/en/exam2019ace.pdf\ndata: https://www.hds.utc.fr/~tdenoeux/dokuwiki/media/en/bikesharing_day.csv.zip","category":"page"},{"location":"tutorials/Regression - bike sharing/betaml_tutorial_regression_sharingBikes.html","page":"A regression task: the prediction of  bike  sharing demand","title":"A regression task: the prediction of  bike  sharing demand","text":"Note that even if we are estimating a time serie, we are not using here a recurrent neural network as we assume the temporal dependence to be negligible (i.e. Y_t = f(X_t) alone).","category":"page"},{"location":"tutorials/Regression - bike sharing/betaml_tutorial_regression_sharingBikes.html#Library-and-data-loading","page":"A regression task: the prediction of  bike  sharing demand","title":"Library and data loading","text":"","category":"section"},{"location":"tutorials/Regression - bike sharing/betaml_tutorial_regression_sharingBikes.html","page":"A regression task: the prediction of  bike  sharing demand","title":"A regression task: the prediction of  bike  sharing demand","text":"We first load all the packages we are going to use","category":"page"},{"location":"tutorials/Regression - bike sharing/betaml_tutorial_regression_sharingBikes.html","page":"A regression task: the prediction of  bike  sharing demand","title":"A regression task: the prediction of  bike  sharing demand","text":"using  LinearAlgebra, Random, Statistics, DataFrames, CSV, Plots, Pipe, BenchmarkTools, BetaML\nimport Distributions: Uniform\nimport DecisionTree, Flux ## For comparisions","category":"page"},{"location":"tutorials/Regression - bike sharing/betaml_tutorial_regression_sharingBikes.html","page":"A regression task: the prediction of  bike  sharing demand","title":"A regression task: the prediction of  bike  sharing demand","text":"Here we load the data from a csv provided by the BataML package","category":"page"},{"location":"tutorials/Regression - bike sharing/betaml_tutorial_regression_sharingBikes.html","page":"A regression task: the prediction of  bike  sharing demand","title":"A regression task: the prediction of  bike  sharing demand","text":"baseDir = joinpath(dirname(pathof(BetaML)),\"..\",\"docs\",\"src\",\"tutorials\",\"Regression - bike sharing\")\ndata    = CSV.File(joinpath(baseDir,\"data\",\"bike_sharing_day.csv\"),delim=',') |> DataFrame\ndescribe(data)","category":"page"},{"location":"tutorials/Regression - bike sharing/betaml_tutorial_regression_sharingBikes.html","page":"A regression task: the prediction of  bike  sharing demand","title":"A regression task: the prediction of  bike  sharing demand","text":"The variable we want to learn to predict is cnt, the total demand of bikes for a given day. Even if it is indeed an integer, we treat it as a continuous variable, so each single prediction will be a scalar Y in mathbbR.","category":"page"},{"location":"tutorials/Regression - bike sharing/betaml_tutorial_regression_sharingBikes.html","page":"A regression task: the prediction of  bike  sharing demand","title":"A regression task: the prediction of  bike  sharing demand","text":"plot(data.cnt, title=\"Daily bike sharing rents (2Y)\", label=nothing)","category":"page"},{"location":"tutorials/Regression - bike sharing/betaml_tutorial_regression_sharingBikes.html#Decision-Trees","page":"A regression task: the prediction of  bike  sharing demand","title":"Decision Trees","text":"","category":"section"},{"location":"tutorials/Regression - bike sharing/betaml_tutorial_regression_sharingBikes.html","page":"A regression task: the prediction of  bike  sharing demand","title":"A regression task: the prediction of  bike  sharing demand","text":"We start our regression task with Decision Trees","category":"page"},{"location":"tutorials/Regression - bike sharing/betaml_tutorial_regression_sharingBikes.html#Data-preparation","page":"A regression task: the prediction of  bike  sharing demand","title":"Data preparation","text":"","category":"section"},{"location":"tutorials/Regression - bike sharing/betaml_tutorial_regression_sharingBikes.html","page":"A regression task: the prediction of  bike  sharing demand","title":"A regression task: the prediction of  bike  sharing demand","text":"The first step is to prepare the data for the analysis. This indeed depends already on the model we want to employ, as some models \"accept\" everything as input, no matter if the data is numerical or categorical, if it has missing values or not... while other models are instead much more exigents, and require more work to \"clean up\" our dataset. Here we start using  Decision Tree and Random Forest models that belong to the first group, so the only things we have to do is to select the variables in input (the \"feature matrix\", we wil lindicate it with \"X\") and those representing our output (the values we want to learn to predict, we call them \"y\"):","category":"page"},{"location":"tutorials/Regression - bike sharing/betaml_tutorial_regression_sharingBikes.html","page":"A regression task: the prediction of  bike  sharing demand","title":"A regression task: the prediction of  bike  sharing demand","text":"x    = Matrix{Float64}(data[:,[:instant,:season,:yr,:mnth,:holiday,:weekday,:workingday,:weathersit,:temp,:atemp,:hum,:windspeed]])\ny    = data[:,16];\nnothing #hide","category":"page"},{"location":"tutorials/Regression - bike sharing/betaml_tutorial_regression_sharingBikes.html","page":"A regression task: the prediction of  bike  sharing demand","title":"A regression task: the prediction of  bike  sharing demand","text":"We can now split the dataset between the data we will use for training the algorithm (xtrain/ytrain), those for selecting the hyperparameters (xval/yval) and finally those for testing the quality of the algoritm with the optimal hyperparameters (xtest/ytest). We use the partition function specifying the share we want to use for these three different subsets, here 75%, 12.5% and 12.5 respectively. As our data represents indeed a time serie, we want our model to be able to predict future demand of bike sharing from past, observed rented bikes, so we do not shuffle the datasets as it would be the default.","category":"page"},{"location":"tutorials/Regression - bike sharing/betaml_tutorial_regression_sharingBikes.html","page":"A regression task: the prediction of  bike  sharing demand","title":"A regression task: the prediction of  bike  sharing demand","text":"((xtrain,xval,xtest),(ytrain,yval,ytest)) = partition([x,y],[0.75,0.125,1-0.75-0.125],shuffle=false)\n(ntrain, nval, ntest) = size.([ytrain,yval,ytest],1)","category":"page"},{"location":"tutorials/Regression - bike sharing/betaml_tutorial_regression_sharingBikes.html","page":"A regression task: the prediction of  bike  sharing demand","title":"A regression task: the prediction of  bike  sharing demand","text":"We can now \"tune\" our model so-called hyperparameters, i.e. choose the best exogenous parameters of our algorithm, where \"best\" refers to some minimisation of a \"loss\" function between the true and the predicted value. To make the comparision we use a specific \"validation\" subset of data (xval and yval).","category":"page"},{"location":"tutorials/Regression - bike sharing/betaml_tutorial_regression_sharingBikes.html","page":"A regression task: the prediction of  bike  sharing demand","title":"A regression task: the prediction of  bike  sharing demand","text":"BetaML doesn't have a dedicated function for hyperparameters optimisation, but it is easy to write some custom julia code, at least for a simple grid-based \"search\". Indeed one of the main reasons that a dedicated function exists in other Machine Learning libraries is that loops in other languages are slow, but this is not a problem in julia, so we can retain the flexibility to write the kind of hyperparameter tuning that best fits our needs.","category":"page"},{"location":"tutorials/Regression - bike sharing/betaml_tutorial_regression_sharingBikes.html","page":"A regression task: the prediction of  bike  sharing demand","title":"A regression task: the prediction of  bike  sharing demand","text":"Below is an example of a possible such function. Note there are more \"elegant\" ways to code it, but this one does the job. We will see the various functions inside tuneHyperParameters() in a moment. For now let's going just to observe that tuneHyperParameters just loops over all the possible hyperparameters and selects the one where the error between xval and yval is minimised. For the meaning of the various hyperparameter, consult the documentation of the buildTree and buildForest functions. The function uses multiple threads, so we calls generateParallelRngs() (in the BetaML.Utils submodule) to generate thread-safe random number generators and locks the comparision step.","category":"page"},{"location":"tutorials/Regression - bike sharing/betaml_tutorial_regression_sharingBikes.html","page":"A regression task: the prediction of  bike  sharing demand","title":"A regression task: the prediction of  bike  sharing demand","text":"function tuneHyperParameters(model,xtrain,ytrain,xval,yval;maxDepthRange=15:15,maxFeaturesRange=size(xtrain,2):size(xtrain,2),nTreesRange=20:20,βRange=0:0,minRecordsRange=2:2,repetitions=5,rng=Random.GLOBAL_RNG)\n    # We start with an infinitely high error\n    bestRme         = +Inf\n    bestMaxDepth    = 1\n    bestMaxFeatures = 1\n    bestMinRecords  = 2\n    bestNTrees      = 1\n    bestβ           = 0\n    compLock        = ReentrantLock()\n\n    # Generate one random number generator per thread\n    masterSeed = rand(rng,100:9999999999999) ## Some RNG have problems with very small seed. Also, the master seed has to be computed _before_ generateParallelRngs\n    rngs = generateParallelRngs(rng,Threads.nthreads())\n\n    # We loop over all possible hyperparameter combinations...\n    parLengths = (length(maxDepthRange),length(maxFeaturesRange),length(minRecordsRange),length(nTreesRange),length(βRange))\n    Threads.@threads for ij in CartesianIndices(parLengths) ## This to avoid many nested for loops\n           (maxDepth,maxFeatures,minRecords,nTrees,β)   = (maxDepthRange[Tuple(ij)[1]], maxFeaturesRange[Tuple(ij)[2]], minRecordsRange[Tuple(ij)[3]], nTreesRange[Tuple(ij)[4]], βRange[Tuple(ij)[5]]) ## The specific hyperparameters of this nested loop\n           tsrng = rngs[Threads.threadid()] ## The random number generator is specific for each thread..\n           joinedIndx = LinearIndices(parLengths)[ij]\n           # And here we make the seeding depending on the id of the loop, not the thread: hence we get the same results indipendently of the number of threads\n           Random.seed!(tsrng,masterSeed+joinedIndx*10)\n           totAttemptError = 0.0\n           # We run several repetitions with the same hyperparameter combination to account for stochasticity...\n           for r in 1:repetitions\n              if model == \"DecisionTree\"\n                 # Here we train the Decition Tree model\n                 myTrainedModel = buildTree(xtrain,ytrain, maxDepth=maxDepth,maxFeatures=maxFeatures,minRecords=minRecords,rng=tsrng)\n              else\n                 # Here we train the Random Forest model\n                 myTrainedModel = buildForest(xtrain,ytrain,nTrees,maxDepth=maxDepth,maxFeatures=maxFeatures,minRecords=minRecords,β=β,rng=tsrng)\n              end\n              # Here we make prediciton with this trained model and we compute its error\n              ŷval   = predict(myTrainedModel, xval,rng=tsrng)\n              rmeVal = meanRelError(ŷval,yval,normRec=false)\n              totAttemptError += rmeVal\n           end\n           avgAttemptedDepthError = totAttemptError / repetitions\n           begin\n               lock(compLock) ## This step can't be run in parallel...\n               try\n                   # Select this specific combination of hyperparameters if the error is the lowest\n                   if avgAttemptedDepthError < bestRme\n                     bestRme         = avgAttemptedDepthError\n                     bestMaxDepth    = maxDepth\n                     bestMaxFeatures = maxFeatures\n                     bestNTrees      = nTrees\n                     bestβ           = β\n                     bestMinRecords  = minRecords\n                   end\n               finally\n                   unlock(compLock)\n               end\n           end\n    end\n    return (bestRme,bestMaxDepth,bestMaxFeatures,bestMinRecords,bestNTrees,bestβ)\nend","category":"page"},{"location":"tutorials/Regression - bike sharing/betaml_tutorial_regression_sharingBikes.html","page":"A regression task: the prediction of  bike  sharing demand","title":"A regression task: the prediction of  bike  sharing demand","text":"We can now run the hyperparameter optimisation function with some \"reasonable\" ranges. To obtain replicable results we call tuneHyperParameters with rng=copy(FIXEDRNG), where FIXEDRNG is a fixed-seeded random number generator guaranteed to maintain the same stream of random numbers even between different julia versions. That's also what we use for our unit tests.","category":"page"},{"location":"tutorials/Regression - bike sharing/betaml_tutorial_regression_sharingBikes.html","page":"A regression task: the prediction of  bike  sharing demand","title":"A regression task: the prediction of  bike  sharing demand","text":"(bestRme,bestMaxDepth,bestMaxFeatures,bestMinRecords) = tuneHyperParameters(\"DecisionTree\",xtrain,ytrain,xval,yval,\n           maxDepthRange=3:7,maxFeaturesRange=10:12,minRecordsRange=2:6,repetitions=10,rng=copy(FIXEDRNG))","category":"page"},{"location":"tutorials/Regression - bike sharing/betaml_tutorial_regression_sharingBikes.html","page":"A regression task: the prediction of  bike  sharing demand","title":"A regression task: the prediction of  bike  sharing demand","text":"Now that we have found the \"optimal\" hyperparameters we can build (\"train\") our model using them:","category":"page"},{"location":"tutorials/Regression - bike sharing/betaml_tutorial_regression_sharingBikes.html","page":"A regression task: the prediction of  bike  sharing demand","title":"A regression task: the prediction of  bike  sharing demand","text":"myTree = buildTree(xtrain,ytrain, maxDepth=bestMaxDepth, maxFeatures=bestMaxFeatures,minRecords=bestMinRecords,rng=copy(FIXEDRNG));\nnothing #hide","category":"page"},{"location":"tutorials/Regression - bike sharing/betaml_tutorial_regression_sharingBikes.html","page":"A regression task: the prediction of  bike  sharing demand","title":"A regression task: the prediction of  bike  sharing demand","text":"Let's benchmark the time and memory usage of the training step of a decision tree:","category":"page"},{"location":"tutorials/Regression - bike sharing/betaml_tutorial_regression_sharingBikes.html","page":"A regression task: the prediction of  bike  sharing demand","title":"A regression task: the prediction of  bike  sharing demand","text":"@btime  buildTree(xtrain,ytrain, maxDepth=bestMaxDepth, maxFeatures=bestMaxFeatures,minRecords=bestMinRecords,rng=copy(FIXEDRNG));\nnothing #hide","category":"page"},{"location":"tutorials/Regression - bike sharing/betaml_tutorial_regression_sharingBikes.html","page":"A regression task: the prediction of  bike  sharing demand","title":"A regression task: the prediction of  bike  sharing demand","text":"Individual decision trees are blazing fast, among the fastest algorithms we could use.","category":"page"},{"location":"tutorials/Regression - bike sharing/betaml_tutorial_regression_sharingBikes.html","page":"A regression task: the prediction of  bike  sharing demand","title":"A regression task: the prediction of  bike  sharing demand","text":"The above buildTreefunction produces a DecisionTree object that can be used to make predictions given some new features, i.e. given some X matrix of (number of observations x dimensions), predict the corresponding Y vector of scalers in R.","category":"page"},{"location":"tutorials/Regression - bike sharing/betaml_tutorial_regression_sharingBikes.html","page":"A regression task: the prediction of  bike  sharing demand","title":"A regression task: the prediction of  bike  sharing demand","text":"(ŷtrain,ŷval,ŷtest) = predict.([myTree], [xtrain,xval,xtest])","category":"page"},{"location":"tutorials/Regression - bike sharing/betaml_tutorial_regression_sharingBikes.html","page":"A regression task: the prediction of  bike  sharing demand","title":"A regression task: the prediction of  bike  sharing demand","text":"Note that the above code uses the \"dot syntax\" to \"broadcast\" predict() over an array of label matrices. It is exactly equivalent to:","category":"page"},{"location":"tutorials/Regression - bike sharing/betaml_tutorial_regression_sharingBikes.html","page":"A regression task: the prediction of  bike  sharing demand","title":"A regression task: the prediction of  bike  sharing demand","text":"ŷtrain = predict(myTree, xtrain);\nŷval   = predict(myTree, xval);\nŷtest  = predict(myTree, xtest);\nnothing #hide","category":"page"},{"location":"tutorials/Regression - bike sharing/betaml_tutorial_regression_sharingBikes.html","page":"A regression task: the prediction of  bike  sharing demand","title":"A regression task: the prediction of  bike  sharing demand","text":"We now compute the relative mean error for the training, the validation and the test set. The meanRelError is a very flexible error function. Without additional parameter, it computes, as the name says, the mean relative error, also known as the \"mean absolute percentage error\" (MAPE)](https://en.wikipedia.org/wiki/Meanabsolutepercentageerror)\") between an estimated and a true vector. However it can also compute the _relative mean error (as we do here), or use a p-norm higher than 1. The mean relative error enfatises the relativeness of the error, i.e. all observations and dimensions weigth the same, wether large or small. Conversly, in the relative mean error the same relative error on larger observations (or dimensions) weights more. In this exercise we use the later, as our data has clearly some outlier days with very small rents, and we care more of avoiding our customers finding empty bike racks than having unrented bikes on the rack. Targeting a low mean average error would push all our predicitons down to try accomodate the low-level predicitons (to avoid a large relative error), and that's not what we want.","category":"page"},{"location":"tutorials/Regression - bike sharing/betaml_tutorial_regression_sharingBikes.html","page":"A regression task: the prediction of  bike  sharing demand","title":"A regression task: the prediction of  bike  sharing demand","text":"For example let'c consider the following example:","category":"page"},{"location":"tutorials/Regression - bike sharing/betaml_tutorial_regression_sharingBikes.html","page":"A regression task: the prediction of  bike  sharing demand","title":"A regression task: the prediction of  bike  sharing demand","text":"y     = [30,28,27,3,32,38];\nŷpref = [32,30,28,10,31,40];\nŷbad  = [29,25,24,5,28,35];\nnothing #hide","category":"page"},{"location":"tutorials/Regression - bike sharing/betaml_tutorial_regression_sharingBikes.html","page":"A regression task: the prediction of  bike  sharing demand","title":"A regression task: the prediction of  bike  sharing demand","text":"Here ŷpref is an ipotetical output of a model that minimise the relative mean error, while ŷbad minimise the mean realative error","category":"page"},{"location":"tutorials/Regression - bike sharing/betaml_tutorial_regression_sharingBikes.html","page":"A regression task: the prediction of  bike  sharing demand","title":"A regression task: the prediction of  bike  sharing demand","text":"meanRelError.([ŷbad, ŷpref],[y,y],normRec=true) ## Mean relative error","category":"page"},{"location":"tutorials/Regression - bike sharing/betaml_tutorial_regression_sharingBikes.html","page":"A regression task: the prediction of  bike  sharing demand","title":"A regression task: the prediction of  bike  sharing demand","text":"meanRelError.([ŷbad, ŷpref],[y,y],normRec=false) ## Relative mean error","category":"page"},{"location":"tutorials/Regression - bike sharing/betaml_tutorial_regression_sharingBikes.html","page":"A regression task: the prediction of  bike  sharing demand","title":"A regression task: the prediction of  bike  sharing demand","text":"plot([y ŷbad ŷpref], colour=[:black :red :green], label=[\"obs\" \"bad est\" \"good est\"])","category":"page"},{"location":"tutorials/Regression - bike sharing/betaml_tutorial_regression_sharingBikes.html","page":"A regression task: the prediction of  bike  sharing demand","title":"A regression task: the prediction of  bike  sharing demand","text":"We can then compute the relative mean error for the decision tree","category":"page"},{"location":"tutorials/Regression - bike sharing/betaml_tutorial_regression_sharingBikes.html","page":"A regression task: the prediction of  bike  sharing demand","title":"A regression task: the prediction of  bike  sharing demand","text":"(rmeTrain, rmeVal, rmeTest) = meanRelError.([ŷtrain,ŷval,ŷtest],[ytrain,yval,ytest],normRec=false)","category":"page"},{"location":"tutorials/Regression - bike sharing/betaml_tutorial_regression_sharingBikes.html","page":"A regression task: the prediction of  bike  sharing demand","title":"A regression task: the prediction of  bike  sharing demand","text":"We can plot the true labels vs the estimated one for the three subsets...","category":"page"},{"location":"tutorials/Regression - bike sharing/betaml_tutorial_regression_sharingBikes.html","page":"A regression task: the prediction of  bike  sharing demand","title":"A regression task: the prediction of  bike  sharing demand","text":"scatter(ytrain,ŷtrain,xlabel=\"daily rides\",ylabel=\"est. daily rides\",label=nothing,title=\"Est vs. obs in training period (DT)\")","category":"page"},{"location":"tutorials/Regression - bike sharing/betaml_tutorial_regression_sharingBikes.html","page":"A regression task: the prediction of  bike  sharing demand","title":"A regression task: the prediction of  bike  sharing demand","text":"scatter(yval,ŷval,xlabel=\"daily rides\",ylabel=\"est. daily rides\",label=nothing,title=\"Est vs. obs in validation period (DT)\")","category":"page"},{"location":"tutorials/Regression - bike sharing/betaml_tutorial_regression_sharingBikes.html","page":"A regression task: the prediction of  bike  sharing demand","title":"A regression task: the prediction of  bike  sharing demand","text":"scatter(ytest,ŷtest,xlabel=\"daily rides\",ylabel=\"est. daily rides\",label=nothing,title=\"Est vs. obs in testing period (DT)\")","category":"page"},{"location":"tutorials/Regression - bike sharing/betaml_tutorial_regression_sharingBikes.html","page":"A regression task: the prediction of  bike  sharing demand","title":"A regression task: the prediction of  bike  sharing demand","text":"Or we can visualise the true vs estimated bike shared on a temporal base. First on the full period (2 years) ...","category":"page"},{"location":"tutorials/Regression - bike sharing/betaml_tutorial_regression_sharingBikes.html","page":"A regression task: the prediction of  bike  sharing demand","title":"A regression task: the prediction of  bike  sharing demand","text":"ŷtrainfull = vcat(ŷtrain,fill(missing,nval+ntest))\nŷvalfull   = vcat(fill(missing,ntrain), ŷval, fill(missing,ntest))\nŷtestfull  = vcat(fill(missing,ntrain+nval), ŷtest)\nplot(data[:,:dteday],[data[:,:cnt] ŷtrainfull ŷvalfull ŷtestfull], label=[\"obs\" \"train\" \"val\" \"test\"], legend=:topleft, ylabel=\"daily rides\", title=\"Daily bike sharing demand observed/estimated across the\\n whole 2-years period (DT)\")","category":"page"},{"location":"tutorials/Regression - bike sharing/betaml_tutorial_regression_sharingBikes.html","page":"A regression task: the prediction of  bike  sharing demand","title":"A regression task: the prediction of  bike  sharing demand","text":"..and then focusing on the testing period","category":"page"},{"location":"tutorials/Regression - bike sharing/betaml_tutorial_regression_sharingBikes.html","page":"A regression task: the prediction of  bike  sharing demand","title":"A regression task: the prediction of  bike  sharing demand","text":"stc = 620\nendc = size(x,1)\nplot(data[stc:endc,:dteday],[data[stc:endc,:cnt] ŷvalfull[stc:endc] ŷtestfull[stc:endc]], label=[\"obs\" \"val\" \"test\"], legend=:bottomleft, ylabel=\"Daily rides\", title=\"Focus on the testing period (DT)\")","category":"page"},{"location":"tutorials/Regression - bike sharing/betaml_tutorial_regression_sharingBikes.html","page":"A regression task: the prediction of  bike  sharing demand","title":"A regression task: the prediction of  bike  sharing demand","text":"The predictions aren't so bad in this case, however decision trees are highly instable, and the output could have depended just from the specific initial random seed.","category":"page"},{"location":"tutorials/Regression - bike sharing/betaml_tutorial_regression_sharingBikes.html#Random-Forests","page":"A regression task: the prediction of  bike  sharing demand","title":"Random Forests","text":"","category":"section"},{"location":"tutorials/Regression - bike sharing/betaml_tutorial_regression_sharingBikes.html","page":"A regression task: the prediction of  bike  sharing demand","title":"A regression task: the prediction of  bike  sharing demand","text":"Rather than trying to solve this problem using a single Decision Tree model, let's not try to use a Random Forest model. Random forests average the results of many different decision trees and provide a more \"stable\" result. Being made of many decision trees, random forests are hovever more computationally expensive to train, but luckily they tend to self-tune (or self-regularise). In particular the default maxDepth andmaxFeatures` shouldn't need tuning. We still tune however the model for other parameters, and in particular the β parameter, a prerogative of BetaML Random Forests that allows to assign more weigth to the best performing trees in the forest. It may be particularly important if there are many outliers in the data we don't want to \"learn\" from.","category":"page"},{"location":"tutorials/Regression - bike sharing/betaml_tutorial_regression_sharingBikes.html","page":"A regression task: the prediction of  bike  sharing demand","title":"A regression task: the prediction of  bike  sharing demand","text":"minRecordsRange=[2,4,5]; nTreesRange=60:10:80; βRange=100:100:300\n(bestRme,bestMaxDepth,bestMaxFeatures,bestMinRecords,bestNTrees,bestβ) = tuneHyperParameters(\"RandomForest\",xtrain,ytrain,xval,yval,\n        maxDepthRange=size(xtrain,1):size(xtrain,1),maxFeaturesRange=Int(round(sqrt(size(xtrain,2)))):Int(round(sqrt(size(xtrain,2)))),\n        minRecordsRange=minRecordsRange,nTreesRange=nTreesRange,βRange=βRange,repetitions=5,rng=copy(FIXEDRNG))","category":"page"},{"location":"tutorials/Regression - bike sharing/betaml_tutorial_regression_sharingBikes.html","page":"A regression task: the prediction of  bike  sharing demand","title":"A regression task: the prediction of  bike  sharing demand","text":"As for decision trees, once the hyper-parameters of the model are tuned we wan refit the model using the optimal parameters.","category":"page"},{"location":"tutorials/Regression - bike sharing/betaml_tutorial_regression_sharingBikes.html","page":"A regression task: the prediction of  bike  sharing demand","title":"A regression task: the prediction of  bike  sharing demand","text":"myForest = buildForest(xtrain,ytrain, bestNTrees, maxDepth=bestMaxDepth,maxFeatures=bestMaxFeatures,minRecords=bestMinRecords,β=bestβ,oob=true,rng=copy(FIXEDRNG));\nnothing #hide","category":"page"},{"location":"tutorials/Regression - bike sharing/betaml_tutorial_regression_sharingBikes.html","page":"A regression task: the prediction of  bike  sharing demand","title":"A regression task: the prediction of  bike  sharing demand","text":"Let's now benchmark of the training of BetaML Random Forest model","category":"page"},{"location":"tutorials/Regression - bike sharing/betaml_tutorial_regression_sharingBikes.html","page":"A regression task: the prediction of  bike  sharing demand","title":"A regression task: the prediction of  bike  sharing demand","text":"@btime buildForest(xtrain,ytrain, bestNTrees, maxDepth=bestMaxDepth,maxFeatures=bestMaxFeatures,minRecords=bestMinRecords,β=bestβ,oob=true,rng=copy(FIXEDRNG));\nnothing #hide","category":"page"},{"location":"tutorials/Regression - bike sharing/betaml_tutorial_regression_sharingBikes.html","page":"A regression task: the prediction of  bike  sharing demand","title":"A regression task: the prediction of  bike  sharing demand","text":"Random forests are evidently slower than individual decision trees but are still relativly fast. We should also consider that they are by default efficiently parallelised, so their speed increases with the number of available cores (in building this documentation page, GitHub allows for a single core).","category":"page"},{"location":"tutorials/Regression - bike sharing/betaml_tutorial_regression_sharingBikes.html","page":"A regression task: the prediction of  bike  sharing demand","title":"A regression task: the prediction of  bike  sharing demand","text":"Random forests support the so-called \"out-of-bag\" error, an estimation of the error that we would have when the model is applied on a testing sample. However in this case the oob reported is much smaller than the testing error we will find. This is due to the fact that the division between training/validation and testing in this exercise is not random, but has a temporal basis. It seems that in this example the data in validation/testing follows a different pattern/variance than those in training (in probabilistic terms, they are not i.i.d.).","category":"page"},{"location":"tutorials/Regression - bike sharing/betaml_tutorial_regression_sharingBikes.html","page":"A regression task: the prediction of  bike  sharing demand","title":"A regression task: the prediction of  bike  sharing demand","text":"oobError, trueTestMeanRelativeError  = myForest.oobError,meanRelError(ŷtest,ytest,normRec=true)","category":"page"},{"location":"tutorials/Regression - bike sharing/betaml_tutorial_regression_sharingBikes.html","page":"A regression task: the prediction of  bike  sharing demand","title":"A regression task: the prediction of  bike  sharing demand","text":"(ŷtrain,ŷval,ŷtest)         = predict.([myForest], [xtrain,xval,xtest])\n(rmeTrain, rmeVal, rmeTest) = meanRelError.([ŷtrain,ŷval,ŷtest],[ytrain,yval,ytest],normRec=false)","category":"page"},{"location":"tutorials/Regression - bike sharing/betaml_tutorial_regression_sharingBikes.html","page":"A regression task: the prediction of  bike  sharing demand","title":"A regression task: the prediction of  bike  sharing demand","text":"In this case we found an error very similar to the one employing a single decision tree. Let's print the observed data vs the estimated one using the random forest and then along the temporal axis:","category":"page"},{"location":"tutorials/Regression - bike sharing/betaml_tutorial_regression_sharingBikes.html","page":"A regression task: the prediction of  bike  sharing demand","title":"A regression task: the prediction of  bike  sharing demand","text":"scatter(ytrain,ŷtrain,xlabel=\"daily rides\",ylabel=\"est. daily rides\",label=nothing,title=\"Est vs. obs in training period (RF)\")","category":"page"},{"location":"tutorials/Regression - bike sharing/betaml_tutorial_regression_sharingBikes.html","page":"A regression task: the prediction of  bike  sharing demand","title":"A regression task: the prediction of  bike  sharing demand","text":"scatter(yval,ŷval,xlabel=\"daily rides\",ylabel=\"est. daily rides\",label=nothing,title=\"Est vs. obs in validation period (RF)\")","category":"page"},{"location":"tutorials/Regression - bike sharing/betaml_tutorial_regression_sharingBikes.html","page":"A regression task: the prediction of  bike  sharing demand","title":"A regression task: the prediction of  bike  sharing demand","text":"scatter(ytest,ŷtest,xlabel=\"daily rides\",ylabel=\"est. daily rides\",label=nothing,title=\"Est vs. obs in testing period (RF)\")","category":"page"},{"location":"tutorials/Regression - bike sharing/betaml_tutorial_regression_sharingBikes.html","page":"A regression task: the prediction of  bike  sharing demand","title":"A regression task: the prediction of  bike  sharing demand","text":"Full period plot (2 years):","category":"page"},{"location":"tutorials/Regression - bike sharing/betaml_tutorial_regression_sharingBikes.html","page":"A regression task: the prediction of  bike  sharing demand","title":"A regression task: the prediction of  bike  sharing demand","text":"ŷtrainfull = vcat(ŷtrain,fill(missing,nval+ntest))\nŷvalfull   = vcat(fill(missing,ntrain), ŷval, fill(missing,ntest))\nŷtestfull  = vcat(fill(missing,ntrain+nval), ŷtest)\nplot(data[:,:dteday],[data[:,:cnt] ŷtrainfull ŷvalfull ŷtestfull], label=[\"obs\" \"train\" \"val\" \"test\"], legend=:topleft, ylabel=\"daily rides\", title=\"Daily bike sharing demand observed/estimated across the\\n whole 2-years period (RF)\")","category":"page"},{"location":"tutorials/Regression - bike sharing/betaml_tutorial_regression_sharingBikes.html","page":"A regression task: the prediction of  bike  sharing demand","title":"A regression task: the prediction of  bike  sharing demand","text":"Focus on the testing period:","category":"page"},{"location":"tutorials/Regression - bike sharing/betaml_tutorial_regression_sharingBikes.html","page":"A regression task: the prediction of  bike  sharing demand","title":"A regression task: the prediction of  bike  sharing demand","text":"stc = 620\nendc = size(x,1)\nplot(data[stc:endc,:dteday],[data[stc:endc,:cnt] ŷvalfull[stc:endc] ŷtestfull[stc:endc]], label=[\"obs\" \"val\" \"test\"], legend=:bottomleft, ylabel=\"Daily rides\", title=\"Focus on the testing period (RF)\")","category":"page"},{"location":"tutorials/Regression - bike sharing/betaml_tutorial_regression_sharingBikes.html#Comparison-with-DecisionTree.jl-random-forest","page":"A regression task: the prediction of  bike  sharing demand","title":"Comparison with DecisionTree.jl random forest","text":"","category":"section"},{"location":"tutorials/Regression - bike sharing/betaml_tutorial_regression_sharingBikes.html","page":"A regression task: the prediction of  bike  sharing demand","title":"A regression task: the prediction of  bike  sharing demand","text":"We now compare our results with those obtained employing the same model in the DecisionTree package, using the default suggested hyperparameters:","category":"page"},{"location":"tutorials/Regression - bike sharing/betaml_tutorial_regression_sharingBikes.html","page":"A regression task: the prediction of  bike  sharing demand","title":"A regression task: the prediction of  bike  sharing demand","text":"Hyperparameters of the DecisionTree.jl random forest model","category":"page"},{"location":"tutorials/Regression - bike sharing/betaml_tutorial_regression_sharingBikes.html","page":"A regression task: the prediction of  bike  sharing demand","title":"A regression task: the prediction of  bike  sharing demand","text":"n_subfeatures=-1; n_trees=bestNTrees; partial_sampling=1; max_depth=26\nmin_samples_leaf=bestMinRecords; min_samples_split=bestMinRecords; min_purity_increase=0.0; seed=3","category":"page"},{"location":"tutorials/Regression - bike sharing/betaml_tutorial_regression_sharingBikes.html","page":"A regression task: the prediction of  bike  sharing demand","title":"A regression task: the prediction of  bike  sharing demand","text":"We train the model..","category":"page"},{"location":"tutorials/Regression - bike sharing/betaml_tutorial_regression_sharingBikes.html","page":"A regression task: the prediction of  bike  sharing demand","title":"A regression task: the prediction of  bike  sharing demand","text":"model = DecisionTree.build_forest(ytrain, convert(Matrix,xtrain),\n                     n_subfeatures,\n                     n_trees,\n                     partial_sampling,\n                     max_depth,\n                     min_samples_leaf,\n                     min_samples_split,\n                     min_purity_increase;\n                     rng = seed)","category":"page"},{"location":"tutorials/Regression - bike sharing/betaml_tutorial_regression_sharingBikes.html","page":"A regression task: the prediction of  bike  sharing demand","title":"A regression task: the prediction of  bike  sharing demand","text":"And we generate predictions and measure their error","category":"page"},{"location":"tutorials/Regression - bike sharing/betaml_tutorial_regression_sharingBikes.html","page":"A regression task: the prediction of  bike  sharing demand","title":"A regression task: the prediction of  bike  sharing demand","text":"(ŷtrain,ŷval,ŷtest) = DecisionTree.apply_forest.([model],[xtrain,xval,xtest]);\nnothing #hide","category":"page"},{"location":"tutorials/Regression - bike sharing/betaml_tutorial_regression_sharingBikes.html","page":"A regression task: the prediction of  bike  sharing demand","title":"A regression task: the prediction of  bike  sharing demand","text":"Let's benchmark the DecisionTrees.jl Random Forest training","category":"page"},{"location":"tutorials/Regression - bike sharing/betaml_tutorial_regression_sharingBikes.html","page":"A regression task: the prediction of  bike  sharing demand","title":"A regression task: the prediction of  bike  sharing demand","text":"@btime  DecisionTree.build_forest(ytrain, convert(Matrix,xtrain),\n                     n_subfeatures,\n                     n_trees,\n                     partial_sampling,\n                     max_depth,\n                     min_samples_leaf,\n                     min_samples_split,\n                     min_purity_increase;\n                     rng = seed);\nnothing #hide","category":"page"},{"location":"tutorials/Regression - bike sharing/betaml_tutorial_regression_sharingBikes.html","page":"A regression task: the prediction of  bike  sharing demand","title":"A regression task: the prediction of  bike  sharing demand","text":"DecisionTrees.jl makes a good job in optimising the Random Forest algorithm, as it is over 3 times faster that BetaML.","category":"page"},{"location":"tutorials/Regression - bike sharing/betaml_tutorial_regression_sharingBikes.html","page":"A regression task: the prediction of  bike  sharing demand","title":"A regression task: the prediction of  bike  sharing demand","text":"(rmeTrain, rmeVal, rmeTest) = meanRelError.([ŷtrain,ŷval,ŷtest],[ytrain,yval,ytest],normRec=false)","category":"page"},{"location":"tutorials/Regression - bike sharing/betaml_tutorial_regression_sharingBikes.html","page":"A regression task: the prediction of  bike  sharing demand","title":"A regression task: the prediction of  bike  sharing demand","text":"However the error on the test set remains relativly high. The very low error level on the training set is a sign that it overspecialised on the training set, and we should have better ran a dedicated hyperparameter optimisation for the model.","category":"page"},{"location":"tutorials/Regression - bike sharing/betaml_tutorial_regression_sharingBikes.html","page":"A regression task: the prediction of  bike  sharing demand","title":"A regression task: the prediction of  bike  sharing demand","text":"Finally we plot the DecisionTree.jl predictions alongside the observed value:","category":"page"},{"location":"tutorials/Regression - bike sharing/betaml_tutorial_regression_sharingBikes.html","page":"A regression task: the prediction of  bike  sharing demand","title":"A regression task: the prediction of  bike  sharing demand","text":"ŷtrainfull = vcat(ŷtrain,fill(missing,nval+ntest))\nŷvalfull   = vcat(fill(missing,ntrain), ŷval, fill(missing,ntest))\nŷtestfull  = vcat(fill(missing,ntrain+nval), ŷtest)\nplot(data[:,:dteday],[data[:,:cnt] ŷtrainfull ŷvalfull ŷtestfull], label=[\"obs\" \"train\" \"val\" \"test\"], legend=:topleft, ylabel=\"daily rides\", title=\"Daily bike sharing demand observed/estimated across the\\n whole 2-years period (DT.jl RF)\")","category":"page"},{"location":"tutorials/Regression - bike sharing/betaml_tutorial_regression_sharingBikes.html","page":"A regression task: the prediction of  bike  sharing demand","title":"A regression task: the prediction of  bike  sharing demand","text":"Again, focusing on the testing data:","category":"page"},{"location":"tutorials/Regression - bike sharing/betaml_tutorial_regression_sharingBikes.html","page":"A regression task: the prediction of  bike  sharing demand","title":"A regression task: the prediction of  bike  sharing demand","text":"stc  = 620\nendc = size(x,1)\nplot(data[stc:endc,:dteday],[data[stc:endc,:cnt] ŷvalfull[stc:endc] ŷtestfull[stc:endc]], label=[\"obs\" \"val\" \"test\"], legend=:bottomleft, ylabel=\"Daily rides\", title=\"Focus on the testing period (DT.jl RF)\")","category":"page"},{"location":"tutorials/Regression - bike sharing/betaml_tutorial_regression_sharingBikes.html#Conclusions-of-Decision-Trees-/-Random-Forests-methods","page":"A regression task: the prediction of  bike  sharing demand","title":"Conclusions of Decision Trees / Random Forests methods","text":"","category":"section"},{"location":"tutorials/Regression - bike sharing/betaml_tutorial_regression_sharingBikes.html","page":"A regression task: the prediction of  bike  sharing demand","title":"A regression task: the prediction of  bike  sharing demand","text":"The error obtained employing DecisionTree.jl is significantly larger than those obtained with the BetaML random forest model, altought to be fair with DecisionTrees.jl we didn't tuned its hyper-parameters. Also, DecisionTree.jl random forest model is much faster. This is partially due by the fact that internally DecisionTree.jl models optimise the algorithm by sorting the observations. BetaML trees/forests don't employ this optimisation and hence it can work with true categorical data for which ordering is not defined. An other explanation of this difference in speed is that BetaML Random Forest models accept missing values within the feature matrix. To sum up, BetaML random forests are ideal algorithms when we want to obtain good predictions in the most simpler way, even without tuning the hyperparameters, and without spending time in cleaning (\"munging\") the feature matrix, as they accept almost \"any kind\" of data as it is.","category":"page"},{"location":"tutorials/Regression - bike sharing/betaml_tutorial_regression_sharingBikes.html#Neural-Networks","page":"A regression task: the prediction of  bike  sharing demand","title":"Neural Networks","text":"","category":"section"},{"location":"tutorials/Regression - bike sharing/betaml_tutorial_regression_sharingBikes.html","page":"A regression task: the prediction of  bike  sharing demand","title":"A regression task: the prediction of  bike  sharing demand","text":"BetaML provides only deep forward neural networks, artificial neural network units where the individual \"nodes\" are arranged in layers, from the input layer, where each unit holds the input coordinate, through various hidden layer transformations, until the actual output of the model:","category":"page"},{"location":"tutorials/Regression - bike sharing/betaml_tutorial_regression_sharingBikes.html","page":"A regression task: the prediction of  bike  sharing demand","title":"A regression task: the prediction of  bike  sharing demand","text":"(Image: Neural Networks)","category":"page"},{"location":"tutorials/Regression - bike sharing/betaml_tutorial_regression_sharingBikes.html","page":"A regression task: the prediction of  bike  sharing demand","title":"A regression task: the prediction of  bike  sharing demand","text":"In this layerwise computation, each unit in a particular layer takes input from all the preceding layer units and it has its own parameters that are adjusted to perform the overall computation. The training of the network consists in retrieving the coefficients that minimise a loss function betwenn the output of the model and the known data. In particular, a deep (feedforward) neural network refers to a neural network that contains not only the input and output layers, but also hidden layers in between.","category":"page"},{"location":"tutorials/Regression - bike sharing/betaml_tutorial_regression_sharingBikes.html","page":"A regression task: the prediction of  bike  sharing demand","title":"A regression task: the prediction of  bike  sharing demand","text":"Neural networks accept only numerical inputs. We hence need to convert all categorical data in numerical units. A common approach is to use the so-called \"one-hot-encoding\" where the catagorical values are converted into indicator variables (0/1), one for each possible value. This can be done in BetaML using the oneHotEncoder function:","category":"page"},{"location":"tutorials/Regression - bike sharing/betaml_tutorial_regression_sharingBikes.html","page":"A regression task: the prediction of  bike  sharing demand","title":"A regression task: the prediction of  bike  sharing demand","text":"seasonDummies  = convert(Array{Float64,2},oneHotEncoder(data[:,:season]))\nweatherDummies = convert(Array{Float64,2},oneHotEncoder(data[:,:weathersit]))\nwdayDummies    = convert(Array{Float64,2},oneHotEncoder(data[:,:weekday] .+ 1 ))\n\n# We compose the feature matrix with the new dimensions obtained from the oneHotEncoder functions\nx = hcat(Matrix{Float64}(data[:,[:instant,:yr,:mnth,:holiday,:workingday,:temp,:atemp,:hum,:windspeed]]),\n         seasonDummies,\n         weatherDummies,\n         wdayDummies)\ny = data[:,16];\nnothing #hide","category":"page"},{"location":"tutorials/Regression - bike sharing/betaml_tutorial_regression_sharingBikes.html","page":"A regression task: the prediction of  bike  sharing demand","title":"A regression task: the prediction of  bike  sharing demand","text":"As usual, we split the data in training, validation and testing sets","category":"page"},{"location":"tutorials/Regression - bike sharing/betaml_tutorial_regression_sharingBikes.html","page":"A regression task: the prediction of  bike  sharing demand","title":"A regression task: the prediction of  bike  sharing demand","text":"((xtrain,xval,xtest),(ytrain,yval,ytest)) = partition([x,y],[0.75,0.125,1-0.75-0.125],shuffle=false)\n(ntrain, nval, ntest) = size.([ytrain,yval,ytest],1)","category":"page"},{"location":"tutorials/Regression - bike sharing/betaml_tutorial_regression_sharingBikes.html","page":"A regression task: the prediction of  bike  sharing demand","title":"A regression task: the prediction of  bike  sharing demand","text":"An other common operation with neural networks is to scale the feature vectors (X) and the labels (Y). The BetaML scale() function, by default, scale the data such that each dimension has mean 0 and variance 1. Note that we can provide the function with different scale factors or specify the columns not to scale (e.g. those resulting from the one-hot encoding). Finally we can reverse the scaling (this is useful to retrieve the unscaled features from a model trained with scaled ones).","category":"page"},{"location":"tutorials/Regression - bike sharing/betaml_tutorial_regression_sharingBikes.html","page":"A regression task: the prediction of  bike  sharing demand","title":"A regression task: the prediction of  bike  sharing demand","text":"colsNotToScale = [2;4;5;10:23]\nxScaleFactors   = getScaleFactors(xtrain,skip=colsNotToScale)\nyScaleFactors   = ([0],[0.001]) # getScaleFactors(ytrain) # This just divide by 1000. Using full scaling of Y we may get negative demand.\nxtrainScaled    = scale(xtrain,xScaleFactors)\nxvalScaled      = scale(xval,xScaleFactors)\nxtestScaled     = scale(xtest,xScaleFactors)\nytrainScaled    = scale(ytrain,yScaleFactors)\nyvalScaled      = scale(yval,yScaleFactors)\nytestScaled     = scale(ytest,yScaleFactors)\nD               = size(xtrain,2)","category":"page"},{"location":"tutorials/Regression - bike sharing/betaml_tutorial_regression_sharingBikes.html","page":"A regression task: the prediction of  bike  sharing demand","title":"A regression task: the prediction of  bike  sharing demand","text":"As before, we select the best hyperparameters by using the validation set (it may take a while)...","category":"page"},{"location":"tutorials/Regression - bike sharing/betaml_tutorial_regression_sharingBikes.html","page":"A regression task: the prediction of  bike  sharing demand","title":"A regression task: the prediction of  bike  sharing demand","text":"function tuneHyperParameters(xtrain,ytrain,xval,yval;epochRange=50:50,hiddenLayerSizeRange=12:12,repetitions=5,rng=Random.GLOBAL_RNG)\n    # We start with an infinititly high error\n    bestRme         = +Inf\n    bestEpoch       = 0\n    bestSize        = 0\n    compLock        = ReentrantLock()\n\n    # Generate one random number generator per thread\n    masterSeed = rand(rng,100:9999999999999) ## Some RNG have problems with very small seed. Also, the master seed has to be computed _before_ generateParallelRngs\n    rngs       = generateParallelRngs(rng,Threads.nthreads())\n\n    # We loop over all possible hyperparameter combinations...\n    parLengths = (length(epochRange),length(hiddenLayerSizeRange))\n    Threads.@threads for ij in CartesianIndices(parLengths)\n       (epoch,hiddenLayerSize)   = (epochRange[Tuple(ij)[1]], hiddenLayerSizeRange[Tuple(ij)[2]])\n       tsrng = rngs[Threads.threadid()]\n       joinedIndx = LinearIndices(parLengths)[ij]\n       # And here we make the seeding depending on the i of the loop, not the thread: hence we get the same results indipendently of the number of threads\n       Random.seed!(tsrng,masterSeed+joinedIndx*10)\n       totAttemptError = 0.0\n       println(\"Testing epochs $epoch, layer size $hiddenLayerSize ...\")\n       # We run several repetitions with the same hyperparameter combination to account for stochasticity...\n       for r in 1:repetitions\n           l1   = DenseLayer(D,hiddenLayerSize,f=relu,rng=tsrng) # Activation function is ReLU\n           l2   = DenseLayer(hiddenLayerSize,hiddenLayerSize,f=identity,rng=tsrng)\n           l3   = DenseLayer(hiddenLayerSize,1,f=relu,rng=tsrng)\n           mynn = buildNetwork([l1,l2,l3],squaredCost,name=\"Bike sharing regression model\") # Build the NN and use the squared cost (aka MSE) as error function\n           # Training it (default to ADAM)\n           res  = train!(mynn,xtrain,ytrain,epochs=epoch,batchSize=8,optAlg=ADAM(),verbosity=NONE, rng=tsrng) # Use optAlg=SGD() to use Stochastic Gradient Descent\n           ŷval = predict(mynn,xval)\n           rmeVal  = meanRelError(ŷval,yval,normRec=false)\n           totAttemptError += rmeVal\n       end\n       avgRme = totAttemptError / repetitions\n       begin\n           lock(compLock) ## This step can't be run in parallel...\n           try\n               # Select this specific combination of hyperparameters if the error is the lowest\n               if avgRme < bestRme\n                 bestRme    = avgRme\n                 bestEpoch  = epoch\n                 bestSize   = hiddenLayerSize\n               end\n           finally\n               unlock(compLock)\n           end\n       end\n    end\n    return (bestRme=bestRme,bestEpoch=bestEpoch,bestSize=bestSize)\nend\n\nepochsToTest     = [100,400]\nhiddenLayerSizes = [5,15,30]\n(bestRme,bestEpoch,bestSize) = tuneHyperParameters(xtrainScaled,ytrainScaled,xvalScaled,yvalScaled;epochRange=epochsToTest,hiddenLayerSizeRange=hiddenLayerSizes,repetitions=3,rng=copy(FIXEDRNG))","category":"page"},{"location":"tutorials/Regression - bike sharing/betaml_tutorial_regression_sharingBikes.html","page":"A regression task: the prediction of  bike  sharing demand","title":"A regression task: the prediction of  bike  sharing demand","text":"We now build our feed-forward neaural network. We create three layers, the first layers will always have a input size equal to the dimensions of our data (the number of columns), and the output layer, for a simple regression where the predictions are scalars, it will always be one. There are already several kind of layers available (and you can build your own kind by defining a new struct and implementing a few functions. See the Nn module documentation for details). Here we use only dense layers, those found in typycal feed-fordward neural networks. For each layer, on top of its size (in \"neurons\") we can specify an activation function. Here we use the relu for the two terminal layers (this will guarantee that our predictions are always positive) and identity for the hidden layer. Again, consult the Nn module documentation for other activation layers already defined, or use any function of your choice. Initial weight parameters can also be specified if needed. By default DenseLayer use the so-called Xavier initialisation.","category":"page"},{"location":"tutorials/Regression - bike sharing/betaml_tutorial_regression_sharingBikes.html","page":"A regression task: the prediction of  bike  sharing demand","title":"A regression task: the prediction of  bike  sharing demand","text":"l1   = DenseLayer(D,bestSize,f=relu,rng=copy(FIXEDRNG)) # Activation function is ReLU\nl2   = DenseLayer(bestSize,bestSize,f=identity,rng=copy(FIXEDRNG))\nl3   = DenseLayer(bestSize,1,f=relu,rng=copy(FIXEDRNG))","category":"page"},{"location":"tutorials/Regression - bike sharing/betaml_tutorial_regression_sharingBikes.html","page":"A regression task: the prediction of  bike  sharing demand","title":"A regression task: the prediction of  bike  sharing demand","text":"Finally we \"chain\" the layer together and we assign a final loss function (agian, you can provide your own, if those available in BetaML don't suit your needs) in order to compose the \"neural network\" object.","category":"page"},{"location":"tutorials/Regression - bike sharing/betaml_tutorial_regression_sharingBikes.html","page":"A regression task: the prediction of  bike  sharing demand","title":"A regression task: the prediction of  bike  sharing demand","text":"mynn = buildNetwork([l1,l2,l3],squaredCost,name=\"Bike sharing regression model\") ## Build the NN and use the squared cost (aka MSE) as error function","category":"page"},{"location":"tutorials/Regression - bike sharing/betaml_tutorial_regression_sharingBikes.html","page":"A regression task: the prediction of  bike  sharing demand","title":"A regression task: the prediction of  bike  sharing demand","text":"The above neural network will use automatic differentiation (using the Zygote package) to compute the gradient to minimise in the training step. Using manual differentiaiton, for the layers that support it, is however really simple. The network below is exactly equivalent to the one above, except it avoids automatic differentiation:","category":"page"},{"location":"tutorials/Regression - bike sharing/betaml_tutorial_regression_sharingBikes.html","page":"A regression task: the prediction of  bike  sharing demand","title":"A regression task: the prediction of  bike  sharing demand","text":"mynnManual = buildNetwork([\n        DenseLayer(D,bestSize,f=relu,df=drelu,rng=copy(FIXEDRNG)),\n        DenseLayer(bestSize,bestSize,f=identity,df=didentity,rng=copy(FIXEDRNG)),\n        DenseLayer(bestSize,1,f=relu,df=drelu,rng=copy(FIXEDRNG))\n    ], squaredCost, name=\"Bike sharing regression model\", dcf=dSquaredCost)","category":"page"},{"location":"tutorials/Regression - bike sharing/betaml_tutorial_regression_sharingBikes.html","page":"A regression task: the prediction of  bike  sharing demand","title":"A regression task: the prediction of  bike  sharing demand","text":"We can now re-do the training with the best hyperparameters. Several optimisation algorithms are available, and each accepts different parameters, like the learning rate for the Stochastic Gradient Descent algorithm (used by default) or the exponential decay rates for the  moments estimates for the ADAM algorithm (that we use here, with the default parameters).","category":"page"},{"location":"tutorials/Regression - bike sharing/betaml_tutorial_regression_sharingBikes.html","page":"A regression task: the prediction of  bike  sharing demand","title":"A regression task: the prediction of  bike  sharing demand","text":"println(\"Final training of $bestEpoch epochs, with layer size $bestSize ...\")\nres  = train!(mynn,xtrainScaled,ytrainScaled,epochs=bestEpoch,batchSize=8,optAlg=ADAM(),rng=copy(FIXEDRNG)) ## Use optAlg=SGD() to use Stochastic Gradient Descent","category":"page"},{"location":"tutorials/Regression - bike sharing/betaml_tutorial_regression_sharingBikes.html","page":"A regression task: the prediction of  bike  sharing demand","title":"A regression task: the prediction of  bike  sharing demand","text":"Let's benchmark the BetaML neural network training","category":"page"},{"location":"tutorials/Regression - bike sharing/betaml_tutorial_regression_sharingBikes.html","page":"A regression task: the prediction of  bike  sharing demand","title":"A regression task: the prediction of  bike  sharing demand","text":"@btime train!(mynnManual,xtrainScaled,ytrainScaled,epochs=bestEpoch,batchSize=8,optAlg=ADAM(),rng=copy(FIXEDRNG), verbosity=NONE);\nnothing #hide","category":"page"},{"location":"tutorials/Regression - bike sharing/betaml_tutorial_regression_sharingBikes.html","page":"A regression task: the prediction of  bike  sharing demand","title":"A regression task: the prediction of  bike  sharing demand","text":"As we can see the model training is one order of magnitude slower than random forests, altought the memory requirement is approximatly the same","category":"page"},{"location":"tutorials/Regression - bike sharing/betaml_tutorial_regression_sharingBikes.html","page":"A regression task: the prediction of  bike  sharing demand","title":"A regression task: the prediction of  bike  sharing demand","text":"To obtain the neural network predictions we apply the function predict to the feature matrix X for which we want to generate previsions, and then, in order to obtain the unscaled unscaled estimates we use the scale function applied to the scaled values with the original scaling factors and the parameter rev set to true. Note the usage of the pipe operator to avoid ugly function1(function2(function3(...))) nested calls:","category":"page"},{"location":"tutorials/Regression - bike sharing/betaml_tutorial_regression_sharingBikes.html","page":"A regression task: the prediction of  bike  sharing demand","title":"A regression task: the prediction of  bike  sharing demand","text":"ŷtrain = @pipe predict(mynn,xtrainScaled) |> scale(_, yScaleFactors,rev=true);\nŷval   = @pipe predict(mynn,xvalScaled)   |> scale(_, yScaleFactors,rev=true);\nŷtest  = @pipe predict(mynn,xtestScaled)  |> scale(_ ,yScaleFactors,rev=true);\nnothing #hide","category":"page"},{"location":"tutorials/Regression - bike sharing/betaml_tutorial_regression_sharingBikes.html","page":"A regression task: the prediction of  bike  sharing demand","title":"A regression task: the prediction of  bike  sharing demand","text":"(mreTrain, mreVal, mreTest) = meanRelError.([ŷtrain,ŷval,ŷtest],[ytrain,yval,ytest],normRec=false)","category":"page"},{"location":"tutorials/Regression - bike sharing/betaml_tutorial_regression_sharingBikes.html","page":"A regression task: the prediction of  bike  sharing demand","title":"A regression task: the prediction of  bike  sharing demand","text":"The error is much lower. Let's plot our predictions:","category":"page"},{"location":"tutorials/Regression - bike sharing/betaml_tutorial_regression_sharingBikes.html","page":"A regression task: the prediction of  bike  sharing demand","title":"A regression task: the prediction of  bike  sharing demand","text":"Again, we can start by plotting the estimated vs the observed value:","category":"page"},{"location":"tutorials/Regression - bike sharing/betaml_tutorial_regression_sharingBikes.html","page":"A regression task: the prediction of  bike  sharing demand","title":"A regression task: the prediction of  bike  sharing demand","text":"scatter(ytrain,ŷtrain,xlabel=\"daily rides\",ylabel=\"est. daily rides\",label=nothing,title=\"Est vs. obs in training period (NN)\")","category":"page"},{"location":"tutorials/Regression - bike sharing/betaml_tutorial_regression_sharingBikes.html","page":"A regression task: the prediction of  bike  sharing demand","title":"A regression task: the prediction of  bike  sharing demand","text":"scatter(yval,ŷval,xlabel=\"daily rides\",ylabel=\"est. daily rides\",label=nothing,title=\"Est vs. obs in validation period (NN)\")","category":"page"},{"location":"tutorials/Regression - bike sharing/betaml_tutorial_regression_sharingBikes.html","page":"A regression task: the prediction of  bike  sharing demand","title":"A regression task: the prediction of  bike  sharing demand","text":"scatter(ytest,ŷtest,xlabel=\"daily rides\",ylabel=\"est. daily rides\",label=nothing,title=\"Est vs. obs in testing period (NN)\")","category":"page"},{"location":"tutorials/Regression - bike sharing/betaml_tutorial_regression_sharingBikes.html","page":"A regression task: the prediction of  bike  sharing demand","title":"A regression task: the prediction of  bike  sharing demand","text":"We now plot across the time dimension, first plotting the whole period (2 years):","category":"page"},{"location":"tutorials/Regression - bike sharing/betaml_tutorial_regression_sharingBikes.html","page":"A regression task: the prediction of  bike  sharing demand","title":"A regression task: the prediction of  bike  sharing demand","text":"ŷtrainfull = vcat(ŷtrain,fill(missing,nval+ntest))\nŷvalfull   = vcat(fill(missing,ntrain), ŷval, fill(missing,ntest))\nŷtestfull  = vcat(fill(missing,ntrain+nval), ŷtest)\nplot(data[:,:dteday],[data[:,:cnt] ŷtrainfull ŷvalfull ŷtestfull], label=[\"obs\" \"train\" \"val\" \"test\"], legend=:topleft, ylabel=\"daily rides\", title=\"Daily bike sharing demand observed/estimated across the\\n whole 2-years period  (NN)\")","category":"page"},{"location":"tutorials/Regression - bike sharing/betaml_tutorial_regression_sharingBikes.html","page":"A regression task: the prediction of  bike  sharing demand","title":"A regression task: the prediction of  bike  sharing demand","text":"...and then focusing on the testing data","category":"page"},{"location":"tutorials/Regression - bike sharing/betaml_tutorial_regression_sharingBikes.html","page":"A regression task: the prediction of  bike  sharing demand","title":"A regression task: the prediction of  bike  sharing demand","text":"stc  = 620\nendc = size(x,1)\nplot(data[stc:endc,:dteday],[data[stc:endc,:cnt] ŷvalfull[stc:endc] ŷtestfull[stc:endc]], label=[\"obs\" \"val\" \"test\"], legend=:bottomleft, ylabel=\"Daily rides\", title=\"Focus on the testing period (NN)\")","category":"page"},{"location":"tutorials/Regression - bike sharing/betaml_tutorial_regression_sharingBikes.html#Comparison-with-Flux","page":"A regression task: the prediction of  bike  sharing demand","title":"Comparison with Flux","text":"","category":"section"},{"location":"tutorials/Regression - bike sharing/betaml_tutorial_regression_sharingBikes.html","page":"A regression task: the prediction of  bike  sharing demand","title":"A regression task: the prediction of  bike  sharing demand","text":"We now to apply the same Neural Network model using the Flux framework, a dedicated neural network library. reusing the optimal parameters that we did found in tuneHyperParameters","category":"page"},{"location":"tutorials/Regression - bike sharing/betaml_tutorial_regression_sharingBikes.html","page":"A regression task: the prediction of  bike  sharing demand","title":"A regression task: the prediction of  bike  sharing demand","text":"We fix the default random number generator so that the Flux example gives a reproducible output","category":"page"},{"location":"tutorials/Regression - bike sharing/betaml_tutorial_regression_sharingBikes.html","page":"A regression task: the prediction of  bike  sharing demand","title":"A regression task: the prediction of  bike  sharing demand","text":"Random.seed!(123)","category":"page"},{"location":"tutorials/Regression - bike sharing/betaml_tutorial_regression_sharingBikes.html","page":"A regression task: the prediction of  bike  sharing demand","title":"A regression task: the prediction of  bike  sharing demand","text":"We define the Flux neural network model and load it with data...","category":"page"},{"location":"tutorials/Regression - bike sharing/betaml_tutorial_regression_sharingBikes.html","page":"A regression task: the prediction of  bike  sharing demand","title":"A regression task: the prediction of  bike  sharing demand","text":"l1         = Flux.Dense(D,bestSize,Flux.relu)\nl2         = Flux.Dense(bestSize,bestSize,identity)\nl3         = Flux.Dense(bestSize,1,Flux.relu)\nFlux_nn    = Flux.Chain(l1,l2,l3)\nloss(x, y) = Flux.mse(Flux_nn(x), y)\nps         = Flux.params(Flux_nn)\nnndata     = Flux.Data.DataLoader((xtrainScaled', ytrainScaled'), batchsize=8,shuffle=true)\n\nFlux_nn2   = deepcopy(Flux_nn)      ## A copy for the time benchmarking\nps2        = Flux.params(Flux_nn2)  ## A copy for the time benchmarking","category":"page"},{"location":"tutorials/Regression - bike sharing/betaml_tutorial_regression_sharingBikes.html","page":"A regression task: the prediction of  bike  sharing demand","title":"A regression task: the prediction of  bike  sharing demand","text":"We do the training of the Flux model...","category":"page"},{"location":"tutorials/Regression - bike sharing/betaml_tutorial_regression_sharingBikes.html","page":"A regression task: the prediction of  bike  sharing demand","title":"A regression task: the prediction of  bike  sharing demand","text":"Flux.@epochs bestEpoch Flux.train!(loss, ps, nndata, Flux.ADAM(0.001, (0.9, 0.8)))","category":"page"},{"location":"tutorials/Regression - bike sharing/betaml_tutorial_regression_sharingBikes.html","page":"A regression task: the prediction of  bike  sharing demand","title":"A regression task: the prediction of  bike  sharing demand","text":"..and we benchmark it..","category":"page"},{"location":"tutorials/Regression - bike sharing/betaml_tutorial_regression_sharingBikes.html","page":"A regression task: the prediction of  bike  sharing demand","title":"A regression task: the prediction of  bike  sharing demand","text":"@btime begin for i in 1:bestEpoch Flux.train!(loss, ps2, nndata, Flux.ADAM(0.001, (0.9, 0.8))) end end","category":"page"},{"location":"tutorials/Regression - bike sharing/betaml_tutorial_regression_sharingBikes.html","page":"A regression task: the prediction of  bike  sharing demand","title":"A regression task: the prediction of  bike  sharing demand","text":"On this small example the speed of Flux is on the same order than BetaML (the actual difference seems to depend on the specific RNG seed and hardware), however I suspect that Flux scales much better with larger networks and/or data.","category":"page"},{"location":"tutorials/Regression - bike sharing/betaml_tutorial_regression_sharingBikes.html","page":"A regression task: the prediction of  bike  sharing demand","title":"A regression task: the prediction of  bike  sharing demand","text":"We obtain the estimates...","category":"page"},{"location":"tutorials/Regression - bike sharing/betaml_tutorial_regression_sharingBikes.html","page":"A regression task: the prediction of  bike  sharing demand","title":"A regression task: the prediction of  bike  sharing demand","text":"ŷtrainf = @pipe Flux_nn(xtrainScaled')' |> scale(_,yScaleFactors,rev=true);\nŷvalf   = @pipe Flux_nn(xvalScaled')'   |> scale(_,yScaleFactors,rev=true);\nŷtestf  = @pipe Flux_nn(xtestScaled')'  |> scale(_,yScaleFactors,rev=true);\nnothing #hide","category":"page"},{"location":"tutorials/Regression - bike sharing/betaml_tutorial_regression_sharingBikes.html","page":"A regression task: the prediction of  bike  sharing demand","title":"A regression task: the prediction of  bike  sharing demand","text":"..and we compute the mean relative errors..","category":"page"},{"location":"tutorials/Regression - bike sharing/betaml_tutorial_regression_sharingBikes.html","page":"A regression task: the prediction of  bike  sharing demand","title":"A regression task: the prediction of  bike  sharing demand","text":"(mreTrain, mreVal, mreTest) = meanRelError.([ŷtrainf,ŷvalf,ŷtestf],[ytrain,yval,ytest],normRec=false)","category":"page"},{"location":"tutorials/Regression - bike sharing/betaml_tutorial_regression_sharingBikes.html","page":"A regression task: the prediction of  bike  sharing demand","title":"A regression task: the prediction of  bike  sharing demand","text":".. finding an error not significantly different than the one obtained from BetaML.Nn.","category":"page"},{"location":"tutorials/Regression - bike sharing/betaml_tutorial_regression_sharingBikes.html","page":"A regression task: the prediction of  bike  sharing demand","title":"A regression task: the prediction of  bike  sharing demand","text":"Plots:","category":"page"},{"location":"tutorials/Regression - bike sharing/betaml_tutorial_regression_sharingBikes.html","page":"A regression task: the prediction of  bike  sharing demand","title":"A regression task: the prediction of  bike  sharing demand","text":"scatter(ytrain,ŷtrainf,xlabel=\"daily rides\",ylabel=\"est. daily rides\",label=nothing,title=\"Est vs. obs in training period (Flux.NN)\")","category":"page"},{"location":"tutorials/Regression - bike sharing/betaml_tutorial_regression_sharingBikes.html","page":"A regression task: the prediction of  bike  sharing demand","title":"A regression task: the prediction of  bike  sharing demand","text":"scatter(yval,ŷvalf,xlabel=\"daily rides\",ylabel=\"est. daily rides\",label=nothing,title=\"Est vs. obs in validation period (Flux.NN)\")","category":"page"},{"location":"tutorials/Regression - bike sharing/betaml_tutorial_regression_sharingBikes.html","page":"A regression task: the prediction of  bike  sharing demand","title":"A regression task: the prediction of  bike  sharing demand","text":"scatter(ytest,ŷtestf,xlabel=\"daily rides\",ylabel=\"est. daily rides\",label=nothing,title=\"Est vs. obs in testing period (Flux.NN)\")","category":"page"},{"location":"tutorials/Regression - bike sharing/betaml_tutorial_regression_sharingBikes.html","page":"A regression task: the prediction of  bike  sharing demand","title":"A regression task: the prediction of  bike  sharing demand","text":"ŷtrainfullf = vcat(ŷtrainf,fill(missing,nval+ntest))\nŷvalfullf   = vcat(fill(missing,ntrain), ŷvalf, fill(missing,ntest))\nŷtestfullf  = vcat(fill(missing,ntrain+nval), ŷtestf)\nplot(data[:,:dteday],[data[:,:cnt] ŷtrainfullf ŷvalfullf ŷtestfullf], label=[\"obs\" \"train\" \"val\" \"test\"], legend=:topleft, ylabel=\"daily rides\", title=\"Daily bike sharing demand observed/estimated across the\\n whole 2-years period (Flux.NN)\")","category":"page"},{"location":"tutorials/Regression - bike sharing/betaml_tutorial_regression_sharingBikes.html","page":"A regression task: the prediction of  bike  sharing demand","title":"A regression task: the prediction of  bike  sharing demand","text":"stc = 620\nendc = size(x,1)\nplot(data[stc:endc,:dteday],[data[stc:endc,:cnt] ŷvalfullf[stc:endc] ŷtestfullf[stc:endc]], label=[\"obs\" \"val\" \"test\"], legend=:bottomleft, ylabel=\"Daily rides\", title=\"Focus on the testing period (Flux.NN)\")","category":"page"},{"location":"tutorials/Regression - bike sharing/betaml_tutorial_regression_sharingBikes.html#Conclusions-of-Neural-Network-models","page":"A regression task: the prediction of  bike  sharing demand","title":"Conclusions of Neural Network models","text":"","category":"section"},{"location":"tutorials/Regression - bike sharing/betaml_tutorial_regression_sharingBikes.html","page":"A regression task: the prediction of  bike  sharing demand","title":"A regression task: the prediction of  bike  sharing demand","text":"If we strive for the most accurate predictions, deep neural networks usually offer the best solution. However they are computationally expensive, so with limited resourses we may get better results by fine tuning and running many repetitions of \"simpler\" decision trees or even random forest models than a large naural network with insufficient hyperparameter tuning. Also, we shoudl consider that decision trees/random forests are much simple to work with.","category":"page"},{"location":"tutorials/Regression - bike sharing/betaml_tutorial_regression_sharingBikes.html","page":"A regression task: the prediction of  bike  sharing demand","title":"A regression task: the prediction of  bike  sharing demand","text":"That said, specialised neural network libraries, like Flux, allow to use GPU and specialised hardware letting neural networks to scale with very large datasets.","category":"page"},{"location":"tutorials/Regression - bike sharing/betaml_tutorial_regression_sharingBikes.html","page":"A regression task: the prediction of  bike  sharing demand","title":"A regression task: the prediction of  bike  sharing demand","text":"Still, for small and medium datasets, BetaML provides simpler yet customisable solutions that are accurate and fast.","category":"page"},{"location":"tutorials/Regression - bike sharing/betaml_tutorial_regression_sharingBikes.html#Summary","page":"A regression task: the prediction of  bike  sharing demand","title":"Summary","text":"","category":"section"},{"location":"tutorials/Regression - bike sharing/betaml_tutorial_regression_sharingBikes.html","page":"A regression task: the prediction of  bike  sharing demand","title":"A regression task: the prediction of  bike  sharing demand","text":"This is the summary of the results we had trying to predict the daily bike sharing demand, given weather and calendar information of the day","category":"page"},{"location":"tutorials/Regression - bike sharing/betaml_tutorial_regression_sharingBikes.html","page":"A regression task: the prediction of  bike  sharing demand","title":"A regression task: the prediction of  bike  sharing demand","text":"Model Train rme Test rme Training time (ms)* Training mem (MB)\nDT 0.1266 0.2223 26.5 58\nRF 0.0651 0.2223 362 971\nRF (DecisionTree.jl) 0.0312 0.3142 36 11\nNN 0.0884 0.1761 1768 758\nNN (Flux.jl) 0.0981 0.1618 1708 282","category":"page"},{"location":"tutorials/Regression - bike sharing/betaml_tutorial_regression_sharingBikes.html","page":"A regression task: the prediction of  bike  sharing demand","title":"A regression task: the prediction of  bike  sharing demand","text":"on a Intel Core i5-8350U laptop","category":"page"},{"location":"tutorials/Regression - bike sharing/betaml_tutorial_regression_sharingBikes.html","page":"A regression task: the prediction of  bike  sharing demand","title":"A regression task: the prediction of  bike  sharing demand","text":"Neural networks can be more precise than random forests models, but are more computationally expensive (and tricky to set up). When we compare BetaML with the algorithm-specific leading packages, we found similar results in terms of accuracy, but often the leading packages are better optimised and run more efficiently (but sometimes at the cost of being less verstatile).","category":"page"},{"location":"tutorials/Regression - bike sharing/betaml_tutorial_regression_sharingBikes.html","page":"A regression task: the prediction of  bike  sharing demand","title":"A regression task: the prediction of  bike  sharing demand","text":"View this file on Github.","category":"page"},{"location":"tutorials/Regression - bike sharing/betaml_tutorial_regression_sharingBikes.html","page":"A regression task: the prediction of  bike  sharing demand","title":"A regression task: the prediction of  bike  sharing demand","text":"","category":"page"},{"location":"tutorials/Regression - bike sharing/betaml_tutorial_regression_sharingBikes.html","page":"A regression task: the prediction of  bike  sharing demand","title":"A regression task: the prediction of  bike  sharing demand","text":"This page was generated using Literate.jl.","category":"page"},{"location":"Clustering.html#The-BetaML.Clustering-Module","page":"Clustering","title":"The BetaML.Clustering Module","text":"","category":"section"},{"location":"Clustering.html","page":"Clustering","title":"Clustering","text":"Clustering\n","category":"page"},{"location":"Clustering.html#BetaML.Clustering","page":"Clustering","title":"BetaML.Clustering","text":"Clustering module (WIP)\n\nProvide clustering methods and missing values imputation / collaborative filtering / reccomendation systems using clustering methods as backend.\n\nThe module provides the following functions. Use ?[function] to access their full signature and detailed documentation:\n\ninitRepresentatives(X,K;initStrategy,Z₀): Initialisation strategies for Kmean and Kmedoids\n`kmeans(X,K;dist,initStrategy,Z₀)`: Classical KMean algorithm\n`kmedoids(X,K;dist,initStrategy,Z₀)`: Kmedoids algorithm\n`gmm(X,K;p₀,mixtures,tol,verbosity,minVariance,minCovariance,initStrategy)`: gmm algorithm over GMM\n`predictMissing(X,K;p₀,mixtures,tol,verbosity,minVariance,minCovariance)`: Fill mixing values / collaborative filtering using gmm as backbone\n\n{Spherical|Diagonal|Full}Gaussian mixtures for gmm / predictMissing are already provided. User defined mixtures can be used defining a struct as subtype of Mixture and implementing for that mixture the following functions:\n\ninitMixtures!(mixtures, X; minVariance, minCovariance, initStrategy)\nlpdf(m,x,mask) (for the e-step)\nupdateParameters!(mixtures, X, pₙₖ; minVariance, minCovariance) (the m-step)\n\n\n\n\n\n","category":"module"},{"location":"Clustering.html#Module-Index","page":"Clustering","title":"Module Index","text":"","category":"section"},{"location":"Clustering.html","page":"Clustering","title":"Clustering","text":"Modules = [Clustering]\nOrder   = [:constant, :type, :function, :macro]","category":"page"},{"location":"Clustering.html#Detailed-API","page":"Clustering","title":"Detailed API","text":"","category":"section"},{"location":"Clustering.html","page":"Clustering","title":"Clustering","text":"Modules = [Clustering]","category":"page"},{"location":"Clustering.html#BetaML.Clustering.gmm-Tuple{Any, Any}","page":"Clustering","title":"BetaML.Clustering.gmm","text":"gmm(X,K;p₀,mixtures,tol,verbosity,minVariance,minCovariance,initStrategy)\n\nCompute Expectation-Maximisation algorithm to identify K clusters of X data, i.e. employ a Generative Mixture Model as the underlying probabilistic model.\n\nX can contain missing values in some or all of its dimensions. In such case the learning is done only with the available data. Implemented in the log-domain for better numerical accuracy with many dimensions.\n\nParameters:\n\nX  :           A (n x d) data to clusterise\nK  :           Number of cluster wanted\np₀ :           Initial probabilities of the categorical distribution (K x 1) [default: nothing]\nmixtures:      An array (of length K) of the mixture to employ (see notes) [def: [DiagonalGaussian() for i in 1:K]]\ntol:           Tolerance to stop the algorithm [default: 10^(-6)]\nverbosity:     A verbosity parameter regulating the information messages frequency [def: STD]\nminVariance:   Minimum variance for the mixtures [default: 0.05]\nminCovariance: Minimum covariance for the mixtures with full covariance matrix [default: 0]. This should be set different than minVariance (see notes).\ninitStrategy:  Mixture initialisation algorithm [def: kmeans]\nmaxIter:       Maximum number of iterations [def: -1, i.e. ∞]\nrng:           Random Number Generator (see FIXEDSEED) [deafult: Random.GLOBAL_RNG]\n\nReturns:\n\nA named touple of:\npₙₖ:      Matrix of size (N x K) of the probabilities of each point i to belong to cluster j\npₖ:       Probabilities of the categorical distribution (K x 1)\nmixtures: Vector (K x 1) of the estimated underlying distributions\nϵ:        Vector of the discrepancy (matrix norm) between pⱼₓ and the lagged pⱼₓ at each iteration\nlL:       The log-likelihood (without considering the last mixture optimisation)\nBIC:      The Bayesian Information Criterion (lower is better)\nAIC:      The Akaike Information Criterion (lower is better)\n\nNotes:\n\nThe mixtures currently implemented are SphericalGaussian(μ,σ²),DiagonalGaussian(μ,σ²) and FullGaussian(μ,σ²)\nReasonable choices for the minVariance/Covariance depends on the mixture. For example 0.25 seems a reasonable value for the SphericalGaussian, 0.05 seems better for the DiagonalGaussian, and FullGaussian seems to prefer either very low values of variance/covariance (e.g. (0.05,0.05) ) or very big but similar ones (e.g. (100,100) ).\nFor initStrategy, look at the documentation of initMixtures! for the mixture you want. The provided gaussian mixtures support grid, kmeans or given. grid is faster (expecially if X contains missing values), but kmeans often provides better results.\n\nResources:\n\nPaper describing gmm with missing values\nClass notes from MITx 6.86x (Sec 15.9)\nLimitations of gmm\n\nExample:\n\njulia> clusters = gmm([1 10.5;1.5 0; 1.8 8; 1.7 15; 3.2 40; 0 0; 3.3 38; 0 -2.3; 5.2 -2.4],3,verbosity=HIGH)\n\n\n\n\n\n","category":"method"},{"location":"Clustering.html#BetaML.Clustering.initMixtures!-Union{Tuple{T}, Tuple{Vector{T}, Any}} where T<:BetaML.Clustering.AbstractGaussian","page":"Clustering","title":"BetaML.Clustering.initMixtures!","text":"initMixtures!(mixtures::Array{T,1}, X; minVariance=0.25, minCovariance=0.0, initStrategy=\"grid\",rng=Random.GLOBAL_RNG)\n\nThe parameter initStrategy can be grid, kmeans or given:\n\ngrid: Uniformly cover the space observed by the data\nkmeans: Use the kmeans algorithm. If the data contains missing values, a first run of predictMissing is done under init=grid to impute the missing values just to allow the kmeans algorithm. Then the em algorithm is used with the output of kmean as init values.\ngiven: Leave the provided set of initial mixtures\n\n\n\n\n\n","category":"method"},{"location":"Clustering.html#BetaML.Clustering.initRepresentatives-Tuple{Any, Any}","page":"Clustering","title":"BetaML.Clustering.initRepresentatives","text":"initRepresentatives(X,K;initStrategy,Z₀)\n\nInitialisate the representatives for a K-Mean or K-Medoids algorithm\n\nParameters:\n\nX: a (N x D) data to clusterise\nK: Number of cluster wonted\ninitStrategy: Whether to select the initial representative vectors:\nrandom: randomly in the X space\ngrid: using a grid approach [default]\nshuffle: selecting randomly within the available points\ngiven: using a provided set of initial representatives provided in the Z₀ parameter\nZ₀: Provided (K x D) matrix of initial representatives (used only together with the given initStrategy) [default: nothing]\nrng: Random Number Generator (see FIXEDSEED) [deafult: Random.GLOBAL_RNG]\n\nReturns:\n\nA (K x D) matrix of initial representatives\n\nExample:\n\njulia> Z₀ = initRepresentatives([1 10.5;1.5 10.8; 1.8 8; 1.7 15; 3.2 40; 3.6 32; 3.6 38],2,initStrategy=\"given\",Z₀=[1.7 15; 3.6 40])\n\n\n\n\n\n","category":"method"},{"location":"Clustering.html#BetaML.Clustering.kmeans-Tuple{Any, Any}","page":"Clustering","title":"BetaML.Clustering.kmeans","text":"kmeans(X,K;dist,initStrategy,Z₀)\n\nCompute K-Mean algorithm to identify K clusters of X using Euclidean distance\n\nParameters:\n\nX: a (N x D) data to clusterise\nK: Number of cluster wonted\ndist: Function to employ as distance (see notes). Default to Euclidean distance.\ninitStrategy: Whether to select the initial representative vectors:\nrandom: randomly in the X space\ngrid: using a grid approach [default]\nshuffle: selecting randomly within the available points\ngiven: using a provided set of initial representatives provided in the Z₀ parameter\nZ₀: Provided (K x D) matrix of initial representatives (used only together with the given initStrategy) [default: nothing]\nrng: Random Number Generator (see FIXEDSEED) [deafult: Random.GLOBAL_RNG]\n\nReturns:\n\nA tuple of two items, the first one being a vector of size N of ids of the clusters associated to each point and the second one the (K x D) matrix of representatives\n\nNotes:\n\nSome returned clusters could be empty\nThe dist parameter can be:\nAny user defined function accepting two vectors and returning a scalar\nAn anonymous function with the same characteristics (e.g. dist = (x,y) -> norm(x-y)^2)\nOne of the above predefined distances: l1_distance, l2_distance, l2²_distance, cosine_distance\n\nExample:\n\njulia> (clIdx,Z) = kmeans([1 10.5;1.5 10.8; 1.8 8; 1.7 15; 3.2 40; 3.6 32; 3.3 38; 5.1 -2.3; 5.2 -2.4],3)\n\n\n\n\n\n","category":"method"},{"location":"Clustering.html#BetaML.Clustering.kmedoids-Tuple{Any, Any}","page":"Clustering","title":"BetaML.Clustering.kmedoids","text":"kmedoids(X,K;dist,initStrategy,Z₀)\n\nCompute K-Medoids algorithm to identify K clusters of X using distance definition dist\n\nParameters:\n\nX: a (n x d) data to clusterise\nK: Number of cluster wonted\ndist: Function to employ as distance (see notes). Default to Euclidean distance.\ninitStrategy: Whether to select the initial representative vectors:\nrandom: randomly in the X space\ngrid: using a grid approach\nshuffle: selecting randomly within the available points [default]\ngiven: using a provided set of initial representatives provided in the Z₀ parameter\nZ₀: Provided (K x D) matrix of initial representatives (used only together with the given initStrategy) [default: nothing]\nrng: Random Number Generator (see FIXEDSEED) [deafult: Random.GLOBAL_RNG]\n\nReturns:\n\nA tuple of two items, the first one being a vector of size N of ids of the clusters associated to each point and the second one the (K x D) matrix of representatives\n\nNotes:\n\nSome returned clusters could be empty\nThe dist parameter can be:\nAny user defined function accepting two vectors and returning a scalar\nAn anonymous function with the same characteristics (e.g. dist = (x,y) -> norm(x-y)^2)\nOne of the above predefined distances: l1_distance, l2_distance, l2²_distance, cosine_distance\n\nExample:\n\njulia> (clIdx,Z) = kmedoids([1 10.5;1.5 10.8; 1.8 8; 1.7 15; 3.2 40; 3.6 32; 3.3 38; 5.1 -2.3; 5.2 -2.4],3,initStrategy=\"grid\")\n\n\n\n\n\n","category":"method"},{"location":"Clustering.html#BetaML.Clustering.lpdf-Tuple{DiagonalGaussian, Any, Any}","page":"Clustering","title":"BetaML.Clustering.lpdf","text":"lpdf(m::DiagonalGaussian,x,mask) - Log PDF of the mixture given the observation x\n\n\n\n\n\n","category":"method"},{"location":"Clustering.html#BetaML.Clustering.lpdf-Tuple{FullGaussian, Any, Any}","page":"Clustering","title":"BetaML.Clustering.lpdf","text":"lpdf(m::FullGaussian,x,mask) - Log PDF of the mixture given the observation x\n\n\n\n\n\n","category":"method"},{"location":"Clustering.html#BetaML.Clustering.lpdf-Tuple{SphericalGaussian, Any, Any}","page":"Clustering","title":"BetaML.Clustering.lpdf","text":"lpdf(m::SphericalGaussian,x,mask) - Log PDF of the mixture given the observation x\n\n\n\n\n\n","category":"method"},{"location":"Clustering.html#BetaML.Clustering.predictMissing","page":"Clustering","title":"BetaML.Clustering.predictMissing","text":"predictMissing(X,K;p₀,mixtures,tol,verbosity,minVariance,minCovariance)\n\nFill missing entries in a sparse matrix assuming an underlying Gaussian Mixture probabilistic Model (GMM) and implementing an Expectation-Maximisation algorithm.\n\nWhile the name of the function is predictMissing, the function can be used also for system reccomendation / collaborative filtering and GMM-based regressions.\n\nImplemented in the log-domain for better numerical accuracy with many dimensions.\n\nParameters:\n\nX  :           A (N x D) sparse matrix of data to fill according to a GMM model\nK  :           Number of mixtures (latent classes) to consider [def: 3]\np₀ :           Initial probabilities of the categorical distribution (K x 1) [default: nothing]\nmixtures:      An array (of length K) of the mixture to employ (see notes) [def: [DiagonalGaussian() for i in 1:K]]\ntol:           Tolerance to stop the algorithm [default: 10^(-6)]\nverbosity:     A verbosity parameter regulating the information messages frequency [def: STD]\nminVariance:   Minimum variance for the mixtures [default: 0.05]\nminCovariance: Minimum covariance for the mixtures with full covariance matrix [default: 0]. This should be set different than minVariance (see notes).\ninitStrategy:  Mixture initialisation algorithm [def: grid]\nmaxIter:       Maximum number of iterations [def: -1, i.e. ∞]\nrng:           Random Number Generator (see FIXEDSEED) [deafult: Random.GLOBAL_RNG]\n\nReturns:\n\nA named touple of:\n̂X̂    : The Filled Matrix of size (N x D)\nnFill: The number of items filled\nlL   : The log-likelihood (without considering the last mixture optimisation)\nBIC :  The Bayesian Information Criterion (lower is better)\nAIC :  The Akaike Information Criterion (lower is better)\nNotes:\nThe mixtures currently implemented are SphericalGaussian(μ,σ²),DiagonalGaussian(μ,σ²) and FullGaussian(μ,σ²)\nFor initStrategy, look at the documentation of initMixtures! for the mixture you want. The provided gaussian mixtures support grid, kmeans or given. grid is faster, but kmeans often provides better results.\nThe algorithm requires to specify a number of \"latent classes\" (mlixtures) to divide the dataset into. If there isn't any prior domain specific knowledge on this point one can test sevaral k and verify which one minimise the BIC or AIC criteria.\n\nExample:\n\njulia>  cFOut = predictMissing([1 10.5;1.5 missing; 1.8 8; 1.7 15; 3.2 40; missing missing; 3.3 38; missing -2.3; 5.2 -2.4],3)\n\n\n\n\n\n","category":"function"},{"location":"Clustering.html#MLJModelInterface.predict-Tuple{Union{KMeans, KMedoids}, Any, Any}","page":"Clustering","title":"MLJModelInterface.predict","text":"predict(m::KMeans, fitResults, X) - Given a trained clustering model and some observations, predict the class of the observation\n\n\n\n\n\n","category":"method"},{"location":"Clustering.html#MLJModelInterface.transform-Tuple{MissingImputator, Any, Any}","page":"Clustering","title":"MLJModelInterface.transform","text":"transform(m::MissingImputator, fitResults, X) - Given a trained imputator model fill the missing data of some new observations\n\n\n\n\n\n","category":"method"},{"location":"Clustering.html#MLJModelInterface.transform-Tuple{Union{KMeans, KMedoids}, Any, Any}","page":"Clustering","title":"MLJModelInterface.transform","text":"fit(m::KMeans, fitResults, X) - Given a trained clustering model and some observations, return the distances to each centroids \n\n\n\n\n\n","category":"method"},{"location":"Perceptron.html#The-BetaML.Perceptron-Module","page":"Perceptron","title":"The BetaML.Perceptron Module","text":"","category":"section"},{"location":"Perceptron.html","page":"Perceptron","title":"Perceptron","text":"Perceptron","category":"page"},{"location":"Perceptron.html#BetaML.Perceptron","page":"Perceptron","title":"BetaML.Perceptron","text":"Perceptron module\n\nProvide linear and kernel classifiers.\n\nSee a runnable example on myBinder\n\nperceptron: Train data using the classical perceptron\nkernelPerceptron: Train data using the kernel perceptron\npegasos: Train data using the pegasos algorithm\npredict: Predict data using parameters from one of the above algorithms\n\nAll algorithms are multiclass, with perceptron and pegasos employing a one-vs-all strategy, while kernelPerceptron employs a one-vs-one approach, and return a \"probability\" for each class in term of a dictionary for each record. Use mode(ŷ) to return a single class prediction per record.\n\nThe binary equivalent algorithms, accepting only {-1,+1} labels, are available as peceptronBinary, kernelPerceptronBinary and pegasosBinary. They are slighly faster as they don't need to be wrapped in the multi-class equivalent and return a more informative output.\n\nThe multi-class versions are available in the MLJ framework as PerceptronClassifier,KernelPerceptronClassifier and PegasosClassifier respectivly.\n\n\n\n\n\n","category":"module"},{"location":"Perceptron.html#Module-Index","page":"Perceptron","title":"Module Index","text":"","category":"section"},{"location":"Perceptron.html","page":"Perceptron","title":"Perceptron","text":"Modules = [Perceptron]\nOrder   = [:constant, :type, :function, :macro]","category":"page"},{"location":"Perceptron.html#Detailed-API","page":"Perceptron","title":"Detailed API","text":"","category":"section"},{"location":"Perceptron.html","page":"Perceptron","title":"Perceptron","text":"Modules = [Perceptron]","category":"page"},{"location":"Perceptron.html#BetaML.Api.predict","page":"Perceptron","title":"BetaML.Api.predict","text":"predict(x,θ,θ₀)\n\nPredict a binary label {-1,1} given the feature vector and the linear coefficients\n\nParameters:\n\nx:        Feature matrix of the training data (n × d)\nθ:        The trained parameters\nθ₀:       The trained bias barameter [def: 0]\n\nReturn :\n\ny: Vector of the predicted labels\n\nExample:\n\njulia> predict([1.1 2.1; 5.3 4.2; 1.8 1.7], [3.2,1.2])\n\n\n\n\n\n","category":"function"},{"location":"Perceptron.html#BetaML.Api.predict-NTuple{4, Any}","page":"Perceptron","title":"BetaML.Api.predict","text":"predict(x,xtrain,ytrain,α;K)\n\nPredict a binary label {-1,1} given the feature vector and the training data together with their errors (as trained by a kernel perceptron algorithm)\n\nParameters:\n\nx:      Feature matrix of the training data (n × d)\nxtrain: The feature vectors used for the training\nytrain: The labels of the training set\nα:      The errors associated to each record\nK:      The kernel function used for the training and to be used for the prediction [def: radialKernel]\n\nReturn :\n\ny: Vector of the predicted labels\n\nExample:\n\njulia> predict([1.1 2.1; 5.3 4.2; 1.8 1.7], [3.2,1.2])\n\n\n\n\n\n","category":"method"},{"location":"Perceptron.html#BetaML.Api.predict-Union{Tuple{Tcl}, Tuple{Any, Any, Any, Any, AbstractVector{Tcl}}} where Tcl","page":"Perceptron","title":"BetaML.Api.predict","text":"predict(x,xtrain,ytrain,α,classes;K)\n\nPredict a multiclass label given the new feature vector and a trained kernel perceptron model.\n\nParameters:\n\nx:      Feature matrix of the training data (n × d)\nxtrain: A vector of the feature matrix used for training each of the one-vs-one class matches (i.e. model.x)\nytrain: A vector of the label vector used for training each of the one-vs-one class matches (i.e. model.y)\nα:      A vector of the errors associated to each record (i.e. model.α)\nclasses: The overal classes encountered in training (i.e. model.classes)\nK:      The kernel function used for the training and to be used for the prediction [def: radialKernel]\n\nReturn :\n\nŷ: Vector of dictionaries label=>probability (warning: it isn't really a probability, it is just the standardized number of matches \"won\" by this class compared with the other classes)\n\nNotes:\n\nUse mode(ŷ) if you want a single predicted label per record\n\nExample:\n\njulia> model  = kernelPerceptron([1.1 2.1; 5.3 4.2; 1.8 1.7], [-1,1,-1])\njulia> ŷtrain = Perceptron.predict([10 10; 2.2 2.5],model.x,model.y,model.α, model.classes,K=model.K)\n\n\n\n\n\n","category":"method"},{"location":"Perceptron.html#BetaML.Api.predict-Union{Tuple{Tcl}, Tuple{T}, Tuple{Any, AbstractVector{T}, AbstractVector{Float64}, Vector{Tcl}}} where {T<:AbstractVector{Float64}, Tcl}","page":"Perceptron","title":"BetaML.Api.predict","text":"predict(x,θ,θ₀,classes)\n\nPredict a multiclass label given the feature vector, the linear coefficients and the classes vector\n\nParameters:\n\nx:       Feature matrix of the training data (n × d)\nθ:       Vector of the trained parameters for each one-vs-all model (i.e. model.θ)\nθ₀:      Vector of the trained bias barameter for each one-vs-all model (i.e. model.θ₀)\nclasses: The overal classes encountered in training (i.e. model.classes)\n\nReturn :\n\nŷ: Vector of dictionaries label=>probability\n\nNotes:\n\nUse mode(ŷ) if you want a single predicted label per record\n\nExample:\n\n```julia julia> model  = perceptron([1.1 2.1; 5.3 4.2; 1.8 1.7], [-1,1,-1]) julia> ŷtrain = predict([10 10; 2.5 2.5],model.θ,model.θ₀, model.classes)\n\n\n\n\n\n","category":"method"},{"location":"Perceptron.html#BetaML.Perceptron.kernelPerceptron-Tuple{Any, Any}","page":"Perceptron","title":"BetaML.Perceptron.kernelPerceptron","text":"kernelPerceptron(x,y;K,T,α,nMsgs,shuffle)\n\nTrain a multiclass kernel classifier \"perceptron\" algorithm based on x and y.\n\nkernelPerceptron is a (potentially) non-linear perceptron-style classifier employing user-defined kernel funcions. Multiclass is supported using a one-vs-one approach.\n\nParameters:\n\nx:        Feature matrix of the training data (n × d)\ny:        Associated labels of the training data, in the format of ⨦ 1\nK:        Kernel function to employ. See ?radialKernel or ?polynomialKernelfor details or check ?BetaML.Utils to verify if other kernels are defined (you can alsways define your own kernel) [def: radialKernel]\nT:        Maximum number of iterations (aka \"epochs\") across the whole set (if the set is not fully classified earlier) [def: 100]\nα:        Initial distribution of the errors [def: zeros(length(y))]\nnMsg:     Maximum number of messages to show if all iterations are done [def: 0]\nshuffle:  Whether to randomly shuffle the data at each iteration [def: false]\nrng:      Random Number Generator (see FIXEDSEED) [deafult: Random.GLOBAL_RNG]\n\nReturn a named tuple with:\n\nx: The x data (eventually shuffled if shuffle=true)\ny: The label\nα: The errors associated to each record\nclasses: The labels classes encountered in the training\n\nNotes:\n\nThe trained model can then be used to make predictions using the function predict().\nThis model is available in the MLJ framework as the KernelPerceptronClassifier\n\nExample:\n\njulia> model  = kernelPerceptron([1.1 2.1; 5.3 4.2; 1.8 1.7], [-1,1,-1])\njulia> ŷtrain = Perceptron.predict(xtrain,model.x,model.y,model.α, model.classes,K=model.K)\njulia> ϵtrain = error(ytrain, mode(ŷtrain))\n\n\n\n\n\n","category":"method"},{"location":"Perceptron.html#BetaML.Perceptron.kernelPerceptronBinary-Tuple{Any, Any}","page":"Perceptron","title":"BetaML.Perceptron.kernelPerceptronBinary","text":"kernelPerceptronBinary(x,y;K,T,α,nMsgs,shuffle)\n\nTrain a multiclass kernel classifier \"perceptron\" algorithm based on x and y\n\nParameters:\n\nx:        Feature matrix of the training data (n × d)\ny:        Associated labels of the training data, in the format of ⨦ 1\nK:        Kernel function to employ. See ?radialKernel or ?polynomialKernelfor details or check ?BetaML.Utils to verify if other kernels are defined (you can alsways define your own kernel) [def: radialKernel]\nT:        Maximum number of iterations across the whole set (if the set is not fully classified earlier) [def: 1000]\nα:        Initial distribution of the errors [def: zeros(length(y))]\nnMsg:     Maximum number of messages to show if all iterations are done\nshuffle:  Whether to randomly shuffle the data at each iteration [def: false]\nrng:      Random Number Generator (see FIXEDSEED) [deafult: Random.GLOBAL_RNG]\n\nReturn a named tuple with:\n\nx: the x data (eventually shuffled if shuffle=true)\ny: the label\nα: the errors associated to each record\nerrors: the number of errors in the last iteration\nbesterrors: the minimum number of errors in classifying the data ever reached\niterations: the actual number of iterations performed\nseparated: a flag if the data has been successfully separated\n\nNotes:\n\nThe trained data can then be used to make predictions using the function predict(). If the option shuffle has been used, it is important to use there the returned (x,y,α) as these would have been shuffle compared with the original (x,y).\nPlease see @kernelPerceptron for a multi-class version\n\nExample:\n\njulia> model = kernelPerceptronBinary([1.1 2.1; 5.3 4.2; 1.8 1.7], [-1,1,-1])\n\n\n\n\n\n","category":"method"},{"location":"Perceptron.html#BetaML.Perceptron.pegasos-Tuple{Any, Any}","page":"Perceptron","title":"BetaML.Perceptron.pegasos","text":"pegasos(x,y;θ,θ₀,λ,η,T,nMsgs,shuffle,forceOrigin,returnMeanHyperplane)\n\nTrain the multiclass classifier \"pegasos\" algorithm according to x (features) and y (labels)\n\nPegasos is a linear, gradient-based classifier. Multiclass is supported using a one-vs-all approach.\n\nParameters:\n\nx:           Feature matrix of the training data (n × d)\ny:           Associated labels of the training data, can be in any format (string, integers..)\nθ:           Initial value of the weights (parameter) [def: zeros(d)]\nθ₀:          Initial value of the weight (parameter) associated to the constant term [def: 0]\nλ:           Multiplicative term of the learning rate\nη:           Learning rate [def: (t -> 1/sqrt(t))]\nT:           Maximum number of iterations across the whole set (if the set is not fully classified earlier) [def: 1000]\nnMsg:        Maximum number of messages to show if all iterations are done\nshuffle:     Whether to randomly shuffle the data at each iteration [def: false]\nforceOrigin: Whehter to force θ₀ to remain zero [def: false]\nreturnMeanHyperplane: Whether to return the average hyperplane coefficients instead of the average ones  [def: false]\nrng:         Random Number Generator (see FIXEDSEED) [deafult: Random.GLOBAL_RNG]\n\nReturn a named tuple with:\n\nθ:          The weights of the classifier\nθ₀:         The weight of the classifier associated to the constant term\nclasses:    The classes (unique values) of y\n\nNotes:\n\nThe trained parameters can then be used to make predictions using the function predict().\nThis model is available in the MLJ framework as the PegasosClassifier\n\nExample:\n\njulia> model = pegasos([1.1 2.1; 5.3 4.2; 1.8 1.7], [-1,1,-1])\njulia> ŷ     = predict([2.1 3.1; 7.3 5.2], model.θ, model.θ₀, model.classes)\n\n\n\n\n\n","category":"method"},{"location":"Perceptron.html#BetaML.Perceptron.pegasosBinary-Tuple{Any, Any}","page":"Perceptron","title":"BetaML.Perceptron.pegasosBinary","text":"pegasosBinary(x,y;θ,θ₀,λ,η,T,nMsgs,shuffle,forceOrigin)\n\nTrain the peagasos algorithm based on x and y (labels)\n\nParameters:\n\nx:           Feature matrix of the training data (n × d)\ny:           Associated labels of the training data, in the format of ⨦ 1\nθ:           Initial value of the weights (parameter) [def: zeros(d)]\nθ₀:          Initial value of the weight (parameter) associated to the constant term [def: 0]\nλ:           Multiplicative term of the learning rate\nη:           Learning rate [def: (t -> 1/sqrt(t))]\nT:           Maximum number of iterations across the whole set (if the set is not fully classified earlier) [def: 1000]\nnMsg:        Maximum number of messages to show if all iterations are done\nshuffle:    Whether to randomly shuffle the data at each iteration [def: false]\nforceOrigin: Whether to force θ₀ to remain zero [def: false]\n\nReturn a named tuple with:\n\nθ:          The final weights of the classifier\nθ₀:         The final weight of the classifier associated to the constant term\navgθ:       The average weights of the classifier\navgθ₀:      The average weight of the classifier associated to the constant term\nerrors:     The number of errors in the last iteration\nbesterrors: The minimum number of errors in classifying the data ever reached\niterations: The actual number of iterations performed\nseparated:  Weather the data has been successfully separated\n\nNotes:\n\nThe trained parameters can then be used to make predictions using the function predict().\n\nExample:\n\njulia> pegasos([1.1 2.1; 5.3 4.2; 1.8 1.7], [-1,1,-1])\n\n\n\n\n\n","category":"method"},{"location":"Perceptron.html#BetaML.Perceptron.perceptron-Tuple{AbstractMatrix{T} where T, AbstractVector{T} where T}","page":"Perceptron","title":"BetaML.Perceptron.perceptron","text":"perceptron(x,y;θ,θ₀,T,nMsgs,shuffle,forceOrigin,returnMeanHyperplane)\n\nTrain the multiclass classifier \"perceptron\" algorithm  based on x and y (labels).\n\nThe perceptron is a linear classifier. Multiclass is supported using a one-vs-all approach.\n\nParameters:\n\nx:           Feature matrix of the training data (n × d)\ny:           Associated labels of the training data, can be in any format (string, integers..)\nθ:           Initial value of the weights (parameter) [def: zeros(d)]\nθ₀:          Initial value of the weight (parameter) associated to the constant                term [def: 0]\nT:           Maximum number of iterations across the whole set (if the set                is not fully classified earlier) [def: 1000]\nnMsg:        Maximum number of messages to show if all iterations are done [def: 0]\nshuffle:     Whether to randomly shuffle the data at each iteration [def: false]\nforceOrigin: Whether to force θ₀ to remain zero [def: false]\nreturnMeanHyperplane: Whether to return the average hyperplane coefficients instead of the final ones  [def: false]\nrng:         Random Number Generator (see FIXEDSEED) [deafult: Random.GLOBAL_RNG]\n\nReturn a named tuple with:\n\nθ:          The weights of the classifier\nθ₀:         The weight of the classifier associated to the constant term\nclasses:    The classes (unique values) of y\n\nNotes:\n\nThe trained parameters can then be used to make predictions using the function predict().\nThis model is available in the MLJ framework as the PerceptronClassifier\n\nExample:\n\njulia> model = perceptron([1.1 2.1; 5.3 4.2; 1.8 1.7], [-1,1,-1])\njulia> ŷ     = predict([2.1 3.1; 7.3 5.2], model.θ, model.θ₀, model.classes)\n\n\n\n\n\n","category":"method"},{"location":"Perceptron.html#BetaML.Perceptron.perceptronBinary-Tuple{Any, Any}","page":"Perceptron","title":"BetaML.Perceptron.perceptronBinary","text":"perceptronBinary(x,y;θ,θ₀,T,nMsgs,shuffle,forceOrigin)\n\nTrain the binary classifier \"perceptron\" algorithm based on x and y (labels)\n\nParameters:\n\nx:           Feature matrix of the training data (n × d)\ny:           Associated labels of the training data, in the format of ⨦ 1\nθ:           Initial value of the weights (parameter) [def: zeros(d)]\nθ₀:          Initial value of the weight (parameter) associated to the constant                term [def: 0]\nT:           Maximum number of iterations across the whole set (if the set                is not fully classified earlier) [def: 1000]\nnMsg:        Maximum number of messages to show if all iterations are done\nshuffle:     Whether to randomly shuffle the data at each iteration [def: false]\nforceOrigin: Whether to force θ₀ to remain zero [def: false]\nrng:         Random Number Generator (see FIXEDSEED) [deafult: Random.GLOBAL_RNG]\n\nReturn a named tuple with:\n\nθ:          The final weights of the classifier\nθ₀:         The final weight of the classifier associated to the constant term\navgθ:       The average weights of the classifier\navgθ₀:      The average weight of the classifier associated to the constant term\nerrors:     The number of errors in the last iteration\nbesterrors: The minimum number of errors in classifying the data ever reached\niterations: The actual number of iterations performed\nseparated:  Weather the data has been successfully separated\n\nNotes:\n\nThe trained parameters can then be used to make predictions using the function predict().\n\nExample:\n\njulia> model = perceptronBinary([1.1 2.1; 5.3 4.2; 1.8 1.7], [-1,1,-1])\n\n\n\n\n\n","category":"method"},{"location":"tutorials/Classification - cars/betaml_tutorial_classification_cars.html","page":"A classification task when labels are known - determining the country of origin of cars given the cars characteristics","title":"A classification task when labels are known - determining the country of origin of cars given the cars characteristics","text":"EditURL = \"https://github.com/sylvaticus/BetaML.jl/blob/master/docs/src/tutorials/Classification - cars/betaml_tutorial_classification_cars.jl\"","category":"page"},{"location":"tutorials/Classification - cars/betaml_tutorial_classification_cars.html#classification_tutorial","page":"A classification task when labels are known - determining the country of origin of cars given the cars characteristics","title":"A classification task when labels are known - determining the country of origin of cars given the cars characteristics","text":"","category":"section"},{"location":"tutorials/Classification - cars/betaml_tutorial_classification_cars.html","page":"A classification task when labels are known - determining the country of origin of cars given the cars characteristics","title":"A classification task when labels are known - determining the country of origin of cars given the cars characteristics","text":"In this exercise we have some car technical characteristics (mpg, horsepower,weight, model year...) and the country of origin and we would like to create a model such that the country of origin can be accurately predicted given the technical characteristics. As the information to predict is a multi-class one, this is a [classification](https://en.wikipedia.org/wiki/Statistical_classification) task. It is a challenging exercise due to the simultaneous presence of three factors: (1) presence of missing data; (2) unbalanced data - 254 out of 406 cars are US made; (3) small dataset.","category":"page"},{"location":"tutorials/Classification - cars/betaml_tutorial_classification_cars.html","page":"A classification task when labels are known - determining the country of origin of cars given the cars characteristics","title":"A classification task when labels are known - determining the country of origin of cars given the cars characteristics","text":"Data origin:","category":"page"},{"location":"tutorials/Classification - cars/betaml_tutorial_classification_cars.html","page":"A classification task when labels are known - determining the country of origin of cars given the cars characteristics","title":"A classification task when labels are known - determining the country of origin of cars given the cars characteristics","text":"dataset description: https://archive.ics.uci.edu/ml/datasets/auto+mpg\ndata source we use here: https://archive.ics.uci.edu/ml/machine-learning-databases/auto-mpg/auto-mpg.data","category":"page"},{"location":"tutorials/Classification - cars/betaml_tutorial_classification_cars.html","page":"A classification task when labels are known - determining the country of origin of cars given the cars characteristics","title":"A classification task when labels are known - determining the country of origin of cars given the cars characteristics","text":"Field description:","category":"page"},{"location":"tutorials/Classification - cars/betaml_tutorial_classification_cars.html","page":"A classification task when labels are known - determining the country of origin of cars given the cars characteristics","title":"A classification task when labels are known - determining the country of origin of cars given the cars characteristics","text":"mpg:           continuous\ncylinders:     multi-valued discrete\ndisplacement:  continuous\nhorsepower:    continuous\nweight:        continuous\nacceleration:  continuous\nmodel year:    multi-valued discrete\norigin:        multi-valued discrete\ncar name:      string (unique for each instance) - not used here","category":"page"},{"location":"tutorials/Classification - cars/betaml_tutorial_classification_cars.html#Library-and-data-loading","page":"A classification task when labels are known - determining the country of origin of cars given the cars characteristics","title":"Library and data loading","text":"","category":"section"},{"location":"tutorials/Classification - cars/betaml_tutorial_classification_cars.html","page":"A classification task when labels are known - determining the country of origin of cars given the cars characteristics","title":"A classification task when labels are known - determining the country of origin of cars given the cars characteristics","text":"We load a buch of packages that we'll use during this tutorial..","category":"page"},{"location":"tutorials/Classification - cars/betaml_tutorial_classification_cars.html","page":"A classification task when labels are known - determining the country of origin of cars given the cars characteristics","title":"A classification task when labels are known - determining the country of origin of cars given the cars characteristics","text":"using Random, HTTP, CSV, DataFrames, BenchmarkTools, BetaML\nimport DecisionTree, Flux\nimport Pipe: @pipe","category":"page"},{"location":"tutorials/Classification - cars/betaml_tutorial_classification_cars.html","page":"A classification task when labels are known - determining the country of origin of cars given the cars characteristics","title":"A classification task when labels are known - determining the country of origin of cars given the cars characteristics","text":"To load the data from the internet our workflow is (1) Retrieve the data –> (2) Clean it –> (3) Load it –> (4) Output it as a DataFrame.","category":"page"},{"location":"tutorials/Classification - cars/betaml_tutorial_classification_cars.html","page":"A classification task when labels are known - determining the country of origin of cars given the cars characteristics","title":"A classification task when labels are known - determining the country of origin of cars given the cars characteristics","text":"For step (1) we use HTTP.get(), for step (2) we usereplace!, for steps (3) and (4) we uses theCSVpackage, and we use the \"pip\"|>` operator to chain these operations:","category":"page"},{"location":"tutorials/Classification - cars/betaml_tutorial_classification_cars.html","page":"A classification task when labels are known - determining the country of origin of cars given the cars characteristics","title":"A classification task when labels are known - determining the country of origin of cars given the cars characteristics","text":"urlDataOriginal = \"https://archive.ics.uci.edu/ml/machine-learning-databases/auto-mpg/auto-mpg.data-original\"\ndata = @pipe HTTP.get(urlDataOriginal).body                                                |>\n             replace!(_, UInt8('\\t') => UInt8(' '))                                        |>\n             CSV.File(_, delim=' ', missingstring=\"NA\", ignorerepeated=true, header=false) |>\n             DataFrame;\nnothing #hide","category":"page"},{"location":"tutorials/Classification - cars/betaml_tutorial_classification_cars.html","page":"A classification task when labels are known - determining the country of origin of cars given the cars characteristics","title":"A classification task when labels are known - determining the country of origin of cars given the cars characteristics","text":"This results in a table where the rows are the observations (the various cars) and the column the fields. All BetaML models expect this layout. As the dataset is ordered, we randomly shuffle the data. Note that we pass to shuffle copy(FIXEDRNG) as the random nuber generator in order to obtain reproducible output ( FIXEDRNG is nothing else than an istance of StableRNG(123) defined in the BetaML.Utils sub-module, but you can choose of course your own \"fixed\" RNG). See the Dealing with stochasticity section in the Getting started tutorial for details.","category":"page"},{"location":"tutorials/Classification - cars/betaml_tutorial_classification_cars.html","page":"A classification task when labels are known - determining the country of origin of cars given the cars characteristics","title":"A classification task when labels are known - determining the country of origin of cars given the cars characteristics","text":"data[shuffle(copy(FIXEDRNG),axes(data, 1)), :]\ndescribe(data)","category":"page"},{"location":"tutorials/Classification - cars/betaml_tutorial_classification_cars.html","page":"A classification task when labels are known - determining the country of origin of cars given the cars characteristics","title":"A classification task when labels are known - determining the country of origin of cars given the cars characteristics","text":"Columns 1 to 7 contain  characteristics of the car, while column 8 encodes the country or origin (\"1\" -> US, \"2\" -> EU, \"3\" -> Japan). That's what we want to be able to predict. Columns 9 contains the car name, but we are not going to use this information in this tutorial. Note also that some fields have missing data. Our first step is hence to divide the dataset in features (the x) and the labels (the y) we want to predict. The x is then a Julia standard Matrix of 406 rows by 7 columns and the y is a vector of the 406 observations:","category":"page"},{"location":"tutorials/Classification - cars/betaml_tutorial_classification_cars.html","page":"A classification task when labels are known - determining the country of origin of cars given the cars characteristics","title":"A classification task when labels are known - determining the country of origin of cars given the cars characteristics","text":"x     = Matrix{Union{Missing,Float64}}(data[:,1:7]);\ny     = Vector{Int64}(data[:,8]);\nnothing #hide","category":"page"},{"location":"tutorials/Classification - cars/betaml_tutorial_classification_cars.html","page":"A classification task when labels are known - determining the country of origin of cars given the cars characteristics","title":"A classification task when labels are known - determining the country of origin of cars given the cars characteristics","text":"Some algorithms that we will use today don't like missing data, so we need to impute them. Foir this we are using the predictMissing function provided by the BetaML.Clustering sub-module. Internally the function uses a Gaussian Mixture Model to assign to the missing walue of a given record an average of the values of the non-missing records weighted for how close they are to our specific record.","category":"page"},{"location":"tutorials/Classification - cars/betaml_tutorial_classification_cars.html","page":"A classification task when labels are known - determining the country of origin of cars given the cars characteristics","title":"A classification task when labels are known - determining the country of origin of cars given the cars characteristics","text":"xFull = predictMissing(x,rng=copy(FIXEDRNG)).X̂;\nnothing #hide","category":"page"},{"location":"tutorials/Classification - cars/betaml_tutorial_classification_cars.html","page":"A classification task when labels are known - determining the country of origin of cars given the cars characteristics","title":"A classification task when labels are known - determining the country of origin of cars given the cars characteristics","text":"Further, some models don't work with categorical data as such, so we need to represent our y as a matrix with a separate column for each possible categorical value (the so called \"one-hot\" representation). For example, within a three classes field, the individual value 2 (or \"Europe\" for what it matters) would be represented as the vector [0 1 0], while 3 (or \"Japan\") wpuld become the vector [0 0 1]. To encode as one-hot we use the function oneHotEncoder in BetaML.Utils","category":"page"},{"location":"tutorials/Classification - cars/betaml_tutorial_classification_cars.html","page":"A classification task when labels are known - determining the country of origin of cars given the cars characteristics","title":"A classification task when labels are known - determining the country of origin of cars given the cars characteristics","text":"y_oh  = oneHotEncoder(y);\nnothing #hide","category":"page"},{"location":"tutorials/Classification - cars/betaml_tutorial_classification_cars.html","page":"A classification task when labels are known - determining the country of origin of cars given the cars characteristics","title":"A classification task when labels are known - determining the country of origin of cars given the cars characteristics","text":"In supervised machine learning it is good practice to partition the available data in a training, validation, and test subsets, where the first one is used to train the ML algorithm, the second one to train any eventual \"hyper-parameters\" of the algorithm and the test subset is finally used to evaluate the quality of the algorithm. Here, for brevity, we use only the train and the test subsets, implicitly assuming we already know the best hyper-parameters. Please refer to the regression tutorial for examples of how to use the validation subset to train the hyper-parameters. We use then the partition function in BetaML.Utils, where we can specify the different data to partition (that must have the same number of observations) and the shares of observation that we want in each subset.","category":"page"},{"location":"tutorials/Classification - cars/betaml_tutorial_classification_cars.html","page":"A classification task when labels are known - determining the country of origin of cars given the cars characteristics","title":"A classification task when labels are known - determining the country of origin of cars given the cars characteristics","text":"((xtrain,xtest),(xtrainFull,xtestFull),(ytrain,ytest),(ytrain_oh,ytest_oh)) = partition([x,xFull,y,y_oh],[0.8,1-0.8],rng=copy(FIXEDRNG));\nnothing #hide","category":"page"},{"location":"tutorials/Classification - cars/betaml_tutorial_classification_cars.html#Random-Forests","page":"A classification task when labels are known - determining the country of origin of cars given the cars characteristics","title":"Random Forests","text":"","category":"section"},{"location":"tutorials/Classification - cars/betaml_tutorial_classification_cars.html","page":"A classification task when labels are known - determining the country of origin of cars given the cars characteristics","title":"A classification task when labels are known - determining the country of origin of cars given the cars characteristics","text":"We are now ready to use our first model, the Random Forests (in the BetaML.Trees sub-module). Random Forests build a \"forest\" of decision trees models and then average their predictions to make an overall prediction out of a feature vector. To \"build\" the forest model (i.e. to \"train\" it) we need to give the model the training feature matrix and the associated \"true\" training labels, and we need to specify the number of trees to employ (this is an example of hyper-parameters). Here we use 30 individual decision trees. As the labels are encoded using integers,  we need also to use the parameter forceClassification=true otherwide the model would undergo a regression job.","category":"page"},{"location":"tutorials/Classification - cars/betaml_tutorial_classification_cars.html","page":"A classification task when labels are known - determining the country of origin of cars given the cars characteristics","title":"A classification task when labels are known - determining the country of origin of cars given the cars characteristics","text":"myForest       = buildForest(xtrain,ytrain,30, rng=copy(FIXEDRNG),forceClassification=true);\nnothing #hide","category":"page"},{"location":"tutorials/Classification - cars/betaml_tutorial_classification_cars.html","page":"A classification task when labels are known - determining the country of origin of cars given the cars characteristics","title":"A classification task when labels are known - determining the country of origin of cars given the cars characteristics","text":"To obtain the predicted values, we can simply use the function BetaML.Trees.predict with our myForest model and either the training or testing data.","category":"page"},{"location":"tutorials/Classification - cars/betaml_tutorial_classification_cars.html","page":"A classification task when labels are known - determining the country of origin of cars given the cars characteristics","title":"A classification task when labels are known - determining the country of origin of cars given the cars characteristics","text":"ŷtrain,ŷtest   = predict.(Ref(myForest), [xtrain,xtest],rng=copy(FIXEDRNG));\nnothing #hide","category":"page"},{"location":"tutorials/Classification - cars/betaml_tutorial_classification_cars.html","page":"A classification task when labels are known - determining the country of origin of cars given the cars characteristics","title":"A classification task when labels are known - determining the country of origin of cars given the cars characteristics","text":"Finally we can measure the accuracy of our predictions with the accuracy function, with the sidenote that we need first to \"parse\" the ŷs as forcing the classification job transformed automatically them in strings from the original integers:","category":"page"},{"location":"tutorials/Classification - cars/betaml_tutorial_classification_cars.html","page":"A classification task when labels are known - determining the country of origin of cars given the cars characteristics","title":"A classification task when labels are known - determining the country of origin of cars given the cars characteristics","text":"trainAccuracy,testAccuracy  = accuracy.([parse.(Int64,mode(ŷtrain,rng=copy(FIXEDRNG))),parse.(Int64,mode(ŷtest,rng=copy(FIXEDRNG)))],[ytrain,ytest])","category":"page"},{"location":"tutorials/Classification - cars/betaml_tutorial_classification_cars.html","page":"A classification task when labels are known - determining the country of origin of cars given the cars characteristics","title":"A classification task when labels are known - determining the country of origin of cars given the cars characteristics","text":"The predictions are quite good, for the training set the algoritm predicted almost all cars' origins correctly, while for the testing set (i.e. those records that has not been used to train the algorithm), the correct prediction level is still quite high, at 80%","category":"page"},{"location":"tutorials/Classification - cars/betaml_tutorial_classification_cars.html","page":"A classification task when labels are known - determining the country of origin of cars given the cars characteristics","title":"A classification task when labels are known - determining the country of origin of cars given the cars characteristics","text":"While accuracy can sometimes suffice, we may often want to better understand which categories our model has trouble to predict correctly. We can investigate the output of a multi-class classifier more in-deep with a ConfusionMatrix where the true values (y) are given in rows and the predicted ones (ŷ) in columns, together to some per-class metrics like the precision (true class i over predicted in class i), the recall (predicted class i over the true class i) and others. We fist build the ConfusionMatrix object between ŷ and y and then we print it (we do it here for the test subset):","category":"page"},{"location":"tutorials/Classification - cars/betaml_tutorial_classification_cars.html","page":"A classification task when labels are known - determining the country of origin of cars given the cars characteristics","title":"A classification task when labels are known - determining the country of origin of cars given the cars characteristics","text":"cm = ConfusionMatrix(parse.(Int64,mode(ŷtest,rng=copy(FIXEDRNG))),ytest,classes=[1,2,3],labels=[\"US\",\"EU\",\"Japan\"])\nprint(cm;what=\"all\")","category":"page"},{"location":"tutorials/Classification - cars/betaml_tutorial_classification_cars.html","page":"A classification task when labels are known - determining the country of origin of cars given the cars characteristics","title":"A classification task when labels are known - determining the country of origin of cars given the cars characteristics","text":"From the report we can see that Japanese cars have more trouble in being correctly classified, and in particular many Japanease cars are classified as US ones. This is likely a result of the class imbalance of the data set, and could be solved by balancing the dataset with various sampling tecniques before training the model.","category":"page"},{"location":"tutorials/Classification - cars/betaml_tutorial_classification_cars.html","page":"A classification task when labels are known - determining the country of origin of cars given the cars characteristics","title":"A classification task when labels are known - determining the country of origin of cars given the cars characteristics","text":"When we benchmark the resourse used (time and memory) we find that Random Forests remain pretty fast, expecially when we compare them with neural networks (see later)","category":"page"},{"location":"tutorials/Classification - cars/betaml_tutorial_classification_cars.html","page":"A classification task when labels are known - determining the country of origin of cars given the cars characteristics","title":"A classification task when labels are known - determining the country of origin of cars given the cars characteristics","text":"@btime buildForest(xtrain,ytrain,30, rng=copy(FIXEDRNG),forceClassification=true);\nnothing #hide","category":"page"},{"location":"tutorials/Classification - cars/betaml_tutorial_classification_cars.html#Comparision-with-DecisionTree.jl","page":"A classification task when labels are known - determining the country of origin of cars given the cars characteristics","title":"Comparision with DecisionTree.jl","text":"","category":"section"},{"location":"tutorials/Classification - cars/betaml_tutorial_classification_cars.html","page":"A classification task when labels are known - determining the country of origin of cars given the cars characteristics","title":"A classification task when labels are known - determining the country of origin of cars given the cars characteristics","text":"DecisionTrees.jl random forests are similar in usage: we first \"build\" (train) the forest and we then make predictions out of the trained model. The main difference is that the model requires data with nonmissing values, so we are going to use the xtrainFull and xtestFull feature labels we created earlier:","category":"page"},{"location":"tutorials/Classification - cars/betaml_tutorial_classification_cars.html","page":"A classification task when labels are known - determining the country of origin of cars given the cars characteristics","title":"A classification task when labels are known - determining the country of origin of cars given the cars characteristics","text":"# We train the model...\nmodel = DecisionTree.build_forest(ytrain, xtrainFull,-1,30,rng=123)\n# ..and we generate predictions and measure their error\n(ŷtrain,ŷtest) = DecisionTree.apply_forest.([model],[xtrainFull,xtestFull]);\n(trainAccuracy,testAccuracy) = accuracy.([ŷtrain,ŷtest],[ytrain,ytest])","category":"page"},{"location":"tutorials/Classification - cars/betaml_tutorial_classification_cars.html","page":"A classification task when labels are known - determining the country of origin of cars given the cars characteristics","title":"A classification task when labels are known - determining the country of origin of cars given the cars characteristics","text":"While the accuracy on the training set is exactly the same as for BetaML random forets, DecisionTree.jl random forests are slighly less accurate in the testing sample. Where however DecisionTrees.jl excell is in the efficiency: they are extremelly fast and memory parse, even if here to this benchmark we should add the resources need to impute the missing values. Also, one of the reasons DecisionTrees are such efficient is that internally they sort the data to avoid repeated comparision, but in this way they work only with features that are sortable, while BetaML random forests accept virtually any kind of input without the need of adapt it.","category":"page"},{"location":"tutorials/Classification - cars/betaml_tutorial_classification_cars.html","page":"A classification task when labels are known - determining the country of origin of cars given the cars characteristics","title":"A classification task when labels are known - determining the country of origin of cars given the cars characteristics","text":"@btime  DecisionTree.build_forest(ytrain, xtrainFull,-1,30,rng=123);\nnothing #hide","category":"page"},{"location":"tutorials/Classification - cars/betaml_tutorial_classification_cars.html#Neural-network","page":"A classification task when labels are known - determining the country of origin of cars given the cars characteristics","title":"Neural network","text":"","category":"section"},{"location":"tutorials/Classification - cars/betaml_tutorial_classification_cars.html","page":"A classification task when labels are known - determining the country of origin of cars given the cars characteristics","title":"A classification task when labels are known - determining the country of origin of cars given the cars characteristics","text":"Neural networks (NN) can be very powerfull, but have two \"inconvenients\" compared with random forests: First, are a bit \"picky\". We need to do a bit of work to provide data in specific format. Note that this is not feature engineering. One of the advantages on neural network is that for the most this is not needed for neural networks. However we still need to \"clean\" the data. One issue is that NN don't like missing data. So we need to provide them with the feature matrix \"clean\" of missing data. Secondly, they work only with numerical data. So we need to use the one-hot encoding we saw earlier.","category":"page"},{"location":"tutorials/Classification - cars/betaml_tutorial_classification_cars.html","page":"A classification task when labels are known - determining the country of origin of cars given the cars characteristics","title":"A classification task when labels are known - determining the country of origin of cars given the cars characteristics","text":"#Further, they work best if the features are scaled such that each feature has mean zero and standard deviation 1. We can achieve it with the function [`scale`](@ref) or, as in this case, [`getScaleFactors`](@ref).\nxScaleFactors   = getScaleFactors(xtrainFull)\nD               = size(xtrainFull,2)\nclasses         = unique(y)\nnCl             = length(classes)","category":"page"},{"location":"tutorials/Classification - cars/betaml_tutorial_classification_cars.html","page":"A classification task when labels are known - determining the country of origin of cars given the cars characteristics","title":"A classification task when labels are known - determining the country of origin of cars given the cars characteristics","text":"The second \"inconvenient\" of NN is that, while not requiring feature engineering, they stil lneed a bit of practice on the way to build the network. It's not as simple as train(model,x,y). We need here to specify how we want our layers, chain the layers together and then decide a loss overall function. Only when we done these steps, we have the model ready for training. Here we define 3 DenseLayer where, for each of them, we specify the number of neurons in input (the first layer being equal to the dimensions of the data), the output layer (for a classification task, the last layer output size beying equal to the number of classes) and an _activation function for each layer (default the identity function).","category":"page"},{"location":"tutorials/Classification - cars/betaml_tutorial_classification_cars.html","page":"A classification task when labels are known - determining the country of origin of cars given the cars characteristics","title":"A classification task when labels are known - determining the country of origin of cars given the cars characteristics","text":"ls   = 50\nl1   = DenseLayer(D,ls,f=relu,rng=copy(FIXEDRNG))\nl2   = DenseLayer(ls,nCl,f=relu,rng=copy(FIXEDRNG))","category":"page"},{"location":"tutorials/Classification - cars/betaml_tutorial_classification_cars.html","page":"A classification task when labels are known - determining the country of origin of cars given the cars characteristics","title":"A classification task when labels are known - determining the country of origin of cars given the cars characteristics","text":"For a classification the last layer is a VectorFunctionLayer that has no learnable parameters but whose activation function is applied to the ensemble of the neurons, rather than individually on each neuron. In particular, for classification we pass the BetaML.Utils.softmax function whose output has the same size as the input (and the number of classes to predict), but we can use the VectorFunctionLayer with any function, including the pool1d function to create a \"pooling\" layer (using maximum, mean or whatever other subfunction we pass to pool1d)","category":"page"},{"location":"tutorials/Classification - cars/betaml_tutorial_classification_cars.html","page":"A classification task when labels are known - determining the country of origin of cars given the cars characteristics","title":"A classification task when labels are known - determining the country of origin of cars given the cars characteristics","text":"l3   = VectorFunctionLayer(nCl,f=softmax) ## Add a (parameterless) layer whose activation function (softMax in this case) is defined to all its nodes at once","category":"page"},{"location":"tutorials/Classification - cars/betaml_tutorial_classification_cars.html","page":"A classification task when labels are known - determining the country of origin of cars given the cars characteristics","title":"A classification task when labels are known - determining the country of origin of cars given the cars characteristics","text":"Finally we chain the layers and assign a loss function","category":"page"},{"location":"tutorials/Classification - cars/betaml_tutorial_classification_cars.html","page":"A classification task when labels are known - determining the country of origin of cars given the cars characteristics","title":"A classification task when labels are known - determining the country of origin of cars given the cars characteristics","text":"mynn = buildNetwork([l1,l2,l3],squaredCost,name=\"Multinomial logistic regression Model Cars\") ## Build the NN and use the squared cost (aka MSE) as error function (crossEntropy could also be used)","category":"page"},{"location":"tutorials/Classification - cars/betaml_tutorial_classification_cars.html","page":"A classification task when labels are known - determining the country of origin of cars given the cars characteristics","title":"A classification task when labels are known - determining the country of origin of cars given the cars characteristics","text":"Now we can train our network using the function train!. It has many options, have a look at the documentation for all the possible arguments. Note that we trained the network based on the scaled feature matrix","category":"page"},{"location":"tutorials/Classification - cars/betaml_tutorial_classification_cars.html","page":"A classification task when labels are known - determining the country of origin of cars given the cars characteristics","title":"A classification task when labels are known - determining the country of origin of cars given the cars characteristics","text":"res  = train!(mynn,scale(xtrainFull,xScaleFactors),ytrain_oh,epochs=500,batchSize=8,optAlg=ADAM(),rng=copy(FIXEDRNG)) ## Use optAlg=SGD() to use Stochastic Gradient Descent instead","category":"page"},{"location":"tutorials/Classification - cars/betaml_tutorial_classification_cars.html","page":"A classification task when labels are known - determining the country of origin of cars given the cars characteristics","title":"A classification task when labels are known - determining the country of origin of cars given the cars characteristics","text":"Once trained, we can predict the label. As the trained was based on the scaled feature matrix, so must be for the predictions","category":"page"},{"location":"tutorials/Classification - cars/betaml_tutorial_classification_cars.html","page":"A classification task when labels are known - determining the country of origin of cars given the cars characteristics","title":"A classification task when labels are known - determining the country of origin of cars given the cars characteristics","text":"(ŷtrain,ŷtest)  = predict.(Ref(mynn),[scale(xtrainFull,xScaleFactors),scale(xtestFull,xScaleFactors)])\ntrainAccuracy   =  accuracy(ŷtrain,ytrain,rng=copy(FIXEDRNG))\ntestAccuracy    = accuracy(ŷtest,ytest,rng=copy(FIXEDRNG))","category":"page"},{"location":"tutorials/Classification - cars/betaml_tutorial_classification_cars.html","page":"A classification task when labels are known - determining the country of origin of cars given the cars characteristics","title":"A classification task when labels are known - determining the country of origin of cars given the cars characteristics","text":"src accuracy(mode(ŷtest,rng=copy(FIXEDRNG)),ytest)","category":"page"},{"location":"tutorials/Classification - cars/betaml_tutorial_classification_cars.html","page":"A classification task when labels are known - determining the country of origin of cars given the cars characteristics","title":"A classification task when labels are known - determining the country of origin of cars given the cars characteristics","text":"cm = ConfusionMatrix(ŷtest,ytest,classes=[1,2,3],labels=[\"US\",\"EU\",\"Japan\"],rng=copy(FIXEDRNG))\nprint(cm)","category":"page"},{"location":"tutorials/Classification - cars/betaml_tutorial_classification_cars.html","page":"A classification task when labels are known - determining the country of origin of cars given the cars characteristics","title":"A classification task when labels are known - determining the country of origin of cars given the cars characteristics","text":"We see a bit the limits of neural networks in this example. While NN can be extremelly performant in many domain, they also require lot of data and computational power, expecially considering the many possible hyper-parameters and hence its large space in the hyper-parameter tuning. In this example we arrive short to the performance of random forests, yet with asignificant numberof neurons.","category":"page"},{"location":"tutorials/Classification - cars/betaml_tutorial_classification_cars.html","page":"A classification task when labels are known - determining the country of origin of cars given the cars characteristics","title":"A classification task when labels are known - determining the country of origin of cars given the cars characteristics","text":"@btime train!(mynn,scale(xtrainFull),ytrain_oh,epochs=300,batchSize=8,rng=copy(FIXEDRNG),verbosity=NONE);\nnothing #hide","category":"page"},{"location":"tutorials/Classification - cars/betaml_tutorial_classification_cars.html#Comparisons-with-Flux","page":"A classification task when labels are known - determining the country of origin of cars given the cars characteristics","title":"Comparisons with Flux","text":"","category":"section"},{"location":"tutorials/Classification - cars/betaml_tutorial_classification_cars.html","page":"A classification task when labels are known - determining the country of origin of cars given the cars characteristics","title":"A classification task when labels are known - determining the country of origin of cars given the cars characteristics","text":"In Flux the input must be in the form (fields, observations), so we transpose our original matrices","category":"page"},{"location":"tutorials/Classification - cars/betaml_tutorial_classification_cars.html","page":"A classification task when labels are known - determining the country of origin of cars given the cars characteristics","title":"A classification task when labels are known - determining the country of origin of cars given the cars characteristics","text":"xtrainT, ytrain_ohT = transpose.([scale(xtrainFull,xScaleFactors), ytrain_oh])\nxtestT, ytest_ohT   = transpose.([scale(xtestFull,xScaleFactors), ytest_oh])","category":"page"},{"location":"tutorials/Classification - cars/betaml_tutorial_classification_cars.html","page":"A classification task when labels are known - determining the country of origin of cars given the cars characteristics","title":"A classification task when labels are known - determining the country of origin of cars given the cars characteristics","text":"We define the Flux neural network model in a similar way than BetaML and load it with data, we train it, predict and measure the accuracies on the training and the test sets:","category":"page"},{"location":"tutorials/Classification - cars/betaml_tutorial_classification_cars.html","page":"A classification task when labels are known - determining the country of origin of cars given the cars characteristics","title":"A classification task when labels are known - determining the country of origin of cars given the cars characteristics","text":"Random.seed!(123)\n\nl1         = Flux.Dense(D,ls,Flux.relu)\n#l2         = Flux.Dense(ls,ls,Flux.relu)\nl3         = Flux.Dense(ls,nCl,Flux.relu)\nFlux_nn    = Flux.Chain(l1,l3)\nloss(x, y) = Flux.logitcrossentropy(Flux_nn(x), y)\nps         = Flux.params(Flux_nn)\nnndata     = Flux.Data.DataLoader((xtrainT, ytrain_ohT), batchsize=8,shuffle=true)\nbegin for i in 1:500  Flux.train!(loss, ps, nndata, Flux.ADAM()) end end\nŷtrain     = Flux.onecold(Flux_nn(xtrainT),1:3)\nŷtest      = Flux.onecold(Flux_nn(xtestT),1:3)\ntrainAccuracy =  accuracy(ŷtrain,ytrain)\ntestAccuracy  = accuracy(ŷtest,ytest)","category":"page"},{"location":"tutorials/Classification - cars/betaml_tutorial_classification_cars.html","page":"A classification task when labels are known - determining the country of origin of cars given the cars characteristics","title":"A classification task when labels are known - determining the country of origin of cars given the cars characteristics","text":"While the train accuracy is little bit higher that BetaML, the test accuracy remains comparable","category":"page"},{"location":"tutorials/Classification - cars/betaml_tutorial_classification_cars.html","page":"A classification task when labels are known - determining the country of origin of cars given the cars characteristics","title":"A classification task when labels are known - determining the country of origin of cars given the cars characteristics","text":"However the time is again lower than BetaML, even if here for \"just\" a factor 2","category":"page"},{"location":"tutorials/Classification - cars/betaml_tutorial_classification_cars.html","page":"A classification task when labels are known - determining the country of origin of cars given the cars characteristics","title":"A classification task when labels are known - determining the country of origin of cars given the cars characteristics","text":"@btime begin for i in 1:500 Flux.train!(loss, ps, nndata, Flux.ADAM()) end end;\nnothing #hide","category":"page"},{"location":"tutorials/Classification - cars/betaml_tutorial_classification_cars.html#Summary","page":"A classification task when labels are known - determining the country of origin of cars given the cars characteristics","title":"Summary","text":"","category":"section"},{"location":"tutorials/Classification - cars/betaml_tutorial_classification_cars.html","page":"A classification task when labels are known - determining the country of origin of cars given the cars characteristics","title":"A classification task when labels are known - determining the country of origin of cars given the cars characteristics","text":"This is the summary of the results we had trying to predict the country of origin of the cars, based on their technical characteristics:","category":"page"},{"location":"tutorials/Classification - cars/betaml_tutorial_classification_cars.html","page":"A classification task when labels are known - determining the country of origin of cars given the cars characteristics","title":"A classification task when labels are known - determining the country of origin of cars given the cars characteristics","text":"Model Train acc Test Acc Training time (ms)* Training mem (MB)\nRF 0.9969 0.8025 134 196\nRF (DecisionTree.jl) 0.9969 0.7531 1.43 1.5\nNN 0.895 0.765 11841 4311\nNN (Flux.jl) 0.938 0.741 5665 1096","category":"page"},{"location":"tutorials/Classification - cars/betaml_tutorial_classification_cars.html","page":"A classification task when labels are known - determining the country of origin of cars given the cars characteristics","title":"A classification task when labels are known - determining the country of origin of cars given the cars characteristics","text":"on a Intel Core i5-8350U laptop","category":"page"},{"location":"tutorials/Classification - cars/betaml_tutorial_classification_cars.html","page":"A classification task when labels are known - determining the country of origin of cars given the cars characteristics","title":"A classification task when labels are known - determining the country of origin of cars given the cars characteristics","text":"We warn that this table just provides a rought idea of the various algorithms performances. Indeed there is a large amount of stochasticity both in the sampling of the data used for training/testing and in the initial settings of the parameters of the algorithm. For a statistically significant comparision we would have to repeat the analysis with multiple sampling (e.g. by cross-validation) and initial random parameters. Neverthless the table above shows that, when we compare BetaML with the algorithm-specific leading packages, we found similar results in terms of accuracy, but often the leading packages are better optimised and run more efficiently (but sometimes at the cost of being less verstatile). Also, for this dataset, Random Forests seems to remain marginally more accurate than Neural Network, altought of course this depends on the hyper-parameters and, with a single run of the models, we don't know if this difference is significant.","category":"page"},{"location":"tutorials/Classification - cars/betaml_tutorial_classification_cars.html","page":"A classification task when labels are known - determining the country of origin of cars given the cars characteristics","title":"A classification task when labels are known - determining the country of origin of cars given the cars characteristics","text":"View this file on Github.","category":"page"},{"location":"tutorials/Classification - cars/betaml_tutorial_classification_cars.html","page":"A classification task when labels are known - determining the country of origin of cars given the cars characteristics","title":"A classification task when labels are known - determining the country of origin of cars given the cars characteristics","text":"","category":"page"},{"location":"tutorials/Classification - cars/betaml_tutorial_classification_cars.html","page":"A classification task when labels are known - determining the country of origin of cars given the cars characteristics","title":"A classification task when labels are known - determining the country of origin of cars given the cars characteristics","text":"This page was generated using Literate.jl.","category":"page"},{"location":"tutorials/Clusterisation - Iris/betaml_tutorial_cluster_iris.html","page":"A classification task: the prediction of  plant species from floreal measures (the iris dataset)","title":"A classification task: the prediction of  plant species from floreal measures (the iris dataset)","text":"EditURL = \"https://github.com/sylvaticus/BetaML.jl/blob/master/docs/src/tutorials/Clusterisation - Iris/betaml_tutorial_cluster_iris.jl\"","category":"page"},{"location":"tutorials/Clusterisation - Iris/betaml_tutorial_cluster_iris.html#classification_clustering","page":"A classification task: the prediction of  plant species from floreal measures (the iris dataset)","title":"A classification task: the prediction of  plant species from floreal measures (the iris dataset)","text":"","category":"section"},{"location":"tutorials/Clusterisation - Iris/betaml_tutorial_cluster_iris.html","page":"A classification task: the prediction of  plant species from floreal measures (the iris dataset)","title":"A classification task: the prediction of  plant species from floreal measures (the iris dataset)","text":"The task is to estimate the species of a plant given some floreal measurements. It use the classical \"Iris\" dataset. Note that in this example we are using clustering approaches, so we try to understand the \"structure\" of our data, without relying to actually knowing the true labels (\"classes\" or \"factors\"). However we have chosen a dataset for which the true labels are actually known, so to compare the accuracy of the algorithms we use, but these labels will not be used during the algorithms training.","category":"page"},{"location":"tutorials/Clusterisation - Iris/betaml_tutorial_cluster_iris.html","page":"A classification task: the prediction of  plant species from floreal measures (the iris dataset)","title":"A classification task: the prediction of  plant species from floreal measures (the iris dataset)","text":"Data origin:","category":"page"},{"location":"tutorials/Clusterisation - Iris/betaml_tutorial_cluster_iris.html","page":"A classification task: the prediction of  plant species from floreal measures (the iris dataset)","title":"A classification task: the prediction of  plant species from floreal measures (the iris dataset)","text":"dataset description: https://en.wikipedia.org/wiki/Irisflowerdata_set\ndata source we use here: https://github.com/JuliaStats/RDatasets.jl","category":"page"},{"location":"tutorials/Clusterisation - Iris/betaml_tutorial_cluster_iris.html#Library-and-data-loading","page":"A classification task: the prediction of  plant species from floreal measures (the iris dataset)","title":"Library and data loading","text":"","category":"section"},{"location":"tutorials/Clusterisation - Iris/betaml_tutorial_cluster_iris.html","page":"A classification task: the prediction of  plant species from floreal measures (the iris dataset)","title":"A classification task: the prediction of  plant species from floreal measures (the iris dataset)","text":"We load the Beta Machine Learning Toolkit as well as some other packages that we use in this tutorial","category":"page"},{"location":"tutorials/Clusterisation - Iris/betaml_tutorial_cluster_iris.html","page":"A classification task: the prediction of  plant species from floreal measures (the iris dataset)","title":"A classification task: the prediction of  plant species from floreal measures (the iris dataset)","text":"using BetaML\nusing Random, Statistics, Logging, BenchmarkTools, RDatasets, Plots, DataFrames","category":"page"},{"location":"tutorials/Clusterisation - Iris/betaml_tutorial_cluster_iris.html","page":"A classification task: the prediction of  plant species from floreal measures (the iris dataset)","title":"A classification task: the prediction of  plant species from floreal measures (the iris dataset)","text":"We are also going to compare our results with two other leading packages in Julia for clustering analysis, Clustering.jl that provides (inter alia) kmeans and kmedoids algorithms and GaussianMixtures.jl that provides, as the name says, Gaussian Mixture Models. So we import them (we \"import\" them, rather than \"use\", not to bound their full names into namespace as some would collide with BetaML).","category":"page"},{"location":"tutorials/Clusterisation - Iris/betaml_tutorial_cluster_iris.html","page":"A classification task: the prediction of  plant species from floreal measures (the iris dataset)","title":"A classification task: the prediction of  plant species from floreal measures (the iris dataset)","text":"import Clustering, GaussianMixtures","category":"page"},{"location":"tutorials/Clusterisation - Iris/betaml_tutorial_cluster_iris.html","page":"A classification task: the prediction of  plant species from floreal measures (the iris dataset)","title":"A classification task: the prediction of  plant species from floreal measures (the iris dataset)","text":"We do a few tweeks for the Clustering and GaussianMixtures packages. Note that in BetaML we can also control both the random seed and the verbosity in the algorithm call, not only globally","category":"page"},{"location":"tutorials/Clusterisation - Iris/betaml_tutorial_cluster_iris.html","page":"A classification task: the prediction of  plant species from floreal measures (the iris dataset)","title":"A classification task: the prediction of  plant species from floreal measures (the iris dataset)","text":"Random.seed!(123)\n#logger  = Logging.SimpleLogger(stdout, Logging.Error); global_logger(logger); ## For suppressing GaussianMixtures output\nnothing #hide","category":"page"},{"location":"tutorials/Clusterisation - Iris/betaml_tutorial_cluster_iris.html","page":"A classification task: the prediction of  plant species from floreal measures (the iris dataset)","title":"A classification task: the prediction of  plant species from floreal measures (the iris dataset)","text":"Differently from the regression tutorial, we load the data here from [RDatasets](https://github.com/JuliaStats/RDatasets.jl](https://github.com/JuliaStats/RDatasets.jl), a package providing standard datasets.","category":"page"},{"location":"tutorials/Clusterisation - Iris/betaml_tutorial_cluster_iris.html","page":"A classification task: the prediction of  plant species from floreal measures (the iris dataset)","title":"A classification task: the prediction of  plant species from floreal measures (the iris dataset)","text":"iris = dataset(\"datasets\", \"iris\")\ndescribe(iris)","category":"page"},{"location":"tutorials/Clusterisation - Iris/betaml_tutorial_cluster_iris.html","page":"A classification task: the prediction of  plant species from floreal measures (the iris dataset)","title":"A classification task: the prediction of  plant species from floreal measures (the iris dataset)","text":"The iris dataset  provides floreal measures in columns 1 to 4 and the assigned species name in column 5. There are no missing values","category":"page"},{"location":"tutorials/Clusterisation - Iris/betaml_tutorial_cluster_iris.html#Data-preparation","page":"A classification task: the prediction of  plant species from floreal measures (the iris dataset)","title":"Data preparation","text":"","category":"section"},{"location":"tutorials/Clusterisation - Iris/betaml_tutorial_cluster_iris.html","page":"A classification task: the prediction of  plant species from floreal measures (the iris dataset)","title":"A classification task: the prediction of  plant species from floreal measures (the iris dataset)","text":"The first step is to prepare the data for the analysis. We collect the first 4 columns as our feature x matrix and the last one as our y label vector. As we are using clustering algorithms, we are not actually using the labels to train the algorithms, we'll behave like we do not know them, we'll just let the algorithm \"learn\" fro mthe structure of the data itself. We'll however use it to judge the accuracy that they did reach.","category":"page"},{"location":"tutorials/Clusterisation - Iris/betaml_tutorial_cluster_iris.html","page":"A classification task: the prediction of  plant species from floreal measures (the iris dataset)","title":"A classification task: the prediction of  plant species from floreal measures (the iris dataset)","text":"x       = Matrix{Float64}(iris[:,1:4]);\nyLabels = unique(iris[:,5]);\nnothing #hide","category":"page"},{"location":"tutorials/Clusterisation - Iris/betaml_tutorial_cluster_iris.html","page":"A classification task: the prediction of  plant species from floreal measures (the iris dataset)","title":"A classification task: the prediction of  plant species from floreal measures (the iris dataset)","text":"As the labels are expressed as strings, the first thing we do is encode them as integers for our analysis using the function integerEncoder.","category":"page"},{"location":"tutorials/Clusterisation - Iris/betaml_tutorial_cluster_iris.html","page":"A classification task: the prediction of  plant species from floreal measures (the iris dataset)","title":"A classification task: the prediction of  plant species from floreal measures (the iris dataset)","text":"y       = integerEncoder(iris[:,5],factors=yLabels);\nnothing #hide","category":"page"},{"location":"tutorials/Clusterisation - Iris/betaml_tutorial_cluster_iris.html","page":"A classification task: the prediction of  plant species from floreal measures (the iris dataset)","title":"A classification task: the prediction of  plant species from floreal measures (the iris dataset)","text":"The dataset from RDatasets is ordered by species, so we need to shuffle it to avoid biases. Shuffling happens by default in crossValidation, but we are keeping here a copy of the shuffled version for later. Note that the version of shuffle that is included in BetaML accepts several n-dimensional arrays and shuffle them (by default on rows, by we can specify the dimension) keeping the association  between the various arrays in the shuffled output.","category":"page"},{"location":"tutorials/Clusterisation - Iris/betaml_tutorial_cluster_iris.html","page":"A classification task: the prediction of  plant species from floreal measures (the iris dataset)","title":"A classification task: the prediction of  plant species from floreal measures (the iris dataset)","text":"(xs,ys) = shuffle([x,y]);\nnothing #hide","category":"page"},{"location":"tutorials/Clusterisation - Iris/betaml_tutorial_cluster_iris.html#Main-analysis","page":"A classification task: the prediction of  plant species from floreal measures (the iris dataset)","title":"Main analysis","text":"","category":"section"},{"location":"tutorials/Clusterisation - Iris/betaml_tutorial_cluster_iris.html","page":"A classification task: the prediction of  plant species from floreal measures (the iris dataset)","title":"A classification task: the prediction of  plant species from floreal measures (the iris dataset)","text":"We will try 3 BetaML models (kmeans, kmedoids and gmm) and we compare them with kmeans from Clusterings.jl and GMM from GaussianMixtures.jl As we are here, we also try different versions of the BetaML models, even if the default \"versions\" should be fine. For kmeans and kmedoids we will try different initialisation strategies (\"gird\", the default one, \"random\" and \"shuffle\"), while for the gmm model we'll choose different distributions of the Gaussain family (SphericalGaussian - where the variance is a scalar, DiagonalGaussian - with a vector variance, and FullGaussian, where the covariance is a matrix).","category":"page"},{"location":"tutorials/Clusterisation - Iris/betaml_tutorial_cluster_iris.html","page":"A classification task: the prediction of  plant species from floreal measures (the iris dataset)","title":"A classification task: the prediction of  plant species from floreal measures (the iris dataset)","text":"As the result would depend on stochasticity both in the data selected and in the random initialisation, we use a cross-validation approach to run our models several times (with different data) and then we average their results. Cross-Validation in BetaML is very flexible and it is done using the crossValidation function. crossValidation works by calling the function f, defined by the user, passing to it the tuple trainData, valData and rng and collecting the result of the function f. The specific method for which trainData, and valData are selected at each iteration depends on the specific sampler. We start by selectign a k-fold sampler that split our data in 5 different parts, it uses 4 for training and 1 part (not used here) for validation. We run the simulations twice and, to be sure to have replicable results, we fix the random seed (at the whole crossValidaiton level, not on each iteration).","category":"page"},{"location":"tutorials/Clusterisation - Iris/betaml_tutorial_cluster_iris.html","page":"A classification task: the prediction of  plant species from floreal measures (the iris dataset)","title":"A classification task: the prediction of  plant species from floreal measures (the iris dataset)","text":"sampler = KFold(nSplits=5,nRepeats=3,shuffle=true, rng=copy(FIXEDRNG))","category":"page"},{"location":"tutorials/Clusterisation - Iris/betaml_tutorial_cluster_iris.html","page":"A classification task: the prediction of  plant species from floreal measures (the iris dataset)","title":"A classification task: the prediction of  plant species from floreal measures (the iris dataset)","text":"We can now run the cross-validation with our models. Note that instead of defining the function f and then calling crossValidation[f(trainData,testData,rng),[x,y],...) we use the Julia do block syntax and we write directly the content of the f function in the do block. Also, by default crossValidation already returns the mean and the standard deviation of the output of the user-provided f function (or the do block). However this requires that the f function return a single scalar. Here we are returning a vector of the accuracies of the different models (so we can run the cross-validation only once), and hence we indicate with returnStatistics=false to crossValidation not to attempt to generate statistics but rather report the whole output. We'll compute the statistics ex-post.","category":"page"},{"location":"tutorials/Clusterisation - Iris/betaml_tutorial_cluster_iris.html","page":"A classification task: the prediction of  plant species from floreal measures (the iris dataset)","title":"A classification task: the prediction of  plant species from floreal measures (the iris dataset)","text":"Inside the do block we do 4 things:","category":"page"},{"location":"tutorials/Clusterisation - Iris/betaml_tutorial_cluster_iris.html","page":"A classification task: the prediction of  plant species from floreal measures (the iris dataset)","title":"A classification task: the prediction of  plant species from floreal measures (the iris dataset)","text":"we recover from trainData (a tuple, as we passed a tuple to crossValidation too) the xtrain features and ytrain labels;\nwe run the various clustering algorithms\nwe use the real labels to compute the model accuracy. Note that the clustering algorithm know nothing about the specific label name or even their order. This is why accuracy has the parameter ignoreLabels to compute the accuracy oven any possible permutation of the classes found.\nwe return the various models' accuracies","category":"page"},{"location":"tutorials/Clusterisation - Iris/betaml_tutorial_cluster_iris.html","page":"A classification task: the prediction of  plant species from floreal measures (the iris dataset)","title":"A classification task: the prediction of  plant species from floreal measures (the iris dataset)","text":"cOut = crossValidation([x,y],sampler,returnStatistics=false) do trainData,testData,rng\n          # For unsupervised learning we use only the train data.\n          # Also, we use the associated labels only to measure the performances\n         (xtrain,ytrain)  = trainData;\n         # We run the clustering algorithm...\n         clusteringOut     = kmeans(xtrain,3,rng=rng) ## init is grid by default\n         # ... and we compute the accuracy using the real labels\n         kMeansAccuracy    = accuracy(clusteringOut[1],ytrain,ignoreLabels=true)\n         clusteringOut     = kmeans(xtrain,3,rng=rng,initStrategy=\"random\")\n         kMeansRAccuracy   = accuracy(clusteringOut[1],ytrain,ignoreLabels=true)\n         clusteringOut     = kmeans(xtrain,3,rng=rng,initStrategy=\"shuffle\")\n         kMeansSAccuracy   = accuracy(clusteringOut[1],ytrain,ignoreLabels=true)\n         clusteringOut     = kmedoids(xtrain,3,rng=rng)   ## init is grid by default\n         kMedoidsAccuracy  = accuracy(clusteringOut[1],ytrain,ignoreLabels=true)\n         clusteringOut     = kmedoids(xtrain,3,rng=rng,initStrategy=\"random\")\n         kMedoidsRAccuracy = accuracy(clusteringOut[1],ytrain,ignoreLabels=true)\n         clusteringOut     = kmedoids(xtrain,3,rng=rng,initStrategy=\"shuffle\")\n         kMedoidsSAccuracy = accuracy(clusteringOut[1],ytrain,ignoreLabels=true)\n         clusteringOut     = gmm(xtrain,3,mixtures=[SphericalGaussian() for i in 1:3], verbosity=NONE, rng=rng)\n         gmmSpherAccuracy  = accuracy(clusteringOut.pₙₖ,ytrain,ignoreLabels=true, rng=rng)\n         clusteringOut     = gmm(xtrain,3,mixtures=[DiagonalGaussian() for i in 1:3], verbosity=NONE, rng=rng)\n         gmmDiagAccuracy   = accuracy(clusteringOut.pₙₖ,ytrain,ignoreLabels=true, rng=rng)\n         clusteringOut     = gmm(xtrain,3,mixtures=[FullGaussian() for i in 1:3], verbosity=NONE, rng=rng)\n         gmmFullAccuracy   = accuracy(clusteringOut.pₙₖ,ytrain,ignoreLabels=true, rng=rng)\n         # For comparision with Clustering.jl\n         clusteringOut     = Clustering.kmeans(xtrain', 3)\n         kMeans2Accuracy   = accuracy(clusteringOut.assignments,ytrain,ignoreLabels=true)\n         # For comparision with GaussianMistures.jl - sometimes GaussianMistures.jl em! fails with a PosDefException\n         dGMM              = GaussianMixtures.GMM(3, xtrain; method=:kmeans, kind=:diag)\n         GaussianMixtures.em!(dGMM, xtrain)\n         gmmDiag2Accuracy  = accuracy(GaussianMixtures.gmmposterior(dGMM, xtrain)[1],ytrain,ignoreLabels=true)\n         fGMM              = GaussianMixtures.GMM(3, xtrain; method=:kmeans, kind=:full)\n         GaussianMixtures.em!(fGMM, xtrain)\n         gmmFull2Accuracy  = accuracy(GaussianMixtures.gmmposterior(fGMM, xtrain)[1],ytrain,ignoreLabels=true)\n         # Returning the accuracies\n         return kMeansAccuracy,kMeansRAccuracy,kMeansSAccuracy,kMedoidsAccuracy,kMedoidsRAccuracy,kMedoidsSAccuracy,gmmSpherAccuracy,gmmDiagAccuracy,gmmFullAccuracy,kMeans2Accuracy,gmmDiag2Accuracy,gmmFull2Accuracy\n end\n\n# We transform the output in matrix for easier analysis\naccuracies = fill(0.0,(length(cOut),length(cOut[1])))\n[accuracies[r,c] = cOut[r][c] for r in 1:length(cOut),c in 1:length(cOut[1])]\nμs = mean(accuracies,dims=1)\nσs = std(accuracies,dims=1)\n\n\nmodelLabels=[\"kMeansG\",\"kMeansR\",\"kMeansS\",\"kMedoidsG\",\"kMedoidsR\",\"kMedoidsS\",\"gmmSpher\",\"gmmDiag\",\"gmmFull\",\"kMeans2\",\"gmmDiag2\",\"gmmFull2\"]\nreport = DataFrame(mName = modelLabels, avgAccuracy = dropdims(round.(μs',digits=3),dims=2), stdAccuracy = dropdims(round.(σs',digits=3),dims=2))","category":"page"},{"location":"tutorials/Clusterisation - Iris/betaml_tutorial_cluster_iris.html#BetaML-model-accuracies","page":"A classification task: the prediction of  plant species from floreal measures (the iris dataset)","title":"BetaML model accuracies","text":"","category":"section"},{"location":"tutorials/Clusterisation - Iris/betaml_tutorial_cluster_iris.html","page":"A classification task: the prediction of  plant species from floreal measures (the iris dataset)","title":"A classification task: the prediction of  plant species from floreal measures (the iris dataset)","text":"From the output We see that the gmm models perform for this dataset generally better than kmeans or kmedoids algorithms, also with very low variances. In detail, it is the (default) grid initialisation that leads to the better results for kmeans and kmedoids, while for the gmm models it is the FullGaussian to perform better.","category":"page"},{"location":"tutorials/Clusterisation - Iris/betaml_tutorial_cluster_iris.html#Comparisions-with-Clustering.jl-and-GaussianMixtures.jl","page":"A classification task: the prediction of  plant species from floreal measures (the iris dataset)","title":"Comparisions with Clustering.jl and GaussianMixtures.jl","text":"","category":"section"},{"location":"tutorials/Clusterisation - Iris/betaml_tutorial_cluster_iris.html","page":"A classification task: the prediction of  plant species from floreal measures (the iris dataset)","title":"A classification task: the prediction of  plant species from floreal measures (the iris dataset)","text":"For this specific case, both Clustering.jl and GaussianMixtures.jl report substantially worst accuracies, and with very high variances. But we maintain the ranking that Full Gaussian gmm > Diagonal Gaussian > Kmeans accuracy. I suspect the reason that BetaML gmm works so weel is in relation to the usage of kmeans algorithm with itself the grid initialisation. The grid initialisation \"guarantee\" indeed that the initial means of the mixture components are well spread across the multidimensional space defined by the data, and it helps avoiding the EM algoritm to converge to a bad local optimus.","category":"page"},{"location":"tutorials/Clusterisation - Iris/betaml_tutorial_cluster_iris.html#Working-without-the-labels","page":"A classification task: the prediction of  plant species from floreal measures (the iris dataset)","title":"Working without the labels","text":"","category":"section"},{"location":"tutorials/Clusterisation - Iris/betaml_tutorial_cluster_iris.html","page":"A classification task: the prediction of  plant species from floreal measures (the iris dataset)","title":"A classification task: the prediction of  plant species from floreal measures (the iris dataset)","text":"Up to now we used the real labels to compare the model accuracies. But in real clustering examples we don't have the true classes, or we wouln't need to do clustering in the first instance, so we don't know the number of classes to use. There are several methods to judge clusters algorithms goodness, perhaps the simplest one, at least for the expectation-maximisation algorithm employed in gmm to fit the data to the unknown mixture, is to use a information criteria that trade the goodness of the lickelyhood with the parameters used to do the fit. BetaML provide by default in the gmm clustering outputs both the Bayesian information criterion  (BIC) and the Akaike information criterion  (AIC), where for both a lower value is better.","category":"page"},{"location":"tutorials/Clusterisation - Iris/betaml_tutorial_cluster_iris.html","page":"A classification task: the prediction of  plant species from floreal measures (the iris dataset)","title":"A classification task: the prediction of  plant species from floreal measures (the iris dataset)","text":"We can then run the model with different number of classes and see which one leads to the lower BIC or AIC. We run hence crossValidation again with the FullGaussian gmm model Note that we use the BIC/AIC criteria here for establishing the \"best\" number of classes but we could have used it also to select the kind of Gaussain distribution to use. This is one example of hyper-parameter tuning that we developed more in detail (but without using cross-validation) in the regression tutorial.","category":"page"},{"location":"tutorials/Clusterisation - Iris/betaml_tutorial_cluster_iris.html","page":"A classification task: the prediction of  plant species from floreal measures (the iris dataset)","title":"A classification task: the prediction of  plant species from floreal measures (the iris dataset)","text":"Let's try up to 8 possible classes:","category":"page"},{"location":"tutorials/Clusterisation - Iris/betaml_tutorial_cluster_iris.html","page":"A classification task: the prediction of  plant species from floreal measures (the iris dataset)","title":"A classification task: the prediction of  plant species from floreal measures (the iris dataset)","text":"K = 8\nsampler = KFold(nSplits=5,nRepeats=2,shuffle=true, rng=copy(FIXEDRNG))\ncOut = crossValidation([x,y],sampler,returnStatistics=false) do trainData,testData,rng\n    (xtrain,ytrain)  = trainData;\n    clusteringOut  = [gmm(xtrain,k,mixtures=[FullGaussian() for i in 1:k], verbosity=NONE, rng=rng) for k in 1:K]\n    BICS           = [clusteringOut[i].BIC for i in 1:K]\n    AICS           = [clusteringOut[i].AIC for i in 1:K]\n    return (BICS,AICS)\nend\n\n# Transforming the output in matrices for easier analysis\nNit = length(cOut)\n\nBICS = fill(0.0,(Nit,K))\nAICS = fill(0.0,(Nit,K))\n[BICS[r,c] = cOut[r][1][c] for r in 1:Nit,c in 1:K]\n[AICS[r,c] = cOut[r][2][c] for r in 1:Nit,c in 1:K]\n\nμsBICS = mean(BICS,dims=1)","category":"page"},{"location":"tutorials/Clusterisation - Iris/betaml_tutorial_cluster_iris.html","page":"A classification task: the prediction of  plant species from floreal measures (the iris dataset)","title":"A classification task: the prediction of  plant species from floreal measures (the iris dataset)","text":"σsBICS = std(BICS,dims=1)","category":"page"},{"location":"tutorials/Clusterisation - Iris/betaml_tutorial_cluster_iris.html","page":"A classification task: the prediction of  plant species from floreal measures (the iris dataset)","title":"A classification task: the prediction of  plant species from floreal measures (the iris dataset)","text":"μsAICS = mean(AICS,dims=1)","category":"page"},{"location":"tutorials/Clusterisation - Iris/betaml_tutorial_cluster_iris.html","page":"A classification task: the prediction of  plant species from floreal measures (the iris dataset)","title":"A classification task: the prediction of  plant species from floreal measures (the iris dataset)","text":"σsAICS = std(AICS,dims=1)","category":"page"},{"location":"tutorials/Clusterisation - Iris/betaml_tutorial_cluster_iris.html","page":"A classification task: the prediction of  plant species from floreal measures (the iris dataset)","title":"A classification task: the prediction of  plant species from floreal measures (the iris dataset)","text":"plot(1:K,[μsBICS' μsAICS'], labels=[\"BIC\" \"AIC\"], title=\"Information criteria by number of classes\", xlabel=\"number of classes\", ylabel=\"lower is better\")","category":"page"},{"location":"tutorials/Clusterisation - Iris/betaml_tutorial_cluster_iris.html","page":"A classification task: the prediction of  plant species from floreal measures (the iris dataset)","title":"A classification task: the prediction of  plant species from floreal measures (the iris dataset)","text":"We see that following the \"lowest AIC\" rule we would indeed choose three classes, while following the \"best AIC\" criteria we would have choosen only two classes. This means that there is two classes that, concerning the floreal measures used in the database, are very similar, and opur models are unsure about them. Perhaps the biologists will end up one day with the conclusion that it is indeed only one specie :-).","category":"page"},{"location":"tutorials/Clusterisation - Iris/betaml_tutorial_cluster_iris.html","page":"A classification task: the prediction of  plant species from floreal measures (the iris dataset)","title":"A classification task: the prediction of  plant species from floreal measures (the iris dataset)","text":"We could study this issue more in detail by analysing the ConfusionMatrix, but the one used in BetaML does not account for the ignoreLabels option (yet).","category":"page"},{"location":"tutorials/Clusterisation - Iris/betaml_tutorial_cluster_iris.html#Benchmarking-computational-efficiency","page":"A classification task: the prediction of  plant species from floreal measures (the iris dataset)","title":"Benchmarking computational efficiency","text":"","category":"section"},{"location":"tutorials/Clusterisation - Iris/betaml_tutorial_cluster_iris.html","page":"A classification task: the prediction of  plant species from floreal measures (the iris dataset)","title":"A classification task: the prediction of  plant species from floreal measures (the iris dataset)","text":"We now benchmark the time and memory required by the various models by using the @btime macro of the BenchmarkTools package:","category":"page"},{"location":"tutorials/Clusterisation - Iris/betaml_tutorial_cluster_iris.html","page":"A classification task: the prediction of  plant species from floreal measures (the iris dataset)","title":"A classification task: the prediction of  plant species from floreal measures (the iris dataset)","text":"@btime kmeans($xs,3);\n# 261.540 μs (3777 allocations: 442.53 KiB)\n@btime kmedoids($xs,3);\n4.576 ms (97356 allocations: 10.42 MiB)\n@btime gmm($xs,3,mixtures=[SphericalGaussian() for i in 1:3], verbosity=NONE);\n# 5.498 ms (133365 allocations: 8.42 MiB)\n@btime gmm($xs,3,mixtures=[DiagonalGaussian() for i in 1:3], verbosity=NONE);\n# 18.901 ms (404333 allocations: 25.65 MiB)\n@btime gmm($xs,3,mixtures=[FullGaussian() for i in 1:3], verbosity=NONE);\n# 49.257 ms (351500 allocations: 61.95 MiB)\n@btime Clustering.kmeans($xs', 3);\n# 17.071 μs (23 allocations: 14.31 KiB)\n@btime begin dGMM = GaussianMixtures.GMM(3, $xs; method=:kmeans, kind=:diag); GaussianMixtures.em!(dGMM, $xs) end;\n# 530.528 μs (2088 allocations: 488.05 KiB)\n@btime begin fGMM = GaussianMixtures.GMM(3, $xs; method=:kmeans, kind=:full); GaussianMixtures.em!(fGMM, $xs) end;\n# 4.166 ms (58910 allocations: 3.59 MiB)","category":"page"},{"location":"tutorials/Clusterisation - Iris/betaml_tutorial_cluster_iris.html","page":"A classification task: the prediction of  plant species from floreal measures (the iris dataset)","title":"A classification task: the prediction of  plant species from floreal measures (the iris dataset)","text":"(note: the values reported here are of a local pc, not of the GitHub CI server, as sometimes - depending on data and random initialisation - GaussainMixtures.em!fails with aPosDefException`. This in turln would lead the whole documentation to fail to compile)","category":"page"},{"location":"tutorials/Clusterisation - Iris/betaml_tutorial_cluster_iris.html","page":"A classification task: the prediction of  plant species from floreal measures (the iris dataset)","title":"A classification task: the prediction of  plant species from floreal measures (the iris dataset)","text":"Like for supervised models, dedicated models are much better optimized than BetaML models, and are order of magnitude more efficient. However even the slowest BetaML clusering model (gmm using full gaussians) is realtively fast and can handle mid-size datasets (tens to hundreds of thousand records) without significant slow downs.","category":"page"},{"location":"tutorials/Clusterisation - Iris/betaml_tutorial_cluster_iris.html#Conclusions","page":"A classification task: the prediction of  plant species from floreal measures (the iris dataset)","title":"Conclusions","text":"","category":"section"},{"location":"tutorials/Clusterisation - Iris/betaml_tutorial_cluster_iris.html","page":"A classification task: the prediction of  plant species from floreal measures (the iris dataset)","title":"A classification task: the prediction of  plant species from floreal measures (the iris dataset)","text":"We have shown in this tutorial how we can easily run clustering almgorithms in BetaML with just one line of code choosenModel(x,k), but also how can we use cross-validation in order to help the model or parameter selection, with or whithout knowing the real classes. We retrieve here what we observed with supervised models. Globally the accuracy of BetaML models are comparable to those of leading specialised packages (in this case they are even better), but there is a significant gap in computational efficiency that restricts the pratical usage of BetaML to mid-size datasets. However we trade this relative inefficiency with very flexible model definition and utility functions (for example the BetaML gmm works with missing data, allowing it to be used as the backbone of the predictMissing missing imputation function, or for collaborative reccomendation systems).","category":"page"},{"location":"tutorials/Clusterisation - Iris/betaml_tutorial_cluster_iris.html","page":"A classification task: the prediction of  plant species from floreal measures (the iris dataset)","title":"A classification task: the prediction of  plant species from floreal measures (the iris dataset)","text":"View this file on Github.","category":"page"},{"location":"tutorials/Clusterisation - Iris/betaml_tutorial_cluster_iris.html","page":"A classification task: the prediction of  plant species from floreal measures (the iris dataset)","title":"A classification task: the prediction of  plant species from floreal measures (the iris dataset)","text":"","category":"page"},{"location":"tutorials/Clusterisation - Iris/betaml_tutorial_cluster_iris.html","page":"A classification task: the prediction of  plant species from floreal measures (the iris dataset)","title":"A classification task: the prediction of  plant species from floreal measures (the iris dataset)","text":"This page was generated using Literate.jl.","category":"page"},{"location":"index.html#![BLogos](assets/BetaML_logo_30x30.png)-BetaML.jl-Documentation","page":"Index","title":"(Image: BLogos) BetaML.jl Documentation","text":"","category":"section"},{"location":"index.html","page":"Index","title":"Index","text":"Welcome to the documentation of the Beta Machine Learning toolkit.","category":"page"},{"location":"index.html#About","page":"Index","title":"About","text":"","category":"section"},{"location":"index.html","page":"Index","title":"Index","text":"The BetaML toolkit provides classical algorithms written in the Julia programming language useful to \"learn\" the relationship between some inputs and some outputs, with the objective to make accurate predictions of the output given new inputs (\"supervised machine learning\") or to better understand the structure of the data, perhaps hidden because of the high dimensionality (\"unsupervised machine learning\").","category":"page"},{"location":"index.html","page":"Index","title":"Index","text":"While specific packages exist for state-of-the art implementations of these algorithms (see the section \"Alternative Packages\"), thanks to the Just-In-Time compilation nature of Julia, BetaML is reasonably fast for datasets that fit in memory while keeping both the code and the usage as simple as possible.","category":"page"},{"location":"index.html","page":"Index","title":"Index","text":"Aside the algorithms themselves, BetaML provides many \"utility\" functions. Because algorithms are all self-contained in the library itself (you are invited to explore their source code by typing @edit functionOfInterest(par1,par2)), the utility functions have APIs that are coordinated with the algorithms, facilitating the \"preparation\" of the data for the analysis, the evaluation of the models or the implementation of several models in chains (pipelines). While BetaML doesn't provide itself tools for hyper-parameters optimisation or complex pipeline building tools, most models have an interface for the MLJ framework that allows it.","category":"page"},{"location":"index.html","page":"Index","title":"Index","text":"Aside Julia, BetaML can be accessed in R or Python using respectively JuliaCall and PyJulia. See here for a tutorial or the examples for some actual use of the Beta Machine Learning Toolkit in R/Python.","category":"page"},{"location":"index.html#Installation","page":"Index","title":"Installation","text":"","category":"section"},{"location":"index.html","page":"Index","title":"Index","text":"The BetaML package is included in the standard Julia register, install it with:","category":"page"},{"location":"index.html","page":"Index","title":"Index","text":"] add BetaML","category":"page"},{"location":"index.html#Loading-the-module(s)","page":"Index","title":"Loading the module(s)","text":"","category":"section"},{"location":"index.html","page":"Index","title":"Index","text":"This package is split in several submodules, but all modules are re-exported at the root module level. This means that you can access their functionality by simply using BetaML.","category":"page"},{"location":"index.html","page":"Index","title":"Index","text":"using BetaML\nmyLayer = DenseLayer(2,3) # DenseLayer is defined in the Nn submodule\nres     = kernelPerceptron([1.1 2.1; 5.3 4.2; 1.8 1.7], [-1,1,-1]) # kernelPerceptron is defined in the Perceptron module\n@edit DenseLayer(2,3)     # Open a text editor with to the relevant source code","category":"page"},{"location":"index.html#Usage","page":"Index","title":"Usage","text":"","category":"section"},{"location":"index.html","page":"Index","title":"Index","text":"Documentation for most algorithms can be retrieved using the inline Julia help system (just press the question mark ? and then, on the special help prompt help?>, type the function name).","category":"page"},{"location":"index.html","page":"Index","title":"Index","text":"For a list of supported algorithms please look at the individual modules:","category":"page"},{"location":"index.html","page":"Index","title":"Index","text":"BetaML.Perceptron: The Perceptron, Kernel Perceptron and Pegasos classification algorithms;\nBetaML.Trees: The Decision Trees and Random Forests algorithms for classification or regression (with missing values supported);\nBetaML.Nn: Implementation of Artificial Neural Networks;\nBetaML.Clustering`: Clustering algorithms (Kmeans, Mdedoids, EM/GMM) and missing imputation / collaborative filtering / recommandation systems using clusters;\nBetaML.Utils`: Various utility functions (scale, one-hot, distances, kernels, pca, accuracy/error measures..).","category":"page"},{"location":"index.html#Examples","page":"Index","title":"Examples","text":"","category":"section"},{"location":"index.html","page":"Index","title":"Index","text":"See the tutorial page for a more hand-to-hand to the below and other examples","category":"page"},{"location":"index.html","page":"Index","title":"Index","text":"Using an Artificial Neural Network for multinomial categorisation","category":"page"},{"location":"index.html","page":"Index","title":"Index","text":"# Load Modules\nusing BetaML, DelimitedFiles, Random, StatsPlots # Load the main module and ausiliary modules\nRandom.seed!(123); # Fix the random seed (to obtain reproducible results)\n\n# Load the data\niris     = readdlm(joinpath(dirname(pathof(BetaML)),\"..\",\"test\",\"data\",\"iris.csv\"),',',skipstart=1)\niris     = iris[shuffle(axes(iris, 1)), :] # Shuffle the records, as they aren't by default\nx        = convert(Array{Float64,2}, iris[:,1:4])\ny        = map(x->Dict(\"setosa\" => 1, \"versicolor\" => 2, \"virginica\" =>3)[x],iris[:, 5]) # Convert the target column to numbers\ny_oh     = oneHotEncoder(y) # Convert to One-hot representation (e.g. 2 => [0 1 0], 3 => [0 0 1])\n\n# Split the data in training/testing sets\n((xtrain,xtest),(ytrain,ytest),(ytrain_oh,ytest_oh)) = partition([x,y,y_oh],[0.8,0.2],shuffle=false)\n(ntrain, ntest) = size.([xtrain,xtest],1)\n\n# Define the Artificial Neural Network model\nl1   = DenseLayer(4,10,f=relu) # Activation function is ReLU\nl2   = DenseLayer(10,3)        # Activation function is identity by default\nl3   = VectorFunctionLayer(3,3,f=softMax) # Add a (parameterless) layer whose activation function (softMax in this case) is defined to all its nodes at once\nmynn = buildNetwork([l1,l2,l3],squaredCost,name=\"Multinomial logistic regression Model Sepal\") # Build the NN and use the squared cost (aka MSE) as error function\n\n# Training it (default to ADAM)\nres = train!(mynn,scale(xtrain),ytrain_oh,epochs=100,batchSize=6) # Use optAlg=SGD (Stochastic Gradient Descent) by default\n\n# Test it\nŷtrain        = predict(mynn,scale(xtrain))   # Note the scaling function\nŷtest         = predict(mynn,scale(xtest))\ntrainAccuracy = accuracy(ŷtrain,ytrain,tol=1) # 0.983\ntestAccuracy  = accuracy(ŷtest,ytest,tol=1)   # 1.0\n\n# Visualise results\ntestSize    = size(ŷtest,1)\nŷtestChosen =  [argmax(ŷtest[i,:]) for i in 1:testSize]\ngroupedbar([ytest ŷtestChosen], label=[\"ytest\" \"ŷtest (est)\"], title=\"True vs estimated categories\") # All records correctly labelled !\nplot(0:res.epochs,res.ϵ_epochs, ylabel=\"epochs\",xlabel=\"error\",legend=nothing,title=\"Avg. error per epoch on the Sepal dataset\")","category":"page"},{"location":"index.html","page":"Index","title":"Index","text":"(Image: results)","category":"page"},{"location":"index.html","page":"Index","title":"Index","text":"(Image: results)","category":"page"},{"location":"index.html","page":"Index","title":"Index","text":"Using the Expectation-Maximisation algorithm for clustering","category":"page"},{"location":"index.html","page":"Index","title":"Index","text":"using BetaML, DelimitedFiles, Random, StatsPlots # Load the main module and ausiliary modules\nRandom.seed!(123); # Fix the random seed (to obtain reproducible results)\n\n# Load the data\niris     = readdlm(joinpath(dirname(pathof(BetaML)),\"..\",\"test\",\"data\",\"iris.csv\"),',',skipstart=1)\niris     = iris[shuffle(axes(iris, 1)), :] # Shuffle the records, as they aren't by default\nx        = convert(Array{Float64,2}, iris[:,1:4])\nx        = scale(x) # normalise all dimensions to (μ=0, σ=1)\ny        = map(x->Dict(\"setosa\" => 1, \"versicolor\" => 2, \"virginica\" =>3)[x],iris[:, 5]) # Convert the target column to numbers\n\n# Get some ranges of minVariance and minCovariance to test\nminVarRange   = collect(0.04:0.05:1.5)\nminCovarRange = collect(0:0.05:1.45)\n\n# Run the gmm(em) algorithm for the various cases...\nsphOut  = [gmm(x,3,mixtures=[SphericalGaussian() for i in 1:3],minVariance=v, minCovariance=cv, verbosity=NONE) for v in minVarRange, cv in minCovarRange[1:1]]\ndiagOut  = [gmm(x,3,mixtures=[DiagonalGaussian() for i in 1:3],minVariance=v, minCovariance=cv, verbosity=NONE)  for v in minVarRange, cv in minCovarRange[1:1]]\nfullOut = [gmm(x,3,mixtures=[FullGaussian() for i in 1:3],minVariance=v, minCovariance=cv, verbosity=NONE)  for v in minVarRange, cv in minCovarRange]\n\n# Get the Bayesian information criterion (AIC is also available)\nsphBIC = [sphOut[v,cv].BIC for v in 1:length(minVarRange), cv in 1:1]\ndiagBIC = [diagOut[v,cv].BIC for v in 1:length(minVarRange), cv in 1:1]\nfullBIC = [fullOut[v,cv].BIC for v in 1:length(minVarRange), cv in 1:length(minCovarRange)]\n\n# Compare the accuracy with true categories\nsphAcc  = [accuracy(sphOut[v,cv].pₙₖ,y,ignoreLabels=true) for v in 1:length(minVarRange), cv in 1:1]\ndiagAcc = [accuracy(diagOut[v,cv].pₙₖ,y,ignoreLabels=true) for v in 1:length(minVarRange), cv in 1:1]\nfullAcc = [accuracy(fullOut[v,cv].pₙₖ,y,ignoreLabels=true) for v in 1:length(minVarRange), cv in 1:length(minCovarRange)]\n\nplot(minVarRange,[sphBIC diagBIC fullBIC[:,1] fullBIC[:,15] fullBIC[:,30]], markershape=:circle, label=[\"sph\" \"diag\" \"full (cov=0)\" \"full (cov=0.7)\" \"full (cov=1.45)\"], title=\"BIC\", xlabel=\"minVariance\")\nplot(minVarRange,[sphAcc diagAcc fullAcc[:,1] fullAcc[:,15] fullAcc[:,30]], markershape=:circle, label=[\"sph\" \"diag\" \"full (cov=0)\" \"full (cov=0.7)\" \"full (cov=1.45)\"], title=\"Accuracies\", xlabel=\"minVariance\")","category":"page"},{"location":"index.html","page":"Index","title":"Index","text":"(Image: results)","category":"page"},{"location":"index.html","page":"Index","title":"Index","text":"(Image: results)","category":"page"},{"location":"index.html","page":"Index","title":"Index","text":"Further examples","category":"page"},{"location":"index.html","page":"Index","title":"Index","text":"We also provide some Jupyter notebooks that can be run online without installing anything, so you can start playing with the library in minutes. Finally, you may want to give a look at the \"test\" folder. While the primary reason of the scripts under the \"test\" folder is to provide automatic testing of the BetaML toolkit, they can also be used to see how functions should be called, as virtually all functions provided by BetaML are tested there.","category":"page"},{"location":"index.html#Acknowledgements","page":"Index","title":"Acknowledgements","text":"","category":"section"},{"location":"index.html","page":"Index","title":"Index","text":"The development of this package at the Bureau d'Economie Théorique et Appliquée (BETA, Nancy) was supported by the French National Research Agency through the Laboratory of Excellence ARBRE, a part of the “Investissements d'Avenir” Program (ANR 11 – LABX-0002-01).","category":"page"},{"location":"index.html","page":"Index","title":"Index","text":"(Image: BLogos)","category":"page"}]
}
