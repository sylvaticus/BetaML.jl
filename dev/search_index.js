var documenterSearchIndex = {"docs":
[{"location":"Api.html#api_module","page":"The Api module","title":"The BetaML.Api Module","text":"","category":"section"},{"location":"Api.html","page":"The Api module","title":"The Api module","text":"Api","category":"page"},{"location":"Api.html#BetaML.Api","page":"The Api module","title":"BetaML.Api","text":"Api\n\nThe Api Module (currently v2)\n\nThis module includes the shared api trough the various BetaML submodules, i.e. names used by more than one submodule.\n\nModules are free to use other functions but these are defined here to avoid name conflicts and allows instead Multiple Dispatch to handle them. For a user-prospective overall description of the BetaML API see the page API V2 → Introduction for users, while for the implementation of the API see the page API V2 → For developers\n\n\n\n\n\n","category":"module"},{"location":"Api.html#Module-Index","page":"The Api module","title":"Module Index","text":"","category":"section"},{"location":"Api.html","page":"The Api module","title":"The Api module","text":"Modules = [Api]\nOrder   = [:constant, :type, :function, :macro]\nPrivate = false","category":"page"},{"location":"Api.html#Detailed-API","page":"The Api module","title":"Detailed API","text":"","category":"section"},{"location":"Api.html","page":"The Api module","title":"The Api module","text":"Modules = [Api]\nPrivate = false","category":"page"},{"location":"Api.html#BetaML.Api.FIXEDRNG","page":"The Api module","title":"BetaML.Api.FIXEDRNG","text":"Fixed ring to allow reproducible results\n\nUse it with:\n\nmyAlgorithm(;rng=FIXEDRNG)         # always produce the same sequence of results on each run of the script (\"pulling\" from the same rng object on different calls)\nmyAlgorithm(;rng=copy(FIXEDRNG))   # always produce the same result (new rng object on each function call)\n\n\n\n\n\n","category":"constant"},{"location":"Api.html#BetaML.Api.FIXEDSEED","page":"The Api module","title":"BetaML.Api.FIXEDSEED","text":"const FIXEDSEED\n\nFixed seed to allow reproducible results. This is the seed used to obtain the same results under unit tests.\n\nUse it with:\n\nmyAlgorithm(;rng=MyChoosenRNG(FIXEDSEED))             # always produce the same sequence of results on each run of the script (\"pulling\" from the same rng object on different calls)\nmyAlgorithm(;rng=copy(MyChoosenRNG(FIXEDSEED)))        # always produce the same result (new rng object on each call)\n\n\n\n\n\n","category":"constant"},{"location":"Api.html#BetaML.Api.BetaMLDefaultOptionsSet","page":"The Api module","title":"BetaML.Api.BetaMLDefaultOptionsSet","text":"mutable struct BetaMLDefaultOptionsSet\n\nA struct defining the options used by default by the algorithms that do not override it with their own option sets.\n\nFields:\n\ncache::Bool\nCache the results of the fitting stage, as to allow predict(mod) [default: true]. Set it to false to save memory for large data.\ndescr::String\nAn optional title and/or description for this model\nverbosity::Verbosity\nThe verbosity level to be used in training or prediction (see ?Verbosity) [deafult: STD]\nrng::Random.AbstractRNG\nRandom Number Generator (see ?FIXEDSEED) [deafult: Random.GLOBAL_RNG]\n\nNotes:\n\neven if a model doesn't override BetaMLDefaultOptionsSet, may not use all its options, for example deterministic models would not make use of the rng parameter. Passing such parameters in these cases would simply have no influence.\n\nExample:\n\njulia> options = BetaMLDefaultOptionsSet(cache=false,descr=\"My model\")\n\n\n\n\n\n","category":"type"},{"location":"Api.html#BetaML.Api.Verbosity","page":"The Api module","title":"BetaML.Api.Verbosity","text":"primitive type Verbosity <: Enum{Int32} 32\n\nMany models and functions accept a verbosity parameter.\n\nChoose between: NONE, LOW, STD [default], HIGH and FULL.\n\n\n\n\n\n","category":"type"},{"location":"Api.html#BetaML.Api.fit!-Tuple{BetaMLModel, Any}","page":"The Api module","title":"BetaML.Api.fit!","text":"fit!(m::BetaMLModel,X,[y])\n\nFit (\"train\") a BetaMLModel (i.e. learn the algorithm's parameters) based on data, either only features or features and labels.\n\nEach specific model implements its own version of fit!(m,X,[Y]), but the usage is consistent across models.\n\nNotes:\n\nFor online algorithms, i.e. models that support updating of the learned parameters with new data, fit! can be repeated as new data arrive, altought not all algorithms guarantee that training each record at the time is equivalent to train all the records at once.\nIf the model has been trained while having the cache option set on true (by default) fit! returns ŷ instead of nothing effectively making it behave like a fit-and-transform function. \n\n\n\n\n\n","category":"method"},{"location":"Api.html#BetaML.Api.hyperparameters-Tuple{BetaMLModel}","page":"The Api module","title":"BetaML.Api.hyperparameters","text":"hyperparameters(m::BetaMLModel)\n\nReturns the hyperparameters of a BetaML model. See also ?options for the parameters that do not directly affect learning.\n\nwarning: Warning\nThe returned object is a reference, so if it is modified, the relative object in the model will change too.\n\n\n\n\n\n","category":"method"},{"location":"Api.html#BetaML.Api.inverse_predict-Tuple{BetaMLModel, Any}","page":"The Api module","title":"BetaML.Api.inverse_predict","text":"inverse_predict(m::BetaMLModel,X)\n\nGiven a model m that fitted on x produces xnew, it takes xnew to return (possibly an approximation of ) x.\n\nFor example, when OneHotEncoder is fitted with a subset of the possible categories and the handle_unknown option is set on infrequent, inverse_transform will aggregate all the other categories as specified in other_categories_name.\n\nNotes:\n\nInplemented only in a few models.\n\n\n\n\n\n","category":"method"},{"location":"Api.html#BetaML.Api.model_load","page":"The Api module","title":"BetaML.Api.model_load","text":"model_load(filename::AbstractString)\nmodel_load(filename::AbstractString,args::AbstractString...)\n\nLoad from file one or more BetaML models (wheter fitted or not).\n\nNotes:\n\nIf no model names to retrieve are specified it returns a dictionary keyed with the model names\nIf multiple models are demanded, a tuple is returned\nFor further options see the documentation of the function load of the JLD2 package\n\nExamples:\n\njulia> models = model_load(\"fittedModels.jl\"; mod1Name=mod1,mod2)\njulia> mod1 = model_load(\"fittedModels.jl\",mod1)\njulia> (mod1,mod2) = model_load(\"fittedModels.jl\",\"mod1\", \"mod2\")\n\n\n\n\n\n","category":"function"},{"location":"Api.html#BetaML.Api.model_save","page":"The Api module","title":"BetaML.Api.model_save","text":"model_save(filename::AbstractString,overwrite_file::Bool=false;kwargs...)\n\nAllow to save one or more BetaML models (wheter fitted or not), eventually specifying a name for each of them.\n\nParameters:\n\nfilename: Name of the destination file\noverwrite_file: Wheter to overrite the file if it alreaxy exist or preserve it (for the objects different than the one that are going to be saved) [def: false, i.e. preserve the file]\nkwargs: model objects to be saved, eventually associated with a different name to save the mwith (e.g. mod1Name=mod1,mod2)  \n\nNotes:\n\nIf an object with the given name already exists on the destination JLD2 file it will be ovenwritten.\nIf the file exists, but it is not a JLD2 file and the option overwrite_file is set to false, an error will be raisen.\nUse the semicolon ; to separate the filename from the model(s) to save\nFor further options see the documentation of the JLD2 package\n\nExamples\n\njulia> model_save(\"fittedModels.jl\"; mod1Name=mod1,mod2)\n\n\n\n\n\n","category":"function"},{"location":"Api.html#BetaML.Api.options-Tuple{BetaMLModel}","page":"The Api module","title":"BetaML.Api.options","text":"options(m::BetaMLModel)\n\nReturns the non-learning related options of a BetaML model. See also ?hyperparameters for the parameters that directly affect learning.\n\nwarning: Warning\nThe returned object is a reference, so if it is modified, the relative object in the model will change too.\n\n\n\n\n\n","category":"method"},{"location":"Api.html#BetaML.Api.parameters-Tuple{BetaMLModel}","page":"The Api module","title":"BetaML.Api.parameters","text":"parameters(m::BetaMLModel)\n\nReturns the learned parameters of a BetaML model.\n\nwarning: Warning\nThe returned object is a reference, so if it is modified, the relative object in the model will change too.\n\n\n\n\n\n","category":"method"},{"location":"Api.html#BetaML.Api.predict-Tuple{BetaMLModel}","page":"The Api module","title":"BetaML.Api.predict","text":"predict(m::BetaMLModel,[X])\n\nPredict new information (including transformation) based on a fitted BetaMLModel, eventually applied to new features when the algorithm generalises to new data.\n\nNotes:\n\nAs a convenience, if the model has been trained while having the cache option set on true (by default) the predictions associated with the last training of the model is retained in the  model object and can be retrieved simply with predict(m).\n\n\n\n\n\n","category":"method"},{"location":"Api.html#BetaML.Api.reset!-Tuple{BetaMLModel}","page":"The Api module","title":"BetaML.Api.reset!","text":"reset!(m::BetaMLModel)\n\nReset the parameters of a trained model.\n\n\n\n\n\n","category":"method"},{"location":"Imputation.html#imputation_module","page":"Imputation","title":"The BetaML.Imputation Module","text":"","category":"section"},{"location":"Imputation.html","page":"Imputation","title":"Imputation","text":"Imputation","category":"page"},{"location":"Imputation.html#BetaML.Imputation","page":"Imputation","title":"BetaML.Imputation","text":"Imputation module\n\nProvide various imputation methods for missing data. Note that the interpretation of \"missing\" can be very wide. For example, reccomendation systems / collaborative filtering (e.g. suggestion of the film to watch) can well be representated as a missing data to impute problem, often with better results than traditional algorithms as k-nearest neighbors (KNN)\n\nProvided imputers:\n\nFeatureBasedImputer: Impute data using the feature (column) mean, optionally normalised by l-norms of the records (rows) (fastest)\nGMMImputer: Impute data using a Generative (Gaussian) Mixture Model (good trade off)\nRFImputer: Impute missing data using Random Forests, with optional replicable multiple imputations (most accurate).\nGenericImputer: Impute missing data using a vector (one per column) of arbitrary learning models (classifiers/regressors) that implement m = Model([options]), fit!(m,X,Y) and predict(m,X).\n\nImputations for all these models can be optained by running mod = ImputatorModel([options]), fit!(mod,X). The data with the missing values imputed can then be obtained with predict(mod). Useinfo(m::Imputer) to retrieve further information concerning the imputation. Trained models can be also used to impute missing values in new data with predict(mox,xNew). Note that if multiple imputations are run (for the supporting imputators) predict() will return a vector of predictions rather than a single one`.\n\nExample\n\njulia> using Statistics, BetaML\n\njulia> X            = [2 missing 10; 2000 4000 1000; 2000 4000 10000; 3 5 12 ; 4 8 20; 1 2 5]\n6×3 Matrix{Union{Missing, Int64}}:\n    2      missing     10\n 2000  4000          1000\n 2000  4000         10000\n    3     5            12\n    4     8            20\n    1     2             5\n\njulia> mod          = RFImputer(multiple_imputations=10,  rng=copy(FIXEDRNG));\n\njulia> fit!(mod,X);\n\njulia> vals         = predict(mod)\n10-element Vector{Matrix{Union{Missing, Int64}}}:\n [2 3 10; 2000 4000 1000; … ; 4 8 20; 1 2 5]\n [2 4 10; 2000 4000 1000; … ; 4 8 20; 1 2 5]\n [2 4 10; 2000 4000 1000; … ; 4 8 20; 1 2 5]\n [2 136 10; 2000 4000 1000; … ; 4 8 20; 1 2 5]\n [2 137 10; 2000 4000 1000; … ; 4 8 20; 1 2 5]\n [2 4 10; 2000 4000 1000; … ; 4 8 20; 1 2 5]\n [2 4 10; 2000 4000 1000; … ; 4 8 20; 1 2 5]\n [2 4 10; 2000 4000 1000; … ; 4 8 20; 1 2 5]\n [2 137 10; 2000 4000 1000; … ; 4 8 20; 1 2 5]\n [2 137 10; 2000 4000 1000; … ; 4 8 20; 1 2 5]\n\njulia> nR,nC        = size(vals[1])\n(6, 3)\n\njulia> medianValues = [median([v[r,c] for v in vals]) for r in 1:nR, c in 1:nC]\n6×3 Matrix{Float64}:\n    2.0     4.0     10.0\n 2000.0  4000.0   1000.0\n 2000.0  4000.0  10000.0\n    3.0     5.0     12.0\n    4.0     8.0     20.0\n    1.0     2.0      5.0\n\njulia> infos        = info(mod);\n\njulia> infos[:n_imputed_values]\n1\n\n\n\n\n\n","category":"module"},{"location":"Imputation.html#Module-Index","page":"Imputation","title":"Module Index","text":"","category":"section"},{"location":"Imputation.html","page":"Imputation","title":"Imputation","text":"Modules = [Imputation]\nOrder   = [:constant, :type, :function, :macro]\nPrivate = false","category":"page"},{"location":"Imputation.html#Detailed-API","page":"Imputation","title":"Detailed API","text":"","category":"section"},{"location":"Imputation.html","page":"Imputation","title":"Imputation","text":"Modules = [Imputation]\nPrivate = false","category":"page"},{"location":"Imputation.html#BetaML.Imputation.FeatureBasedImputer","page":"Imputation","title":"BetaML.Imputation.FeatureBasedImputer","text":"mutable struct FeatureBasedImputer <: Imputer\n\nSimple imputer using the missing data's feature (column) statistic (def: mean), optionally normalised by l-norms of the records (rows)\n\nParameters:\n\nstatistics: The descriptive statistic of the column (feature) to use as imputed value [def: mean]\nnorm: Normalise the feature mean by l-norm norm of the records [default: nothing]. Use it (e.g. norm=1 to use the l-1 norm) if the records are highly heterogeneus (e.g. quantity exports of different countries).  \n\nLimitations:\n\ndata must be numerical\n\n\n\n\n\n","category":"type"},{"location":"Imputation.html#BetaML.Imputation.FeatureBasedImputerHyperParametersSet","page":"Imputation","title":"BetaML.Imputation.FeatureBasedImputerHyperParametersSet","text":"mutable struct FeatureBasedImputerHyperParametersSet <: BetaMLHyperParametersSet\n\nHyperparameters for the FeatureBasedImputer model\n\nParameters:\n\nstatistic::Function\nThe descriptive statistic of the column (feature) to use as imputed value [def: mean]\nnorm::Union{Nothing, Int64}\nNormalise the feature mean by l-norm norm of the records [default: nothing]. Use it (e.g. norm=1 to use the l-1 norm) if the records are highly heterogeneus (e.g. quantity exports of different countries).\n\n\n\n\n\n","category":"type"},{"location":"Imputation.html#BetaML.Imputation.GMMImputer","page":"Imputation","title":"BetaML.Imputation.GMMImputer","text":"mutable struct GMMImputer <: Imputer\n\nMissing data imputer that uses a Generative (Gaussian) Mixture Model.\n\nFor the parameters (n_classes,mixtures,..) see  GMMImputerLearnableParameters.\n\nLimitations:\n\ndata must be numerical\nthe resulted matrix is a Matrix{Float64}\ncurrently the Mixtures available do not support random initialisation for missing imputation, and the rest of the algorithm (Expectation-Maximisation) is deterministic, so there is no random component involved (i.e. no multiple imputations)\n\n\n\n\n\n","category":"type"},{"location":"Imputation.html#BetaML.Imputation.RFImputer","page":"Imputation","title":"BetaML.Imputation.RFImputer","text":"mutable struct RFImputer <: Imputer\n\nImpute missing data using Random Forests, with optional replicable multiple imputations. \n\nSee RFImputerHyperParametersSet, RFHyperParametersSet and BetaMLDefaultOptionsSet for the parameters.\n\nNotes:\n\nGiven a certain RNG and its status (e.g. RFImputer(...,rng=StableRNG(FIXEDSEED))), the algorithm is completely deterministic, i.e. replicable. \nThe algorithm accepts virtually any kind of data, sortable or not\n\n\n\n\n\n","category":"type"},{"location":"Imputation.html#BetaML.Imputation.RFImputerHyperParametersSet","page":"Imputation","title":"BetaML.Imputation.RFImputerHyperParametersSet","text":"mutable struct RFImputerHyperParametersSet <: BetaMLHyperParametersSet\n\nHyperparameters for RFImputer\n\nParameters:\n\nrfhpar::Any\nFor the underlying random forest algorithm parameters (n_trees,max_depth,min_gain,min_records,max_features:,splitting_criterion,β,initialisation_strategy, oob and rng) see RFHyperParametersSet for the specific RF algorithm parameters\nforced_categorical_cols::Vector{Int64}\nSpecify the positions of the integer columns to treat as categorical instead of cardinal. [Default: empty vector (all numerical cols are treated as cardinal by default and the others as categorical)]\nrecursive_passages::Int64\nDefine the times to go trough the various columns to impute their data. Useful when there are data to impute on multiple columns. The order of the first passage is given by the decreasing number of missing values per column, the other passages are random [default: 1].\nmultiple_imputations::Int64\nDetermine the number of independent imputation of the whole dataset to make. Note that while independent, the imputations share the same random number generator (RNG).\n\nExample:\n\njulia>mod = RFImputer(n_trees=20,max_depth=10,recursive_passages=3)\n\n\n\n\n\n","category":"type"},{"location":"Imputation.html#BetaML.Imputation.UniversalImputer","page":"Imputation","title":"BetaML.Imputation.UniversalImputer","text":"mutable struct UniversalImputer <: Imputer\n\nImpute missing data using any regressor/classifier (not necessarily from BetaML) that implements m=Model([options]), fit!(m,X,Y) and predict(m,X)\n\nSee UniversalImputerHyperParametersSet for the hyper-parameters.\n\n\n\n\n\n","category":"type"},{"location":"Imputation.html#BetaML.Imputation.UniversalImputerHyperParametersSet","page":"Imputation","title":"BetaML.Imputation.UniversalImputerHyperParametersSet","text":"mutable struct UniversalImputerHyperParametersSet <: BetaMLHyperParametersSet\n\nHyperparameters for UniversalImputer\n\nParameters:\n\nestimators\nSpecify a regressor or classifier model (and its options/hyper-parameters) per each column of the matrix to impute. Default to random forests.\nrecursive_passages\nDefine the times to go trough the various columns to impute their data. Useful when there are data to impute on multiple columns. The order of the first passage is given by the decreasing number of missing values per column, the other passages are random [default: 1].\nmultiple_imputations\nDetermine the number of independent imputation of the whole dataset to make. Note that while independent, the imputations share the same random number generator (RNG).\n\n\n\n\n\n","category":"type"},{"location":"Imputation.html#BetaML.Api.fit!-Tuple{FeatureBasedImputer, Any}","page":"Imputation","title":"BetaML.Api.fit!","text":"fit!(\n    imputer::FeatureBasedImputer,\n    X\n) -> Union{Nothing, Matrix{Float64}}\n\n\nFit a matrix with missing data using FeatureBasedImputer\n\n\n\n\n\n","category":"method"},{"location":"Imputation.html#BetaML.Api.fit!-Tuple{GMMImputer, Any}","page":"Imputation","title":"BetaML.Api.fit!","text":"fit!(m::GMMImputer, X) -> Union{Nothing, Matrix{Float64}}\n\n\nFit a matrix with missing data using GMMImputer\n\n\n\n\n\n","category":"method"},{"location":"Imputation.html#BetaML.Api.fit!-Tuple{RFImputer, Any}","page":"Imputation","title":"BetaML.Api.fit!","text":"fit!(m::RFImputer, X) -> Any\n\n\nFit a matrix with missing data using RFImputer\n\n\n\n\n\n","category":"method"},{"location":"Imputation.html#BetaML.Api.fit!-Tuple{UniversalImputer, Any}","page":"Imputation","title":"BetaML.Api.fit!","text":"fit!(m::UniversalImputer, X) -> Any\n\n\nFit a matrix with missing data using UniversalImputer\n\n\n\n\n\n","category":"method"},{"location":"Imputation.html#BetaML.Api.predict-Tuple{FeatureBasedImputer, Any}","page":"Imputation","title":"BetaML.Api.predict","text":"predict(m::FeatureBasedImputer, X) -> Any\n\n\nPredict the missing data using the feature averages (eventually normalised) learned by fitting a FeatureBasedImputer model\n\n\n\n\n\n","category":"method"},{"location":"Imputation.html#BetaML.Api.predict-Tuple{GMMImputer, Any}","page":"Imputation","title":"BetaML.Api.predict","text":"predict(m::GMMImputer, X) -> Any\n\n\nPredict the missing data using the mixtures learned by fitting a GMMImputer model\n\n\n\n\n\n","category":"method"},{"location":"Imputation.html#BetaML.Api.predict-Tuple{RFImputer, Any}","page":"Imputation","title":"BetaML.Api.predict","text":"predict(m::RFImputer, X) -> Any\n\n\nReturn the data with the missing values replaced with the imputed ones using the non-linear structure learned fitting a RFImputer model.\n\nNotes:\n\nIf multiple_imputations was set > 1 this is a vector of matrices (the individual imputations) instead of a single matrix.\n\n\n\n\n\n","category":"method"},{"location":"Imputation.html#BetaML.Api.predict-Tuple{UniversalImputer, Any}","page":"Imputation","title":"BetaML.Api.predict","text":"predict(m::UniversalImputer, X) -> Any\n\n\nReturn the data with the missing values replaced with the imputed ones using the non-linear structure learned fitting a UniversalImputer model.\n\nNotes:\n\nIf multiple_imputations was set > 1 this is a vector of matrices (the individual imputations) instead of a single matrix.\n\n\n\n\n\n","category":"method"},{"location":"Imputation.html#BetaML.Imputation.predictMissing","page":"Imputation","title":"BetaML.Imputation.predictMissing","text":"predictMissing(X,K;initial_probmixtures,mixtures,tol,verbosity,minimum_variance,minimum_covariance)\n\nwarning: Warning\nThis function is deprecated and will possibly be removed in BetaML 0.9. Use the model GMMClusterer instead. \n\nFill missing entries in a sparse matrix (i.e. perform a \"matrix completion\") assuming an underlying Gaussian Mixture probabilistic Model (GMM) fitted using an Expectation-Maximisation algorithm.\n\nWhile the name of the function is predictMissing, the function can be also used for system reccomendation / collaborative filtering and GMM-based regressions. The advantage over traditional algorithms as k-nearest neighbors (KNN) is that GMM can \"detect\" the hidden structure of the observed data, where some observation can be similar to a certain pool of other observvations for a certain characteristic, but similar to an other pool of observations for other characteristics.\n\nImplemented in the log-domain for better numerical accuracy with many dimensions.\n\nParameters:\n\nX  :           A (N x D) sparse matrix of data to fill according to a GMM model\nK  :           Number of mixtures (latent classes) to consider [def: 3]\ninitial_probmixtures :           Initial probabilities of the categorical distribution (K x 1) [default: []]\nmixtures:      An array (of length K) of the mixture to employ (see notes) [def: [DiagonalGaussian() for i in 1:K]]\ntol:           Tolerance to stop the algorithm [default: 10^(-6)]\nverbosity:     A verbosity parameter regulating the information messages frequency [def: STD]\nminimum_variance:   Minimum variance for the mixtures [default: 0.05]\nminimum_covariance: Minimum covariance for the mixtures with full covariance matrix [default: 0]. This should be set different than minimum_variance (see notes).\ninitialisation_strategy:  Mixture initialisation algorithm [def: grid]\nmaximum_iterations:       Maximum number of iterations [def: typemax(Int64), i.e. ∞]\nrng:           Random Number Generator (see FIXEDSEED) [deafult: Random.GLOBAL_RNG]\n\nReturns:\n\nA named touple of:\n̂X̂    : The Filled Matrix of size (N x D)\nnFill: The number of items filled\nlL   : The log-likelihood (without considering the last mixture optimisation)\nBIC :  The Bayesian Information Criterion (lower is better)\nAIC :  The Akaike Information Criterion (lower is better)\n\nNotes:\n\nThe mixtures currently implemented are SphericalGaussian(μ,σ²),DiagonalGaussian(μ,σ²) and FullGaussian(μ,σ²)\nFor initialisation_strategy, look at the documentation of init_mixtures! for the mixture you want. The provided gaussian mixtures support grid, kmeans or given. grid is faster, but kmeans often provides better results.\nThe algorithm requires to specify a number of \"latent classes\" (mlixtures) to divide the dataset into. If there isn't any prior domain specific knowledge on this point one can test sevaral k and verify which one minimise the BIC or AIC criteria.\n\nExample:\n\njulia>  cFOut = predictMissing([1 10.5;1.5 missing; 1.8 8; 1.7 15; 3.2 40; missing missing; 3.3 38; missing -2.3; 5.2 -2.4],3)\n\n\n\n\n\n","category":"function"},{"location":"Examples.html#Examples","page":"Examples","title":"Examples","text":"","category":"section"},{"location":"Examples.html#Supervised-learning","page":"Examples","title":"Supervised learning","text":"","category":"section"},{"location":"Examples.html#Regression","page":"Examples","title":"Regression","text":"","category":"section"},{"location":"Examples.html#Estimating-the-bike-sharing-demand","page":"Examples","title":"Estimating the bike sharing demand","text":"","category":"section"},{"location":"Examples.html","page":"Examples","title":"Examples","text":"The task is to estimate the influence of several variables (like the weather, the season, the day of the week..) on the demand of shared bicycles, so that the authority in charge of the service can organise the service in the best way.","category":"page"},{"location":"Examples.html","page":"Examples","title":"Examples","text":"Data origin:","category":"page"},{"location":"Examples.html","page":"Examples","title":"Examples","text":"original full dataset (by hour, not used here): https://archive.ics.uci.edu/ml/datasets/Bike+Sharing+Dataset\nsimplified dataset (by day, with some simple scaling): https://www.hds.utc.fr/~tdenoeux/dokuwiki/en/aec\ndescription: https://www.hds.utc.fr/~tdenoeux/dokuwiki/media/en/exam2019ace.pdf\ndata: https://www.hds.utc.fr/~tdenoeux/dokuwiki/media/en/bikesharing_day.csv.zip","category":"page"},{"location":"Examples.html","page":"Examples","title":"Examples","text":"Note that even if we are estimating a time serie, we are not using here a recurrent neural network as we assume the temporal dependence to be negligible (i.e. Y_t = f(X_t) alone).","category":"page"},{"location":"Examples.html#Classification","page":"Examples","title":"Classification","text":"","category":"section"},{"location":"Examples.html#Unsupervised-lerarning","page":"Examples","title":"Unsupervised lerarning","text":"","category":"section"},{"location":"Examples.html#Notebooks","page":"Examples","title":"Notebooks","text":"","category":"section"},{"location":"Examples.html","page":"Examples","title":"Examples","text":"The following notebooks provide runnable examples of the package functionality:","category":"page"},{"location":"Examples.html","page":"Examples","title":"Examples","text":"Pegasus classifiers: [Static notebook] - [myBinder]\nDecision Trees and Random Forest regression on Bike sharing demand forecast (daily data): [Static notebook] - [myBinder]\nNeural Networks: [Static notebook] - [myBinder]\nBike sharing demand forecast (daily data): [Static notebook] - [myBinder]\nClustering: [Static notebook] - [myBinder]","category":"page"},{"location":"Examples.html","page":"Examples","title":"Examples","text":"Note: the live, runnable computational environment is a temporary new copy made at each connection. The first time after a commit is done on this repository a new environment has to be set (instead of just being copied), and the server may take several minutes.","category":"page"},{"location":"Examples.html","page":"Examples","title":"Examples","text":"This is only if you are the unlucky user triggering the rebuild of the environment after the commit.","category":"page"},{"location":"Nn.html#nn_module","page":"Nn","title":"The BetaML.Nn Module","text":"","category":"section"},{"location":"Nn.html","page":"Nn","title":"Nn","text":"Nn","category":"page"},{"location":"Nn.html#BetaML.Nn","page":"Nn","title":"BetaML.Nn","text":"BetaML.Nn module\n\nImplement the functionality required to define an artificial Neural Network, train it with data, forecast data and assess its performances.\n\nCommon type of layers and optimisation algorithms are already provided, but you can define your own ones subclassing respectively the AbstractLayer and OptimisationAlgorithm abstract types.\n\nThe module provide the following types or functions. Use ?[type or function] to access their full signature and detailed documentation:\n\nModel definition:\n\nDenseLayer: Classical feed-forward layer with user-defined activation function\nDenseNoBiasLayer: Classical layer without the bias parameter\nVectorFunctionLayer: Parameterless layer whose activation function run over the ensable of its nodes rather than on each one individually\nNeuralNetworkEstimator: Build the chained network and define a cost function\n\nEach layer can use a default activation function, one of the functions provided in the Utils module (relu, tanh, softmax,...) or you can specify your own function. The derivative of the activation function can be optionally be provided, in such case training will be quicker, altought this difference tends to vanish with bigger datasets. You can alternativly implement your own layer defining a new type as subtype of the abstract type AbstractLayer. Each user-implemented layer must define the following methods:\n\nA suitable constructor\nforward(layer,x)\nbackward(layer,x,next_gradient)\nget_params(layer)\nget_gradient(layer,x,next_gradient)\nset_params!(layer,w)\nsize(layer)\n\nModel fitting:\n\nfit!(nn,X,Y):  fitting function\nfitting_info(nn): Default callback function during fitting\nSGD:  The classical optimisation algorithm\nADAM: A faster moment-based optimisation algorithm \n\nTo define your own optimisation algorithm define a subtype of OptimisationAlgorithm and implement the function single_update!(θ,▽;opt_alg) and eventually init_optalg(⋅) specific for it.\n\nModel predictions and assessment:\n\npredict(nn) or predict(nn,X): Return the output given the data\nloss(nn): Compute avg. network loss on a test set\naccuracy(ŷ,y): Categorical output accuracy\n\nWhile high-level functions operating on the dataset expect it to be in the standard format (nrecords × ndimensions matrices) it is customary to represent the chain of a neural network as a flow of column vectors, so all low-level operations (operating on a single datapoint) expect both the input and the output as a column vector.\n\n\n\n\n\n","category":"module"},{"location":"Nn.html#Module-Index","page":"Nn","title":"Module Index","text":"","category":"section"},{"location":"Nn.html","page":"Nn","title":"Nn","text":"Modules = [Nn]\nOrder   = [:constant, :type, :function, :macro]\nPrivate = false","category":"page"},{"location":"Nn.html#Detailed-API","page":"Nn","title":"Detailed API","text":"","category":"section"},{"location":"Nn.html","page":"Nn","title":"Nn","text":"Modules = [Nn]\nPrivate = false","category":"page"},{"location":"Nn.html#BetaML.Nn.ADAM","page":"Nn","title":"BetaML.Nn.ADAM","text":"ADAM(;η, λ, β₁, β₂, ϵ)\n\nThe ADAM algorithm, an adaptive moment estimation optimiser.\n\nFields:\n\nη:  Learning rate (stepsize, α in the paper), as a function of the current epoch [def: t -> 0.001 (i.e. fixed)]\nλ:  Multiplicative constant to the learning rate [def: 1]\nβ₁: Exponential decay rate for the first moment estimate [range: ∈ [0,1], def: 0.9]\nβ₂: Exponential decay rate for the second moment estimate [range: ∈ [0,1], def: 0.999]\nϵ:  Epsilon value to avoid division by zero [def: 10^-8]\n\n\n\n\n\n","category":"type"},{"location":"Nn.html#BetaML.Nn.DenseLayer","page":"Nn","title":"BetaML.Nn.DenseLayer","text":"DenseLayer\n\nRepresentation of a layer in the network\n\nFields:\n\nw:  Weigths matrix with respect to the input from previous layer or data (n x n pr. layer)\nwb: Biases (n)\nf:  Activation function\ndf: Derivative of the activation function\n\n\n\n\n\n","category":"type"},{"location":"Nn.html#BetaML.Nn.DenseNoBiasLayer","page":"Nn","title":"BetaML.Nn.DenseNoBiasLayer","text":"DenseNoBiasLayer\n\nRepresentation of a layer without bias in the network\n\nFields:\n\nw:  Weigths matrix with respect to the input from previous layer or data (n x n pr. layer)\nf:  Activation function\ndf: Derivative of the activation function\n\n\n\n\n\n","category":"type"},{"location":"Nn.html#BetaML.Nn.Learnable","page":"Nn","title":"BetaML.Nn.Learnable","text":"Learnable(data)\n\nStructure representing the learnable parameters of a layer or its gradient.\n\nThe learnable parameters of a layers are given in the form of a N-tuple of Array{Float64,N2} where N2 can change (e.g. we can have a layer with the first parameter being a matrix, and the second one being a scalar). We wrap the tuple on its own structure a bit for some efficiency gain, but above all to define standard mathematic operations on the gradients without doing \"type pyracy\" with respect to Base tuples.\n\n\n\n\n\n","category":"type"},{"location":"Nn.html#BetaML.Nn.MultitargetNeuralNetworkRegressor","page":"Nn","title":"BetaML.Nn.MultitargetNeuralNetworkRegressor","text":"FeedfordwardNeuralNetwork\n\nA simple but flexible Feedforward Neural Network, from the Beta Machine Learning Toolkit (BetaML) that can be used for both regression tasks or classification ones.\n\nParameters:\n\nlayers\nArray of layer objects [def: nothing, i.e. basic network]. See subtypes(BetaML.AbstractLayer) for supported layers\nloss\nLoss (cost) function [def: squared_cost].\nwarning: Warning\nIf you change the parameter loss, you need to either provide its derivative on the parameter dloss or use autodiff with dloss=nothing.\n\ndloss\nDerivative of the loss function [def: dSquaredCost, i.e. use the derivative of the squared cost]. Use nothing for autodiff.\nepochs\nNumber of epochs, i.e. passages trough the whole training sample [def: 1000]\nbatch_size\nSize of each individual batch [def: 32]\nopt_alg\nThe optimisation algorithm to update the gradient at each batch [def: ADAM()]\nshuffle\nWhether to randomly shuffle the data at each iteration (epoch) [def: true]\ndescr\nAn optional title and/or description for this model\ncb\nA call back function to provide information during training [def: fitting_info\nrng\nRandom Number Generator (see FIXEDSEED) [deafult: Random.GLOBAL_RNG]\n\nNotes:\n\ndata must be numerical\nthe label can be a n-records vector or a n-records by n-dimensions matrix (e.g. a one-hot-encoded data for classification), but the result is always a matrix.\nFor one-dimension regressions drop the unnecessary dimension with dropdims(ŷ,dims=2)\nFor classification tasks the columns should be interpreted as the probabilities for each categories\n\n\n\n\n\n","category":"type"},{"location":"Nn.html#BetaML.Nn.NN","page":"Nn","title":"BetaML.Nn.NN","text":"NN\n\nRepresentation of a Neural Network\n\nFields:\n\nlayers:  Array of layers objects\ncf:      Cost function\ndcf:     Derivative of the cost function\ntrained: Control flag for trained networks\n\n\n\n\n\n","category":"type"},{"location":"Nn.html#BetaML.Nn.NeuralNetworkClassifier","page":"Nn","title":"BetaML.Nn.NeuralNetworkClassifier","text":"NeuralNetworkClassifier\n\nA simple but flexible Feedforward Neural Network, from the Beta Machine Learning Toolkit (BetaML) for classification  problems.\n\nParameters:\n\nlayers\nArray of layer objects [def: nothing, i.e. basic network]. See subtypes(BetaML.AbstractLayer) for supported layers. The last \"softmax\" layer is automatically added.\nloss\nLoss (cost) function [def: crossentropy].\nwarning: Warning\nIf you change the parameter loss, you need to either provide its derivative on the parameter dloss or use autodiff with dloss=nothing.\n\ndloss\nDerivative of the loss function [def: dcrossentropy, i.e. the derivative of the cross-entropy]. Use nothing for autodiff.\nepochs\nNumber of epochs, i.e. passages trough the whole training sample [def: 1000]\nbatch_size\nSize of each individual batch [def: 32]\nopt_alg\nThe optimisation algorithm to update the gradient at each batch [def: ADAM()]\nshuffle\nWhether to randomly shuffle the data at each iteration (epoch) [def: true]\ndescr\nAn optional title and/or description for this model\ncb\nA call back function to provide information during training [def: fitting_info\ncategories\nThe categories to represent as columns. [def: nothing, i.e. unique training values].\nhandle_unknown\nHow to handle categories not seens in training or not present in the provided categories array? \"error\" (default) rises an error, \"infrequent\" adds a specific column for these categories.\nother_categories_name\nWhich value during prediction to assign to this \"other\" category (i.e. categories not seen on training or not present in the provided categories array? [def: nothing, i.e. typemax(Int64) for integer vectors and \"other\" for other types]. This setting is active only if handle_unknown=\"infrequent\" and in that case it MUST be specified if Y is neither integer or strings\nrng\nRandom Number Generator [deafult: Random.GLOBAL_RNG]\n\nNotes:\n\ndata must be numerical\nthe label can be a n-records vector or a n-records by n-dimensions matrix (e.g. a one-hot-encoded data for classification), but the result is always a matrix.\nFor one-dimension regressions drop the unnecessary dimension with dropdims(ŷ,dims=2)\nFor classification tasks the columns should be interpreted as the probabilities for each categories\n\n\n\n\n\n","category":"type"},{"location":"Nn.html#BetaML.Nn.NeuralNetworkEstimator","page":"Nn","title":"BetaML.Nn.NeuralNetworkEstimator","text":"NeuralNetworkEstimator\n\nA \"feedforward\" neural network (supervised).\n\nFor the parameters see NeuralNetworkEstimatorLearnableParameters.\n\nNotes:\n\ndata must be numerical\nthe label can be a n-records vector or a n-records by n-dimensions matrix, but the result is always a matrix.\nFor one-dimension regressions drop the unnecessary dimension with dropdims(ŷ,dims=2)\nFor classification tasks the columns should be interpreted as the probabilities for each categories\n\n\n\n\n\n","category":"type"},{"location":"Nn.html#BetaML.Nn.NeuralNetworkEstimatorHyperParametersSet","page":"Nn","title":"BetaML.Nn.NeuralNetworkEstimatorHyperParametersSet","text":"**`\n\nmutable struct NeuralNetworkEstimatorHyperParametersSet <: BetaMLHyperParametersSet\n\n`**\n\nHyperparameters for the Feedforward neural network model\n\nParameters:\n\nlayers\nArray of layer objects [def: nothing, i.e. basic network]. See subtypes(BetaML.AbstractLayer) for supported layers\nloss\nLoss (cost) function [def: squared_cost].\nwarning: Warning\nIf you change the parameter loss, you need to either provide its derivative on the parameter dloss or use autodiff with dloss=nothing.\n\ndloss\nDerivative of the loss function [def: dSquaredCost, i.e. use the derivative of the squared cost]. Use nothing for autodiff.\nepochs\nNumber of epochs, i.e. passages trough the whole training sample [def: 1000]\nbatch_size\nSize of each individual batch [def: 32]\nopt_alg\nThe optimisation algorithm to update the gradient at each batch [def: ADAM()]\nshuffle\nWhether to randomly shuffle the data at each iteration (epoch) [def: true]\n\nTo know the available layers type subtypes(AbstractLayer)) and then type ?LayerName for information on how to use each layer.\n\n\n\n\n\n","category":"type"},{"location":"Nn.html#BetaML.Nn.NeuralNetworkEstimatorOptionsSet","page":"Nn","title":"BetaML.Nn.NeuralNetworkEstimatorOptionsSet","text":"NeuralNetworkEstimatorOptionsSet\n\nA struct defining the options used by the Feedforward neural network model\n\nParameters:\n\ncache\nCache the results of the fitting stage, as to allow predict(mod) [default: true]. Set it to false to save memory for large data.\ndescr\nAn optional title and/or description for this model\nverbosity\nThe verbosity level to be used in training or prediction (see Verbosity) [deafult: STD]\ncb\nA call back function to provide information during training [def: fitting_info\nrng\nRandom Number Generator (see FIXEDSEED) [deafult: Random.GLOBAL_RNG]\n\n\n\n\n\n","category":"type"},{"location":"Nn.html#BetaML.Nn.SGD","page":"Nn","title":"BetaML.Nn.SGD","text":"SGD(;η=t -> 1/(1+t), λ=2)\n\nStochastic Gradient Descent algorithm (default)\n\nFields:\n\nη: Learning rate, as a function of the current epoch [def: t -> 1/(1+t)]\nλ: Multiplicative constant to the learning rate [def: 2]\n\n\n\n\n\n","category":"type"},{"location":"Nn.html#BetaML.Nn.ScalarFunctionLayer","page":"Nn","title":"BetaML.Nn.ScalarFunctionLayer","text":"ScalarFunctionLayer\n\nRepresentation of a ScalarFunction layer in the network. ScalarFunctionLayer applies the activation function directly to the output of the previous layer (i.e., without passing for a weigth matrix), but using an  optional learnable parameter (an array) used as second argument, similarly to [VectorFunctionLayer(@ref). Differently from VectorFunctionLayer, the function is applied scalarwise to each node. \n\nThe number of nodes in input must be set to the same as in the previous layer\n\nFields:\n\nw:   Weigths (parameter) array passes as second argument to the activation        function (if not empty)\nn:   Number of nodes in output (≡ number of nodes in input )\nf:   Activation function (vector)\ndfx: Derivative of the (vector) activation function with respect to the        layer inputs (x)\ndfw: Derivative of the (vector) activation function with respect to the        optional learnable weigths (w)         \n\nNotes:\n\nThe output size of this layer is the same as those of the previous layers.\n\n\n\n\n\n","category":"type"},{"location":"Nn.html#BetaML.Nn.VectorFunctionLayer","page":"Nn","title":"BetaML.Nn.VectorFunctionLayer","text":"VectorFunctionLayer\n\nRepresentation of a VectorFunction layer in the network. Vector function layer expects a vector activation function, i.e. a function taking the whole output of the previous layer an input rather than working on a single node as \"normal\" activation functions would do. Useful for example with the SoftMax function in classification or with the pool1D function to implement a \"pool\" layer in 1 dimensions. By default it is weightless, i.e. it doesn't apply any transformation to the output coming from the previous layer except the activation function. However, by passing the parameter wsize (a touple or array - tested only 1D) you can pass the learnable parameter to the activation function too. It is your responsability to be sure the activation function accept only X or also this  learnable array (as second argument).    The number of nodes in input must be set to the same as in the previous layer (and if you are using this for classification, to the number of classes, i.e. the previous layer must be set equal to the number of classes in the predictions).\n\nFields:\n\nw:   Weigths (parameter) array passes as second argument to the activation        function (if not empty)\nnₗ:  Number of nodes in input (i.e. length of previous layer)\nn:   Number of nodes in output (automatically inferred in the constructor)\nf:   Activation function (vector)\ndfx: Derivative of the (vector) activation function with respect to the        layer inputs (x)\ndfw: Derivative of the (vector) activation function with respect to the        optional learnable weigths (w)         \n\nNotes:\n\nThe output size of this layer is given by the size of the output function,\n\nthat not necessarily is the same as the previous layers.\n\n\n\n\n\n","category":"type"},{"location":"Nn.html#Base.show-Tuple{NN}","page":"Nn","title":"Base.show","text":"show(nn)\n\nPrint a representation of the Neural Network (layers, dimensions..)\n\nParameters:\n\nnn: Worker network\n\n\n\n\n\n","category":"method"},{"location":"Nn.html#Base.size-Tuple{AbstractLayer}","page":"Nn","title":"Base.size","text":"size(layer)\n\nGet the dimensions of the layers in terms of (dimensions in input , dimensions in output)\n\nNotes:\n\nYou need to use import Base.size before defining this function for your layer\n\n\n\n\n\n","category":"method"},{"location":"Nn.html#BetaML.Nn.backward-Tuple{AbstractLayer, Any, Any}","page":"Nn","title":"BetaML.Nn.backward","text":"backward(layer,x,next_gradient)\n\nCompute backpropagation for this layer\n\nParameters:\n\nlayer:        Worker layer\nx:            Input to the layer\nnext_gradient: Derivative of the overal loss with respect to the input of the next layer (output of this layer)\n\nReturn:\n\nThe evaluated gradient of the loss with respect to this layer inputs\n\n\n\n\n\n","category":"method"},{"location":"Nn.html#BetaML.Nn.buildNetwork-Tuple{Any, Any}","page":"Nn","title":"BetaML.Nn.buildNetwork","text":"buildNetwork(layers,cf;dcf,name)\n\nInstantiate a new Feedforward Neural Network\n\nwarning: Warning\nThis function is deprecated and will possibly be removed in BetaML 0.9. Use the model NeuralNetworkEstimator instead. \n\nParameters:\n\nlayers: Array of layers objects\ncf:     Cost function\ndcf:    Derivative of the cost function [def: nothing]\nname:   Name of the network [def: \"Neural Network\"]\n\nNotes:\n\nEven if the network ends with a single output note, the cost function and its derivative should always expect y and ŷ as column vectors.\n\n\n\n\n\n","category":"method"},{"location":"Nn.html#BetaML.Nn.forward-Tuple{AbstractLayer, Any}","page":"Nn","title":"BetaML.Nn.forward","text":"forward(layer,x)\n\nPredict the output of the layer given the input\n\nParameters:\n\nlayer:  Worker layer\nx:      Input to the layer\n\nReturn:\n\nAn Array{T,1} of the prediction (even for a scalar)\n\n\n\n\n\n","category":"method"},{"location":"Nn.html#BetaML.Nn.get_gradient-Tuple{AbstractLayer, Any, Any}","page":"Nn","title":"BetaML.Nn.get_gradient","text":"get_gradient(layer,x,next_gradient)\n\nCompute backpropagation for this layer\n\nParameters:\n\nlayer:        Worker layer\nx:            Input to the layer\nnext_gradient: Derivative of the overaall loss with respect to the input of the next layer (output of this layer)\n\nReturn:\n\nThe evaluated gradient of the loss with respect to this layer's trainable parameters as tuple of matrices. It is up to you to decide how to organise this tuple, as long you are consistent with the get_params() and set_params() functions. Note that starting from BetaML 0.2.2 this tuple needs to be wrapped in its Learnable type.\n\n\n\n\n\n","category":"method"},{"location":"Nn.html#BetaML.Nn.get_gradient-Union{Tuple{T2}, Tuple{T}, Tuple{Any, AbstractMatrix{T}, AbstractMatrix{T2}}} where {T<:Number, T2<:Number}","page":"Nn","title":"BetaML.Nn.get_gradient","text":"get_gradient(nn,xbatch,ybatch)\n\nRetrieve the current gradient of the weigthts (i.e. derivative of the cost with respect to the weigths)\n\nParameters:\n\nnn:      Worker network\nxbatch:  Input to the network (n,d)\nybatch:  Label input (n,d)\n\n#Notes:\n\nThe output is a vector of tuples of each layer's input weigths and bias weigths\n\n\n\n\n\n","category":"method"},{"location":"Nn.html#BetaML.Nn.get_gradient-Union{Tuple{T2}, Tuple{T}, Tuple{NN, Union{AbstractVector{T}, T}, Union{AbstractVector{T2}, T2}}} where {T<:Number, T2<:Number}","page":"Nn","title":"BetaML.Nn.get_gradient","text":"get_gradient(nn,x,y)\n\nRetrieve the current gradient of the weigthts (i.e. derivative of the cost with respect to the weigths)\n\nParameters:\n\nnn: Worker network\nx:   Input to the network (d,1)\ny:   Label input (d,1)\n\n#Notes:\n\nThe output is a vector of tuples of each layer's input weigths and bias weigths\n\n\n\n\n\n","category":"method"},{"location":"Nn.html#BetaML.Nn.get_nparams-Tuple{AbstractLayer}","page":"Nn","title":"BetaML.Nn.get_nparams","text":"get_nparams(layer)\n\nReturn the number of parameters of a layer.\n\nIt doesn't need to be implemented by each layer type, as it uses get_params().\n\n\n\n\n\n","category":"method"},{"location":"Nn.html#BetaML.Nn.get_nparams-Tuple{NN}","page":"Nn","title":"BetaML.Nn.get_nparams","text":"get_nparams(nn) - Return the number of trainable parameters of the neural network.\n\n\n\n\n\n","category":"method"},{"location":"Nn.html#BetaML.Nn.get_params-Tuple{AbstractLayer}","page":"Nn","title":"BetaML.Nn.get_params","text":"get_params(layer)\n\nGet the layers current value of its trainable parameters\n\nParameters:\n\nlayer:  Worker layer\n\nReturn:\n\nThe current value of the layer's trainable parameters as tuple of matrices. It is up to you to decide how to organise this tuple, as long you are consistent with the get_gradient() and set_params() functions. Note that starting from BetaML 0.2.2 this tuple needs to be wrapped in its Learnable type.\n\n\n\n\n\n","category":"method"},{"location":"Nn.html#BetaML.Nn.get_params-Tuple{NN}","page":"Nn","title":"BetaML.Nn.get_params","text":"get_params(nn)\n\nRetrieve current weigthts\n\nParameters:\n\nnn: Worker network\n\nNotes:\n\nThe output is a vector of tuples of each layer's input weigths and bias weigths\n\n\n\n\n\n","category":"method"},{"location":"Nn.html#BetaML.Nn.init_optalg!-Tuple{ADAM}","page":"Nn","title":"BetaML.Nn.init_optalg!","text":"initoptalg!(optalg::ADAM;θ,batch_size,x,y,rng)\n\nInitialize the ADAM algorithm with the parameters m and v as zeros and check parameter bounds\n\n\n\n\n\n","category":"method"},{"location":"Nn.html#BetaML.Nn.init_optalg!-Tuple{BetaML.Nn.OptimisationAlgorithm}","page":"Nn","title":"BetaML.Nn.init_optalg!","text":"initoptalg!(optalg;θ,batch_size,x,y)\n\nInitialize the optimisation algorithm\n\nParameters:\n\nopt_alg:    The Optimisation algorithm to use\nθ:         Current parameters\nbatch_size:    The size of the batch\nx:   The training (input) data\ny:   The training \"labels\" to match\nrng: Random Number Generator (see FIXEDSEED) [deafult: Random.GLOBAL_RNG]\n\nNotes:\n\nOnly a few optimizers need this function and consequently ovverride it. By default it does nothing, so if you want write your own optimizer and don't need to initialise it, you don't have to override this method\n\n\n\n\n\n","category":"method"},{"location":"Nn.html#BetaML.Nn.loss-Tuple{NN, Any, Any}","page":"Nn","title":"BetaML.Nn.loss","text":"loss(fnn,x,y)\n\nCompute avg. network loss on a test set (or a single (1 × d) data point)\n\nParameters:\n\nfnn: Worker network\nx:   Input to the network (n) or (n x d)\ny:   Label input (n) or (n x d)\n\n\n\n\n\n","category":"method"},{"location":"Nn.html#BetaML.Nn.set_params!-Tuple{AbstractLayer, Any}","page":"Nn","title":"BetaML.Nn.set_params!","text":"set_params!(layer,w)\n\nSet the trainable parameters of the layer with the given values\n\nParameters:\n\nlayer: Worker layer\nw:     The new parameters to set (Learnable)\n\nNotes:\n\nThe format of the tuple wrapped by Learnable must be consistent with those of the get_params() and get_gradient() functions.\n\n\n\n\n\n","category":"method"},{"location":"Nn.html#BetaML.Nn.set_params!-Tuple{NN, Any}","page":"Nn","title":"BetaML.Nn.set_params!","text":"set_params!(nn,w)\n\nUpdate weigths of the network\n\nParameters:\n\nnn: Worker network\nw:  The new weights to set\n\n\n\n\n\n","category":"method"},{"location":"Nn.html#BetaML.Nn.single_update!-Tuple{Any, Any}","page":"Nn","title":"BetaML.Nn.single_update!","text":"singleupdate!(θ,▽;nepoch,nbatch,batchsize,xbatch,ybatch,opt_alg)\n\nPerform the parameters update based on the average batch gradient.\n\nParameters:\n\nθ:         Current parameters\n▽:         Average gradient of the batch\nn_epoch:    Count of current epoch\nn_batch:    Count of current batch\nn_batches:  Number of batches per epoch\nxbatch:    Data associated to the current batch\nybatch:    Labels associated to the current batch\nopt_alg:    The Optimisation algorithm to use for the update\n\nNotes:\n\nThis function is overridden so that each optimisation algorithm implement their\n\nown version\n\nMost parameters are not used by any optimisation algorithm. They are provided\n\nto support the largest possible class of optimisation algorithms\n\nSome optimisation algorithms may change their internal structure in this function\n\n\n\n\n\n","category":"method"},{"location":"Nn.html#BetaML.Nn.train!-Tuple{NN, Any, Any}","page":"Nn","title":"BetaML.Nn.train!","text":"train!(nn,x,y;epochs,batchsize,sequential,optalg,verbosity,cb)\n\nTrain a neural network with the given x,y data\n\nwarning: Warning\nThis function is deprecated and will possibly be removed in BetaML 0.9. Use the model NeuralNetworkEstimator instead. \n\nParameters:\n\nnn:         Worker network\nx:          Training input to the network (records x dimensions)\ny:          Label input (records x dimensions)\nepochs:     Number of passages over the training set [def: 100]\nbatch_size:  Size of each individual batch [def: min(size(x,1),32)]\nsequential: Wether to run all data sequentially instead of random [def: false]\nopt_alg:     The optimisation algorithm to update the gradient at each batch [def: ADAM()]\nverbosity:  A verbosity parameter for the trade off information / efficiency [def: STD]\ncb:         A callback to provide information. [def: fitting_info]\nrng:        Random Number Generator (see FIXEDSEED) [deafult: Random.GLOBAL_RNG]\n\nReturn:\n\nA named tuple with the following information\nepochs: Number of epochs actually ran\nϵ_epochs: The average error on each epoch (if verbosity > LOW)\nθ_epochs: The parameters at each epoch (if verbosity > STD)\n\nNotes:\n\nCurrently supported algorithms:\nSGD, the classical (Stochastic) Gradient Descent optimiser\nADAM,  an adaptive moment estimation optimiser\nLook at the individual optimisation algorithm (?[Name OF THE ALGORITHM]) for info on its parameter, e.g. ?SGD for the Stochastic Gradient Descent.\nYou can implement your own optimisation algorithm using a subtype of OptimisationAlgorithm and implementing its constructor and the update function single_update!(⋅) (type ?single_update! for details).\nYou can implement your own callback function, altought the one provided by default is already pretty generic (its output depends on the verbosity parameter). See fitting_info for informations on the cb parameters.\nBoth the callback function and the single_update! function of the optimisation algorithm can be used to stop the training algorithm, respectively returning true or stop=true.\nThe verbosity can be set to any of NONE,LOW,STD,HIGH,FULL.\nThe update is done computing the average gradient for each batch and then calling single_update! to let the optimisation algorithm perform the parameters update\n\n\n\n\n\n","category":"method"},{"location":"Trees.html#trees_module","page":"Trees","title":"The BetaML.Trees Module","text":"","category":"section"},{"location":"Trees.html","page":"Trees","title":"Trees","text":"Trees","category":"page"},{"location":"Trees.html#BetaML.Trees","page":"Trees","title":"BetaML.Trees","text":"BetaML.Trees module\n\nImplement the DecisionTreeEstimator and RandomForestEstimator models (Decision Trees and Random Forests).\n\nBoth Decision Trees and Random Forests can be used for regression or classification problems, based on the type of the labels (numerical or not). The automatic selection can be overridden with the parameter force_classification=true, typically if labels are integer representing some categories rather than numbers. For classification problems the output of predict is a dictionary with the key being the labels with non-zero probabilitity and the corresponding value its probability; for regression it is a numerical value.\n\nPlease be aware that, differently from most other implementations, the Random Forest algorithm collects and averages the probabilities from the trees, rather than just repording the mode, i.e. no information is lost and the output of the forest classifier is still a PMF.\n\nTo retrieve the prediction with the highest probability use mode over the prediciton returned by the model. Most error/accuracy measures in the Util BetaML module works diretly with this format.\n\nMissing data and trully unordered types are supported on the features, both on training and on prediction.\n\nThe module provide the following functions. Use ?[type or function] to access their full signature and detailed documentation:\n\nFeatures are expected to be in the standard format (nRecords × nDimensions matrices) and the labels (either categorical or numerical) as a nRecords column vector.\n\nAcknowlegdments: originally based on the Josh Gordon's code\n\n\n\n\n\n","category":"module"},{"location":"Trees.html#Module-Index","page":"Trees","title":"Module Index","text":"","category":"section"},{"location":"Trees.html","page":"Trees","title":"Trees","text":"Modules = [Trees]\nOrder   = [:constant, :type, :function, :macro]\nPrivate = false","category":"page"},{"location":"Trees.html#Detailed-API","page":"Trees","title":"Detailed API","text":"","category":"section"},{"location":"Trees.html","page":"Trees","title":"Trees","text":"Modules = [Trees]\nPrivate = false","category":"page"},{"location":"Trees.html#BetaML.Trees.DTHyperParametersSet","page":"Trees","title":"BetaML.Trees.DTHyperParametersSet","text":"mutable struct DTHyperParametersSet <: BetaMLHyperParametersSet\n\nHyperparameters for DecisionTreeEstimator (Decision Tree).\n\nParameters:\n\nmax_depth::Union{Nothing, Int64}\nThe maximum depth the tree is allowed to reach. When this is reached the node is forced to become a leaf [def: nothing, i.e. no limits]\nmin_gain::Float64\nThe minimum information gain to allow for a node's partition [def: 0]\nmin_records::Int64\nThe minimum number of records a node must holds to consider for a partition of it [def: 2]\nmax_features::Union{Nothing, Int64}\nThe maximum number of (random) features to consider at each partitioning [def: nothing, i.e. look at all features]\nforce_classification::Bool\nWhether to force a classification task even if the labels are numerical (typically when labels are integers encoding some feature rather than representing a real cardinal measure) [def: false]\nsplitting_criterion::Union{Nothing, Function}\nThis is the name of the function to be used to compute the information gain of a specific partition. This is done by measuring the difference betwwen the \"impurity\" of the labels of the parent node with those of the two child nodes, weighted by the respective number of items. [def: nothing, i.e. gini for categorical labels (classification task) and variance for numerical labels(regression task)]. Either gini, entropy, variance or a custom function. It can also be an anonymous function.\n\n\n\n\n\n","category":"type"},{"location":"Trees.html#BetaML.Trees.DecisionTreeClassifier","page":"Trees","title":"BetaML.Trees.DecisionTreeClassifier","text":"mutable struct DecisionTreeClassifier <: MLJModelInterface.Probabilistic\n\nA simple Decision Tree for classification with support for Missing data, from the Beta Machine Learning Toolkit (BetaML).\n\nHyperparameters:\n\nmax_depth::Int64\nThe maximum depth the tree is allowed to reach. When this is reached the node is forced to become a leaf [def: 0, i.e. no limits]\nmin_gain::Float64\nThe minimum information gain to allow for a node's partition [def: 0]\nmin_records::Int64\nThe minimum number of records a node must holds to consider for a partition of it [def: 2]\nmax_features::Int64\nThe maximum number of (random) features to consider at each partitioning [def: 0, i.e. look at all features]\nsplitting_criterion::Function\nThis is the name of the function to be used to compute the information gain of a specific partition. This is done by measuring the difference betwwen the \"impurity\" of the labels of the parent node with those of the two child nodes, weighted by the respective number of items. [def: gini]. Either gini, entropy or a custom function. It can also be an anonymous function.\nrng::Random.AbstractRNG\nA Random Number Generator to be used in stochastic parts of the code [deafult: Random.GLOBAL_RNG]\n\n\n\n\n\n","category":"type"},{"location":"Trees.html#BetaML.Trees.DecisionTreeEstimator","page":"Trees","title":"BetaML.Trees.DecisionTreeEstimator","text":"mutable struct DecisionTreeEstimator <: BetaMLSupervisedModel\n\nA Decision Tree classifier and regressor (supervised).\n\nDecision Tree works by finding the \"best\" question to split the fitting data (according to the metric specified by the parameter splitting_criterion on the associated labels) untill either all the dataset is separated or a terminal condition is reached. \n\nFor the parameters see ?DTHyperParametersSet and ?BetaMLDefaultOptionsSet.\n\nNotes:\n\nOnline fitting (re-fitting with new data) is not supported\nMissing data (in the feature dataset) is supported.\n\n\n\n\n\n","category":"type"},{"location":"Trees.html#BetaML.Trees.DecisionTreeRegressor","page":"Trees","title":"BetaML.Trees.DecisionTreeRegressor","text":"mutable struct DecisionTreeRegressor <: MLJModelInterface.Deterministic\n\nA simple Decision Tree for regression with support for Missing data, from the Beta Machine Learning Toolkit (BetaML).\n\nHyperparameters:\n\nmax_depth::Int64\nThe maximum depth the tree is allowed to reach. When this is reached the node is forced to become a leaf [def: 0, i.e. no limits]\nmin_gain::Float64\nThe minimum information gain to allow for a node's partition [def: 0]\nmin_records::Int64\nThe minimum number of records a node must holds to consider for a partition of it [def: 2]\nmax_features::Int64\nThe maximum number of (random) features to consider at each partitioning [def: 0, i.e. look at all features]\nsplitting_criterion::Function\nThis is the name of the function to be used to compute the information gain of a specific partition. This is done by measuring the difference betwwen the \"impurity\" of the labels of the parent node with those of the two child nodes, weighted by the respective number of items. [def: variance]. Either variance or a custom function. It can also be an anonymous function.\nrng::Random.AbstractRNG\nA Random Number Generator to be used in stochastic parts of the code [deafult: Random.GLOBAL_RNG]\n\n\n\n\n\n","category":"type"},{"location":"Trees.html#BetaML.Trees.InfoNode","page":"Trees","title":"BetaML.Trees.InfoNode","text":"These types are introduced so that additional information currently not present in  a DecisionTree-structure – namely the feature names  –  can be used for visualization.\n\n\n\n\n\n","category":"type"},{"location":"Trees.html#BetaML.Trees.RFHyperParametersSet","page":"Trees","title":"BetaML.Trees.RFHyperParametersSet","text":"mutable struct RFHyperParametersSet <: BetaMLHyperParametersSet\n\nHyperparameters for RandomForestEstimator (Random Forest).\n\nParameters:\n\nn_trees::Int64\nNumber of (decision) trees in the forest [def: 30]\nmax_depth::Union{Nothing, Int64}\nThe maximum depth the tree is allowed to reach. When this is reached the node is forced to become a leaf [def: nothing, i.e. no limits]\nmin_gain::Float64\nThe minimum information gain to allow for a node's partition [def: 0]\nmin_records::Int64\nThe minimum number of records a node must holds to consider for a partition of it [def: 2]\nmax_features::Union{Nothing, Int64}\nThe maximum number of (random) features to consider when choosing the optimal partition of the dataset [def: nothing, i.e. square root of the dimensions of the training data`]\nforce_classification::Bool\nWhether to force a classification task even if the labels are numerical (typically when labels are integers encoding some feature rather than representing a real cardinal measure) [def: false]\nsplitting_criterion::Union{Nothing, Function}\nEither gini, entropy or variance. This is the name of the function to be used to compute the information gain of a specific partition. This is done by measuring the difference betwwen the \"impurity\" of the labels of the parent node with those of the two child nodes, weighted by the respective number of items. [def: nothing, i.e. gini for categorical labels (classification task) and variance for numerical labels(regression task)]. It can be an anonymous function.\nbeta::Float64\nParameter that regulate the weights of the scoring of each tree, to be (optionally) used in prediction based on the error of the individual trees computed on the records on which trees have not been trained. Higher values favour \"better\" trees, but too high values will cause overfitting [def: 0, i.e. uniform weigths]\noob::Bool\nWheter to compute the Out-Of-Bag error, an estimation of the validation error (the mismatching error for classification and the relative mean error for regression jobs).\n\n\n\n\n\n","category":"type"},{"location":"Trees.html#BetaML.Trees.RandomForestClassifier","page":"Trees","title":"BetaML.Trees.RandomForestClassifier","text":"mutable struct RandomForestClassifier <: MLJModelInterface.Probabilistic\n\nA simple Random Forest for classification with support for Missing data, from the Beta Machine Learning Toolkit (BetaML).\n\nHyperparameters:\n\nn_trees::Int64\nmax_depth::Int64\nThe maximum depth the tree is allowed to reach. When this is reached the node is forced to become a leaf [def: 0, i.e. no limits]\nmin_gain::Float64\nThe minimum information gain to allow for a node's partition [def: 0]\nmin_records::Int64\nThe minimum number of records a node must holds to consider for a partition of it [def: 2]\nmax_features::Int64\nThe maximum number of (random) features to consider at each partitioning [def: 0, i.e. square root of the data dimensions]\nsplitting_criterion::Function\nThis is the name of the function to be used to compute the information gain of a specific partition. This is done by measuring the difference betwwen the \"impurity\" of the labels of the parent node with those of the two child nodes, weighted by the respective number of items. [def: gini]. Either gini, entropy or a custom function. It can also be an anonymous function.\nβ::Float64\nParameter that regulate the weights of the scoring of each tree, to be (optionally) used in prediction based on the error of the individual trees computed on the records on which trees have not been trained. Higher values favour \"better\" trees, but too high values will cause overfitting [def: 0, i.e. uniform weigths]\nrng::Random.AbstractRNG\nA Random Number Generator to be used in stochastic parts of the code [deafult: Random.GLOBAL_RNG]\n\n\n\n\n\n","category":"type"},{"location":"Trees.html#BetaML.Trees.RandomForestEstimator","page":"Trees","title":"BetaML.Trees.RandomForestEstimator","text":"mutable struct RandomForestEstimator <: BetaMLSupervisedModel\n\nA Random Forest classifier and regressor (supervised).\n\nRandom forests are ensemble of Decision Trees models (see ?DecisionTreeEstimator).\n\nFor the parameters see ?RFHyperParametersSet and ?BetaMLDefaultOptionsSet.\n\nNotes :\n\nEach individual decision tree is built using bootstrap over the data, i.e. \"sampling N records with replacement\" (hence, some records appear multiple times and some records do not appear in the specific tree training). The maxx_feature injects further variability and reduces the correlation between the forest trees.\nThe predictions of the \"forest\" (using the function predict()) are then the aggregated predictions of the individual trees (from which the name \"bagging\": boostrap aggregating).\nThe performances of each individual trees,  as measured using the records they have not being trained with, can then be (optionally) used as weights in the predict function. The parameter beta ≥ 0 regulate the distribution of these weights: larger is β, the greater the importance (hence the weights) attached to the best-performing trees compared to the low-performing ones. Using these weights can significantly improve the forest performances (especially using small forests), however the correct value of beta depends on the problem under exam (and the chosen caratteristics of the random forest estimator) and should be cross-validated to avoid over-fitting.\nNote that training RandomForestEstimator uses multiple threads if these are available. You can check the number of threads available with Threads.nthreads(). To set the number of threads in Julia either set the environmental variable JULIA_NUM_THREADS (before starting Julia) or start Julia with the command line option --threads (most integrated development editors for Julia already set the number of threads to 4).\nOnline fitting (re-fitting with new data) is not supported\nMissing data (in the feature dataset) is supported.\n\n\n\n\n\n","category":"type"},{"location":"Trees.html#BetaML.Trees.RandomForestRegressor","page":"Trees","title":"BetaML.Trees.RandomForestRegressor","text":"mutable struct RandomForestRegressor <: MLJModelInterface.Deterministic\n\nA simple Random Forest for regression with support for Missing data, from the Beta Machine Learning Toolkit (BetaML).\n\nHyperparameters:\n\nn_trees::Int64\nmax_depth::Int64\nThe maximum depth the tree is allowed to reach. When this is reached the node is forced to become a leaf [def: 0, i.e. no limits]\nmin_gain::Float64\nThe minimum information gain to allow for a node's partition [def: 0]\nmin_records::Int64\nThe minimum number of records a node must holds to consider for a partition of it [def: 2]\nmax_features::Int64\nThe maximum number of (random) features to consider at each partitioning [def: 0, i.e. square root of the data dimension]\nsplitting_criterion::Function\nThis is the name of the function to be used to compute the information gain of a specific partition. This is done by measuring the difference betwwen the \"impurity\" of the labels of the parent node with those of the two child nodes, weighted by the respective number of items. [def: variance]. Either variance or a custom function. It can also be an anonymous function.\nβ::Float64\nParameter that regulate the weights of the scoring of each tree, to be (optionally) used in prediction based on the error of the individual trees computed on the records on which trees have not been trained. Higher values favour \"better\" trees, but too high values will cause overfitting [def: 0, i.e. uniform weigths]\nrng::Random.AbstractRNG\nA Random Number Generator to be used in stochastic parts of the code [deafult: Random.GLOBAL_RNG]\n\n\n\n\n\n","category":"type"},{"location":"Trees.html#BetaML.Trees.buildForest-Union{Tuple{Ty}, Tuple{Any, AbstractVector{Ty}}, Tuple{Any, AbstractVector{Ty}, Any}} where Ty","page":"Trees","title":"BetaML.Trees.buildForest","text":"buildForest(x, y, ntrees; maxdepth, mingain, minrecords, maxfeatures, splittingcriterion, force_classification)\n\nBuilds (define and train) a \"forest\" of Decision Trees.\n\nwarning: Warning\nThis function is deprecated and will possibly be removed in BetaML 0.9. Use RandomForestEstimator instead. \n\nParameters:\n\nSee buildTree. The function has all the parameters of bildTree (with the max_features defaulting to √D instead of D) plus the following parameters:\n\nn_trees: Number of trees in the forest [def: 30]\nβ: Parameter that regulate the weights of the scoring of each tree, to be (optionally) used in prediction (see later) [def: 0, i.e. uniform weigths]\noob: Whether to coompute the out-of-bag error, an estimation of the generalization accuracy [def: false]\nrng: Random Number Generator (see FIXEDSEED) [deafult: Random.GLOBAL_RNG]\n\nOutput:\n\nThe function returns a Forest object (see Forest).\nThe forest weights default to array of ones if β ≤ 0 and the oob error to +Inf if oob == false.\n\nNotes :\n\nEach individual decision tree is built using bootstrap over the data, i.e. \"sampling N records with replacement\" (hence, some records appear multiple times and some records do not appear in the specific tree training). The maxFeature injects further variability and reduces the correlation between the forest trees.\nThe predictions of the \"forest\" (using the function predict()) are then the aggregated predictions of the individual trees (from which the name \"bagging\": boostrap aggregating).\nThis function optionally reports a weight distribution of the performances of eanch individual trees, as measured using the records he has not being trained with. These weights can then be (optionally) used in the predict function. The parameter β ≥ 0 regulate the distribution of these weights: larger is β, the greater the importance (hence the weights) attached to the best-performing trees compared to the low-performing ones. Using these weights can significantly improve the forest performances (especially using small forests), however the correct value of β depends on the problem under exam (and the chosen caratteristics of the random forest estimator) and should be cross-validated to avoid over-fitting.\nNote that this function uses multiple threads if these are available. You can check the number of threads available with Threads.nthreads(). To set the number of threads in Julia either set the environmental variable JULIA_NUM_THREADS (before starting Julia) or start Julia with the command line option --threads (most integrated development editors for Julia already set the number of threads to 4).\n\n\n\n\n\n","category":"method"},{"location":"Trees.html#BetaML.Trees.buildTree-Union{Tuple{Ty}, Tuple{Any, AbstractVector{Ty}}} where Ty","page":"Trees","title":"BetaML.Trees.buildTree","text":"buildTree(x, y, depth; maxdepth, mingain, minrecords, maxfeatures, splittingcriterion, forceclassification)\n\nBuilds (define and train) a Decision Tree.\n\nwarning: Warning\nThis function is deprecated and will possibly be removed in BetaML 0.9. Use DecisionTreeEstimator instead. \n\nGiven a dataset of features x and the corresponding dataset of labels y, recursivelly build a decision tree by finding at each node the best question to split the data untill either all the dataset is separated or a terminal condition is reached. The given tree is then returned.\n\nParameters:\n\nx: The dataset's features (N × D)\ny: The dataset's labels (N × 1)\nmax_depth: The maximum depth the tree is allowed to reach. When this is reached the node is forced to become a leaf [def: N, i.e. no limits]\nmin_gain: The minimum information gain to allow for a node's partition [def: 0]\nmin_records:  The minimum number of records a node must holds to consider for a partition of it [def: 2]\nmax_features: The maximum number of (random) features to consider at each partitioning [def: D, i.e. look at all features]\nsplitting_criterion: Either gini, entropy or variance (see infoGain ) [def: gini for categorical labels (classification task) and variance for numerical labels(regression task)]\nforce_classification: Whether to force a classification task even if the labels are numerical (typically when labels are integers encoding some feature rather than representing a real cardinal measure) [def: false]\nrng: Random Number Generator ((see FIXEDSEED)) [deafult: Random.GLOBAL_RNG]\n\nNotes:\n\nMissing data (in the feature dataset) are supported.\n\n\n\n\n\n","category":"method"},{"location":"Trees.html#BetaML.Trees.wrap","page":"Trees","title":"BetaML.Trees.wrap","text":"wrap(node:: DecisionNode, ...)\n\nCalled on the root node of a DecsionTree dc in order to add visualization information. In case of a BetaML/DecisionTree this is typically a list of feature names as follows:\n\nwdc = wrap(dc, (featurenames = feature_names, ))\n\n\n\n\n\n","category":"function"},{"location":"Utils.html#utils_module","page":"Utils","title":"The BetaML.Utils Module","text":"","category":"section"},{"location":"Utils.html","page":"Utils","title":"Utils","text":"Utils\n","category":"page"},{"location":"Utils.html#BetaML.Utils","page":"Utils","title":"BetaML.Utils","text":"Utils module\n\nProvide shared utility functions for various machine learning algorithms.\n\nFor the complete list of functions provided see below. The main ones are:\n\nHelper functions for logging\n\nMost BetAML functions accept a parameter verbosity that expect one of the element in the Verbosity enoum (NONE, LOW, STD, HIGH or FULL)\nWriting complex code and need to find where something is executed ? Use the macro @codelocation\n\nStochasticity management\n\nUtils provide [FIXEDSEED], [FIXEDRNG] and generate_parallel_rngs. All stochastic functions accept a rng paraemter. See the \"Getting started\" section in the tutorial for details.\n\nData processing\n\nVarious small and large utilities for helping processing the data, expecially before running a ML algorithm\nIncludes getpermutations, onehotencoder, integerencoder (and integerdecoder), partition, scale (and get_scalefactors), pca, cross_validation\n\nSamplers\n\nUtilities to sample from data (e.g. for neural network training or for cross-validation)\nInclude the \"generic\" type SamplerWithData, together with the sampler implementation KFold and the function batch\n\nTransformers\n\nFuntions that \"transform\" a single input (that can be also a vector or a matrix)\nIncludes varios NN \"activation\" functions (relu, celu, sigmoid, softmax, pool1d) and their derivatives (d[FunctionName]), but also gini, entropy, variance, BIC, AIC\n\nMeasures\n\nSeveral functions of a pair of parameters (often y and ŷ) to measure the goodness of ŷ, the distance between the two elements of the pair, ...\nIncludes \"classical\" distance functions (l1_distance, l2_distance, l2squared_distance cosine_distance), \"cost\" functions for continuous variables (squared_cost, mean_relative_error) and comparision functions for multui-class variables (crossentropy, accuracy, ConfMatrix).\n\nImputers\n\nImputers of missing values\n\n\n\n\n\n","category":"module"},{"location":"Utils.html#Module-Index","page":"Utils","title":"Module Index","text":"","category":"section"},{"location":"Utils.html","page":"Utils","title":"Utils","text":"Modules = [Utils]\nOrder   = [:constant, :type, :function, :macro]\nPrivate = false","category":"page"},{"location":"Utils.html#Detailed-API","page":"Utils","title":"Detailed API","text":"","category":"section"},{"location":"Utils.html","page":"Utils","title":"Utils","text":"Modules = [Utils]\nPrivate = false","category":"page"},{"location":"Utils.html#BetaML.Utils.ConfMatrix","page":"Utils","title":"BetaML.Utils.ConfMatrix","text":"ConfMatrix\n\nScores and measures resulting from a comparation between true and predicted categorical variables\n\nUse the function ConfMatrix(ŷ,y;classes,labels,rng) to build it and report(cm::ConfMatrix;what) to visualise it, or use the individual parts of interest, e.g. display(cm.scores).\n\nFields:\n\nlabels: Array of categorical labels\naccuracy: Overall accuracy rate\nmisclassification: Overall misclassification rate\nactual_count: Array of counts per lebel in the actual data\npredicted_count: Array of counts per label in the predicted data\nscores: Matrix actual (rows) vs predicted (columns)\nnormalised_scores: Normalised scores\ntp: True positive (by class)\ntn: True negative (by class)\nfp: False positive (by class), aka \"type I error\" or \"false allarm\"\nfn: False negative (by class), aka \"type II error\" or \"miss\"\nprecision: True class i over predicted class i (by class)\nrecall: Predicted class i over true class i (by class), aka \"True Positive Rate (TPR)\", \"Sensitivity\" or \"Probability of detection\"\nspecificity: Predicted not class i over true not class i (by class), aka \"True Negative Rate (TNR)\"\nf1score: Harmonic mean of precision and recall\nmean_precision: Mean by class, respectively unweighted and weighted by actual_count\nmean_recall: Mean by class, respectively unweighted and weighted by actual_count\nmean_specificity: Mean by class, respectively unweighted and weighted by actual_count\nmean_f1score: Mean by class, respectively unweighted and weighted by actual_count\n\n\n\n\n\n","category":"type"},{"location":"Utils.html#BetaML.Utils.ConfMatrix-Union{Tuple{T}, Tuple{Any, AbstractArray{T, N} where N}} where T","page":"Utils","title":"BetaML.Utils.ConfMatrix","text":"ConfMatrix(ŷ,y;classes,labels,rng)\n\nBuild a \"confusion matrix\" between predicted (columns) vs actual (rows) categorical values\n\nParameters:\n\nŷ: Vector of predicted categorical data\ny: Vector of actual categorical data\nclasses: The full set of possible classes (useful to give a specicif order or if not al lclasses are represented in y) [def: unique(y) ]\nlabels: String representation of the classes [def: string.(classes)]\nrng: Random number generator. Used only if ŷ is given in terms of a PMF and there are multi-modal values, as these are assigned randomply [def: Random.GLOBAL_RNG]\n\nReturn and example:\n\na ConfMatrix object to be printed e.g with print or plotted with heatmap:\n\njulia> using BetaML, Plots\njulia> y = [\"orange\",\"apple\",\"banana\",\"orange\",\"orange\",\"banana\"]\njulia> ŷ = [\"orange\",\"apple\",\"apple\",\"apple\",\"orange\",\"banana\"]\njulia> cm = ConfMatrix(ŷ,y)\n\n\n\n\n\n\n","category":"method"},{"location":"Utils.html#BetaML.Utils.ConfusionMatrix","page":"Utils","title":"BetaML.Utils.ConfusionMatrix","text":"mutable struct ConfusionMatrix <: BetaMLUnsupervisedModel\n\nCompute a confusion matrix detailing the mismatch between observations and predictions of a categorical variable\n\nFor the parameters see ConfusionMatrixHyperParametersSet and BetaMLDefaultOptionsSet.\n\nThe \"predicted\" values are either the scores or the normalised scores (depending on the parameter normalise_scores [def: true]).\n\nNotes:\n\nThe Confusion matrix report can be printed (i.e. print(cm_model). If you plan to print the Confusion Matrix report, be sure that the type of the data in y and ŷ can be converted to String.\nInformation in a structured way is available trought the info(cm) function that returns the following dictionary:\naccuracy:           Oveall accuracy rate\nmisclassification:  Overall misclassification rate\nactual_count:       Array of counts per lebel in the actual data\npredicted_count:    Array of counts per label in the predicted data\nscores:             Matrix actual (rows) vs predicted (columns)\nnormalised_scores:  Normalised scores\ntp:                 True positive (by class)\ntn:                 True negative (by class)\nfp:                 False positive (by class)\nfn:                 False negative (by class)\nprecision:          True class i over predicted class i (by class)\nrecall:             Predicted class i over true class i (by class)\nspecificity:        Predicted not class i over true not class i (by class)\nf1score:            Harmonic mean of precision and recall\nmean_precision:     Mean by class, respectively unweighted and weighted by actual_count\nmean_recall:        Mean by class, respectively unweighted and weighted by actual_count\nmean_specificity:   Mean by class, respectively unweighted and weighted by actual_count\nmean_f1score:       Mean by class, respectively unweighted and weighted by actual_count\ncategories:         The categories considered\nfitted_records:     Number of records considered\nn_categories:       Number of categories considered\nThe confusion matrix can also be plotted, e.g.:\n\nusing Plots, BetaML \ny  = [1,2,2,1,3,2,3]\nŷ  = [1,3,2,2,3,3,3]\ncm = ConfusionMatrix()\nfit!(cm,y,ŷ)\nres = info(cm)\nheatmap(string.(res[:categories]),string.(res[:categories]),res[:normalised_scores],seriescolor=cgrad([:white,:blue]),xlabel=\"Predicted\",ylabel=\"Actual\", title=\"Confusion Matrix (normalised scores)\")\n\n\n\n\n\n","category":"type"},{"location":"Utils.html#BetaML.Utils.ConfusionMatrixHyperParametersSet","page":"Utils","title":"BetaML.Utils.ConfusionMatrixHyperParametersSet","text":"mutable struct ConfusionMatrixHyperParametersSet <: BetaMLHyperParametersSet\n\nHyperparameters for ConfusionMatrix\n\nParameters:\n\ncategories\nThe categories (aka \"levels\") to represent. [def: nothing, i.e. unique ground true values].\nhandle_unknown\nHow to handle categories not seen in the ground true values or not present in the provided categories array? \"error\" (default) rises an error, \"infrequent\" adds a specific category for these values.\nhandle_missing\nHow to handle missing values in either ground true or predicted values ? \"error\" [default] will rise an error, \"drop\" will drop the record\nother_categories_name\nWhich value to assign to the \"other\" category (i.e. categories not seen in the gound truth or not present in the provided categories array? [def: nothing, i.e. typemax(Int64) for integer vectors and \"other\" for other types]. This setting is active only if handle_unknown=\"infrequent\" and in that case it MUST be specified if the vector to one-hot encode is neither integer or strings\nnormalise_scores\nWheter predict should return the normalised scores. Note that both unnormalised and normalised scores remain available using info. [def: true]\n\n\n\n\n\n","category":"type"},{"location":"Utils.html#BetaML.Utils.KFold","page":"Utils","title":"BetaML.Utils.KFold","text":"KFold(nSplits=5,nRepeats=1,shuffle=true,rng=Random.GLOBAL_RNG)\n\nIterator for k-fold cross_validation strategy.\n\n\n\n\n\n","category":"type"},{"location":"Utils.html#BetaML.Utils.MinMaxScaler","page":"Utils","title":"BetaML.Utils.MinMaxScaler","text":"mutable struct MinMaxScaler <: BetaML.Utils.AbstractScaler\n\nScale the data to a given (def: unit) hypercube\n\nParameters\n\ninputRange\nThe range of the input. [def: (minimum,maximum)]. Both ranges are functions of the data. You can consider other relative of absolute ranges using e.g. inputRange=(x->minimum(x)*0.8,x->100)\noutputRange\nThe range of the scaled output [def: (0,1)]\n\n\n\n\n\n","category":"type"},{"location":"Utils.html#BetaML.Utils.OneHotEncoder","page":"Utils","title":"BetaML.Utils.OneHotEncoder","text":"mutable struct OneHotEncoder <: BetaMLUnsupervisedModel\n\nEncode a vector of categorical values as one-hot columns.\n\nThe algorithm distinguishes between missing values, for which it returns a one-hot encoded row of missing values, and other categories not in the provided list or not seen during training that are handled according to the handle_unknown parameter. \n\nFor the parameters see OneHotEncoderHyperParametersSet and BetaMLDefaultOptionsSet.  This model supports inverse_predict.\n\n\n\n\n\n","category":"type"},{"location":"Utils.html#BetaML.Utils.OneHotEncoderHyperParametersSet","page":"Utils","title":"BetaML.Utils.OneHotEncoderHyperParametersSet","text":"mutable struct OneHotEncoderHyperParametersSet <: BetaMLHyperParametersSet\n\nHyperparameters for both OneHotEncoder and OrdinalEncoder\n\nParameters:\n\ncategories\nThe categories to represent as columns. [def: nothing, i.e. unique training values]. Do not include missing in this list.\nhandle_unknown\nHow to handle categories not seen in training or not present in the provided categories array? \"error\" (default) rises an error, \"missing\" labels the whole output with missing values, \"infrequent\" adds a specific column for these categories in one-hot encoding or a single new category for ordinal one.\nother_categories_name\nWhich value during inverse transformation to assign to the \"other\" category (i.e. categories not seen on training or not present in the provided categories array? [def: nothing, i.e. typemax(Int64) for integer vectors and \"other\" for other types]. This setting is active only if handle_unknown=\"infrequent\" and in that case it MUST be specified if the vector to one-hot encode is neither integer or strings\n\n\n\n\n\n","category":"type"},{"location":"Utils.html#BetaML.Utils.OrdinalEncoder","page":"Utils","title":"BetaML.Utils.OrdinalEncoder","text":"mutable struct OrdinalEncoder <: BetaMLUnsupervisedModel\n\nEncode a vector of categorical values as integers.\n\nThe algorithm distinguishes between missing values, for which it propagate the missing, and other categories not in the provided list or not seen during training that are handled according to the handle_unknown parameter. \n\nFor the parameters see OneHotEncoderHyperParametersSet and BetaMLDefaultOptionsSet. This model supports inverse_predict.\n\n\n\n\n\n","category":"type"},{"location":"Utils.html#BetaML.Utils.PCA","page":"Utils","title":"BetaML.Utils.PCA","text":"mutable struct PCA <: BetaMLUnsupervisedModel\n\nPerform a Principal Component Analysis, a dimensionality reduction tecnique employing a linear trasformation of the original matrix by the eigenvectors of the covariance matrix.\n\nPCA returns the matrix reprojected among the dimensions of maximum variance.\n\nFor the parameters see PCAHyperParametersSet and BetaMLDefaultOptionSet \n\nNotes:\n\nPCA doesn't automatically scale the data. It is suggested to apply the Scaler model before running it. \nmissing data are not supported. Impute them first, see the Imputation module.\n\n\n\n\n\n","category":"type"},{"location":"Utils.html#BetaML.Utils.PCAHyperParametersSet","page":"Utils","title":"BetaML.Utils.PCAHyperParametersSet","text":"mutable struct PCAHyperParametersSet <: BetaMLHyperParametersSet\n\nHyperparameters for the PCA transformer\n\nParameters\n\noutdims\nmax_prop_unexplained_var\n\n\n\n\n\n","category":"type"},{"location":"Utils.html#BetaML.Utils.SamplerWithData","page":"Utils","title":"BetaML.Utils.SamplerWithData","text":"SamplerWithData{Tsampler}\n\nAssociate an instance of an AbstractDataSampler with the actual data to sample.\n\n\n\n\n\n","category":"type"},{"location":"Utils.html#BetaML.Utils.Scaler","page":"Utils","title":"BetaML.Utils.Scaler","text":"mutable struct Scaler <: BetaMLUnsupervisedModel\n\nScale the data according to the specific chosen method (def: StandardScaler) \n\nFor the parameters see ScalerHyperParametersSet and BetaMLDefaultOptionSet \n\njulia>m = Scaler(MinMaxScaler(inputRange=(x->minimum(x)*0.8,maximum),outputRange=(0,256)),skip=[3,7,8])\n\n\n\n\n\n","category":"type"},{"location":"Utils.html#BetaML.Utils.ScalerHyperParametersSet","page":"Utils","title":"BetaML.Utils.ScalerHyperParametersSet","text":"mutable struct ScalerHyperParametersSet <: BetaMLHyperParametersSet\n\nHyperparameters for the Scaler transformer\n\nParameters\n\nmethod\nThe specific scaler method to employ with its own parameters. See StandardScaler [def] or MinMaxScaler.\nskip\nThe ids of the columns to skip scaling (eg. categorical columns, dummies,...) [def: []]\n\n\n\n\n\n","category":"type"},{"location":"Utils.html#BetaML.Utils.StandardScaler","page":"Utils","title":"BetaML.Utils.StandardScaler","text":"mutable struct StandardScaler <: BetaML.Utils.AbstractScaler\n\nStandardise the input to zero mean (unless center=false is used) and unit standard deviation (unless scale=false is used), aka \"Z-score\". Note that missing values are skipped.\n\n\n\n\n\n","category":"type"},{"location":"Utils.html#Base.error-Union{Tuple{T}, Tuple{AbstractVector{T}, AbstractVector{T}}} where T","page":"Utils","title":"Base.error","text":"error(ŷ,y;ignorelabels=false) - Categorical error (T vs T)\n\n\n\n\n\n","category":"method"},{"location":"Utils.html#Base.error-Union{Tuple{T}, Tuple{Array{Dict{T, Float64}, 1}, Vector{T}}} where T","page":"Utils","title":"Base.error","text":"error(ŷ,y) - Categorical error with with probabilistic predictions of a dataset given in terms of a dictionary of probabilities (Dict{T,Float64} vs T). \n\n\n\n\n\n","category":"method"},{"location":"Utils.html#Base.error-Union{Tuple{T}, Tuple{Matrix{T}, Vector{Int64}}} where T<:Number","page":"Utils","title":"Base.error","text":"error(ŷ,y) - Categorical error with probabilistic predictions of a dataset (PMF vs Int). \n\n\n\n\n\n","category":"method"},{"location":"Utils.html#Base.error-Union{Tuple{T}, Tuple{Vector{T}, Int64}} where T<:Number","page":"Utils","title":"Base.error","text":"error(ŷ,y) - Categorical error with probabilistic prediction of a single datapoint (PMF vs Int). \n\n\n\n\n\n","category":"method"},{"location":"Utils.html#Base.reshape-Union{Tuple{T}, Tuple{T, Vararg{Any, N} where N}} where T<:Number","page":"Utils","title":"Base.reshape","text":"reshape(myNumber, dims..) - Reshape a number as a n dimensional Array \n\n\n\n\n\n","category":"method"},{"location":"Utils.html#BetaML.Utils.accuracy-Union{Tuple{T}, Tuple{AbstractVector{T}, AbstractVector{T}}} where T","page":"Utils","title":"BetaML.Utils.accuracy","text":"accuracy(ŷ,y;ignorelabels=false) - Categorical accuracy between two vectors (T vs T). \n\n\n\n\n\n","category":"method"},{"location":"Utils.html#BetaML.Utils.accuracy-Union{Tuple{T}, Tuple{Array{Dict{T, Float64}, 1}, Vector{T}}} where T","page":"Utils","title":"BetaML.Utils.accuracy","text":"accuracy(ŷ,y;tol)\n\nCategorical accuracy with probabilistic predictions of a dataset given in terms of a dictionary of probabilities (Dict{T,Float64} vs T).\n\nParameters:\n\nŷ: An array where each item is the estimated probability mass function in terms of a Dictionary(Item1 => Prob1, Item2 => Prob2, ...)\ny: The N array with the correct category for each point n.\ntol: The tollerance to the prediction, i.e. if considering \"correct\" only a prediction where the value with highest probability is the true value (tol = 1), or consider instead the set of tol maximum values [def: 1].\n\n\n\n\n\n","category":"method"},{"location":"Utils.html#BetaML.Utils.accuracy-Union{Tuple{T}, Tuple{Dict{T, Float64}, T}} where T","page":"Utils","title":"BetaML.Utils.accuracy","text":"accuracy(ŷ,y;tol)\n\nCategorical accuracy with probabilistic prediction of a single datapoint given in terms of a dictionary of probabilities (Dict{T,Float64} vs T).\n\nParameters:\n\nŷ: The returned probability mass function in terms of a Dictionary(Item1 => Prob1, Item2 => Prob2, ...)\ntol: The tollerance to the prediction, i.e. if considering \"correct\" only a prediction where the value with highest probability is the true value (tol = 1), or consider instead the set of tol maximum values [def: 1].\n\n\n\n\n\n","category":"method"},{"location":"Utils.html#BetaML.Utils.accuracy-Union{Tuple{T}, Tuple{Matrix{T}, Vector{Int64}}} where T<:Number","page":"Utils","title":"BetaML.Utils.accuracy","text":"accuracy(ŷ,y;tol,ignorelabels)\n\nCategorical accuracy with probabilistic predictions of a dataset (PMF vs Int).\n\nParameters:\n\nŷ: An (N,K) matrix of probabilities that each hat y_n record with n in 1N  being of category k with k in 1K.\ny: The N array with the correct category for each point n.\ntol: The tollerance to the prediction, i.e. if considering \"correct\" only a prediction where the value with highest probability is the true value (tol = 1), or consider instead the set of tol maximum values [def: 1].\nignorelabels: Whether to ignore the specific label order in y. Useful for unsupervised learning algorithms where the specific label order don't make sense [def: false]\n\n\n\n\n\n","category":"method"},{"location":"Utils.html#BetaML.Utils.accuracy-Union{Tuple{T}, Tuple{Vector{T}, Int64}} where T<:Number","page":"Utils","title":"BetaML.Utils.accuracy","text":"accuracy(ŷ,y;tol)\n\nCategorical accuracy with probabilistic prediction of a single datapoint (PMF vs Int).\n\nUse the parameter tol [def: 1] to determine the tollerance of the prediction, i.e. if considering \"correct\" only a prediction where the value with highest probability is the true value (tol = 1), or consider instead the set of tol maximum values.\n\n\n\n\n\n","category":"method"},{"location":"Utils.html#BetaML.Utils.aic-Tuple{Any, Any}","page":"Utils","title":"BetaML.Utils.aic","text":"aic(lL,k) -  Akaike information criterion (lower is better)\n\n\n\n\n\n","category":"method"},{"location":"Utils.html#BetaML.Utils.autojacobian-Tuple{Any, Any}","page":"Utils","title":"BetaML.Utils.autojacobian","text":"autojacobian(f,x;nY)\n\nEvaluate the Jacobian using AD in the form of a (nY,nX) matrix of first derivatives\n\nParameters:\n\nf: The function to compute the Jacobian\nx: The input to the function where the jacobian has to be computed\nnY: The number of outputs of the function f [def: length(f(x))]\n\nReturn values:\n\nAn Array{Float64,2} of the locally evaluated Jacobian\n\nNotes:\n\nThe nY parameter is optional. If provided it avoids having to compute f(x)\n\n\n\n\n\n","category":"method"},{"location":"Utils.html#BetaML.Utils.batch-Tuple{Integer, Integer}","page":"Utils","title":"BetaML.Utils.batch","text":"batch(n,bsize;sequential=false,rng)\n\nReturn a vector of bsize vectors of indeces from 1 to n. Randomly unless the optional parameter sequential is used.\n\nExample:\n\njulia julia> Utils.batch(6,2,sequential=true) 3-element Array{Array{Int64,1},1}:  [1, 2]  [3, 4]  [5, 6]\n\n\n\n\n\n","category":"method"},{"location":"Utils.html#BetaML.Utils.bic-Tuple{Any, Any, Any}","page":"Utils","title":"BetaML.Utils.bic","text":"bic(lL,k,n) -  Bayesian information criterion (lower is better)\n\n\n\n\n\n","category":"method"},{"location":"Utils.html#BetaML.Utils.celu-Tuple{Any}","page":"Utils","title":"BetaML.Utils.celu","text":"celu(x; α=1) \n\nhttps://arxiv.org/pdf/1704.07483.pdf\n\n\n\n\n\n","category":"method"},{"location":"Utils.html#BetaML.Utils.class_counts-Tuple{Any}","page":"Utils","title":"BetaML.Utils.class_counts","text":"class_counts(x;classes=nothing)\n\nReturn a (unsorted) vector with the counts of each unique item (element or rows) in a dataset.\n\nIf order is important or not all classes are present in the data, a preset vectors of classes can be given in the parameter classes\n\n\n\n\n\n","category":"method"},{"location":"Utils.html#BetaML.Utils.class_counts_with_labels-Tuple{Any}","page":"Utils","title":"BetaML.Utils.class_counts_with_labels","text":"classcountswith_labels(x)\n\nReturn a dictionary that counts the number of each unique item (rows) in a dataset.\n\n\n\n\n\n","category":"method"},{"location":"Utils.html#BetaML.Utils.cols_with_missing-Tuple{Any}","page":"Utils","title":"BetaML.Utils.cols_with_missing","text":"cols_with_missing(x)\n\nRetuyrn an array with the ids of the columns where there is at least a missing value.\n\n\n\n\n\n","category":"method"},{"location":"Utils.html#BetaML.Utils.cosine_distance-Tuple{Any, Any}","page":"Utils","title":"BetaML.Utils.cosine_distance","text":"Cosine distance\n\n\n\n\n\n","category":"method"},{"location":"Utils.html#BetaML.Utils.cross_validation","page":"Utils","title":"BetaML.Utils.cross_validation","text":"cross_validation(f,data,sampler;dims,verbosity,returnStatistics)\n\nPerform cross_validation according to sampler rule by calling the function f and collecting its output\n\nParameters\n\nf: The user-defined function that consume the specific train and validation data and return somehting (often the associated validation error). See later\ndata: A single n-dimenasional array or a vector of them (e.g. X,Y), depending on the tasks required by f.\nsampler: An istance of a AbstractDataSampler, defining the \"rules\" for sampling at each iteration. [def: KFold(nSplits=5,nRepeats=1,shuffle=true,rng=Random.GLOBAL_RNG) ]\ndims: The dimension over performing the cross_validation i.e. the dimension containing the observations [def: 1]\nverbosity: The verbosity to print information during each iteration (this can also be printed in the f function) [def: STD]\nreturnStatistics: Wheter cross_validation should return the statistics of the output of f (mean and standard deviation) or the whole outputs [def: true].\n\nNotes\n\ncross_validation works by calling the function f, defined by the user, passing to it the tuple trainData, valData and rng and collecting the result of the function f. The specific method for which trainData, and valData are selected at each iteration depends on the specific sampler, whith a single 5 k-fold rule being the default.\n\nThis approach is very flexible because the specific model to employ or the metric to use is left within the user-provided function. The only thing that cross_validation does is provide the model defined in the function f with the opportune data (and the random number generator).\n\nInput of the user-provided function trainData and valData are both themselves tuples. In supervised models, cross_validations data should be a tuple of (X,Y) and trainData and valData will be equivalent to (xtrain, ytrain) and (xval, yval). In unsupervised models data is a single array, but the training and validation data should still need to be accessed as  trainData[1] and valData[1]. Output of the user-provided function The user-defined function can return whatever. However, if returnStatistics is left on its default true value the user-defined function must return a single scalar (e.g. some error measure) so that the mean and the standard deviation are returned.\n\nNote that cross_validation can beconveniently be employed using the do syntax, as Julia automatically rewrite cross_validation(data,...) trainData,valData,rng  ...user defined body... end as cross_validation(f(trainData,valData,rng ), data,...)\n\nExample\n\njulia> X = [11:19 21:29 31:39 41:49 51:59 61:69];\njulia> Y = [1:9;];\njulia> sampler = KFold(nSplits=3);\njulia> (μ,σ) = cross_validation([X,Y],sampler) do trainData,valData,rng\n                 (xtrain,ytrain) = trainData; (xval,yval) = valData\n                 trainedModel    = buildForest(xtrain,ytrain,30)\n                 predictions     = predict(trainedModel,xval)\n                 ϵ               = mean_relative_error(predictions,yval,normrec=false)\n                 return ϵ\n               end\n(0.3202242202242202, 0.04307662219315022)\n\n\n\n\n\n","category":"function"},{"location":"Utils.html#BetaML.Utils.crossentropy-Tuple{Any, Any}","page":"Utils","title":"BetaML.Utils.crossentropy","text":"crossentropy(ŷ, y; weight)\n\nCompute the (weighted) cross-entropy between the predicted and the sampled probability distributions.\n\nTo be used in classification problems.\n\n\n\n\n\n","category":"method"},{"location":"Utils.html#BetaML.Utils.dcelu-Tuple{Any}","page":"Utils","title":"BetaML.Utils.dcelu","text":"dcelu(x; α=1) \n\nhttps://arxiv.org/pdf/1704.07483.pdf\n\n\n\n\n\n","category":"method"},{"location":"Utils.html#BetaML.Utils.delu-Tuple{Any}","page":"Utils","title":"BetaML.Utils.delu","text":"delu(x; α=1) with α > 0 \n\nhttps://arxiv.org/pdf/1511.07289.pdf\n\n\n\n\n\n","category":"method"},{"location":"Utils.html#BetaML.Utils.dmish-Tuple{Any}","page":"Utils","title":"BetaML.Utils.dmish","text":"dmish(x) \n\nhttps://arxiv.org/pdf/1908.08681v1.pdf\n\n\n\n\n\n","category":"method"},{"location":"Utils.html#BetaML.Utils.dplu-Tuple{Any}","page":"Utils","title":"BetaML.Utils.dplu","text":"dplu(x;α=0.1,c=1) \n\nPiecewise Linear Unit derivative \n\nhttps://arxiv.org/pdf/1809.09534.pdf\n\n\n\n\n\n","category":"method"},{"location":"Utils.html#BetaML.Utils.drelu-Tuple{Any}","page":"Utils","title":"BetaML.Utils.drelu","text":"drelu(x) \n\nRectified Linear Unit \n\nhttps://www.cs.toronto.edu/~hinton/absps/reluICML.pdf\n\n\n\n\n\n","category":"method"},{"location":"Utils.html#BetaML.Utils.dsigmoid-Tuple{Any}","page":"Utils","title":"BetaML.Utils.dsigmoid","text":"dsigmoid(x)\n\n\n\n\n\n","category":"method"},{"location":"Utils.html#BetaML.Utils.dsoftmax-Tuple{Any}","page":"Utils","title":"BetaML.Utils.dsoftmax","text":"dsoftmax(x; β=1) \n\nDerivative of the softmax function \n\nhttps://eli.thegreenplace.net/2016/the-softmax-function-and-its-derivative/\n\n\n\n\n\n","category":"method"},{"location":"Utils.html#BetaML.Utils.dsoftplus-Tuple{Any}","page":"Utils","title":"BetaML.Utils.dsoftplus","text":"dsoftplus(x) \n\nhttps://en.wikipedia.org/wiki/Rectifier(neuralnetworks)#Softplus\n\n\n\n\n\n","category":"method"},{"location":"Utils.html#BetaML.Utils.dtanh-Tuple{Any}","page":"Utils","title":"BetaML.Utils.dtanh","text":"dtanh(x)\n\n\n\n\n\n","category":"method"},{"location":"Utils.html#BetaML.Utils.elu-Tuple{Any}","page":"Utils","title":"BetaML.Utils.elu","text":"elu(x; α=1) with α > 0 \n\nhttps://arxiv.org/pdf/1511.07289.pdf\n\n\n\n\n\n","category":"method"},{"location":"Utils.html#BetaML.Utils.entropy-Tuple{Any}","page":"Utils","title":"BetaML.Utils.entropy","text":"entropy(x)\n\nCalculate the entropy for a list of items (or rows).\n\nSee: https://en.wikipedia.org/wiki/Decisiontreelearning#Gini_impurity\n\n\n\n\n\n","category":"method"},{"location":"Utils.html#BetaML.Utils.generate_parallel_rngs-Tuple{Random.AbstractRNG, Integer}","page":"Utils","title":"BetaML.Utils.generate_parallel_rngs","text":"generate_parallel_rngs(rng::AbstractRNG, n::Integer;reSeed=false)\n\nFor multi-threaded models, return n independent random number generators (one per thread) to be used in threaded computations.\n\nNote that each ring is a copy of the original random ring. This means that code that use these RNGs will not change the original RNG state.\n\nUse it with rngs = generate_parallel_rngs(rng,Threads.nthreads()) to have a separate rng per thread. By default the function doesn't re-seed the RNG, as you may want to have a loop index based re-seeding strategy rather than a threadid-based one (to guarantee the same result independently of the number of threads). If you prefer, you can instead re-seed the RNG here (using the parameter reSeed=true), such that each thread has a different seed. Be aware however that the stream  of number generated will depend from the number of threads at run time.\n\n\n\n\n\n","category":"method"},{"location":"Utils.html#BetaML.Utils.get_scalefactors-Tuple{Any}","page":"Utils","title":"BetaML.Utils.get_scalefactors","text":"get_scalefactors(x;skip)\n\nReturn the scale factors (for each dimensions) in order to scale a matrix X (n,d) such that each dimension has mean 0 and variance 1. Note that missing values are skipped.\n\nwarning: Warning\nThis function is deprecated and will possibly be removed in BetaML 0.9. Use the model Scaler() instead.\n\nParameters\n\nx: the (n × d) dimension matrix to scale on each dimension d\nskip: an array of dimension index to skip the scaling [def: []]\n\nReturn\n\nA touple whose first elmement is the shift and the second the multiplicative\n\nterm to make the scale.\n\n\n\n\n\n","category":"method"},{"location":"Utils.html#BetaML.Utils.getpermutations-Union{Tuple{AbstractVector{T}}, Tuple{T}} where T","page":"Utils","title":"BetaML.Utils.getpermutations","text":"getpermutations(v::AbstractArray{T,1};keepStructure=false)\n\nReturn a vector of either (a) all possible permutations (uncollected) or (b) just those based on the unique values of the vector\n\nUseful to measure accuracy where you don't care about the actual name of the labels, like in unsupervised classifications (e.g. clustering)\n\n\n\n\n\n","category":"method"},{"location":"Utils.html#BetaML.Utils.gini-Tuple{Any}","page":"Utils","title":"BetaML.Utils.gini","text":"gini(x)\n\nCalculate the Gini Impurity for a list of items (or rows).\n\nSee: https://en.wikipedia.org/wiki/Decisiontreelearning#Information_gain\n\n\n\n\n\n","category":"method"},{"location":"Utils.html#BetaML.Utils.integerdecoder-Union{Tuple{T}, Tuple{Any, AbstractVector{T}}} where T","page":"Utils","title":"BetaML.Utils.integerdecoder","text":"integerdecoder(x,factors::AbstractVector{T};unique)\n\nDecode an array of integers to an array of T corresponding to the elements of factors\n\nwarning: Warning\nThis function is deprecated and will possibly be removed in BetaML 0.9. Use the model OrdinalEncoder() instead.\n\nParameters:\n\nx: The vector to decode\nfactors: The vector of elements to use for the encoding\nunique: Wether factors is already made of unique elements [def: true]\n\nReturn:\n\nA vector of length(x) elements corresponding to the (unique) factors elements at the position x\n\nExample:\n\njulia> integerdecoder([1, 2, 2, 3, 2, 1],[\"aa\",\"cc\",\"bb\"]) # out: [\"aa\",\"cc\",\"cc\",\"bb\",\"cc\",\"aa\"]\n\n\n\n\n\n","category":"method"},{"location":"Utils.html#BetaML.Utils.integerencoder-Tuple{AbstractVector{T} where T}","page":"Utils","title":"BetaML.Utils.integerencoder","text":"integerencoder(x;factors=unique(x))\n\nEncode an array of T to an array of integers using the their position in factor vector (default to the unique vector of the input array)\n\nwarning: Warning\nThis function is deprecated and will possibly be removed in BetaML 0.9. Use the model OrdinalEncoder() instead.\n\nParameters:\n\nx: The vector to encode\nfactors: The vector of factors whose position is the result of the encoding [def: unique(x)]\n\nReturn:\n\nA vector of [1,length(x)] integers corresponding to the position of each element in the factors vector`\n\nNote:\n\nAttention that while this function creates a ordered (and sortable) set, it is up to the user to be sure that this \"property\" is not indeed used in his code if the unencoded data is indeed unordered.\n\nExample:\n\njulia> integerencoder([\"a\",\"e\",\"b\",\"e\"],factors=[\"a\",\"b\",\"c\",\"d\",\"e\"]) # out: [1,5,2,5]\n\n\n\n\n\n","category":"method"},{"location":"Utils.html#BetaML.Utils.issortable-Union{Tuple{AbstractArray{T, N}}, Tuple{N}, Tuple{T}} where {T, N}","page":"Utils","title":"BetaML.Utils.issortable","text":"Return wheather an array is sortable, i.e. has methos issort defined\n\n\n\n\n\n","category":"method"},{"location":"Utils.html#BetaML.Utils.l1_distance-Tuple{Any, Any}","page":"Utils","title":"BetaML.Utils.l1_distance","text":"L1 norm distance (aka Manhattan Distance)\n\n\n\n\n\n","category":"method"},{"location":"Utils.html#BetaML.Utils.l2_distance-Tuple{Any, Any}","page":"Utils","title":"BetaML.Utils.l2_distance","text":"Euclidean (L2) distance\n\n\n\n\n\n","category":"method"},{"location":"Utils.html#BetaML.Utils.l2squared_distance-Tuple{Any, Any}","page":"Utils","title":"BetaML.Utils.l2squared_distance","text":"Squared Euclidean (L2) distance\n\n\n\n\n\n","category":"method"},{"location":"Utils.html#BetaML.Utils.lse-Tuple{Any}","page":"Utils","title":"BetaML.Utils.lse","text":"LogSumExp for efficiently computing log(sum(exp.(x))) \n\n\n\n\n\n","category":"method"},{"location":"Utils.html#BetaML.Utils.makematrix-Tuple{AbstractArray}","page":"Utils","title":"BetaML.Utils.makematrix","text":"Transform an Array{T,1} in an Array{T,2} and leave unchanged Array{T,2}.\n\n\n\n\n\n","category":"method"},{"location":"Utils.html#BetaML.Utils.mean_dicts-Tuple{Any}","page":"Utils","title":"BetaML.Utils.mean_dicts","text":"mean_dicts(dicts)\n\nCompute the mean of the values of an array of dictionaries.\n\nGiven dicts an array of dictionaries, mean_dicts first compute the union of the keys and then average the values. If the original valueas are probabilities (non-negative items summing to 1), the result is also a probability distribution.\n\n\n\n\n\n","category":"method"},{"location":"Utils.html#BetaML.Utils.mean_relative_error-Tuple{Any, Any}","page":"Utils","title":"BetaML.Utils.mean_relative_error","text":"meanrelativeerror(ŷ,y;normdim=true,normrec=true,p=1)\n\nCompute the mean relative error (l-1 based by default) between ŷ and y.\n\nThere are many ways to compute a mean relative error. In particular, if normrec (normdim) is set to true, the records (dimensions) are normalised, in the sense that it doesn't matter if a record (dimension) is bigger or smaller than the others, the relative error is first computed for each record (dimension) and then it is averaged. With both normdim and normrec set to false the function returns the relative mean error; with both set to true (default) it returns the mean relative error (i.e. with p=1 the \"mean absolute percentage error (MAPE)\") The parameter p [def: 1] controls the p-norm used to define the error.\n\nThe mean relative error enfatises the relativeness of the error, i.e. all observations and dimensions weigth the same, wether large or small. Conversly, in the relative mean error the same relative error on larger observations (or dimensions) weights more.\n\nFor example, given y = [1,44,3] and ŷ = [2,45,2], the mean relative error mean_relative_error(ŷ,y) is 0.452, while the relative mean error mean_relative_error(ŷ,y, normrec=false) is \"only\" 0.0625.\n\n\n\n\n\n","category":"method"},{"location":"Utils.html#BetaML.Utils.mish-Tuple{Any}","page":"Utils","title":"BetaML.Utils.mish","text":"mish(x) \n\nhttps://arxiv.org/pdf/1908.08681v1.pdf\n\n\n\n\n\n","category":"method"},{"location":"Utils.html#BetaML.Utils.mode-Union{Tuple{AbstractArray{Dict{T, Float64}, N} where N}, Tuple{T}} where T","page":"Utils","title":"BetaML.Utils.mode","text":"mode(elements,rng)\n\nGiven a vector of dictionaries whose key is numerical (e.g. probabilities), a vector of vectors or a matrix, it returns the mode of each element (dictionary, vector or row) in terms of the key or the position.\n\nUse it to return a unique value from a multiclass classifier returning probabilities.\n\nNote:\n\nIf multiple classes have the highest mode, one is returned at random (use the parameter rng to fix the stochasticity)\n\n\n\n\n\n","category":"method"},{"location":"Utils.html#BetaML.Utils.mode-Union{Tuple{AbstractVector{T}}, Tuple{T}} where T<:Number","page":"Utils","title":"BetaML.Utils.mode","text":"mode(v::AbstractVector{T};rng)\n\nReturn the position with the highest value in an array, interpreted as mode (using rand in case of multimodal values)\n\n\n\n\n\n","category":"method"},{"location":"Utils.html#BetaML.Utils.mode-Union{Tuple{Dict{T, Float64}}, Tuple{T}} where T","page":"Utils","title":"BetaML.Utils.mode","text":"mode(dict::Dict{T,Float64};rng)\n\nReturn the key with highest mode (using rand in case of multimodal values)\n\n\n\n\n\n","category":"method"},{"location":"Utils.html#BetaML.Utils.mse-Tuple{Any, Any}","page":"Utils","title":"BetaML.Utils.mse","text":"mse(ŷ,y)\n\nCompute the mean squared error (MSE) (aka mean squared deviation - MSD) between two vectors ŷ and y. Note that while the deviation is averaged by the length of y is is not scaled to give it a relative meaning.\n\n\n\n\n\n","category":"method"},{"location":"Utils.html#BetaML.Utils.onehotdecoder-Tuple{Any}","page":"Utils","title":"BetaML.Utils.onehotdecoder","text":"onehotdecoder(x)\n\nGiven a matrix of one-hot encoded values (e.g. [0 1 0; 1 0 0]) returns a vector of the integer positions (e.g. [2,1]).\n\nwarning: Warning\nThis function is deprecated and will possibly be removed in BetaML 0.9. Use the model OneHotEncoder() instead.\n\n\n\n\n\n","category":"method"},{"location":"Utils.html#BetaML.Utils.onehotencoder-Union{Tuple{Union{AbstractVector{T}, T}}, Tuple{T}} where T","page":"Utils","title":"BetaML.Utils.onehotencoder","text":"onehotencoder(x;d,factors,count)\n\nEncode arrays (or arrays of arrays) of categorical data as matrices of one column per factor.\n\nwarning: Warning\nThis function is deprecated and will possibly be removed in BetaML 0.9. Use the model OneHotEncoder() instead.\n\nThe case of arrays of arrays is for when at each record you have more than one categorical output. You can then decide to encode just the presence of the factors or their counting\n\nParameters:\n\nx: The data to convert (array or array of arrays)\nd: The number of dimensions in the output matrix [def: maximum(x) for integers and length(factors) otherwise]\nfactors: The factors from which to encode [def: 1:d for integer x or unique(x) otherwise]\ncount: Wether to count multiple instances on the same dimension/record (true) or indicate just presence. [def: false]\n\nExamples\n\njulia> onehotencoder([\"a\",\"c\",\"c\"],factors=[\"a\",\"b\",\"c\",\"d\"])\n3×4 Matrix{Int64}:\n 1  0  0  0\n 0  0  1  0\n 0  0  1  0\njulia> onehotencoder([2,4,4])\n3×4 Matrix{Int64}:\n 0  1  0  0\n 0  0  0  1\n 0  0  0  1\n julia> onehotencoder([[2,2,1],[2,4,4]],count=true)\n2×4 Matrix{Int64}:\n 1  2  0  0\n 0  1  0  2\n\n\n\n\n\n","category":"method"},{"location":"Utils.html#BetaML.Utils.partition-Union{Tuple{T}, Tuple{AbstractVector{T}, AbstractVector{Float64}}} where T<:AbstractArray","page":"Utils","title":"BetaML.Utils.partition","text":"partition(data,parts;shuffle,dims,rng)\n\nPartition (by rows) one or more matrices according to the shares in parts.\n\nParameters\n\ndata: A matrix/vector or a vector of matrices/vectors\nparts: A vector of the required shares (must sum to 1)\nshufle: Whether to randomly shuffle the matrices (preserving the relative order between matrices)\ndims: The dimension for which to partition [def: 1]\ncopy: Wheter to copy the actual data or only create a reference [def: true]\nrng: Random Number Generator (see FIXEDSEED) [deafult: Random.GLOBAL_RNG]\n\nNotes:\n\nThe sum of parts must be equal to 1\nThe number of elements in the specified dimension must be the same for all the arrays in data\n\nExample:\n\njulia julia> x = [1:10 11:20] julia> y = collect(31:40) julia> ((xtrain,xtest),(ytrain,ytest)) = partition([x,y],[0.7,0.3])\n\n\n\n\n\n","category":"method"},{"location":"Utils.html#BetaML.Utils.pca-Tuple{Any}","page":"Utils","title":"BetaML.Utils.pca","text":"pca(X;K,error)\n\nPerform Principal Component Analysis returning the matrix reprojected among the dimensions of maximum variance.\n\nwarning: Warning\nThis function is deprecated and will possibly be removed in BetaML 0.9. Use the model PCA() instead.\n\nParameters:\n\nX : The (N,D) data to reproject\nK : The number of dimensions to maintain (with K<=D) [def: nothing]\nerror: The maximum approximation error that we are willing to accept [def: 0.05]\n\nReturn:\n\nA named tuple with:\nX: The reprojected (NxK) matrix with the column dimensions organized in descending order of of the proportion of explained variance\nK: The number of dimensions retieved\nerror: The actual proportion of variance not explained in the reprojected dimensions\nP: The (D,K) matrix of the eigenvectors associated to the K-largest eigenvalues used to reproject the data matrix\nexplVarByDim: An array of dimensions D with the share of the cumulative variance explained by dimensions (the last element being always 1.0)\n\nNotes:\n\nIf K is provided, the parameter error has no effect.\nIf one doesn't know a priori the error that she/he is willling to accept, nor the wished number of dimensions, he/she can run this pca function with out = pca(X,K=size(X,2)) (i.e. with K=D), analise the proportions of explained cumulative variance by dimensions in out.explVarByDim, choose the number of dimensions K according to his/her needs and finally pick from the reprojected matrix only the number of dimensions needed, i.e. out.X[:,1:K].\n\nExample:\n\njulia> X = [1 10 100; 1.1 15 120; 0.95 23 90; 0.99 17 120; 1.05 8 90; 1.1 12 95]\n6×3 Matrix{Float64}:\n 1.0   10.0  100.0\n 1.1   15.0  120.0\n 0.95  23.0   90.0\n 0.99  17.0  120.0\n 1.05   8.0   90.0\n 1.1   12.0   95.0\njulia> X = pca(X,error=0.05).X\n6×2 Matrix{Float64}:\n 100.449    3.1783\n 120.743    6.80764\n  91.3551  16.8275\n 120.878    8.80372\n  90.3363   1.86179\n  95.5965   5.51254\n\n\n\n\n\n","category":"method"},{"location":"Utils.html#BetaML.Utils.plu-Tuple{Any}","page":"Utils","title":"BetaML.Utils.plu","text":"plu(x;α=0.1,c=1) \n\nPiecewise Linear Unit \n\nhttps://arxiv.org/pdf/1809.09534.pdf\n\n\n\n\n\n","category":"method"},{"location":"Utils.html#BetaML.Utils.polynomial_kernel-Tuple{Any, Any}","page":"Utils","title":"BetaML.Utils.polynomial_kernel","text":"Polynomial kernel parametrised with c=0 and d=2 (i.e. a quadratic kernel). For other cᵢ and dᵢ use K = (x,y) -> polynomial_kernel(x,y,c=cᵢ,d=dᵢ) as kernel function in the supporting algorithms\n\n\n\n\n\n","category":"method"},{"location":"Utils.html#BetaML.Utils.pool1d","page":"Utils","title":"BetaML.Utils.pool1d","text":"pool1d(x,poolsize=2;f=mean)\n\nApply funtion f to a rolling poolsize contiguous (in 1d) neurons.\n\nApplicable to VectorFunctionLayer, e.g. layer2  = VectorFunctionLayer(nₗ,f=(x->pool1d(x,4,f=mean)) Attention: to apply this funciton as activation function in a neural network you will need Julia version >= 1.6, otherwise you may experience a segmentation fault (see this bug report)\n\n\n\n\n\n","category":"function"},{"location":"Utils.html#BetaML.Utils.radial_kernel-Tuple{Any, Any}","page":"Utils","title":"BetaML.Utils.radial_kernel","text":"Radial Kernel (aka RBF kernel) parametrised with γ=1/2. For other gammas γᵢ use K = (x,y) -> radial_kernel(x,y,γ=γᵢ) as kernel function in the supporting algorithms\n\n\n\n\n\n","category":"method"},{"location":"Utils.html#BetaML.Utils.relu-Tuple{Any}","page":"Utils","title":"BetaML.Utils.relu","text":"relu(x) \n\nRectified Linear Unit \n\nhttps://www.cs.toronto.edu/~hinton/absps/reluICML.pdf\n\n\n\n\n\n","category":"method"},{"location":"Utils.html#BetaML.Utils.scale","page":"Utils","title":"BetaML.Utils.scale","text":"scale(x,scalefactors;rev)\n\nPerform a linear scaling of x using scaling factors scalefactors.\n\nwarning: Warning\nThis function is deprecated and will possibly be removed in BetaML 0.9. Use the model Scaler() instead.\n\nParameters\n\nx: The (n × d) dimension matrix to scale on each dimension d\nscalingFactors: A tuple of the constant and multiplicative scaling factor\n\nrespectively [def: the scaling factors needed to scale x to mean 0 and variance 1]\n\nrev: Whether to invert the scaling [def: false]\n\nReturn\n\nThe scaled matrix\n\nNotes:\n\nAlso available scale!(x,scalefactors) for in-place scaling\nRetrieve the scale factors with the get_scalefactors() function\nNote that missing values are skipped\n\n\n\n\n\n","category":"function"},{"location":"Utils.html#BetaML.Utils.sigmoid-Tuple{Any}","page":"Utils","title":"BetaML.Utils.sigmoid","text":"sigmoid(x)\n\n\n\n\n\n","category":"method"},{"location":"Utils.html#BetaML.Utils.softmax-Tuple{Any}","page":"Utils","title":"BetaML.Utils.softmax","text":"softmax (x; β=1) \n\nThe input x is a vector. Return a PMF\n\n\n\n\n\n","category":"method"},{"location":"Utils.html#BetaML.Utils.softplus-Tuple{Any}","page":"Utils","title":"BetaML.Utils.softplus","text":"softplus(x) \n\nhttps://en.wikipedia.org/wiki/Rectifier(neuralnetworks)#Softplus\n\n\n\n\n\n","category":"method"},{"location":"Utils.html#BetaML.Utils.squared_cost-Tuple{Any, Any}","page":"Utils","title":"BetaML.Utils.squared_cost","text":"squared_cost(ŷ,y)\n\nCompute the squared costs between a vector of prediction and one of observations as (1/2)*norm(y - ŷ)^2.\n\nAside the 1/2 term, it correspond to the squared l-2 norm distance and when it is averaged on multiple datapoints corresponds to the Mean Squared Error (MSE). It is mostly used for regression problems.\n\n\n\n\n\n","category":"method"},{"location":"Utils.html#BetaML.Utils.sterling-Tuple{BigInt, BigInt}","page":"Utils","title":"BetaML.Utils.sterling","text":"Sterling number: number of partitions of a set of n elements in k sets \n\n\n\n\n\n","category":"method"},{"location":"Utils.html#BetaML.Utils.variance-Tuple{Any}","page":"Utils","title":"BetaML.Utils.variance","text":"variance(x) - population variance\n\n\n\n\n\n","category":"method"},{"location":"Utils.html#Random.shuffle-Union{Tuple{AbstractVector{T}}, Tuple{T}} where T<:AbstractArray","page":"Utils","title":"Random.shuffle","text":"shuffle(data;dims,rng)\n\nShuffle a vector of n-dimensional arrays across dimension dims keeping the same order between the arrays\n\nParameters\n\ndata: The vector of arrays to shuffle\ndims: The dimension over to apply the shuffle [def: 1]\nrng:  An AbstractRNG to apply for the shuffle\n\nNotes\n\nAll the arrays must have the same size for the dimension to shuffle\n\nExample\n\njulia> a = [1 2 30; 10 20 30]; b = [100 200 300]; julia> (aShuffled, bShuffled) = shuffle([a,b],dims=2) 2-element Vector{Matrix{Int64}}:  [1 30 2; 10 30 20]  [100 300 200]\n\n\n\n\n\n","category":"method"},{"location":"Utils.html#BetaML.Utils.@codelocation-Tuple{}","page":"Utils","title":"BetaML.Utils.@codelocation","text":"@codelocation()\n\nHelper macro to print during runtime an info message concerning the code being executed position\n\n\n\n\n\n","category":"macro"},{"location":"tutorials/Clustering - Iris/betaml_tutorial_cluster_iris.html","page":"A clustering task: the prediction of  plant species from floreal measures (the iris dataset)","title":"A clustering task: the prediction of  plant species from floreal measures (the iris dataset)","text":"EditURL = \"https://github.com/sylvaticus/BetaML.jl/blob/master/docs/src/tutorials/Clustering - Iris/betaml_tutorial_cluster_iris.jl\"","category":"page"},{"location":"tutorials/Clustering - Iris/betaml_tutorial_cluster_iris.html#clustering_tutorial","page":"A clustering task: the prediction of  plant species from floreal measures (the iris dataset)","title":"A clustering task: the prediction of  plant species from floreal measures (the iris dataset)","text":"","category":"section"},{"location":"tutorials/Clustering - Iris/betaml_tutorial_cluster_iris.html","page":"A clustering task: the prediction of  plant species from floreal measures (the iris dataset)","title":"A clustering task: the prediction of  plant species from floreal measures (the iris dataset)","text":"The task is to estimate the species of a plant given some floreal measurements. It use the classical \"Iris\" dataset. Note that in this example we are using clustering approaches, so we try to understand the \"structure\" of our data, without relying to actually knowing the true labels (\"classes\" or \"factors\"). However we have chosen a dataset for which the true labels are actually known, so to compare the accuracy of the algorithms we use, but these labels will not be used during the algorithms training.","category":"page"},{"location":"tutorials/Clustering - Iris/betaml_tutorial_cluster_iris.html","page":"A clustering task: the prediction of  plant species from floreal measures (the iris dataset)","title":"A clustering task: the prediction of  plant species from floreal measures (the iris dataset)","text":"Data origin:","category":"page"},{"location":"tutorials/Clustering - Iris/betaml_tutorial_cluster_iris.html","page":"A clustering task: the prediction of  plant species from floreal measures (the iris dataset)","title":"A clustering task: the prediction of  plant species from floreal measures (the iris dataset)","text":"dataset description: https://en.wikipedia.org/wiki/Irisflowerdata_set\ndata source we use here: https://github.com/JuliaStats/RDatasets.jl","category":"page"},{"location":"tutorials/Clustering - Iris/betaml_tutorial_cluster_iris.html","page":"A clustering task: the prediction of  plant species from floreal measures (the iris dataset)","title":"A clustering task: the prediction of  plant species from floreal measures (the iris dataset)","text":"!!! warning As the above example is automatically executed by GitHub on every code update, it uses parameters (epoch numbers, parameter space of hyperparameter validation, number of trees,...) that minimise the computation. In real case you will want to use better but more computationally intensive ones. For the same reason benchmarks codes are commented and the pre-run output reported rather than actually being executed.","category":"page"},{"location":"tutorials/Clustering - Iris/betaml_tutorial_cluster_iris.html#Library-and-data-loading","page":"A clustering task: the prediction of  plant species from floreal measures (the iris dataset)","title":"Library and data loading","text":"","category":"section"},{"location":"tutorials/Clustering - Iris/betaml_tutorial_cluster_iris.html","page":"A clustering task: the prediction of  plant species from floreal measures (the iris dataset)","title":"A clustering task: the prediction of  plant species from floreal measures (the iris dataset)","text":"We load the Beta Machine Learning Toolkit as well as some other packages that we use in this tutorial","category":"page"},{"location":"tutorials/Clustering - Iris/betaml_tutorial_cluster_iris.html","page":"A clustering task: the prediction of  plant species from floreal measures (the iris dataset)","title":"A clustering task: the prediction of  plant species from floreal measures (the iris dataset)","text":"using BetaML\nusing Random, Statistics, Logging, BenchmarkTools, RDatasets, Plots, DataFrames","category":"page"},{"location":"tutorials/Clustering - Iris/betaml_tutorial_cluster_iris.html","page":"A clustering task: the prediction of  plant species from floreal measures (the iris dataset)","title":"A clustering task: the prediction of  plant species from floreal measures (the iris dataset)","text":"We are also going to compare our results with two other leading packages in Julia for clustering analysis, Clustering.jl that provides (inter alia) kmeans and kmedoids algorithms and GaussianMixtures.jl that provides, as the name says, Gaussian Mixture Models. So we import them (we \"import\" them, rather than \"use\", not to bound their full names into namespace as some would collide with BetaML).","category":"page"},{"location":"tutorials/Clustering - Iris/betaml_tutorial_cluster_iris.html","page":"A clustering task: the prediction of  plant species from floreal measures (the iris dataset)","title":"A clustering task: the prediction of  plant species from floreal measures (the iris dataset)","text":"import Clustering, GaussianMixtures","category":"page"},{"location":"tutorials/Clustering - Iris/betaml_tutorial_cluster_iris.html","page":"A clustering task: the prediction of  plant species from floreal measures (the iris dataset)","title":"A clustering task: the prediction of  plant species from floreal measures (the iris dataset)","text":"We do a few tweeks for the Clustering and GaussianMixtures packages. Note that in BetaML we can also control both the random seed and the verbosity in the algorithm call, not only globally","category":"page"},{"location":"tutorials/Clustering - Iris/betaml_tutorial_cluster_iris.html","page":"A clustering task: the prediction of  plant species from floreal measures (the iris dataset)","title":"A clustering task: the prediction of  plant species from floreal measures (the iris dataset)","text":"Random.seed!(123)\n#logger  = Logging.SimpleLogger(stdout, Logging.Error); global_logger(logger); ## For suppressing GaussianMixtures output\nnothing #hide","category":"page"},{"location":"tutorials/Clustering - Iris/betaml_tutorial_cluster_iris.html","page":"A clustering task: the prediction of  plant species from floreal measures (the iris dataset)","title":"A clustering task: the prediction of  plant species from floreal measures (the iris dataset)","text":"Differently from the regression tutorial, we load the data here from [RDatasets](https://github.com/JuliaStats/RDatasets.jl](https://github.com/JuliaStats/RDatasets.jl), a package providing standard datasets.","category":"page"},{"location":"tutorials/Clustering - Iris/betaml_tutorial_cluster_iris.html","page":"A clustering task: the prediction of  plant species from floreal measures (the iris dataset)","title":"A clustering task: the prediction of  plant species from floreal measures (the iris dataset)","text":"iris = dataset(\"datasets\", \"iris\")\ndescribe(iris)","category":"page"},{"location":"tutorials/Clustering - Iris/betaml_tutorial_cluster_iris.html","page":"A clustering task: the prediction of  plant species from floreal measures (the iris dataset)","title":"A clustering task: the prediction of  plant species from floreal measures (the iris dataset)","text":"The iris dataset  provides floreal measures in columns 1 to 4 and the assigned species name in column 5. There are no missing values","category":"page"},{"location":"tutorials/Clustering - Iris/betaml_tutorial_cluster_iris.html#Data-preparation","page":"A clustering task: the prediction of  plant species from floreal measures (the iris dataset)","title":"Data preparation","text":"","category":"section"},{"location":"tutorials/Clustering - Iris/betaml_tutorial_cluster_iris.html","page":"A clustering task: the prediction of  plant species from floreal measures (the iris dataset)","title":"A clustering task: the prediction of  plant species from floreal measures (the iris dataset)","text":"The first step is to prepare the data for the analysis. We collect the first 4 columns as our feature x matrix and the last one as our y label vector. As we are using clustering algorithms, we are not actually using the labels to train the algorithms, we'll behave like we do not know them, we'll just let the algorithm \"learn\" fro mthe structure of the data itself. We'll however use it to judge the accuracy that they did reach.","category":"page"},{"location":"tutorials/Clustering - Iris/betaml_tutorial_cluster_iris.html","page":"A clustering task: the prediction of  plant species from floreal measures (the iris dataset)","title":"A clustering task: the prediction of  plant species from floreal measures (the iris dataset)","text":"x       = Matrix{Float64}(iris[:,1:4]);\nyLabels = unique(iris[:,5]);\nnothing #hide","category":"page"},{"location":"tutorials/Clustering - Iris/betaml_tutorial_cluster_iris.html","page":"A clustering task: the prediction of  plant species from floreal measures (the iris dataset)","title":"A clustering task: the prediction of  plant species from floreal measures (the iris dataset)","text":"As the labels are expressed as strings, the first thing we do is encode them as integers for our analysis using the function integerencoder.","category":"page"},{"location":"tutorials/Clustering - Iris/betaml_tutorial_cluster_iris.html","page":"A clustering task: the prediction of  plant species from floreal measures (the iris dataset)","title":"A clustering task: the prediction of  plant species from floreal measures (the iris dataset)","text":"y       = integerencoder(iris[:,5],factors=yLabels);\nnothing #hide","category":"page"},{"location":"tutorials/Clustering - Iris/betaml_tutorial_cluster_iris.html","page":"A clustering task: the prediction of  plant species from floreal measures (the iris dataset)","title":"A clustering task: the prediction of  plant species from floreal measures (the iris dataset)","text":"The dataset from RDatasets is ordered by species, so we need to shuffle it to avoid biases. Shuffling happens by default in cross_validation, but we are keeping here a copy of the shuffled version for later. Note that the version of shuffle that is included in BetaML accepts several n-dimensional arrays and shuffle them (by default on rows, by we can specify the dimension) keeping the association  between the various arrays in the shuffled output.","category":"page"},{"location":"tutorials/Clustering - Iris/betaml_tutorial_cluster_iris.html","page":"A clustering task: the prediction of  plant species from floreal measures (the iris dataset)","title":"A clustering task: the prediction of  plant species from floreal measures (the iris dataset)","text":"(xs,ys) = shuffle([x,y]);\nnothing #hide","category":"page"},{"location":"tutorials/Clustering - Iris/betaml_tutorial_cluster_iris.html#Main-analysis","page":"A clustering task: the prediction of  plant species from floreal measures (the iris dataset)","title":"Main analysis","text":"","category":"section"},{"location":"tutorials/Clustering - Iris/betaml_tutorial_cluster_iris.html","page":"A clustering task: the prediction of  plant species from floreal measures (the iris dataset)","title":"A clustering task: the prediction of  plant species from floreal measures (the iris dataset)","text":"We will try 3 BetaML models (kmeans, kmedoids and gmm) and we compare them with kmeans from Clusterings.jl and GMM from GaussianMixtures.jl Kmeans and kmedoids works by first initialising the centers of the k-clusters (the \"representative\" (step a ) . For kmeans they must be selected within one of the data, for kmeans they are the geometrical center) n a nutshell. Then ( b ) iterate for each point to assign the point to the cluster of the closest representative (according with a user defined distance metric, default to Euclidean), and ( c ) move each representative at the center of its newly acquired cluster (where \"center\" depends again from the metric). Steps ( b ) and ( c ) are reiterated until the algorithm converge, i.e. the tentative k representative points (and their relative clusters) don't move any more. The result (output of the algorithm) is that each point is assigned to one of the clusters (classes). The gmm algorithm is similar in that it employs an iterative approach (the ExpectationMinimisation algorithm, \"em\") but here we make the hipothesis that the data points are the observed outcomes of some _mixture probabilistic models where we have first a k-categorical variables whose outcomes are the (unobservble) parameters of a probabilistic distribution from which the data is finally drawn. Because the parameters of each of the k-possible distributions is unobservable this is also called a model with latent variables. Most gmm models use the Gaussain distribution as the family of the mixture components, so we can tought the gmm acronym to indicate Gaussian Mixture Model. In BetaML we do implemented only Gaussain components, but any distribution could be used by just subclassing AbstractMixture and implementing a couple of methids (you are invited to contribute or just ask for a distribution family you are interested), so I prefer to think \"gmm\" as an acronym for Generative Mixture Model. The algorithm try to find the mixture that maximises the likelihood that the data has been generated indeed from such mixture, where the \"E\" step refers to computing the probability that each point belongs to each of the k-composants (somehow similar to the step b in the kmeans/kmedoids algorithm), and the \"M\" step estimates, giving the association probabilities in step \"M\", the parameters of the mixture and of the individual components (similar to step c). The result here is that each point has a categorical distribution (PMF) representing the probabilities that it belongs to any of the k-components (our classes or clusters). This is interesting, as gmm can be used for many other things that clustering. It forms the backbone of the predictMissing function to impute missing values (on some or all dimensions) based to how close the record seems to its pears. For the same reasons, predictMissing can also be used to predict user's behaviours (or users' appreciation) according to the behaviour/ranking made by pears (\"collaborative filtering\"). While the result of gmm is a vector of PMFs (one for each record), error measures and reports with the true values (if known) can be directly applied, as in BetaML they internally call mode() to retrieve the class with the highest probability for each record.","category":"page"},{"location":"tutorials/Clustering - Iris/betaml_tutorial_cluster_iris.html","page":"A clustering task: the prediction of  plant species from floreal measures (the iris dataset)","title":"A clustering task: the prediction of  plant species from floreal measures (the iris dataset)","text":"As we are here, we also try different versions of the BetaML models, even if the default \"versions\" should be fine. For kmeans and kmedoids we will try different initialisation strategies (\"gird\", the default one, \"random\" and \"shuffle\"), while for the gmm model we'll choose different distributions of the Gaussain family (SphericalGaussian - where the variance is a scalar, DiagonalGaussian - with a vector variance, and FullGaussian, where the covariance is a matrix).","category":"page"},{"location":"tutorials/Clustering - Iris/betaml_tutorial_cluster_iris.html","page":"A clustering task: the prediction of  plant species from floreal measures (the iris dataset)","title":"A clustering task: the prediction of  plant species from floreal measures (the iris dataset)","text":"As the result would depend on stochasticity both in the data selected and in the random initialisation, we use a cross-validation approach to run our models several times (with different data) and then we average their results. Cross-Validation in BetaML is very flexible and it is done using the cross_validation function. cross_validation works by calling the function f, defined by the user, passing to it the tuple trainData, valData and rng and collecting the result of the function f. The specific method for which trainData, and valData are selected at each iteration depends on the specific sampler. We start by selectign a k-fold sampler that split our data in 5 different parts, it uses 4 for training and 1 part (not used here) for validation. We run the simulations twice and, to be sure to have replicable results, we fix the random seed (at the whole crossValidaiton level, not on each iteration).","category":"page"},{"location":"tutorials/Clustering - Iris/betaml_tutorial_cluster_iris.html","page":"A clustering task: the prediction of  plant species from floreal measures (the iris dataset)","title":"A clustering task: the prediction of  plant species from floreal measures (the iris dataset)","text":"sampler = KFold(nSplits=5,nRepeats=3,shuffle=true, rng=copy(FIXEDRNG))","category":"page"},{"location":"tutorials/Clustering - Iris/betaml_tutorial_cluster_iris.html","page":"A clustering task: the prediction of  plant species from floreal measures (the iris dataset)","title":"A clustering task: the prediction of  plant species from floreal measures (the iris dataset)","text":"We can now run the cross-validation with our models. Note that instead of defining the function f and then calling cross_validation[f(trainData,testData,rng),[x,y],...) we use the Julia do block syntax and we write directly the content of the f function in the do block. Also, by default crossvalidation already returns the mean and the standard deviation of the output of the user-provided f function (or the do block). However this requires that the f function return a single scalar. Here we are returning a vector of the accuracies of the different models (so we can run the cross-validation only once), and hence we indicate with returnStatistics=false to crossvalidation not to attempt to generate statistics but rather report the whole output. We'll compute the statistics ex-post.","category":"page"},{"location":"tutorials/Clustering - Iris/betaml_tutorial_cluster_iris.html","page":"A clustering task: the prediction of  plant species from floreal measures (the iris dataset)","title":"A clustering task: the prediction of  plant species from floreal measures (the iris dataset)","text":"Inside the do block we do 4 things:","category":"page"},{"location":"tutorials/Clustering - Iris/betaml_tutorial_cluster_iris.html","page":"A clustering task: the prediction of  plant species from floreal measures (the iris dataset)","title":"A clustering task: the prediction of  plant species from floreal measures (the iris dataset)","text":"we recover from trainData (a tuple, as we passed a tuple to cross_validation too) the xtrain features and ytrain labels;\nwe run the various clustering algorithms\nwe use the real labels to compute the model accuracy. Note that the clustering algorithm know nothing about the specific label name or even their order. This is why accuracy has the parameter ignorelabels to compute the accuracy oven any possible permutation of the classes found.\nwe return the various models' accuracies","category":"page"},{"location":"tutorials/Clustering - Iris/betaml_tutorial_cluster_iris.html","page":"A clustering task: the prediction of  plant species from floreal measures (the iris dataset)","title":"A clustering task: the prediction of  plant species from floreal measures (the iris dataset)","text":"cOut = cross_validation([x,y],sampler,returnStatistics=false) do trainData,testData,rng\n          # For unsupervised learning we use only the train data.\n          # Also, we use the associated labels only to measure the performances\n         (xtrain,ytrain)  = trainData;\n         # We run the clustering algorithm...\n         clusteringOut     = kmeans(xtrain,3,rng=rng) ## init is grid by default\n         # ... and we compute the accuracy using the real labels\n         kMeansAccuracy    = accuracy(clusteringOut[1],ytrain,ignorelabels=true)\n         clusteringOut     = kmeans(xtrain,3,rng=rng,initialisation_strategy=\"random\")\n         kMeansRAccuracy   = accuracy(clusteringOut[1],ytrain,ignorelabels=true)\n         clusteringOut     = kmeans(xtrain,3,rng=rng,initialisation_strategy=\"shuffle\")\n         kMeansSAccuracy   = accuracy(clusteringOut[1],ytrain,ignorelabels=true)\n         clusteringOut     = kmedoids(xtrain,3,rng=rng)   ## init is grid by default\n         kMedoidsAccuracy  = accuracy(clusteringOut[1],ytrain,ignorelabels=true)\n         clusteringOut     = kmedoids(xtrain,3,rng=rng,initialisation_strategy=\"random\")\n         kMedoidsRAccuracy = accuracy(clusteringOut[1],ytrain,ignorelabels=true)\n         clusteringOut     = kmedoids(xtrain,3,rng=rng,initialisation_strategy=\"shuffle\")\n         kMedoidsSAccuracy = accuracy(clusteringOut[1],ytrain,ignorelabels=true)\n         clusteringOut     = gmm(xtrain,3,mixtures=[SphericalGaussian() for i in 1:3], verbosity=NONE, rng=rng)\n         gmmSpherAccuracy  = accuracy(clusteringOut.pₙₖ,ytrain,ignorelabels=true, rng=rng)\n         clusteringOut     = gmm(xtrain,3,mixtures=[DiagonalGaussian() for i in 1:3], verbosity=NONE, rng=rng)\n         gmmDiagAccuracy   = accuracy(clusteringOut.pₙₖ,ytrain,ignorelabels=true, rng=rng)\n         clusteringOut     = gmm(xtrain,3,mixtures=[FullGaussian() for i in 1:3], verbosity=NONE, rng=rng)\n         gmmFullAccuracy   = accuracy(clusteringOut.pₙₖ,ytrain,ignorelabels=true, rng=rng)\n         # For comparision with Clustering.jl\n         clusteringOut     = Clustering.kmeans(xtrain', 3)\n         kMeans2Accuracy   = accuracy(clusteringOut.assignments,ytrain,ignorelabels=true)\n         # For comparision with GaussianMistures.jl - sometimes GaussianMistures.jl em! fails with a PosDefException\n         dGMM              = GaussianMixtures.GMM(3, xtrain; method=:kmeans, kind=:diag)\n         GaussianMixtures.em!(dGMM, xtrain)\n         gmmDiag2Accuracy  = accuracy(GaussianMixtures.gmmposterior(dGMM, xtrain)[1],ytrain,ignorelabels=true)\n         fGMM              = GaussianMixtures.GMM(3, xtrain; method=:kmeans, kind=:full)\n         GaussianMixtures.em!(fGMM, xtrain)\n         gmmFull2Accuracy  = accuracy(GaussianMixtures.gmmposterior(fGMM, xtrain)[1],ytrain,ignorelabels=true)\n         # Returning the accuracies\n         return kMeansAccuracy,kMeansRAccuracy,kMeansSAccuracy,kMedoidsAccuracy,kMedoidsRAccuracy,kMedoidsSAccuracy,gmmSpherAccuracy,gmmDiagAccuracy,gmmFullAccuracy,kMeans2Accuracy,gmmDiag2Accuracy,gmmFull2Accuracy\n end\n\n# We transform the output in matrix for easier analysis\naccuracies = fill(0.0,(length(cOut),length(cOut[1])))\n[accuracies[r,c] = cOut[r][c] for r in 1:length(cOut),c in 1:length(cOut[1])]\nμs = mean(accuracies,dims=1)\nσs = std(accuracies,dims=1)\n\n\nmodelLabels=[\"kMeansG\",\"kMeansR\",\"kMeansS\",\"kMedoidsG\",\"kMedoidsR\",\"kMedoidsS\",\"gmmSpher\",\"gmmDiag\",\"gmmFull\",\"kMeans (Clustering.jl)\",\"gmmDiag (GaussianMixtures.jl)\",\"gmmFull (GaussianMixtures.jl)\"]\nreport = DataFrame(mName = modelLabels, avgAccuracy = dropdims(round.(μs',digits=3),dims=2), stdAccuracy = dropdims(round.(σs',digits=3),dims=2))","category":"page"},{"location":"tutorials/Clustering - Iris/betaml_tutorial_cluster_iris.html#BetaML-model-accuracies","page":"A clustering task: the prediction of  plant species from floreal measures (the iris dataset)","title":"BetaML model accuracies","text":"","category":"section"},{"location":"tutorials/Clustering - Iris/betaml_tutorial_cluster_iris.html","page":"A clustering task: the prediction of  plant species from floreal measures (the iris dataset)","title":"A clustering task: the prediction of  plant species from floreal measures (the iris dataset)","text":"From the output We see that the gmm models perform for this dataset generally better than kmeans or kmedoids algorithms, also with very low variances. In detail, it is the (default) grid initialisation that leads to the better results for kmeans and kmedoids, while for the gmm models it is the FullGaussian to perform better.","category":"page"},{"location":"tutorials/Clustering - Iris/betaml_tutorial_cluster_iris.html#Comparisions-with-Clustering.jl-and-GaussianMixtures.jl","page":"A clustering task: the prediction of  plant species from floreal measures (the iris dataset)","title":"Comparisions with Clustering.jl and GaussianMixtures.jl","text":"","category":"section"},{"location":"tutorials/Clustering - Iris/betaml_tutorial_cluster_iris.html","page":"A clustering task: the prediction of  plant species from floreal measures (the iris dataset)","title":"A clustering task: the prediction of  plant species from floreal measures (the iris dataset)","text":"For this specific case, both Clustering.jl and GaussianMixtures.jl report substantially worst accuracies, and with very high variances. But we maintain the ranking that Full Gaussian gmm > Diagonal Gaussian > Kmeans accuracy. I suspect the reason that BetaML gmm works so weel is in relation to the usage of kmeans algorithm with itself the grid initialisation. The grid initialisation \"guarantee\" indeed that the initial means of the mixture components are well spread across the multidimensional space defined by the data, and it helps avoiding the EM algoritm to converge to a bad local optimus.","category":"page"},{"location":"tutorials/Clustering - Iris/betaml_tutorial_cluster_iris.html#Working-without-the-labels","page":"A clustering task: the prediction of  plant species from floreal measures (the iris dataset)","title":"Working without the labels","text":"","category":"section"},{"location":"tutorials/Clustering - Iris/betaml_tutorial_cluster_iris.html","page":"A clustering task: the prediction of  plant species from floreal measures (the iris dataset)","title":"A clustering task: the prediction of  plant species from floreal measures (the iris dataset)","text":"Up to now we used the real labels to compare the model accuracies. But in real clustering examples we don't have the true classes, or we wouln't need to do clustering in the first instance, so we don't know the number of classes to use. There are several methods to judge clusters algorithms goodness, perhaps the simplest one, at least for the expectation-maximisation algorithm employed in gmm to fit the data to the unknown mixture, is to use a information criteria that trade the goodness of the lickelyhood with the parameters used to do the fit. BetaML provide by default in the gmm clustering outputs both the Bayesian information criterion  (BIC) and the Akaike information criterion  (AIC), where for both a lower value is better.","category":"page"},{"location":"tutorials/Clustering - Iris/betaml_tutorial_cluster_iris.html","page":"A clustering task: the prediction of  plant species from floreal measures (the iris dataset)","title":"A clustering task: the prediction of  plant species from floreal measures (the iris dataset)","text":"We can then run the model with different number of classes and see which one leads to the lower BIC or AIC. We run hence cross_validation again with the FullGaussian gmm model Note that we use the BIC/AIC criteria here for establishing the \"best\" number of classes but we could have used it also to select the kind of Gaussain distribution to use. This is one example of hyper-parameter tuning that we developed more in detail (but without using cross-validation) in the regression tutorial.","category":"page"},{"location":"tutorials/Clustering - Iris/betaml_tutorial_cluster_iris.html","page":"A clustering task: the prediction of  plant species from floreal measures (the iris dataset)","title":"A clustering task: the prediction of  plant species from floreal measures (the iris dataset)","text":"Let's try up to 4 possible classes:","category":"page"},{"location":"tutorials/Clustering - Iris/betaml_tutorial_cluster_iris.html","page":"A clustering task: the prediction of  plant species from floreal measures (the iris dataset)","title":"A clustering task: the prediction of  plant species from floreal measures (the iris dataset)","text":"K = 4\nsampler = KFold(nSplits=5,nRepeats=2,shuffle=true, rng=copy(FIXEDRNG))\ncOut = cross_validation([x,y],sampler,returnStatistics=false) do trainData,testData,rng\n    (xtrain,ytrain)  = trainData;\n    clusteringOut  = [gmm(xtrain,k,mixtures=[FullGaussian() for i in 1:k], verbosity=NONE, rng=rng) for k in 1:K]\n    BICS           = [clusteringOut[i].BIC for i in 1:K]\n    AICS           = [clusteringOut[i].AIC for i in 1:K]\n    return (BICS,AICS)\nend\n\n# Transforming the output in matrices for easier analysis\nNit = length(cOut)\n\nBICS = fill(0.0,(Nit,K))\nAICS = fill(0.0,(Nit,K))\n[BICS[r,c] = cOut[r][1][c] for r in 1:Nit,c in 1:K]\n[AICS[r,c] = cOut[r][2][c] for r in 1:Nit,c in 1:K]\n\nμsBICS = mean(BICS,dims=1)","category":"page"},{"location":"tutorials/Clustering - Iris/betaml_tutorial_cluster_iris.html","page":"A clustering task: the prediction of  plant species from floreal measures (the iris dataset)","title":"A clustering task: the prediction of  plant species from floreal measures (the iris dataset)","text":"σsBICS = std(BICS,dims=1)","category":"page"},{"location":"tutorials/Clustering - Iris/betaml_tutorial_cluster_iris.html","page":"A clustering task: the prediction of  plant species from floreal measures (the iris dataset)","title":"A clustering task: the prediction of  plant species from floreal measures (the iris dataset)","text":"μsAICS = mean(AICS,dims=1)","category":"page"},{"location":"tutorials/Clustering - Iris/betaml_tutorial_cluster_iris.html","page":"A clustering task: the prediction of  plant species from floreal measures (the iris dataset)","title":"A clustering task: the prediction of  plant species from floreal measures (the iris dataset)","text":"σsAICS = std(AICS,dims=1)","category":"page"},{"location":"tutorials/Clustering - Iris/betaml_tutorial_cluster_iris.html","page":"A clustering task: the prediction of  plant species from floreal measures (the iris dataset)","title":"A clustering task: the prediction of  plant species from floreal measures (the iris dataset)","text":"plot(1:K,[μsBICS' μsAICS'], labels=[\"BIC\" \"AIC\"], title=\"Information criteria by number of classes\", xlabel=\"number of classes\", ylabel=\"lower is better\")","category":"page"},{"location":"tutorials/Clustering - Iris/betaml_tutorial_cluster_iris.html","page":"A clustering task: the prediction of  plant species from floreal measures (the iris dataset)","title":"A clustering task: the prediction of  plant species from floreal measures (the iris dataset)","text":"We see that following the \"lowest AIC\" rule we would indeed choose three classes, while following the \"best AIC\" criteria we would have choosen only two classes. This means that there is two classes that, concerning the floreal measures used in the database, are very similar, and opur models are unsure about them. Perhaps the biologists will end up one day with the conclusion that it is indeed only one specie :-).","category":"page"},{"location":"tutorials/Clustering - Iris/betaml_tutorial_cluster_iris.html","page":"A clustering task: the prediction of  plant species from floreal measures (the iris dataset)","title":"A clustering task: the prediction of  plant species from floreal measures (the iris dataset)","text":"We could study this issue more in detail by analysing the ConfMatrix, but the one used in BetaML does not account for the ignorelabels option (yet).","category":"page"},{"location":"tutorials/Clustering - Iris/betaml_tutorial_cluster_iris.html#Benchmarking-computational-efficiency","page":"A clustering task: the prediction of  plant species from floreal measures (the iris dataset)","title":"Benchmarking computational efficiency","text":"","category":"section"},{"location":"tutorials/Clustering - Iris/betaml_tutorial_cluster_iris.html","page":"A clustering task: the prediction of  plant species from floreal measures (the iris dataset)","title":"A clustering task: the prediction of  plant species from floreal measures (the iris dataset)","text":"We now benchmark the time and memory required by the various models by using the @btime macro of the BenchmarkTools package:","category":"page"},{"location":"tutorials/Clustering - Iris/betaml_tutorial_cluster_iris.html","page":"A clustering task: the prediction of  plant species from floreal measures (the iris dataset)","title":"A clustering task: the prediction of  plant species from floreal measures (the iris dataset)","text":"@btime kmeans($xs,3);\n# 261.540 μs (3777 allocations: 442.53 KiB)\n@btime kmedoids($xs,3);\n4.576 ms (97356 allocations: 10.42 MiB)\n@btime gmm($xs,3,mixtures=[SphericalGaussian() for i in 1:3], verbosity=NONE);\n# 5.498 ms (133365 allocations: 8.42 MiB)\n@btime gmm($xs,3,mixtures=[DiagonalGaussian() for i in 1:3], verbosity=NONE);\n# 18.901 ms (404333 allocations: 25.65 MiB)\n@btime gmm($xs,3,mixtures=[FullGaussian() for i in 1:3], verbosity=NONE);\n# 49.257 ms (351500 allocations: 61.95 MiB)\n@btime Clustering.kmeans($xs', 3);\n# 17.071 μs (23 allocations: 14.31 KiB)\n@btime begin dGMM = GaussianMixtures.GMM(3, $xs; method=:kmeans, kind=:diag); GaussianMixtures.em!(dGMM, $xs) end;\n# 530.528 μs (2088 allocations: 488.05 KiB)\n@btime begin fGMM = GaussianMixtures.GMM(3, $xs; method=:kmeans, kind=:full); GaussianMixtures.em!(fGMM, $xs) end;\n# 4.166 ms (58910 allocations: 3.59 MiB)","category":"page"},{"location":"tutorials/Clustering - Iris/betaml_tutorial_cluster_iris.html","page":"A clustering task: the prediction of  plant species from floreal measures (the iris dataset)","title":"A clustering task: the prediction of  plant species from floreal measures (the iris dataset)","text":"(note: the values reported here are of a local pc, not of the GitHub CI server, as sometimes - depending on data and random initialisation - GaussainMixtures.em!fails with aPosDefException`. This in turn would lead the whole documentation to fail to compile)","category":"page"},{"location":"tutorials/Clustering - Iris/betaml_tutorial_cluster_iris.html","page":"A clustering task: the prediction of  plant species from floreal measures (the iris dataset)","title":"A clustering task: the prediction of  plant species from floreal measures (the iris dataset)","text":"Like for supervised models, dedicated models are much better optimized than BetaML models, and are order of magnitude more efficient. However even the slowest BetaML clusering model (gmm using full gaussians) is realtively fast and can handle mid-size datasets (tens to hundreds of thousand records) without significant slow downs.","category":"page"},{"location":"tutorials/Clustering - Iris/betaml_tutorial_cluster_iris.html#Conclusions","page":"A clustering task: the prediction of  plant species from floreal measures (the iris dataset)","title":"Conclusions","text":"","category":"section"},{"location":"tutorials/Clustering - Iris/betaml_tutorial_cluster_iris.html","page":"A clustering task: the prediction of  plant species from floreal measures (the iris dataset)","title":"A clustering task: the prediction of  plant species from floreal measures (the iris dataset)","text":"We have shown in this tutorial how we can easily run clustering almgorithms in BetaML with just one line of code choosenModel(x,k), but also how can we use cross-validation in order to help the model or parameter selection, with or whithout knowing the real classes. We retrieve here what we observed with supervised models. Globally the accuracy of BetaML models are comparable to those of leading specialised packages (in this case they are even better), but there is a significant gap in computational efficiency that restricts the pratical usage of BetaML to mid-size datasets. However we trade this relative inefficiency with very flexible model definition and utility functions (for example the BetaML gmm works with missing data, allowing it to be used as the backbone of the predictMissing missing imputation function, or for collaborative reccomendation systems).","category":"page"},{"location":"tutorials/Clustering - Iris/betaml_tutorial_cluster_iris.html","page":"A clustering task: the prediction of  plant species from floreal measures (the iris dataset)","title":"A clustering task: the prediction of  plant species from floreal measures (the iris dataset)","text":"View this file on Github.","category":"page"},{"location":"tutorials/Clustering - Iris/betaml_tutorial_cluster_iris.html","page":"A clustering task: the prediction of  plant species from floreal measures (the iris dataset)","title":"A clustering task: the prediction of  plant species from floreal measures (the iris dataset)","text":"","category":"page"},{"location":"tutorials/Clustering - Iris/betaml_tutorial_cluster_iris.html","page":"A clustering task: the prediction of  plant species from floreal measures (the iris dataset)","title":"A clustering task: the prediction of  plant species from floreal measures (the iris dataset)","text":"This page was generated using Literate.jl.","category":"page"},{"location":"tutorials/Regression - bike sharing/betaml_tutorial_regression_sharingBikes.html","page":"A regression task: the prediction of  bike  sharing demand","title":"A regression task: the prediction of  bike  sharing demand","text":"EditURL = \"https://github.com/sylvaticus/BetaML.jl/blob/master/docs/src/tutorials/Regression - bike sharing/betaml_tutorial_regression_sharingBikes.jl\"","category":"page"},{"location":"tutorials/Regression - bike sharing/betaml_tutorial_regression_sharingBikes.html#regression_tutorial","page":"A regression task: the prediction of  bike  sharing demand","title":"A regression task: the prediction of  bike  sharing demand","text":"","category":"section"},{"location":"tutorials/Regression - bike sharing/betaml_tutorial_regression_sharingBikes.html","page":"A regression task: the prediction of  bike  sharing demand","title":"A regression task: the prediction of  bike  sharing demand","text":"The task is to estimate the influence of several variables (like the weather, the season, the day of the week..) on the demand of shared bicycles, so that the authority in charge of the service can organise the service in the best way.","category":"page"},{"location":"tutorials/Regression - bike sharing/betaml_tutorial_regression_sharingBikes.html","page":"A regression task: the prediction of  bike  sharing demand","title":"A regression task: the prediction of  bike  sharing demand","text":"Data origin:","category":"page"},{"location":"tutorials/Regression - bike sharing/betaml_tutorial_regression_sharingBikes.html","page":"A regression task: the prediction of  bike  sharing demand","title":"A regression task: the prediction of  bike  sharing demand","text":"original full dataset (by hour, not used here): https://archive.ics.uci.edu/ml/datasets/Bike+Sharing+Dataset\nsimplified dataset (by day, with some simple scaling): https://www.hds.utc.fr/~tdenoeux/dokuwiki/en/aec\ndescription: https://www.hds.utc.fr/~tdenoeux/dokuwiki/media/en/exam2019ace.pdf\ndata: https://www.hds.utc.fr/~tdenoeux/dokuwiki/media/en/bikesharing_day.csv.zip","category":"page"},{"location":"tutorials/Regression - bike sharing/betaml_tutorial_regression_sharingBikes.html","page":"A regression task: the prediction of  bike  sharing demand","title":"A regression task: the prediction of  bike  sharing demand","text":"Note that even if we are estimating a time serie, we are not using here a recurrent neural network as we assume the temporal dependence to be negligible (i.e. Y_t = f(X_t) alone).","category":"page"},{"location":"tutorials/Regression - bike sharing/betaml_tutorial_regression_sharingBikes.html","page":"A regression task: the prediction of  bike  sharing demand","title":"A regression task: the prediction of  bike  sharing demand","text":"!!! warning As the above example is automatically executed by GitHub on every code update, it uses parameters (epoch numbers, parameter space of hyperparameter validation, number of trees,...) that minimise the computation. In real case you will want to use better but more computationally intensive ones. For the same reason benchmarks codes are commented and the pre-run output reported rather than actually being executed.","category":"page"},{"location":"tutorials/Regression - bike sharing/betaml_tutorial_regression_sharingBikes.html#Library-and-data-loading","page":"A regression task: the prediction of  bike  sharing demand","title":"Library and data loading","text":"","category":"section"},{"location":"tutorials/Regression - bike sharing/betaml_tutorial_regression_sharingBikes.html","page":"A regression task: the prediction of  bike  sharing demand","title":"A regression task: the prediction of  bike  sharing demand","text":"We first load all the packages we are going to use","category":"page"},{"location":"tutorials/Regression - bike sharing/betaml_tutorial_regression_sharingBikes.html","page":"A regression task: the prediction of  bike  sharing demand","title":"A regression task: the prediction of  bike  sharing demand","text":"using  LinearAlgebra, Random, Statistics, DataFrames, CSV, Plots, Pipe, BenchmarkTools, BetaML\nimport Distributions: Uniform\nimport DecisionTree, Flux ## For comparisions","category":"page"},{"location":"tutorials/Regression - bike sharing/betaml_tutorial_regression_sharingBikes.html","page":"A regression task: the prediction of  bike  sharing demand","title":"A regression task: the prediction of  bike  sharing demand","text":"Here we load the data from a csv provided by the BataML package","category":"page"},{"location":"tutorials/Regression - bike sharing/betaml_tutorial_regression_sharingBikes.html","page":"A regression task: the prediction of  bike  sharing demand","title":"A regression task: the prediction of  bike  sharing demand","text":"baseDir = joinpath(dirname(pathof(BetaML)),\"..\",\"docs\",\"src\",\"tutorials\",\"Regression - bike sharing\")\ndata    = CSV.File(joinpath(baseDir,\"data\",\"bike_sharing_day.csv\"),delim=',') |> DataFrame\ndescribe(data)","category":"page"},{"location":"tutorials/Regression - bike sharing/betaml_tutorial_regression_sharingBikes.html","page":"A regression task: the prediction of  bike  sharing demand","title":"A regression task: the prediction of  bike  sharing demand","text":"The variable we want to learn to predict is cnt, the total demand of bikes for a given day. Even if it is indeed an integer, we treat it as a continuous variable, so each single prediction will be a scalar Y in mathbbR.","category":"page"},{"location":"tutorials/Regression - bike sharing/betaml_tutorial_regression_sharingBikes.html","page":"A regression task: the prediction of  bike  sharing demand","title":"A regression task: the prediction of  bike  sharing demand","text":"plot(data.cnt, title=\"Daily bike sharing rents (2Y)\", label=nothing)","category":"page"},{"location":"tutorials/Regression - bike sharing/betaml_tutorial_regression_sharingBikes.html#Decision-Trees","page":"A regression task: the prediction of  bike  sharing demand","title":"Decision Trees","text":"","category":"section"},{"location":"tutorials/Regression - bike sharing/betaml_tutorial_regression_sharingBikes.html","page":"A regression task: the prediction of  bike  sharing demand","title":"A regression task: the prediction of  bike  sharing demand","text":"We start our regression task with Decision Trees.","category":"page"},{"location":"tutorials/Regression - bike sharing/betaml_tutorial_regression_sharingBikes.html","page":"A regression task: the prediction of  bike  sharing demand","title":"A regression task: the prediction of  bike  sharing demand","text":"Decision trees training consist in choosing the set of questions (in a hierarcical way, so to form indeed a \"decision tree\") that \"best\" split the dataset given for training, in the sense that the split generate the sub-samples (always 2 subsamples in the BetaML implementation) that are, for the characteristic we want to predict, the most homogeneous possible. Decision trees are one of the few ML algorithms that has an intuitive interpretation and can be used for both regression or classification tasks.","category":"page"},{"location":"tutorials/Regression - bike sharing/betaml_tutorial_regression_sharingBikes.html#Data-preparation","page":"A regression task: the prediction of  bike  sharing demand","title":"Data preparation","text":"","category":"section"},{"location":"tutorials/Regression - bike sharing/betaml_tutorial_regression_sharingBikes.html","page":"A regression task: the prediction of  bike  sharing demand","title":"A regression task: the prediction of  bike  sharing demand","text":"The first step is to prepare the data for the analysis. This indeed depends already on the model we want to employ, as some models \"accept\" almost everything as input, no matter if the data is numerical or categorical, if it has missing values or not... while other models are instead much more exigents, and require more work to \"clean up\" our dataset.","category":"page"},{"location":"tutorials/Regression - bike sharing/betaml_tutorial_regression_sharingBikes.html","page":"A regression task: the prediction of  bike  sharing demand","title":"A regression task: the prediction of  bike  sharing demand","text":"Here we start using  Decision Tree and Random Forest models that definitly belong to the first group, so the only thing we have to do is to select the variables in input (the \"feature matrix\", that we will indicate with \"X\") and the variable representing our output (the information we want to learn to predict, we call it \"y\"):","category":"page"},{"location":"tutorials/Regression - bike sharing/betaml_tutorial_regression_sharingBikes.html","page":"A regression task: the prediction of  bike  sharing demand","title":"A regression task: the prediction of  bike  sharing demand","text":"x    = Matrix{Float64}(data[:,[:instant,:season,:yr,:mnth,:holiday,:weekday,:workingday,:weathersit,:temp,:atemp,:hum,:windspeed]])\ny    = data[:,16];\nnothing #hide","category":"page"},{"location":"tutorials/Regression - bike sharing/betaml_tutorial_regression_sharingBikes.html","page":"A regression task: the prediction of  bike  sharing demand","title":"A regression task: the prediction of  bike  sharing demand","text":"We can now split the dataset between the data we will use for training the algorithm (xtrain/ytrain), those for selecting the hyperparameters (xval/yval) and finally those for testing the quality of the algoritm with the optimal hyperparameters (xtest/ytest). We use the partition function specifying the share we want to use for these three different subsets, here 75%, 12.5% and 12.5 respectively. As our data represents indeed a time serie, we want our model to be able to predict future demand of bike sharing from past, observed rented bikes, so we do not shuffle the datasets as it would be the default.","category":"page"},{"location":"tutorials/Regression - bike sharing/betaml_tutorial_regression_sharingBikes.html","page":"A regression task: the prediction of  bike  sharing demand","title":"A regression task: the prediction of  bike  sharing demand","text":"((xtrain,xval,xtest),(ytrain,yval,ytest)) = partition([x,y],[0.75,0.125,1-0.75-0.125],shuffle=false)\n(ntrain, nval, ntest) = size.([ytrain,yval,ytest],1)","category":"page"},{"location":"tutorials/Regression - bike sharing/betaml_tutorial_regression_sharingBikes.html","page":"A regression task: the prediction of  bike  sharing demand","title":"A regression task: the prediction of  bike  sharing demand","text":"We can now \"tune\" our model so-called hyper-parameters, i.e. choose the best exogenous parameters of our algorithm, where \"best\" refers to some minimisation of a \"loss\" function between the true and the predicted values. We compute this loss function on a specific subset of data, that we call the \"validation\" subset (xval and yval).","category":"page"},{"location":"tutorials/Regression - bike sharing/betaml_tutorial_regression_sharingBikes.html","page":"A regression task: the prediction of  bike  sharing demand","title":"A regression task: the prediction of  bike  sharing demand","text":"BetaML doesn't have a dedicated function for hyper-parameters optimisation, but it is easy to write some custom julia code, at least for a simple grid-based \"search\". Indeed one of the main reasons that a dedicated function exists in other Machine Learning libraries is that loops in other languages are slow, but this is not a problem in julia, so we can retain the flexibility to write the kind of hyper-parameter tuning that best fits our needs.","category":"page"},{"location":"tutorials/Regression - bike sharing/betaml_tutorial_regression_sharingBikes.html","page":"A regression task: the prediction of  bike  sharing demand","title":"A regression task: the prediction of  bike  sharing demand","text":"Below is an example of a possible such function. Note there are more \"elegant\" ways to code it, but this one does the job. In particular, for simplicity, this hyper-paramerter tuning function just run multiple repetitions. In real world it is better to use cross-validation in the hyper-parameter tuning, expecially when the observations are small. The Clustering tutorial shows an example on how to use cross_validation.","category":"page"},{"location":"tutorials/Regression - bike sharing/betaml_tutorial_regression_sharingBikes.html","page":"A regression task: the prediction of  bike  sharing demand","title":"A regression task: the prediction of  bike  sharing demand","text":"We will see the various functions inside tuneHyperParameters() in a moment. For now let's going just to observe that tuneHyperParameters just loops over all the possible hyper-parameters and selects the ones where the error between xval and yval is minimised. For the meaning of the various hyper-parameter, consult the documentation of the buildTree and buildForest functions. The function uses multiple threads, so we calls generate_parallel_rngs() (in the BetaML.Utils submodule) to generate thread-safe random number generators and locks the comparision step.","category":"page"},{"location":"tutorials/Regression - bike sharing/betaml_tutorial_regression_sharingBikes.html","page":"A regression task: the prediction of  bike  sharing demand","title":"A regression task: the prediction of  bike  sharing demand","text":"function tuneHyperParameters(model,xtrain,ytrain,xval,yval;max_depthRange=15:15,max_featuresRange=size(xtrain,2):size(xtrain,2),n_treesRange=20:20,βRange=0:0,min_recordsRange=2:2,repetitions=5,rng=Random.GLOBAL_RNG)\n    # We start with an infinitely high error\n    bestRme         = +Inf\n    bestmax_depth    = 1\n    bestmax_features = 1\n    bestmin_records  = 2\n    bestn_trees      = 1\n    bestβ           = 0\n    compLock        = ReentrantLock()\n\n    # Generate one random number generator per thread\n    masterSeed = rand(rng,100:9999999999999) ## Some RNG have problems with very small seed. Also, the master seed has to be computed _before_ generate_parallel_rngs\n    rngs = generate_parallel_rngs(rng,Threads.nthreads())\n\n    # We loop over all possible hyperparameter combinations...\n    parLengths = (length(max_depthRange),length(max_featuresRange),length(min_recordsRange),length(n_treesRange),length(βRange))\n    Threads.@threads for ij in CartesianIndices(parLengths) ## This to avoid many nested for loops\n           (max_depth,max_features,min_records,n_trees,β)   = (max_depthRange[Tuple(ij)[1]], max_featuresRange[Tuple(ij)[2]], min_recordsRange[Tuple(ij)[3]], n_treesRange[Tuple(ij)[4]], βRange[Tuple(ij)[5]]) ## The specific hyperparameters of this nested loop\n           tsrng = rngs[Threads.threadid()] ## The random number generator is specific for each thread..\n           joinedIndx = LinearIndices(parLengths)[ij]\n           # And here we make the seeding depending on the id of the loop, not the thread: hence we get the same results indipendently of the number of threads\n           Random.seed!(tsrng,masterSeed+joinedIndx*10)\n           totAttemptError = 0.0\n           # We run several repetitions with the same hyperparameter combination to account for stochasticity...\n           for r in 1:repetitions\n              if model == \"DecisionTree\"\n                 # Here we train the Decition Tree model\n                 myTrainedModel = buildTree(xtrain,ytrain, max_depth=max_depth,max_features=max_features,min_records=min_records,rng=tsrng)\n              else\n                 # Here we train the Random Forest model\n                 myTrainedModel = buildForest(xtrain,ytrain,n_trees,max_depth=max_depth,max_features=max_features,min_records=min_records,β=β,rng=tsrng)\n              end\n              # Here we make prediciton with this trained model and we compute its error\n              ŷval   = predict(myTrainedModel, xval,rng=tsrng)\n              rmeVal = mean_relative_error(ŷval,yval,normrec=false)\n              totAttemptError += rmeVal\n           end\n           avgAttemptedDepthError = totAttemptError / repetitions\n           begin\n               lock(compLock) ## This step can't be run in parallel...\n               try\n                   # Select this specific combination of hyperparameters if the error is the lowest\n                   if avgAttemptedDepthError < bestRme\n                     bestRme         = avgAttemptedDepthError\n                     bestmax_depth    = max_depth\n                     bestmax_features = max_features\n                     bestn_trees      = n_trees\n                     bestβ           = β\n                     bestmin_records  = min_records\n                   end\n               finally\n                   unlock(compLock)\n               end\n           end\n    end\n    return (bestRme,bestmax_depth,bestmax_features,bestmin_records,bestn_trees,bestβ)\nend","category":"page"},{"location":"tutorials/Regression - bike sharing/betaml_tutorial_regression_sharingBikes.html","page":"A regression task: the prediction of  bike  sharing demand","title":"A regression task: the prediction of  bike  sharing demand","text":"We can now run the hyperparameter optimisation function with some \"reasonable\" ranges. To obtain replicable results we call tuneHyperParameters with rng=copy(FIXEDRNG), where FIXEDRNG is a fixed-seeded random number generator guaranteed to maintain the same stream of random numbers even between different julia versions. That's also what we use for our unit tests (see the Getting started for more details).","category":"page"},{"location":"tutorials/Regression - bike sharing/betaml_tutorial_regression_sharingBikes.html","page":"A regression task: the prediction of  bike  sharing demand","title":"A regression task: the prediction of  bike  sharing demand","text":"(bestRme,bestmax_depth,bestmax_features,bestmin_records) = tuneHyperParameters(\"DecisionTree\",xtrain,ytrain,xval,yval,\n           max_depthRange=4:5,max_featuresRange=11:12,min_recordsRange=5:5,repetitions=3,rng=copy(FIXEDRNG))","category":"page"},{"location":"tutorials/Regression - bike sharing/betaml_tutorial_regression_sharingBikes.html","page":"A regression task: the prediction of  bike  sharing demand","title":"A regression task: the prediction of  bike  sharing demand","text":"Now that we have found the \"optimal\" hyperparameters we can build (\"train\") our model using them:","category":"page"},{"location":"tutorials/Regression - bike sharing/betaml_tutorial_regression_sharingBikes.html","page":"A regression task: the prediction of  bike  sharing demand","title":"A regression task: the prediction of  bike  sharing demand","text":"myTree = buildTree(xtrain,ytrain, max_depth=bestmax_depth, max_features=bestmax_features,min_records=bestmin_records,rng=copy(FIXEDRNG));\nnothing #hide","category":"page"},{"location":"tutorials/Regression - bike sharing/betaml_tutorial_regression_sharingBikes.html","page":"A regression task: the prediction of  bike  sharing demand","title":"A regression task: the prediction of  bike  sharing demand","text":"Let's benchmark the time and memory usage of the training step of a decision tree:","category":"page"},{"location":"tutorials/Regression - bike sharing/betaml_tutorial_regression_sharingBikes.html","page":"A regression task: the prediction of  bike  sharing demand","title":"A regression task: the prediction of  bike  sharing demand","text":"@btime  buildTree(xtrain,ytrain, max_depth=bestmax_depth, max_features=bestmax_features,min_records=bestmin_records,rng=copy(FIXEDRNG));\n26.538 ms (55753 allocations: 58.57 MiB)","category":"page"},{"location":"tutorials/Regression - bike sharing/betaml_tutorial_regression_sharingBikes.html","page":"A regression task: the prediction of  bike  sharing demand","title":"A regression task: the prediction of  bike  sharing demand","text":"Individual decision trees are blazing fast, among the fastest algorithms we could use.","category":"page"},{"location":"tutorials/Regression - bike sharing/betaml_tutorial_regression_sharingBikes.html","page":"A regression task: the prediction of  bike  sharing demand","title":"A regression task: the prediction of  bike  sharing demand","text":"The above buildTree function produces a DecisionTree object that can be used to make predictions given some new features, i.e. given some X matrix of (number of observations x dimensions), predict the corresponding Y vector of scalers in R.","category":"page"},{"location":"tutorials/Regression - bike sharing/betaml_tutorial_regression_sharingBikes.html","page":"A regression task: the prediction of  bike  sharing demand","title":"A regression task: the prediction of  bike  sharing demand","text":"(ŷtrain,ŷval,ŷtest) = predict.([myTree], [xtrain,xval,xtest])","category":"page"},{"location":"tutorials/Regression - bike sharing/betaml_tutorial_regression_sharingBikes.html","page":"A regression task: the prediction of  bike  sharing demand","title":"A regression task: the prediction of  bike  sharing demand","text":"Note that the above code uses the \"dot syntax\" to \"broadcast\" predict() over an array of label matrices. It is exactly equivalent to:","category":"page"},{"location":"tutorials/Regression - bike sharing/betaml_tutorial_regression_sharingBikes.html","page":"A regression task: the prediction of  bike  sharing demand","title":"A regression task: the prediction of  bike  sharing demand","text":"ŷtrain = predict(myTree, xtrain);\nŷval   = predict(myTree, xval);\nŷtest  = predict(myTree, xtest);\nnothing #hide","category":"page"},{"location":"tutorials/Regression - bike sharing/betaml_tutorial_regression_sharingBikes.html","page":"A regression task: the prediction of  bike  sharing demand","title":"A regression task: the prediction of  bike  sharing demand","text":"We now compute the relative mean error for the training, the validation and the test set. The mean_relative_error is a very flexible error function. Without additional parameter, it computes, as the name says, the mean relative error, also known as the \"mean absolute percentage error\" (MAPE) between an estimated and a true vector. However it can also compute the relative mean error (as we do here), or use a p-norm higher than 1. The mean relative error enfatises the relativeness of the error, i.e. all observations and dimensions weigth the same, wether large or small. Conversly, in the relative mean error the same relative error on larger observations (or dimensions) weights more. In this exercise we use the later, as our data has clearly some outlier days with very small rents, and we care more of avoiding our customers finding empty bike racks than having unrented bikes on the rack. Targeting a low mean average error would push all our predicitons down to try accomodate the low-level predicitons (to avoid a large relative error), and that's not what we want.","category":"page"},{"location":"tutorials/Regression - bike sharing/betaml_tutorial_regression_sharingBikes.html","page":"A regression task: the prediction of  bike  sharing demand","title":"A regression task: the prediction of  bike  sharing demand","text":"For example let's consider the following example:","category":"page"},{"location":"tutorials/Regression - bike sharing/betaml_tutorial_regression_sharingBikes.html","page":"A regression task: the prediction of  bike  sharing demand","title":"A regression task: the prediction of  bike  sharing demand","text":"y     = [30,28,27,3,32,38];\nŷpref = [32,30,28,10,31,40];\nŷbad  = [29,25,24,5,28,35];\nnothing #hide","category":"page"},{"location":"tutorials/Regression - bike sharing/betaml_tutorial_regression_sharingBikes.html","page":"A regression task: the prediction of  bike  sharing demand","title":"A regression task: the prediction of  bike  sharing demand","text":"Here ŷpref is an ipotetical output of a model that minimises the relative mean error, while ŷbad minimises the mean realative error.","category":"page"},{"location":"tutorials/Regression - bike sharing/betaml_tutorial_regression_sharingBikes.html","page":"A regression task: the prediction of  bike  sharing demand","title":"A regression task: the prediction of  bike  sharing demand","text":"mean_relative_error.([ŷbad, ŷpref],[y,y],normrec=true) ## Mean relative error","category":"page"},{"location":"tutorials/Regression - bike sharing/betaml_tutorial_regression_sharingBikes.html","page":"A regression task: the prediction of  bike  sharing demand","title":"A regression task: the prediction of  bike  sharing demand","text":"mean_relative_error.([ŷbad, ŷpref],[y,y],normrec=false) ## Relative mean error","category":"page"},{"location":"tutorials/Regression - bike sharing/betaml_tutorial_regression_sharingBikes.html","page":"A regression task: the prediction of  bike  sharing demand","title":"A regression task: the prediction of  bike  sharing demand","text":"plot([y ŷbad ŷpref], colour=[:black :red :green], label=[\"obs\" \"bad est\" \"good est\"])","category":"page"},{"location":"tutorials/Regression - bike sharing/betaml_tutorial_regression_sharingBikes.html","page":"A regression task: the prediction of  bike  sharing demand","title":"A regression task: the prediction of  bike  sharing demand","text":"We can then compute the relative mean error for the decision tree","category":"page"},{"location":"tutorials/Regression - bike sharing/betaml_tutorial_regression_sharingBikes.html","page":"A regression task: the prediction of  bike  sharing demand","title":"A regression task: the prediction of  bike  sharing demand","text":"(rmeTrain, rmeVal, rmeTest) = mean_relative_error.([ŷtrain,ŷval,ŷtest],[ytrain,yval,ytest],normrec=false)","category":"page"},{"location":"tutorials/Regression - bike sharing/betaml_tutorial_regression_sharingBikes.html","page":"A regression task: the prediction of  bike  sharing demand","title":"A regression task: the prediction of  bike  sharing demand","text":"We can plot the true labels vs the estimated one for the three subsets...","category":"page"},{"location":"tutorials/Regression - bike sharing/betaml_tutorial_regression_sharingBikes.html","page":"A regression task: the prediction of  bike  sharing demand","title":"A regression task: the prediction of  bike  sharing demand","text":"scatter(ytrain,ŷtrain,xlabel=\"daily rides\",ylabel=\"est. daily rides\",label=nothing,title=\"Est vs. obs in training period (DT)\")","category":"page"},{"location":"tutorials/Regression - bike sharing/betaml_tutorial_regression_sharingBikes.html","page":"A regression task: the prediction of  bike  sharing demand","title":"A regression task: the prediction of  bike  sharing demand","text":"scatter(yval,ŷval,xlabel=\"daily rides\",ylabel=\"est. daily rides\",label=nothing,title=\"Est vs. obs in validation period (DT)\")","category":"page"},{"location":"tutorials/Regression - bike sharing/betaml_tutorial_regression_sharingBikes.html","page":"A regression task: the prediction of  bike  sharing demand","title":"A regression task: the prediction of  bike  sharing demand","text":"scatter(ytest,ŷtest,xlabel=\"daily rides\",ylabel=\"est. daily rides\",label=nothing,title=\"Est vs. obs in testing period (DT)\")","category":"page"},{"location":"tutorials/Regression - bike sharing/betaml_tutorial_regression_sharingBikes.html","page":"A regression task: the prediction of  bike  sharing demand","title":"A regression task: the prediction of  bike  sharing demand","text":"Or we can visualise the true vs estimated bike shared on a temporal base. First on the full period (2 years) ...","category":"page"},{"location":"tutorials/Regression - bike sharing/betaml_tutorial_regression_sharingBikes.html","page":"A regression task: the prediction of  bike  sharing demand","title":"A regression task: the prediction of  bike  sharing demand","text":"ŷtrainfull = vcat(ŷtrain,fill(missing,nval+ntest))\nŷvalfull   = vcat(fill(missing,ntrain), ŷval, fill(missing,ntest))\nŷtestfull  = vcat(fill(missing,ntrain+nval), ŷtest)\nplot(data[:,:dteday],[data[:,:cnt] ŷtrainfull ŷvalfull ŷtestfull], label=[\"obs\" \"train\" \"val\" \"test\"], legend=:topleft, ylabel=\"daily rides\", title=\"Daily bike sharing demand observed/estimated across the\\n whole 2-years period (DT)\")","category":"page"},{"location":"tutorials/Regression - bike sharing/betaml_tutorial_regression_sharingBikes.html","page":"A regression task: the prediction of  bike  sharing demand","title":"A regression task: the prediction of  bike  sharing demand","text":"..and then focusing on the testing period","category":"page"},{"location":"tutorials/Regression - bike sharing/betaml_tutorial_regression_sharingBikes.html","page":"A regression task: the prediction of  bike  sharing demand","title":"A regression task: the prediction of  bike  sharing demand","text":"stc = 620\nendc = size(x,1)\nplot(data[stc:endc,:dteday],[data[stc:endc,:cnt] ŷvalfull[stc:endc] ŷtestfull[stc:endc]], label=[\"obs\" \"val\" \"test\"], legend=:bottomleft, ylabel=\"Daily rides\", title=\"Focus on the testing period (DT)\")","category":"page"},{"location":"tutorials/Regression - bike sharing/betaml_tutorial_regression_sharingBikes.html","page":"A regression task: the prediction of  bike  sharing demand","title":"A regression task: the prediction of  bike  sharing demand","text":"The predictions aren't so bad in this case, however decision trees are highly instable, and the output could have depended just from the specific initial random seed.","category":"page"},{"location":"tutorials/Regression - bike sharing/betaml_tutorial_regression_sharingBikes.html#Random-Forests","page":"A regression task: the prediction of  bike  sharing demand","title":"Random Forests","text":"","category":"section"},{"location":"tutorials/Regression - bike sharing/betaml_tutorial_regression_sharingBikes.html","page":"A regression task: the prediction of  bike  sharing demand","title":"A regression task: the prediction of  bike  sharing demand","text":"Rather than trying to solve this problem using a single Decision Tree model, let's not try to use a Random Forest model. Random forests average the results of many different decision trees and provide a more \"stable\" result. Being made of many decision trees, random forests are hovever more computationally expensive to train, but luckily they tend to self-tune (or self-regularise). In particular the parameters max_depth and max_features shouldn't need tuning.","category":"page"},{"location":"tutorials/Regression - bike sharing/betaml_tutorial_regression_sharingBikes.html","page":"A regression task: the prediction of  bike  sharing demand","title":"A regression task: the prediction of  bike  sharing demand","text":"We still tune however the model for other parameters, and in particular the β parameter, a prerogative of BetaML Random Forests that allows to assign more weigth to the best performing trees in the forest. It may be particularly important if there are many outliers in the data we don't want to \"learn\" from.","category":"page"},{"location":"tutorials/Regression - bike sharing/betaml_tutorial_regression_sharingBikes.html","page":"A regression task: the prediction of  bike  sharing demand","title":"A regression task: the prediction of  bike  sharing demand","text":"min_recordsRange=[5]; n_treesRange=[60]; βRange=100:100:300\n(bestRme,bestmax_depth,bestmax_features,bestmin_records,bestn_trees,bestβ) = tuneHyperParameters(\"RandomForest\",xtrain,ytrain,xval,yval,\n        max_depthRange=size(xtrain,1):size(xtrain,1),max_featuresRange=Int(round(sqrt(size(xtrain,2)))):Int(round(sqrt(size(xtrain,2)))),\n        min_recordsRange=min_recordsRange,n_treesRange=n_treesRange,βRange=βRange,repetitions=5,rng=copy(FIXEDRNG))","category":"page"},{"location":"tutorials/Regression - bike sharing/betaml_tutorial_regression_sharingBikes.html","page":"A regression task: the prediction of  bike  sharing demand","title":"A regression task: the prediction of  bike  sharing demand","text":"As for decision trees, once the hyper-parameters of the model are tuned we wan train again the model using the optimal parameters.","category":"page"},{"location":"tutorials/Regression - bike sharing/betaml_tutorial_regression_sharingBikes.html","page":"A regression task: the prediction of  bike  sharing demand","title":"A regression task: the prediction of  bike  sharing demand","text":"myForest = buildForest(xtrain,ytrain, bestn_trees, max_depth=bestmax_depth,max_features=bestmax_features,min_records=bestmin_records,β=bestβ,oob=true,rng=copy(FIXEDRNG));\nnothing #hide","category":"page"},{"location":"tutorials/Regression - bike sharing/betaml_tutorial_regression_sharingBikes.html","page":"A regression task: the prediction of  bike  sharing demand","title":"A regression task: the prediction of  bike  sharing demand","text":"Let's now benchmark the training of the BetaML Random Forest model @btime buildForest(xtrain,ytrain, bestntrees, maxdepth=bestmaxdepth,maxfeatures=bestmaxfeatures,minrecords=bestmin_records,β=bestβ,oob=true,rng=copy(FIXEDRNG)); 863.842 ms (2451894 allocations: 971.33 MiB) Random forests are evidently slower than individual decision trees but are still relativly fast. We should also consider that they are by default efficiently parallelised, so their speed increases with the number of available cores (in building this documentation page, GitHub CI servers allow for a single core, so all the bechmark you see in this tutorial are run with a single core available).","category":"page"},{"location":"tutorials/Regression - bike sharing/betaml_tutorial_regression_sharingBikes.html","page":"A regression task: the prediction of  bike  sharing demand","title":"A regression task: the prediction of  bike  sharing demand","text":"Random forests support the so-called \"out-of-bag\" error, an estimation of the error that we would have when the model is applied on a testing sample. However in this case the oob reported is much smaller than the testing error we will actually find. This is due to the fact that the division between training/validation and testing in this exercise is not random, but has a temporal basis. It seems that in this example the data in validation/testing follows a different pattern/variance than those in training (in probabilistic terms, the daily observations are not i.i.d.).","category":"page"},{"location":"tutorials/Regression - bike sharing/betaml_tutorial_regression_sharingBikes.html","page":"A regression task: the prediction of  bike  sharing demand","title":"A regression task: the prediction of  bike  sharing demand","text":"oobError, trueTestMeanRelativeError  = myForest.oobError,mean_relative_error(ŷtest,ytest,normrec=true)","category":"page"},{"location":"tutorials/Regression - bike sharing/betaml_tutorial_regression_sharingBikes.html","page":"A regression task: the prediction of  bike  sharing demand","title":"A regression task: the prediction of  bike  sharing demand","text":"(ŷtrain,ŷval,ŷtest)         = predict.([myForest], [xtrain,xval,xtest])\n(rmeTrain, rmeVal, rmeTest) = mean_relative_error.([ŷtrain,ŷval,ŷtest],[ytrain,yval,ytest],normrec=false)","category":"page"},{"location":"tutorials/Regression - bike sharing/betaml_tutorial_regression_sharingBikes.html","page":"A regression task: the prediction of  bike  sharing demand","title":"A regression task: the prediction of  bike  sharing demand","text":"In this case we found an error very similar to the one employing a single decision tree. Let's print the observed data vs the estimated one using the random forest and then along the temporal axis:","category":"page"},{"location":"tutorials/Regression - bike sharing/betaml_tutorial_regression_sharingBikes.html","page":"A regression task: the prediction of  bike  sharing demand","title":"A regression task: the prediction of  bike  sharing demand","text":"scatter(ytrain,ŷtrain,xlabel=\"daily rides\",ylabel=\"est. daily rides\",label=nothing,title=\"Est vs. obs in training period (RF)\")","category":"page"},{"location":"tutorials/Regression - bike sharing/betaml_tutorial_regression_sharingBikes.html","page":"A regression task: the prediction of  bike  sharing demand","title":"A regression task: the prediction of  bike  sharing demand","text":"scatter(yval,ŷval,xlabel=\"daily rides\",ylabel=\"est. daily rides\",label=nothing,title=\"Est vs. obs in validation period (RF)\")","category":"page"},{"location":"tutorials/Regression - bike sharing/betaml_tutorial_regression_sharingBikes.html","page":"A regression task: the prediction of  bike  sharing demand","title":"A regression task: the prediction of  bike  sharing demand","text":"scatter(ytest,ŷtest,xlabel=\"daily rides\",ylabel=\"est. daily rides\",label=nothing,title=\"Est vs. obs in testing period (RF)\")","category":"page"},{"location":"tutorials/Regression - bike sharing/betaml_tutorial_regression_sharingBikes.html","page":"A regression task: the prediction of  bike  sharing demand","title":"A regression task: the prediction of  bike  sharing demand","text":"Full period plot (2 years):","category":"page"},{"location":"tutorials/Regression - bike sharing/betaml_tutorial_regression_sharingBikes.html","page":"A regression task: the prediction of  bike  sharing demand","title":"A regression task: the prediction of  bike  sharing demand","text":"ŷtrainfull = vcat(ŷtrain,fill(missing,nval+ntest))\nŷvalfull   = vcat(fill(missing,ntrain), ŷval, fill(missing,ntest))\nŷtestfull  = vcat(fill(missing,ntrain+nval), ŷtest)\nplot(data[:,:dteday],[data[:,:cnt] ŷtrainfull ŷvalfull ŷtestfull], label=[\"obs\" \"train\" \"val\" \"test\"], legend=:topleft, ylabel=\"daily rides\", title=\"Daily bike sharing demand observed/estimated across the\\n whole 2-years period (RF)\")","category":"page"},{"location":"tutorials/Regression - bike sharing/betaml_tutorial_regression_sharingBikes.html","page":"A regression task: the prediction of  bike  sharing demand","title":"A regression task: the prediction of  bike  sharing demand","text":"Focus on the testing period:","category":"page"},{"location":"tutorials/Regression - bike sharing/betaml_tutorial_regression_sharingBikes.html","page":"A regression task: the prediction of  bike  sharing demand","title":"A regression task: the prediction of  bike  sharing demand","text":"stc = 620\nendc = size(x,1)\nplot(data[stc:endc,:dteday],[data[stc:endc,:cnt] ŷvalfull[stc:endc] ŷtestfull[stc:endc]], label=[\"obs\" \"val\" \"test\"], legend=:bottomleft, ylabel=\"Daily rides\", title=\"Focus on the testing period (RF)\")","category":"page"},{"location":"tutorials/Regression - bike sharing/betaml_tutorial_regression_sharingBikes.html#Comparison-with-DecisionTree.jl-random-forest","page":"A regression task: the prediction of  bike  sharing demand","title":"Comparison with DecisionTree.jl random forest","text":"","category":"section"},{"location":"tutorials/Regression - bike sharing/betaml_tutorial_regression_sharingBikes.html","page":"A regression task: the prediction of  bike  sharing demand","title":"A regression task: the prediction of  bike  sharing demand","text":"We now compare our results with those obtained employing the same model in the DecisionTree package, using the default suggested hyperparameters:","category":"page"},{"location":"tutorials/Regression - bike sharing/betaml_tutorial_regression_sharingBikes.html","page":"A regression task: the prediction of  bike  sharing demand","title":"A regression task: the prediction of  bike  sharing demand","text":"Hyperparameters of the DecisionTree.jl random forest model","category":"page"},{"location":"tutorials/Regression - bike sharing/betaml_tutorial_regression_sharingBikes.html","page":"A regression task: the prediction of  bike  sharing demand","title":"A regression task: the prediction of  bike  sharing demand","text":"n_subfeatures=-1; n_trees=bestn_trees; partial_sampling=1; max_depth=26\nmin_samples_leaf=bestmin_records; min_samples_split=bestmin_records; min_purity_increase=0.0; seed=3","category":"page"},{"location":"tutorials/Regression - bike sharing/betaml_tutorial_regression_sharingBikes.html","page":"A regression task: the prediction of  bike  sharing demand","title":"A regression task: the prediction of  bike  sharing demand","text":"We train the model..","category":"page"},{"location":"tutorials/Regression - bike sharing/betaml_tutorial_regression_sharingBikes.html","page":"A regression task: the prediction of  bike  sharing demand","title":"A regression task: the prediction of  bike  sharing demand","text":"model = DecisionTree.build_forest(ytrain, convert(Matrix,xtrain),\n                     n_subfeatures,\n                     n_trees,\n                     partial_sampling,\n                     max_depth,\n                     min_samples_leaf,\n                     min_samples_split,\n                     min_purity_increase;\n                     rng = seed)","category":"page"},{"location":"tutorials/Regression - bike sharing/betaml_tutorial_regression_sharingBikes.html","page":"A regression task: the prediction of  bike  sharing demand","title":"A regression task: the prediction of  bike  sharing demand","text":"And we generate predictions and measure their error","category":"page"},{"location":"tutorials/Regression - bike sharing/betaml_tutorial_regression_sharingBikes.html","page":"A regression task: the prediction of  bike  sharing demand","title":"A regression task: the prediction of  bike  sharing demand","text":"(ŷtrain,ŷval,ŷtest) = DecisionTree.apply_forest.([model],[xtrain,xval,xtest]);\nnothing #hide","category":"page"},{"location":"tutorials/Regression - bike sharing/betaml_tutorial_regression_sharingBikes.html","page":"A regression task: the prediction of  bike  sharing demand","title":"A regression task: the prediction of  bike  sharing demand","text":"Let's benchmark the DecisionTrees.jl Random Forest training","category":"page"},{"location":"tutorials/Regression - bike sharing/betaml_tutorial_regression_sharingBikes.html","page":"A regression task: the prediction of  bike  sharing demand","title":"A regression task: the prediction of  bike  sharing demand","text":"@btime  DecisionTree.build_forest(ytrain, convert(Matrix,xtrain),\n                     n_subfeatures,\n                     n_trees,\n                     partial_sampling,\n                     max_depth,\n                     min_samples_leaf,\n                     min_samples_split,\n                     min_purity_increase;\n                     rng = seed);\n144.026 ms (41085 allocations: 11.01 MiB)","category":"page"},{"location":"tutorials/Regression - bike sharing/betaml_tutorial_regression_sharingBikes.html","page":"A regression task: the prediction of  bike  sharing demand","title":"A regression task: the prediction of  bike  sharing demand","text":"DecisionTrees.jl makes a good job in optimising the Random Forest algorithm, as it is over 3 times faster that BetaML.","category":"page"},{"location":"tutorials/Regression - bike sharing/betaml_tutorial_regression_sharingBikes.html","page":"A regression task: the prediction of  bike  sharing demand","title":"A regression task: the prediction of  bike  sharing demand","text":"(rmeTrain, rmeVal, rmeTest) = mean_relative_error.([ŷtrain,ŷval,ŷtest],[ytrain,yval,ytest],normrec=false)","category":"page"},{"location":"tutorials/Regression - bike sharing/betaml_tutorial_regression_sharingBikes.html","page":"A regression task: the prediction of  bike  sharing demand","title":"A regression task: the prediction of  bike  sharing demand","text":"However the error on the test set remains relativly high. The very low error level on the training set is a sign that it overspecialised on the training set, and we should have better ran a dedicated hyper-parameter tuning function for the model.","category":"page"},{"location":"tutorials/Regression - bike sharing/betaml_tutorial_regression_sharingBikes.html","page":"A regression task: the prediction of  bike  sharing demand","title":"A regression task: the prediction of  bike  sharing demand","text":"Finally we plot the DecisionTree.jl predictions alongside the observed value:","category":"page"},{"location":"tutorials/Regression - bike sharing/betaml_tutorial_regression_sharingBikes.html","page":"A regression task: the prediction of  bike  sharing demand","title":"A regression task: the prediction of  bike  sharing demand","text":"ŷtrainfull = vcat(ŷtrain,fill(missing,nval+ntest))\nŷvalfull   = vcat(fill(missing,ntrain), ŷval, fill(missing,ntest))\nŷtestfull  = vcat(fill(missing,ntrain+nval), ŷtest)\nplot(data[:,:dteday],[data[:,:cnt] ŷtrainfull ŷvalfull ŷtestfull], label=[\"obs\" \"train\" \"val\" \"test\"], legend=:topleft, ylabel=\"daily rides\", title=\"Daily bike sharing demand observed/estimated across the\\n whole 2-years period (DT.jl RF)\")","category":"page"},{"location":"tutorials/Regression - bike sharing/betaml_tutorial_regression_sharingBikes.html","page":"A regression task: the prediction of  bike  sharing demand","title":"A regression task: the prediction of  bike  sharing demand","text":"Again, focusing on the testing data:","category":"page"},{"location":"tutorials/Regression - bike sharing/betaml_tutorial_regression_sharingBikes.html","page":"A regression task: the prediction of  bike  sharing demand","title":"A regression task: the prediction of  bike  sharing demand","text":"stc  = 620\nendc = size(x,1)\nplot(data[stc:endc,:dteday],[data[stc:endc,:cnt] ŷvalfull[stc:endc] ŷtestfull[stc:endc]], label=[\"obs\" \"val\" \"test\"], legend=:bottomleft, ylabel=\"Daily rides\", title=\"Focus on the testing period (DT.jl RF)\")","category":"page"},{"location":"tutorials/Regression - bike sharing/betaml_tutorial_regression_sharingBikes.html#Conclusions-of-Decision-Trees-/-Random-Forests-methods","page":"A regression task: the prediction of  bike  sharing demand","title":"Conclusions of Decision Trees / Random Forests methods","text":"","category":"section"},{"location":"tutorials/Regression - bike sharing/betaml_tutorial_regression_sharingBikes.html","page":"A regression task: the prediction of  bike  sharing demand","title":"A regression task: the prediction of  bike  sharing demand","text":"The error obtained employing DecisionTree.jl is significantly larger than those obtained using a BetaML random forest model, altought to be fair with DecisionTrees.jl we didn't tuned its hyper-parameters. Also, the DecisionTree.jl random forest model is much faster. This is partially due by the fact that, internally, DecisionTree.jl models optimise the algorithm by sorting the observations. BetaML trees/forests don't employ this optimisation and hence they can work with true categorical data for which ordering is not defined. An other explanation of this difference in speed is that BetaML Random Forest models accept missing values within the feature matrix. To sum up, BetaML random forests are ideal algorithms when we want to obtain good predictions in the most simpler way, even without tuning the hyper-parameters, and without spending time in cleaning (\"munging\") the feature matrix, as they accept almost \"any kind\" of data as it is.","category":"page"},{"location":"tutorials/Regression - bike sharing/betaml_tutorial_regression_sharingBikes.html#Neural-Networks","page":"A regression task: the prediction of  bike  sharing demand","title":"Neural Networks","text":"","category":"section"},{"location":"tutorials/Regression - bike sharing/betaml_tutorial_regression_sharingBikes.html","page":"A regression task: the prediction of  bike  sharing demand","title":"A regression task: the prediction of  bike  sharing demand","text":"BetaML provides only deep forward neural networks, artificial neural network units where the individual \"nodes\" are arranged in layers, from the input layer, where each unit holds the input coordinate, through various hidden layer transformations, until the actual output of the model:","category":"page"},{"location":"tutorials/Regression - bike sharing/betaml_tutorial_regression_sharingBikes.html","page":"A regression task: the prediction of  bike  sharing demand","title":"A regression task: the prediction of  bike  sharing demand","text":"(Image: Neural Networks)","category":"page"},{"location":"tutorials/Regression - bike sharing/betaml_tutorial_regression_sharingBikes.html","page":"A regression task: the prediction of  bike  sharing demand","title":"A regression task: the prediction of  bike  sharing demand","text":"In this layerwise computation, each unit in a particular layer takes input from all the preceding layer units and it has its own parameters that are adjusted to perform the overall computation. The training of the network consists in retrieving the coefficients that minimise a loss function between the output of the model and the known data. In particular, a deep (feedforward) neural network refers to a neural network that contains not only the input and output layers, but also hidden layers in between.","category":"page"},{"location":"tutorials/Regression - bike sharing/betaml_tutorial_regression_sharingBikes.html","page":"A regression task: the prediction of  bike  sharing demand","title":"A regression task: the prediction of  bike  sharing demand","text":"Neural networks accept only numerical inputs. We hence need to convert all categorical data in numerical units. A common approach is to use the so-called \"one-hot-encoding\" where the catagorical values are converted into indicator variables (0/1), one for each possible value. This can be done in BetaML using the onehotencoder function:","category":"page"},{"location":"tutorials/Regression - bike sharing/betaml_tutorial_regression_sharingBikes.html","page":"A regression task: the prediction of  bike  sharing demand","title":"A regression task: the prediction of  bike  sharing demand","text":"seasonDummies  = convert(Array{Float64,2},onehotencoder(data[:,:season]))\nweatherDummies = convert(Array{Float64,2},onehotencoder(data[:,:weathersit]))\nwdayDummies    = convert(Array{Float64,2},onehotencoder(data[:,:weekday] .+ 1 ))\n\n# We compose the feature matrix with the new dimensions obtained from the onehotencoder functions\nx = hcat(Matrix{Float64}(data[:,[:instant,:yr,:mnth,:holiday,:workingday,:temp,:atemp,:hum,:windspeed]]),\n         seasonDummies,\n         weatherDummies,\n         wdayDummies)\ny = data[:,16];\nnothing #hide","category":"page"},{"location":"tutorials/Regression - bike sharing/betaml_tutorial_regression_sharingBikes.html","page":"A regression task: the prediction of  bike  sharing demand","title":"A regression task: the prediction of  bike  sharing demand","text":"As usual, we split the data in training, validation and testing sets","category":"page"},{"location":"tutorials/Regression - bike sharing/betaml_tutorial_regression_sharingBikes.html","page":"A regression task: the prediction of  bike  sharing demand","title":"A regression task: the prediction of  bike  sharing demand","text":"((xtrain,xval,xtest),(ytrain,yval,ytest)) = partition([x,y],[0.75,0.125,1-0.75-0.125],shuffle=false)\n(ntrain, nval, ntest) = size.([ytrain,yval,ytest],1)","category":"page"},{"location":"tutorials/Regression - bike sharing/betaml_tutorial_regression_sharingBikes.html","page":"A regression task: the prediction of  bike  sharing demand","title":"A regression task: the prediction of  bike  sharing demand","text":"An other common operation with neural networks is to scale the feature vectors (X) and the labels (Y). The BetaML scale function, by default, scales the data such that each dimension has mean 0 and variance 1.","category":"page"},{"location":"tutorials/Regression - bike sharing/betaml_tutorial_regression_sharingBikes.html","page":"A regression task: the prediction of  bike  sharing demand","title":"A regression task: the prediction of  bike  sharing demand","text":"Note that we can provide the function with different scale factors or specify the columns that shoudn't be scaled (e.g. those resulting from the one-hot encoding). Finally we can reverse the scaling (this is useful to retrieve the unscaled features from a model trained with scaled ones).","category":"page"},{"location":"tutorials/Regression - bike sharing/betaml_tutorial_regression_sharingBikes.html","page":"A regression task: the prediction of  bike  sharing demand","title":"A regression task: the prediction of  bike  sharing demand","text":"colsNotToScale = [2;4;5;10:23]\nxScaleFactors   = get_scalefactors(xtrain,skip=colsNotToScale)\nyScaleFactors   = ([0],[0.001]) # get_scalefactors(ytrain) # This just divide by 1000. Using full scaling of Y we may get negative demand.\nxtrainScaled    = scale(xtrain,xScaleFactors)\nxvalScaled      = scale(xval,xScaleFactors)\nxtestScaled     = scale(xtest,xScaleFactors)\nytrainScaled    = scale(ytrain,yScaleFactors)\nyvalScaled      = scale(yval,yScaleFactors)\nytestScaled     = scale(ytest,yScaleFactors)\nD               = size(xtrain,2)","category":"page"},{"location":"tutorials/Regression - bike sharing/betaml_tutorial_regression_sharingBikes.html","page":"A regression task: the prediction of  bike  sharing demand","title":"A regression task: the prediction of  bike  sharing demand","text":"As we did above for decision trees and random forests, we select the best hyper-parameters by using the validation set. We consider here two hyper-parameters, the first one (hiddenLayerSizeRange) concerns the structure of the neural network, and in particular the size (in nodes) of the hidden layer, the second one (epochRange) concerns the training itself, and in particular the number of so-called \"epochs\" (number of iterations trough the whole dataset) to train the model.","category":"page"},{"location":"tutorials/Regression - bike sharing/betaml_tutorial_regression_sharingBikes.html","page":"A regression task: the prediction of  bike  sharing demand","title":"A regression task: the prediction of  bike  sharing demand","text":"Again, we use here repetitions for simplicity, but a cross-validation approach would have bee nmore appropriate:","category":"page"},{"location":"tutorials/Regression - bike sharing/betaml_tutorial_regression_sharingBikes.html","page":"A regression task: the prediction of  bike  sharing demand","title":"A regression task: the prediction of  bike  sharing demand","text":"function tuneHyperParameters(xtrain,ytrain,xval,yval;epochRange=50:50,hiddenLayerSizeRange=12:12,repetitions=5,rng=Random.GLOBAL_RNG)\n    # We start with an infinititly high error\n    bestRme         = +Inf\n    bestEpoch       = 0\n    bestSize        = 0\n    compLock        = ReentrantLock()\n\n    # Generate one random number generator per thread\n    masterSeed = rand(rng,100:9999999999999) ## Some RNG have problems with very small seed. Also, the master seed has to be computed _before_ generate_parallel_rngs\n    rngs       = generate_parallel_rngs(rng,Threads.nthreads())\n\n    # We loop over all possible hyperparameter combinations...\n    parLengths = (length(epochRange),length(hiddenLayerSizeRange))\n    Threads.@threads for ij in CartesianIndices(parLengths)\n       (epoch,hiddenLayerSize)   = (epochRange[Tuple(ij)[1]], hiddenLayerSizeRange[Tuple(ij)[2]])\n       tsrng = rngs[Threads.threadid()]\n       joinedIndx = LinearIndices(parLengths)[ij]\n       # And here we make the seeding depending on the i of the loop, not the thread: hence we get the same results indipendently of the number of threads\n       Random.seed!(tsrng,masterSeed+joinedIndx*10)\n       totAttemptError = 0.0\n       println(\"Testing epochs $epoch, layer size $hiddenLayerSize ...\")\n       # We run several repetitions with the same hyperparameter combination to account for stochasticity...\n       for r in 1:repetitions\n           l1   = DenseLayer(D,hiddenLayerSize,f=relu,rng=tsrng) # Activation function is ReLU\n           l2   = DenseLayer(hiddenLayerSize,hiddenLayerSize,f=identity,rng=tsrng)\n           l3   = DenseLayer(hiddenLayerSize,1,f=relu,rng=tsrng)\n           mynn = buildNetwork([l1,l2,l3],squared_cost,name=\"Bike sharing regression model\") # Build the NN and use the squared cost (aka MSE) as error function\n           # Training it (default to ADAM)\n           res  = train!(mynn,xtrain,ytrain,epochs=epoch,batch_size=8,opt_alg=ADAM(),verbosity=NONE, rng=tsrng) # Use opt_alg=SGD() to use Stochastic Gradient Descent\n           ŷval = predict(mynn,xval)\n           rmeVal  = mean_relative_error(ŷval,yval,normrec=false)\n           totAttemptError += rmeVal\n       end\n       avgRme = totAttemptError / repetitions\n       begin\n           lock(compLock) ## This step can't be run in parallel...\n           try\n               # Select this specific combination of hyperparameters if the error is the lowest\n               if avgRme < bestRme\n                 bestRme    = avgRme\n                 bestEpoch  = epoch\n                 bestSize   = hiddenLayerSize\n               end\n           finally\n               unlock(compLock)\n           end\n       end\n    end\n    return (bestRme=bestRme,bestEpoch=bestEpoch,bestSize=bestSize)\nend","category":"page"},{"location":"tutorials/Regression - bike sharing/betaml_tutorial_regression_sharingBikes.html","page":"A regression task: the prediction of  bike  sharing demand","title":"A regression task: the prediction of  bike  sharing demand","text":"# Note: the following code block may take a bit to run...\nepochsToTest     = [100]\nhiddenLayerSizes = [5,10]\n(bestRme,bestEpoch,bestSize) = tuneHyperParameters(xtrainScaled,ytrainScaled,xvalScaled,yvalScaled;epochRange=epochsToTest,hiddenLayerSizeRange=hiddenLayerSizes,repetitions=3,rng=copy(FIXEDRNG))","category":"page"},{"location":"tutorials/Regression - bike sharing/betaml_tutorial_regression_sharingBikes.html","page":"A regression task: the prediction of  bike  sharing demand","title":"A regression task: the prediction of  bike  sharing demand","text":"We can now build our feed-forward neaural network. We create three layers, the first layers will always have a input size equal to the dimensions of our data (the number of columns), and the output layer, for a simple regression where the predictions are scalars, it will always be one. The middle layer has the size we obtained from tuneHyperParameters.","category":"page"},{"location":"tutorials/Regression - bike sharing/betaml_tutorial_regression_sharingBikes.html","page":"A regression task: the prediction of  bike  sharing demand","title":"A regression task: the prediction of  bike  sharing demand","text":"There are already several kind of layers available (and you can build your own kind by defining a new struct and implementing a few functions. See the Nn module documentation for details). Here we use only dense layers, those found in typycal feed-fordward neural networks.","category":"page"},{"location":"tutorials/Regression - bike sharing/betaml_tutorial_regression_sharingBikes.html","page":"A regression task: the prediction of  bike  sharing demand","title":"A regression task: the prediction of  bike  sharing demand","text":"For each layer, on top of its size (in \"neurons\") we can specify an activation function. Here we use the relu for the terminal layer (this will guarantee that our predictions are always positive) and identity for the hidden layer. Again, consult the Nn module documentation for other activation layers already defined, or use any function of your choice.","category":"page"},{"location":"tutorials/Regression - bike sharing/betaml_tutorial_regression_sharingBikes.html","page":"A regression task: the prediction of  bike  sharing demand","title":"A regression task: the prediction of  bike  sharing demand","text":"Initial weight parameters can also be specified if needed. By default DenseLayer use the so-called Xavier initialisation.","category":"page"},{"location":"tutorials/Regression - bike sharing/betaml_tutorial_regression_sharingBikes.html","page":"A regression task: the prediction of  bike  sharing demand","title":"A regression task: the prediction of  bike  sharing demand","text":"l1   = DenseLayer(D,bestSize,f=relu,rng=copy(FIXEDRNG)) # Activation function is ReLU\nl2   = DenseLayer(bestSize,bestSize,f=identity,rng=copy(FIXEDRNG))\nl3   = DenseLayer(bestSize,1,f=relu,rng=copy(FIXEDRNG))","category":"page"},{"location":"tutorials/Regression - bike sharing/betaml_tutorial_regression_sharingBikes.html","page":"A regression task: the prediction of  bike  sharing demand","title":"A regression task: the prediction of  bike  sharing demand","text":"Finally we \"chain\" the layers together and we assign a final loss function (agian, you can provide your own loss function, if those available in BetaML don't suit your needs) in order to create the \"neural network\" NN object with the [buildNetwork][@ref] function:","category":"page"},{"location":"tutorials/Regression - bike sharing/betaml_tutorial_regression_sharingBikes.html","page":"A regression task: the prediction of  bike  sharing demand","title":"A regression task: the prediction of  bike  sharing demand","text":"mynn = buildNetwork([l1,l2,l3],squared_cost,name=\"Bike sharing regression model\") ## Build the NN and use the squared cost (aka MSE) as error function","category":"page"},{"location":"tutorials/Regression - bike sharing/betaml_tutorial_regression_sharingBikes.html","page":"A regression task: the prediction of  bike  sharing demand","title":"A regression task: the prediction of  bike  sharing demand","text":"The above neural network will use automatic differentiation (using the Zygote package) to compute the gradient used in the loss minimisation during the training step. It is also possible, for the layers that support it, to use manual differentiation. The network below is exactly equivalent to the one above, except it avoids automatic differentiation as we tell BetaML the functions to use as derivatives of the activation functions and of the overall network loss function:","category":"page"},{"location":"tutorials/Regression - bike sharing/betaml_tutorial_regression_sharingBikes.html","page":"A regression task: the prediction of  bike  sharing demand","title":"A regression task: the prediction of  bike  sharing demand","text":"mynnManual = buildNetwork([\n        DenseLayer(D,bestSize,f=relu,df=drelu,rng=copy(FIXEDRNG)),\n        DenseLayer(bestSize,bestSize,f=identity,df=didentity,rng=copy(FIXEDRNG)),\n        DenseLayer(bestSize,1,f=relu,df=drelu,rng=copy(FIXEDRNG))\n    ], squared_cost, name=\"Bike sharing regression model\", dcf=dSquaredCost)","category":"page"},{"location":"tutorials/Regression - bike sharing/betaml_tutorial_regression_sharingBikes.html","page":"A regression task: the prediction of  bike  sharing demand","title":"A regression task: the prediction of  bike  sharing demand","text":"We can now train the neural network with the best hyper-parameters using the function train!. Note the esclamation point. By convention in julia, functions that end with an exclamation mark modify some of their inputs, normally the first one.","category":"page"},{"location":"tutorials/Regression - bike sharing/betaml_tutorial_regression_sharingBikes.html","page":"A regression task: the prediction of  bike  sharing demand","title":"A regression task: the prediction of  bike  sharing demand","text":"Here train! has a question mark as indeed it modifies the neural network object, by updating its weights. If you want to keep a copy of the network before training (for example, if you want to train it in different independent ways), make a deepcopy of the NN object or first save its parameters with get_params and then re-apply the saved parameters with set_params!.","category":"page"},{"location":"tutorials/Regression - bike sharing/betaml_tutorial_regression_sharingBikes.html","page":"A regression task: the prediction of  bike  sharing demand","title":"A regression task: the prediction of  bike  sharing demand","text":"Several optimisation algorithms are available, and each accepts different parameters, like the learning rate for the Stochastic Gradient Descent algorithm (SGD or the exponential decay rates for the  moments estimates for the ADAM algorithm (that we use here, with the default parameters).","category":"page"},{"location":"tutorials/Regression - bike sharing/betaml_tutorial_regression_sharingBikes.html","page":"A regression task: the prediction of  bike  sharing demand","title":"A regression task: the prediction of  bike  sharing demand","text":"println(\"Final training of $bestEpoch epochs, with layer size $bestSize ...\")\nres  = train!(mynn,xtrainScaled,ytrainScaled,epochs=bestEpoch,batch_size=8,opt_alg=ADAM(),rng=copy(FIXEDRNG)) ## Use opt_alg=SGD() to use Stochastic Gradient Descent","category":"page"},{"location":"tutorials/Regression - bike sharing/betaml_tutorial_regression_sharingBikes.html","page":"A regression task: the prediction of  bike  sharing demand","title":"A regression task: the prediction of  bike  sharing demand","text":"Let's benchmark the BetaML neural network training","category":"page"},{"location":"tutorials/Regression - bike sharing/betaml_tutorial_regression_sharingBikes.html","page":"A regression task: the prediction of  bike  sharing demand","title":"A regression task: the prediction of  bike  sharing demand","text":"@btime train!(mynnManual,xtrainScaled,ytrainScaled,epochs=bestEpoch,batch_size=8,opt_alg=ADAM(),rng=copy(FIXEDRNG), verbosity=NONE);\n1.951 s (8716955 allocations: 702.71 MiB)","category":"page"},{"location":"tutorials/Regression - bike sharing/betaml_tutorial_regression_sharingBikes.html","page":"A regression task: the prediction of  bike  sharing demand","title":"A regression task: the prediction of  bike  sharing demand","text":"As we can see the model training is one order of magnitude slower than random forests, altought the memory requirement is approximatly the same.","category":"page"},{"location":"tutorials/Regression - bike sharing/betaml_tutorial_regression_sharingBikes.html","page":"A regression task: the prediction of  bike  sharing demand","title":"A regression task: the prediction of  bike  sharing demand","text":"To obtain the neural network predictions we apply the function predict to the feature matrix X for which we want to generate previsions, and then, in order to obtain the unscaled estimates we use the scale function applied to the scaled values with the original scaling factors and the parameter rev set to true. Note the usage of the pipe operator to avoid ugly function1(function2(function3(...))) nested calls:","category":"page"},{"location":"tutorials/Regression - bike sharing/betaml_tutorial_regression_sharingBikes.html","page":"A regression task: the prediction of  bike  sharing demand","title":"A regression task: the prediction of  bike  sharing demand","text":"ŷtrain = @pipe predict(mynn,xtrainScaled) |> scale(_, yScaleFactors,rev=true);\nŷval   = @pipe predict(mynn,xvalScaled)   |> scale(_, yScaleFactors,rev=true);\nŷtest  = @pipe predict(mynn,xtestScaled)  |> scale(_ ,yScaleFactors,rev=true);\nnothing #hide","category":"page"},{"location":"tutorials/Regression - bike sharing/betaml_tutorial_regression_sharingBikes.html","page":"A regression task: the prediction of  bike  sharing demand","title":"A regression task: the prediction of  bike  sharing demand","text":"(mreTrain, mreVal, mreTest) = mean_relative_error.([ŷtrain,ŷval,ŷtest],[ytrain,yval,ytest],normrec=false)","category":"page"},{"location":"tutorials/Regression - bike sharing/betaml_tutorial_regression_sharingBikes.html","page":"A regression task: the prediction of  bike  sharing demand","title":"A regression task: the prediction of  bike  sharing demand","text":"The error is much lower. Let's plot our predictions:","category":"page"},{"location":"tutorials/Regression - bike sharing/betaml_tutorial_regression_sharingBikes.html","page":"A regression task: the prediction of  bike  sharing demand","title":"A regression task: the prediction of  bike  sharing demand","text":"Again, we can start by plotting the estimated vs the observed value:","category":"page"},{"location":"tutorials/Regression - bike sharing/betaml_tutorial_regression_sharingBikes.html","page":"A regression task: the prediction of  bike  sharing demand","title":"A regression task: the prediction of  bike  sharing demand","text":"scatter(ytrain,ŷtrain,xlabel=\"daily rides\",ylabel=\"est. daily rides\",label=nothing,title=\"Est vs. obs in training period (NN)\")","category":"page"},{"location":"tutorials/Regression - bike sharing/betaml_tutorial_regression_sharingBikes.html","page":"A regression task: the prediction of  bike  sharing demand","title":"A regression task: the prediction of  bike  sharing demand","text":"scatter(yval,ŷval,xlabel=\"daily rides\",ylabel=\"est. daily rides\",label=nothing,title=\"Est vs. obs in validation period (NN)\")","category":"page"},{"location":"tutorials/Regression - bike sharing/betaml_tutorial_regression_sharingBikes.html","page":"A regression task: the prediction of  bike  sharing demand","title":"A regression task: the prediction of  bike  sharing demand","text":"scatter(ytest,ŷtest,xlabel=\"daily rides\",ylabel=\"est. daily rides\",label=nothing,title=\"Est vs. obs in testing period (NN)\")","category":"page"},{"location":"tutorials/Regression - bike sharing/betaml_tutorial_regression_sharingBikes.html","page":"A regression task: the prediction of  bike  sharing demand","title":"A regression task: the prediction of  bike  sharing demand","text":"We now plot across the time dimension, first plotting the whole period (2 years):","category":"page"},{"location":"tutorials/Regression - bike sharing/betaml_tutorial_regression_sharingBikes.html","page":"A regression task: the prediction of  bike  sharing demand","title":"A regression task: the prediction of  bike  sharing demand","text":"ŷtrainfull = vcat(ŷtrain,fill(missing,nval+ntest))\nŷvalfull   = vcat(fill(missing,ntrain), ŷval, fill(missing,ntest))\nŷtestfull  = vcat(fill(missing,ntrain+nval), ŷtest)\nplot(data[:,:dteday],[data[:,:cnt] ŷtrainfull ŷvalfull ŷtestfull], label=[\"obs\" \"train\" \"val\" \"test\"], legend=:topleft, ylabel=\"daily rides\", title=\"Daily bike sharing demand observed/estimated across the\\n whole 2-years period  (NN)\")","category":"page"},{"location":"tutorials/Regression - bike sharing/betaml_tutorial_regression_sharingBikes.html","page":"A regression task: the prediction of  bike  sharing demand","title":"A regression task: the prediction of  bike  sharing demand","text":"...and then focusing on the testing data","category":"page"},{"location":"tutorials/Regression - bike sharing/betaml_tutorial_regression_sharingBikes.html","page":"A regression task: the prediction of  bike  sharing demand","title":"A regression task: the prediction of  bike  sharing demand","text":"stc  = 620\nendc = size(x,1)\nplot(data[stc:endc,:dteday],[data[stc:endc,:cnt] ŷvalfull[stc:endc] ŷtestfull[stc:endc]], label=[\"obs\" \"val\" \"test\"], legend=:bottomleft, ylabel=\"Daily rides\", title=\"Focus on the testing period (NN)\")","category":"page"},{"location":"tutorials/Regression - bike sharing/betaml_tutorial_regression_sharingBikes.html#Comparison-with-Flux","page":"A regression task: the prediction of  bike  sharing demand","title":"Comparison with Flux","text":"","category":"section"},{"location":"tutorials/Regression - bike sharing/betaml_tutorial_regression_sharingBikes.html","page":"A regression task: the prediction of  bike  sharing demand","title":"A regression task: the prediction of  bike  sharing demand","text":"We now apply the same Neural Network model using the Flux framework, a dedicated neural network library, reusing the optimal parameters that we did learn in tuneHyperParameters","category":"page"},{"location":"tutorials/Regression - bike sharing/betaml_tutorial_regression_sharingBikes.html","page":"A regression task: the prediction of  bike  sharing demand","title":"A regression task: the prediction of  bike  sharing demand","text":"We fix the default random number generator so that the Flux example gives a reproducible output","category":"page"},{"location":"tutorials/Regression - bike sharing/betaml_tutorial_regression_sharingBikes.html","page":"A regression task: the prediction of  bike  sharing demand","title":"A regression task: the prediction of  bike  sharing demand","text":"Random.seed!(123)","category":"page"},{"location":"tutorials/Regression - bike sharing/betaml_tutorial_regression_sharingBikes.html","page":"A regression task: the prediction of  bike  sharing demand","title":"A regression task: the prediction of  bike  sharing demand","text":"We define the Flux neural network model and load it with data...","category":"page"},{"location":"tutorials/Regression - bike sharing/betaml_tutorial_regression_sharingBikes.html","page":"A regression task: the prediction of  bike  sharing demand","title":"A regression task: the prediction of  bike  sharing demand","text":"l1         = Flux.Dense(D,bestSize,Flux.relu)\nl2         = Flux.Dense(bestSize,bestSize,identity)\nl3         = Flux.Dense(bestSize,1,Flux.relu)\nFlux_nn    = Flux.Chain(l1,l2,l3)\nloss(x, y) = Flux.mse(Flux_nn(x), y)\nps         = Flux.params(Flux_nn)\nnndata     = Flux.Data.DataLoader((xtrainScaled', ytrainScaled'), batchsize=8,shuffle=true)\n\nFlux_nn2   = deepcopy(Flux_nn)      ## A copy for the time benchmarking\nps2        = Flux.params(Flux_nn2)  ## A copy for the time benchmarking","category":"page"},{"location":"tutorials/Regression - bike sharing/betaml_tutorial_regression_sharingBikes.html","page":"A regression task: the prediction of  bike  sharing demand","title":"A regression task: the prediction of  bike  sharing demand","text":"We do the training of the Flux model...","category":"page"},{"location":"tutorials/Regression - bike sharing/betaml_tutorial_regression_sharingBikes.html","page":"A regression task: the prediction of  bike  sharing demand","title":"A regression task: the prediction of  bike  sharing demand","text":"Flux.@epochs bestEpoch Flux.train!(loss, ps, nndata, Flux.ADAM(0.001, (0.9, 0.8)));\nnothing #hide","category":"page"},{"location":"tutorials/Regression - bike sharing/betaml_tutorial_regression_sharingBikes.html","page":"A regression task: the prediction of  bike  sharing demand","title":"A regression task: the prediction of  bike  sharing demand","text":"..and we benchmark it..","category":"page"},{"location":"tutorials/Regression - bike sharing/betaml_tutorial_regression_sharingBikes.html","page":"A regression task: the prediction of  bike  sharing demand","title":"A regression task: the prediction of  bike  sharing demand","text":"@btime begin for i in 1:bestEpoch Flux.train!(loss, ps2, nndata, Flux.ADAM(0.001, (0.9, 0.8))) end end\n690.231 ms (3349901 allocations: 266.76 MiB)","category":"page"},{"location":"tutorials/Regression - bike sharing/betaml_tutorial_regression_sharingBikes.html","page":"A regression task: the prediction of  bike  sharing demand","title":"A regression task: the prediction of  bike  sharing demand","text":"On this small example the speed of Flux is on the same order than BetaML (the actual difference seems to depend on the specific RNG seed and hardware), however I suspect that Flux scales much better with larger networks and/or data.","category":"page"},{"location":"tutorials/Regression - bike sharing/betaml_tutorial_regression_sharingBikes.html","page":"A regression task: the prediction of  bike  sharing demand","title":"A regression task: the prediction of  bike  sharing demand","text":"We obtain the estimates...","category":"page"},{"location":"tutorials/Regression - bike sharing/betaml_tutorial_regression_sharingBikes.html","page":"A regression task: the prediction of  bike  sharing demand","title":"A regression task: the prediction of  bike  sharing demand","text":"ŷtrainf = @pipe Flux_nn(xtrainScaled')' |> scale(_,yScaleFactors,rev=true);\nŷvalf   = @pipe Flux_nn(xvalScaled')'   |> scale(_,yScaleFactors,rev=true);\nŷtestf  = @pipe Flux_nn(xtestScaled')'  |> scale(_,yScaleFactors,rev=true);\nnothing #hide","category":"page"},{"location":"tutorials/Regression - bike sharing/betaml_tutorial_regression_sharingBikes.html","page":"A regression task: the prediction of  bike  sharing demand","title":"A regression task: the prediction of  bike  sharing demand","text":"..and we compute the mean relative errors..","category":"page"},{"location":"tutorials/Regression - bike sharing/betaml_tutorial_regression_sharingBikes.html","page":"A regression task: the prediction of  bike  sharing demand","title":"A regression task: the prediction of  bike  sharing demand","text":"(mreTrain, mreVal, mreTest) = mean_relative_error.([ŷtrainf,ŷvalf,ŷtestf],[ytrain,yval,ytest],normrec=false)","category":"page"},{"location":"tutorials/Regression - bike sharing/betaml_tutorial_regression_sharingBikes.html","page":"A regression task: the prediction of  bike  sharing demand","title":"A regression task: the prediction of  bike  sharing demand","text":".. finding an error not significantly different than the one obtained from BetaML.Nn.","category":"page"},{"location":"tutorials/Regression - bike sharing/betaml_tutorial_regression_sharingBikes.html","page":"A regression task: the prediction of  bike  sharing demand","title":"A regression task: the prediction of  bike  sharing demand","text":"Plots:","category":"page"},{"location":"tutorials/Regression - bike sharing/betaml_tutorial_regression_sharingBikes.html","page":"A regression task: the prediction of  bike  sharing demand","title":"A regression task: the prediction of  bike  sharing demand","text":"scatter(ytrain,ŷtrainf,xlabel=\"daily rides\",ylabel=\"est. daily rides\",label=nothing,title=\"Est vs. obs in training period (Flux.NN)\")","category":"page"},{"location":"tutorials/Regression - bike sharing/betaml_tutorial_regression_sharingBikes.html","page":"A regression task: the prediction of  bike  sharing demand","title":"A regression task: the prediction of  bike  sharing demand","text":"scatter(yval,ŷvalf,xlabel=\"daily rides\",ylabel=\"est. daily rides\",label=nothing,title=\"Est vs. obs in validation period (Flux.NN)\")","category":"page"},{"location":"tutorials/Regression - bike sharing/betaml_tutorial_regression_sharingBikes.html","page":"A regression task: the prediction of  bike  sharing demand","title":"A regression task: the prediction of  bike  sharing demand","text":"scatter(ytest,ŷtestf,xlabel=\"daily rides\",ylabel=\"est. daily rides\",label=nothing,title=\"Est vs. obs in testing period (Flux.NN)\")","category":"page"},{"location":"tutorials/Regression - bike sharing/betaml_tutorial_regression_sharingBikes.html","page":"A regression task: the prediction of  bike  sharing demand","title":"A regression task: the prediction of  bike  sharing demand","text":"ŷtrainfullf = vcat(ŷtrainf,fill(missing,nval+ntest))\nŷvalfullf   = vcat(fill(missing,ntrain), ŷvalf, fill(missing,ntest))\nŷtestfullf  = vcat(fill(missing,ntrain+nval), ŷtestf)\nplot(data[:,:dteday],[data[:,:cnt] ŷtrainfullf ŷvalfullf ŷtestfullf], label=[\"obs\" \"train\" \"val\" \"test\"], legend=:topleft, ylabel=\"daily rides\", title=\"Daily bike sharing demand observed/estimated across the\\n whole 2-years period (Flux.NN)\")","category":"page"},{"location":"tutorials/Regression - bike sharing/betaml_tutorial_regression_sharingBikes.html","page":"A regression task: the prediction of  bike  sharing demand","title":"A regression task: the prediction of  bike  sharing demand","text":"stc = 620\nendc = size(x,1)\nplot(data[stc:endc,:dteday],[data[stc:endc,:cnt] ŷvalfullf[stc:endc] ŷtestfullf[stc:endc]], label=[\"obs\" \"val\" \"test\"], legend=:bottomleft, ylabel=\"Daily rides\", title=\"Focus on the testing period (Flux.NN)\")","category":"page"},{"location":"tutorials/Regression - bike sharing/betaml_tutorial_regression_sharingBikes.html#Conclusions-of-Neural-Network-models","page":"A regression task: the prediction of  bike  sharing demand","title":"Conclusions of Neural Network models","text":"","category":"section"},{"location":"tutorials/Regression - bike sharing/betaml_tutorial_regression_sharingBikes.html","page":"A regression task: the prediction of  bike  sharing demand","title":"A regression task: the prediction of  bike  sharing demand","text":"If we strive for the most accurate predictions, deep neural networks are usually the best choice. However they are computationally expensive, so with limited resourses we may get better results by fine tuning and running many repetitions of \"simpler\" decision trees or even random forest models than a large naural network with insufficient hyper-parameter tuning. Also, we shoudl consider that decision trees/random forests are much simpler to work with.","category":"page"},{"location":"tutorials/Regression - bike sharing/betaml_tutorial_regression_sharingBikes.html","page":"A regression task: the prediction of  bike  sharing demand","title":"A regression task: the prediction of  bike  sharing demand","text":"That said, specialised neural network libraries, like Flux, allow to use GPU and specialised hardware letting neural networks to scale with very large datasets.","category":"page"},{"location":"tutorials/Regression - bike sharing/betaml_tutorial_regression_sharingBikes.html","page":"A regression task: the prediction of  bike  sharing demand","title":"A regression task: the prediction of  bike  sharing demand","text":"Still, for small and medium datasets, BetaML provides simpler yet customisable solutions that are accurate and fast.","category":"page"},{"location":"tutorials/Regression - bike sharing/betaml_tutorial_regression_sharingBikes.html#Using-GMM-based-regressors","page":"A regression task: the prediction of  bike  sharing demand","title":"Using GMM-based regressors","text":"","category":"section"},{"location":"tutorials/Regression - bike sharing/betaml_tutorial_regression_sharingBikes.html","page":"A regression task: the prediction of  bike  sharing demand","title":"A regression task: the prediction of  bike  sharing demand","text":"These are newly addded regression algorithms based on Gaussian Mixture Model. There are two variants available, GMMRegressor1 and GMMRegressor2 This example uses the V2 API","category":"page"},{"location":"tutorials/Regression - bike sharing/betaml_tutorial_regression_sharingBikes.html","page":"A regression task: the prediction of  bike  sharing demand","title":"A regression task: the prediction of  bike  sharing demand","text":"First we define the model with its hyperparameters and the options (random seed, verbosity level..)","category":"page"},{"location":"tutorials/Regression - bike sharing/betaml_tutorial_regression_sharingBikes.html","page":"A regression task: the prediction of  bike  sharing demand","title":"A regression task: the prediction of  bike  sharing demand","text":"m = GMMRegressor1(rng=copy(FIXEDRNG), verbosity=NONE)","category":"page"},{"location":"tutorials/Regression - bike sharing/betaml_tutorial_regression_sharingBikes.html","page":"A regression task: the prediction of  bike  sharing demand","title":"A regression task: the prediction of  bike  sharing demand","text":"@btime begin fit!(m,xtrainScaled,ytrainScaled); reset!(m) end 13.584 ms (103690 allocations: 25.08 MiB)","category":"page"},{"location":"tutorials/Regression - bike sharing/betaml_tutorial_regression_sharingBikes.html","page":"A regression task: the prediction of  bike  sharing demand","title":"A regression task: the prediction of  bike  sharing demand","text":"Here we fit the model to the training data..","category":"page"},{"location":"tutorials/Regression - bike sharing/betaml_tutorial_regression_sharingBikes.html","page":"A regression task: the prediction of  bike  sharing demand","title":"A regression task: the prediction of  bike  sharing demand","text":"fit!(m,xtrainScaled,ytrainScaled)","category":"page"},{"location":"tutorials/Regression - bike sharing/betaml_tutorial_regression_sharingBikes.html","page":"A regression task: the prediction of  bike  sharing demand","title":"A regression task: the prediction of  bike  sharing demand","text":"And here we predict...","category":"page"},{"location":"tutorials/Regression - bike sharing/betaml_tutorial_regression_sharingBikes.html","page":"A regression task: the prediction of  bike  sharing demand","title":"A regression task: the prediction of  bike  sharing demand","text":"ŷtrainGMM = @pipe predict(m,xtrainScaled) |> scale(_, yScaleFactors,rev=true);\nŷvalGMM   = @pipe predict(m,xvalScaled)   |> scale(_, yScaleFactors,rev=true);\nŷtestGMM  = @pipe predict(m,xtestScaled)  |> scale(_ ,yScaleFactors,rev=true);\n\n(mreTrainGMM, mreValGMM, mreTestGMM) = mean_relative_error.([ŷtrainGMM,ŷvalGMM,ŷtestGMM],[ytrain,yval,ytest],normrec=false)","category":"page"},{"location":"tutorials/Regression - bike sharing/betaml_tutorial_regression_sharingBikes.html","page":"A regression task: the prediction of  bike  sharing demand","title":"A regression task: the prediction of  bike  sharing demand","text":"Better (test MRE ≈ 0.25) can be obtained by using more mixtures, but at a much larger computational costs","category":"page"},{"location":"tutorials/Regression - bike sharing/betaml_tutorial_regression_sharingBikes.html","page":"A regression task: the prediction of  bike  sharing demand","title":"A regression task: the prediction of  bike  sharing demand","text":"m = GMMRegressor2(rng=copy(FIXEDRNG), verbosity=NONE)\n@btime begin fit!(m,xtrainScaled,ytrainScaled); reset!(m) end","category":"page"},{"location":"tutorials/Regression - bike sharing/betaml_tutorial_regression_sharingBikes.html","page":"A regression task: the prediction of  bike  sharing demand","title":"A regression task: the prediction of  bike  sharing demand","text":"10.704 ms (84754 allocations: 19.54 MiB)","category":"page"},{"location":"tutorials/Regression - bike sharing/betaml_tutorial_regression_sharingBikes.html","page":"A regression task: the prediction of  bike  sharing demand","title":"A regression task: the prediction of  bike  sharing demand","text":"fit!(m,xtrainScaled,ytrainScaled)\nŷtrainGMM = @pipe predict(m,xtrainScaled) |> scale(_, yScaleFactors,rev=true);\nŷvalGMM   = @pipe predict(m,xvalScaled)   |> scale(_, yScaleFactors,rev=true);\nŷtestGMM  = @pipe predict(m,xtestScaled)  |> scale(_ ,yScaleFactors,rev=true);\n\n(mreTrainGMM, mreValGMM, mreTestGMM) = mean_relative_error.([ŷtrainGMM,ŷvalGMM,ŷtestGMM],[ytrain,yval,ytest],normrec=false)","category":"page"},{"location":"tutorials/Regression - bike sharing/betaml_tutorial_regression_sharingBikes.html#Summary","page":"A regression task: the prediction of  bike  sharing demand","title":"Summary","text":"","category":"section"},{"location":"tutorials/Regression - bike sharing/betaml_tutorial_regression_sharingBikes.html","page":"A regression task: the prediction of  bike  sharing demand","title":"A regression task: the prediction of  bike  sharing demand","text":"This is the summary of the results we had trying to predict the daily bike sharing demand, given weather and calendar information of the day","category":"page"},{"location":"tutorials/Regression - bike sharing/betaml_tutorial_regression_sharingBikes.html","page":"A regression task: the prediction of  bike  sharing demand","title":"A regression task: the prediction of  bike  sharing demand","text":"Model Train rme Test rme Training time (ms)* Training mem (MB)*\nDT 0.1266 0.2223 26.5 58\nRF 0.0651 0.2223 362 971\nRF (DecisionTree.jl) 0.0312 0.3142 36 11\nNN 0.0884 0.1761 1768 758\nNN (Flux.jl) 0.0981 0.1618 1708 282\nGMMRegressor1 0.2388 0.4394 13.5 25.1\nGMMRegressor2 0.2588 0.2763 10.7 19.5","category":"page"},{"location":"tutorials/Regression - bike sharing/betaml_tutorial_regression_sharingBikes.html","page":"A regression task: the prediction of  bike  sharing demand","title":"A regression task: the prediction of  bike  sharing demand","text":"on a Intel Core i5-8350U laptop","category":"page"},{"location":"tutorials/Regression - bike sharing/betaml_tutorial_regression_sharingBikes.html","page":"A regression task: the prediction of  bike  sharing demand","title":"A regression task: the prediction of  bike  sharing demand","text":"Neural networks can be more precise than random forests models, but are more computationally expensive (and tricky to set up). When we compare BetaML with the algorithm-specific leading packages, we found similar results in terms of accuracy, but often the leading packages are better optimised and run more efficiently (but sometimes at the cost of being less versatile). GMM regressors are very computationally cheap and a good choice if accuracy can be traded off for performances.","category":"page"},{"location":"tutorials/Regression - bike sharing/betaml_tutorial_regression_sharingBikes.html","page":"A regression task: the prediction of  bike  sharing demand","title":"A regression task: the prediction of  bike  sharing demand","text":"View this file on Github.","category":"page"},{"location":"tutorials/Regression - bike sharing/betaml_tutorial_regression_sharingBikes.html","page":"A regression task: the prediction of  bike  sharing demand","title":"A regression task: the prediction of  bike  sharing demand","text":"","category":"page"},{"location":"tutorials/Regression - bike sharing/betaml_tutorial_regression_sharingBikes.html","page":"A regression task: the prediction of  bike  sharing demand","title":"A regression task: the prediction of  bike  sharing demand","text":"This page was generated using Literate.jl.","category":"page"},{"location":"Clustering.html#clustering_module","page":"Clustering","title":"The BetaML.Clustering Module","text":"","category":"section"},{"location":"Clustering.html","page":"Clustering","title":"Clustering","text":"Clustering","category":"page"},{"location":"Clustering.html#BetaML.Clustering","page":"Clustering","title":"BetaML.Clustering","text":"Clustering module (WIP)\n\n(Hard) Clustering algorithms\n\nProvide hard clustering methods using K-means and k-medoids. Please see also the GMM module for GMM-based soft clustering (i.e. where a probability distribution to be part of the various classes is assigned to each record instead of a single class), missing values imputation / collaborative filtering / reccomendation systems using clustering methods as backend.\n\nThe module provides the following models. Use ?[model] to access their documentation:\n\nKMeansClusterer: Classical KMean algorithm\nKMedoidsClusterer: Kmedoids algorithm with configurable distance metric\n\n\n\n\n\n","category":"module"},{"location":"Clustering.html#Module-Index","page":"Clustering","title":"Module Index","text":"","category":"section"},{"location":"Clustering.html","page":"Clustering","title":"Clustering","text":"Modules = [Clustering]\nOrder   = [:constant, :type, :function, :macro]\nPrivate = false","category":"page"},{"location":"Clustering.html#Detailed-API","page":"Clustering","title":"Detailed API","text":"","category":"section"},{"location":"Clustering.html","page":"Clustering","title":"Clustering","text":"Modules = [Clustering]\nPrivate = false","category":"page"},{"location":"Clustering.html#BetaML.Clustering.KMeans","page":"Clustering","title":"BetaML.Clustering.KMeans","text":"mutable struct KMeans <: MLJModelInterface.Unsupervised\n\nThe classical KMeans clustering algorithm, from the Beta Machine Learning Toolkit (BetaML).\n\nParameters:\n\nn_classes::Int64\nNumber of classes to discriminate the data [def: 3]\ndist::Function\nFunction to employ as distance. Default to the Euclidean distance. Can be one of the predefined distances (l1_distance, l2_distance, l2squared_distance),  cosine_distance), any user defined function accepting two vectors and returning a scalar or an anonymous function with the same characteristics. Attention that, contrary to KMedoids, the KMeansClusterer algorithm is not guaranteed to converge with other distances than the Euclidean one.\ninitialisation_strategy::String\nThe computation method of the vector of the initial representatives. One of the following:\n\"random\": randomly in the X space\n\"grid\": using a grid approach\n\"shuffle\": selecting randomly within the available points [default]\n\"given\": using a provided set of initial representatives provided in the initial_representatives parameter\n\ninitial_representatives::Union{Nothing, Matrix{Float64}}\nProvided (K x D) matrix of initial representatives (useful only with initialisation_strategy=\"given\") [default: nothing]\nrng::Random.AbstractRNG\nRandom Number Generator [deafult: Random.GLOBAL_RNG]\n\nNotes:\n\ndata must be numerical\nonline fitting (re-fitting with new data) is supported\n\n\n\n\n\n","category":"type"},{"location":"Clustering.html#BetaML.Clustering.KMeansClusterer","page":"Clustering","title":"BetaML.Clustering.KMeansClusterer","text":"mutable struct KMeansClusterer <: BetaMLUnsupervisedModel\n\nThe classical \"K-Means\" clustering algorithm (unsupervised).\n\nFor the parameters see ?KMeansMedoidsHyperParametersSet and ?BetaMLDefaultOptionsSet.\n\nNotes:\n\ndata must be numerical\nonline fitting (re-fitting with new data) is supported\n\n\n\n\n\n","category":"type"},{"location":"Clustering.html#BetaML.Clustering.KMeansMedoidsHyperParametersSet","page":"Clustering","title":"BetaML.Clustering.KMeansMedoidsHyperParametersSet","text":"mutable struct KMeansMedoidsHyperParametersSet <: BetaMLHyperParametersSet\n\nHyperparameters for both the KMeansClusterer and KMedoidsClusterer models\n\nParameters:\n\nn_classes::Int64\nNumber of classes to discriminate the data [def: 3]\ndist::Function\nFunction to employ as distance. Default to the Euclidean distance. Can be one of the predefined distances (l1_distance, l2_distance, l2squared_distance),  cosine_distance), any user defined function accepting two vectors and returning a scalar or an anonymous function with the same characteristics. Attention that the KMeansClusterer algorithm is not guaranteed to converge with other distances than the Euclidean one.\ninitialisation_strategy::String\nThe computation method of the vector of the initial representatives. One of the following:\n\"random\": randomly in the X space [default]\n\"grid\": using a grid approach\n\"shuffle\": selecting randomly within the available points\n\"given\": using a provided set of initial representatives provided in the initial_representatives parameter\n\ninitial_representatives::Union{Nothing, Matrix{Float64}}\nProvided (K x D) matrix of initial representatives (useful only with initialisation_strategy=\"given\") [default: nothing]\n\n\n\n\n\n","category":"type"},{"location":"Clustering.html#BetaML.Clustering.KMedoids","page":"Clustering","title":"BetaML.Clustering.KMedoids","text":"mutable struct KMedoids <: MLJModelInterface.Unsupervised\n\nParameters:\n\nn_classes::Int64\nNumber of classes to discriminate the data [def: 3]\ndist::Function\nFunction to employ as distance. Default to the Euclidean distance. Can be one of the predefined distances (l1_distance, l2_distance, l2squared_distance),  cosine_distance), any user defined function accepting two vectors and returning a scalar or an anonymous function with the same characteristics.\ninitialisation_strategy::String\nThe computation method of the vector of the initial representatives. One of the following:\n\"random\": randomly in the X space\n\"grid\": using a grid approach\n\"shuffle\": selecting randomly within the available points [default]\n\"given\": using a provided set of initial representatives provided in the initial_representatives parameter\n\ninitial_representatives::Union{Nothing, Matrix{Float64}}\nProvided (K x D) matrix of initial representatives (useful only with initialisation_strategy=\"given\") [default: nothing]\nrng::Random.AbstractRNG\nRandom Number Generator [deafult: Random.GLOBAL_RNG]\n\nThe K-medoids clustering algorithm with customisable distance function, from the Beta Machine Learning Toolkit (BetaML).\n\nSimilar to K-Means, but the \"representatives\" (the cetroids) are guaranteed to be one of the training points. The algorithm work with any arbitrary distance measure.\n\nNotes:\n\ndata must be numerical\nonline fitting (re-fitting with new data) is supported\n\n\n\n\n\n","category":"type"},{"location":"Clustering.html#BetaML.Clustering.KMedoidsClusterer","page":"Clustering","title":"BetaML.Clustering.KMedoidsClusterer","text":"mutable struct KMedoidsClusterer <: BetaMLUnsupervisedModel\n\nThe classical \"K-Medoids\" clustering algorithm (unsupervised).\n\nSimilar to K-Means, but the \"representatives\" (the cetroids) are guaranteed to be one of the training points. The algorithm work with any arbitrary distance measure.\n\nFor the parameters see ?KMeansMedoidsHyperParametersSet and ?BetaMLDefaultOptionsSet.\n\nNotes:\n\ndata must be numerical\nonline fitting (re-fitting with new data) is supported\n\n\n\n\n\n","category":"type"},{"location":"Clustering.html#BetaML.Clustering.kmeans-Tuple{Any, Any}","page":"Clustering","title":"BetaML.Clustering.kmeans","text":"kmeans(X,K;dist,initialisationstrategy,initialrepresentatives)\n\nCompute K-Mean algorithm to identify K clusters of X using Euclidean distance\n\nwarning: Warning\nThis function is deprecated and will possibly be removed in BetaML 0.9. Use KMeansClusterer instead. \n\nParameters:\n\nX: a (N x D) data to clusterise\nK: Number of cluster wonted\ndist: Function to employ as distance (see notes). Default to Euclidean distance.\ninitialisation_strategy: Whether to select the initial representative vectors:\nrandom: randomly in the X space\ngrid: using a grid approach [default]\nshuffle: selecting randomly within the available points\ngiven: using a provided set of initial representatives provided in the initial_representatives parameter\ninitial_representatives: Provided (K x D) matrix of initial representatives (used only together with the given initialisation_strategy) [default: nothing]\nrng: Random Number Generator (see FIXEDSEED) [deafult: Random.GLOBAL_RNG]\n\nReturns:\n\nA tuple of two items, the first one being a vector of size N of ids of the clusters associated to each point and the second one the (K x D) matrix of representatives\n\nNotes:\n\nSome returned clusters could be empty\nThe dist parameter can be:\nAny user defined function accepting two vectors and returning a scalar\nAn anonymous function with the same characteristics (e.g. dist = (x,y) -> norm(x-y)^2)\nOne of the above predefined distances: l1_distance, l2_distance, l2squared_distance, cosine_distance\n\nExample:\n\njulia> (clIdx,Z) = kmeans([1 10.5;1.5 10.8; 1.8 8; 1.7 15; 3.2 40; 3.6 32; 3.3 38; 5.1 -2.3; 5.2 -2.4],3)\n\n\n\n\n\n","category":"method"},{"location":"Clustering.html#BetaML.Clustering.kmedoids-Tuple{Any, Any}","page":"Clustering","title":"BetaML.Clustering.kmedoids","text":"kmedoids(X,K;dist,initialisationstrategy,initialrepresentatives)\n\nCompute K-Medoids algorithm to identify K clusters of X using distance definition dist\n\nwarning: Warning\nThis function is deprecated and will possibly be removed in BetaML 0.9. Use KMedoidsClusterer instead. \n\nParameters:\n\nX: a (n x d) data to clusterise\nK: Number of cluster wonted\ndist: Function to employ as distance (see notes). Default to Euclidean distance.\ninitialisation_strategy: Whether to select the initial representative vectors:\nrandom: randomly in the X space\ngrid: using a grid approach\nshuffle: selecting randomly within the available points [default]\ngiven: using a provided set of initial representatives provided in the initial_representatives parameter\ninitial_representatives: Provided (K x D) matrix of initial representatives (used only together with the given initialisation_strategy) [default: nothing]\nrng: Random Number Generator (see FIXEDSEED) [deafult: Random.GLOBAL_RNG]\n\nReturns:\n\nA tuple of two items, the first one being a vector of size N of ids of the clusters associated to each point and the second one the (K x D) matrix of representatives\n\nNotes:\n\nSome returned clusters could be empty\nThe dist parameter can be:\nAny user defined function accepting two vectors and returning a scalar\nAn anonymous function with the same characteristics (e.g. dist = (x,y) -> norm(x-y)^2)\nOne of the above predefined distances: l1_distance, l2_distance, l2squared_distance, cosine_distance\n\nExample:\n\njulia> (clIdx,Z) = kmedoids([1 10.5;1.5 10.8; 1.8 8; 1.7 15; 3.2 40; 3.6 32; 3.3 38; 5.1 -2.3; 5.2 -2.4],3,initialisation_strategy=\"grid\")\n\n\n\n\n\n","category":"method"},{"location":"Perceptron.html#perceptron_module","page":"Perceptron","title":"The BetaML.Perceptron Module","text":"","category":"section"},{"location":"Perceptron.html","page":"Perceptron","title":"Perceptron","text":"Perceptron","category":"page"},{"location":"Perceptron.html#BetaML.Perceptron","page":"Perceptron","title":"BetaML.Perceptron","text":"Perceptron module\n\nProvide linear and kernel classifiers.\n\nProvide the following supervised models:\n\nPerceptronClassifier: Train data using the classical perceptron\nKernelPerceptronClassifier: Train data using the kernel perceptron\nPegasosClassifier: Train data using the pegasos algorithm\n\nAll algorithms are multiclass, with PerceptronClassifier and PegasosClassifier employing a one-vs-all strategy, while KernelPerceptronClassifier employs a one-vs-one approach, and return a \"probability\" for each class in term of a dictionary for each record. Use mode(ŷ) to return a single class prediction per record.\n\nThese models are available in the MLJ framework as LinearPerceptron,KernelPerceptron and Pegasos respectivly.\n\n\n\n\n\n","category":"module"},{"location":"Perceptron.html#Module-Index","page":"Perceptron","title":"Module Index","text":"","category":"section"},{"location":"Perceptron.html","page":"Perceptron","title":"Perceptron","text":"Modules = [Perceptron]\nOrder   = [:constant, :type, :function, :macro]\nPrivate = false","category":"page"},{"location":"Perceptron.html#Detailed-API","page":"Perceptron","title":"Detailed API","text":"","category":"section"},{"location":"Perceptron.html","page":"Perceptron","title":"Perceptron","text":"Modules = [Perceptron]\nPrivate = false","category":"page"},{"location":"Perceptron.html#BetaML.Perceptron.KernelPerceptron","page":"Perceptron","title":"BetaML.Perceptron.KernelPerceptron","text":"The kernel perceptron algorithm using one-vs-one for multiclass, from the Beta Machine Learning Toolkit (BetaML).\n\n\n\n\n\n","category":"type"},{"location":"Perceptron.html#BetaML.Perceptron.KernelPerceptronClassifier","page":"Perceptron","title":"BetaML.Perceptron.KernelPerceptronClassifier","text":"mutable struct KernelPerceptronClassifier <: BetaMLSupervisedModel\n\nA \"kernel\" version of the Perceptron model (supervised) with user configurable kernel function.\n\nFor the parameters see ?KernelPerceptronClassifierHyperParametersSet and ?BetaMLDefaultOptionsSet\n\nLimitations:\n\ndata must be numerical\nonline training (retraining) is not supported\n\n\n\n\n\n","category":"type"},{"location":"Perceptron.html#BetaML.Perceptron.KernelPerceptronClassifierHyperParametersSet","page":"Perceptron","title":"BetaML.Perceptron.KernelPerceptronClassifierHyperParametersSet","text":"mutable struct KernelPerceptronClassifierHyperParametersSet <: BetaMLHyperParametersSet\n\nHyperparameters for the KernelPerceptronClassifier model\n\nParameters:\n\nkernel\nKernel function to employ. See ?radial_kernel or ?polynomial_kernel for details or check ?BetaML.Utils to verify if other kernels are defined (you can alsways define your own kernel) [def: radial_kernel]\ninitial_errors\nInitial distribution of the number of errors errors [def: nothing, i.e. zeros]. If provided, this should be a nModels-lenght vector of nRecords integer values vectors , where nModels is computed as (n_classes  * (n_classes - 1)) / 2\nepochs\nMaximum number of epochs, i.e. passages trough the whole training sample [def: 100]\nshuffle\nWhether to randomly shuffle the data at each iteration (epoch) [def: false]\n\n\n\n\n\n","category":"type"},{"location":"Perceptron.html#BetaML.Perceptron.LinearPerceptron","page":"Perceptron","title":"BetaML.Perceptron.LinearPerceptron","text":"The classical perceptron algorithm using one-vs-all for multiclass, from the Beta Machine Learning Toolkit (BetaML).\n\n\n\n\n\n","category":"type"},{"location":"Perceptron.html#BetaML.Perceptron.Pegasos","page":"Perceptron","title":"BetaML.Perceptron.Pegasos","text":"The gradient-based linear \"pegasos\" classifier using one-vs-all for multiclass, from the Beta Machine Learning Toolkit (BetaML).\n\n\n\n\n\n","category":"type"},{"location":"Perceptron.html#BetaML.Perceptron.PegasosClassifier","page":"Perceptron","title":"BetaML.Perceptron.PegasosClassifier","text":"mutable struct PegasosClassifier <: BetaMLSupervisedModel\n\nThe PegasosClassifier model, a linear, gradient-based classifier. Multiclass is supported using a one-vs-all approach.\n\nSee ?PegasosClassifierHyperParametersSet and ?BetaMLDefaultOptionsSet for applicable hyperparameters and options. \n\n\n\n\n\n","category":"type"},{"location":"Perceptron.html#BetaML.Perceptron.PegasosClassifierHyperParametersSet","page":"Perceptron","title":"BetaML.Perceptron.PegasosClassifierHyperParametersSet","text":"mutable struct PegasosClassifierHyperParametersSet <: BetaMLHyperParametersSet\n\nHyperparameters for the PegasosClassifier model.\n\nParameters:\n\nlearning_rate::Function\nLearning rate [def: (epoch -> 1/sqrt(epoch))]\nlearning_rate_multiplicative::Float64\nMultiplicative term of the learning rate [def: 0.5]\ninitial_parameters::Union{Nothing, Matrix{Float64}}\nInitial parameters. If given, should be a matrix of n-classes by feature dimension + 1 (to include the constant term as the first element) [def: nothing, i.e. zeros]\nepochs::Int64\nMaximum number of epochs, i.e. passages trough the whole training sample [def: 1000]\nshuffle::Bool\nWhether to randomly shuffle the data at each iteration (epoch) [def: false]\nforce_origin::Bool\nWhether to force the parameter associated with the constant term to remain zero [def: false]\nreturn_mean_hyperplane::Bool\nWhether to return the average hyperplane coefficients instead of the final ones  [def: false]\n\n\n\n\n\n","category":"type"},{"location":"Perceptron.html#BetaML.Perceptron.PerceptronClassifier","page":"Perceptron","title":"BetaML.Perceptron.PerceptronClassifier","text":"mutable struct PerceptronClassifier <: BetaMLSupervisedModel\n\nThe classical \"perceptron\" linear classifier (supervised).\n\nFor the parameters see ?PerceptronClassifierHyperParametersSet and ?BetaMLDefaultOptionsSet.\n\nNotes:\n\ndata must be numerical\nonline fitting (re-fitting with new data) is not supported\n\n\n\n\n\n","category":"type"},{"location":"Perceptron.html#BetaML.Perceptron.PerceptronClassifierHyperParametersSet","page":"Perceptron","title":"BetaML.Perceptron.PerceptronClassifierHyperParametersSet","text":"mutable struct PerceptronClassifierHyperParametersSet <: BetaMLHyperParametersSet\n\nHyperparameters for the PerceptronClassifier model\n\nParameters:\n\ninitial_parameters::Union{Nothing, Matrix{Float64}}\nInitial parameters. If given, should be a matrix of n-classes by feature dimension + 1 (to include the constant term as the first element) [def: nothing, i.e. zeros]\nepochs::Int64\nMaximum number of epochs, i.e. passages trough the whole training sample [def: 1000]\nshuffle::Bool\nWhether to randomly shuffle the data at each iteration (epoch) [def: false]\nforce_origin::Bool\nWhether to force the parameter associated with the constant term to remain zero [def: false]\nreturn_mean_hyperplane::Bool\nWhether to return the average hyperplane coefficients instead of the final ones  [def: false]\n\n\n\n\n\n","category":"type"},{"location":"Perceptron.html#BetaML.Api.predict","page":"Perceptron","title":"BetaML.Api.predict","text":"predict(x,θ,θ₀)\n\nPredict a binary label {-1,1} given the feature vector and the linear coefficients\n\nwarning: Warning\nThis function is deprecated and will possibly be removed in BetaML 0.9. Use the predict function with your desired model instead. \n\nParameters:\n\nx:        Feature matrix of the training data (n × d)\nθ:        The trained parameters\nθ₀:       The trained bias barameter [def: 0]\n\nReturn :\n\ny: Vector of the predicted labels\n\nExample:\n\njulia> predict([1.1 2.1; 5.3 4.2; 1.8 1.7], [3.2,1.2])\n\n\n\n\n\n","category":"function"},{"location":"Perceptron.html#BetaML.Api.predict-NTuple{4, Any}","page":"Perceptron","title":"BetaML.Api.predict","text":"predict(x,xtrain,ytrain,α;K)\n\nPredict a binary label {-1,1} given the feature vector and the training data together with their errors (as trained by a kernel perceptron algorithm)\n\nwarning: Warning\nThis function is deprecated and will possibly be removed in BetaML 0.9. Use the predict function with your desired model instead. \n\nParameters:\n\nx:      Feature matrix of the data to predict (n × d)\nxtrain: The feature vectors used for the training\nytrain: The labels of the training set\nα:      The errors associated to each record\nK:      The kernel function used for the training and to be used for the prediction [def: radial_kernel]\n\nReturn :\n\ny: Vector of the predicted labels\n\nExample:\n\njulia> predict([1.1 2.1; 5.3 4.2; 1.8 1.7], [3.2,1.2])\n\n\n\n\n\n","category":"method"},{"location":"Perceptron.html#BetaML.Api.predict-Tuple{KernelPerceptronClassifier, Any}","page":"Perceptron","title":"BetaML.Api.predict","text":"predict(m::KernelPerceptronClassifier, X) -> Any\n\n\nPredict labels using a fitted KernelPerceptronClassifier model.\n\n\n\n\n\n","category":"method"},{"location":"Perceptron.html#BetaML.Api.predict-Tuple{PegasosClassifier, Any}","page":"Perceptron","title":"BetaML.Api.predict","text":"predict(m::PegasosClassifier, X) -> Any\n\n\nPredict labels using a fitted PegasosClassifier model.\n\n\n\n\n\n","category":"method"},{"location":"Perceptron.html#BetaML.Api.predict-Tuple{PerceptronClassifier, Any}","page":"Perceptron","title":"BetaML.Api.predict","text":"predict(m::PerceptronClassifier, X) -> Any\n\n\nPredict the labels associated to some feature data using the linear coefficients learned by fitting a PerceptronClassifier model\n\n\n\n\n\n","category":"method"},{"location":"Perceptron.html#BetaML.Api.predict-Union{Tuple{Tcl}, Tuple{Any, Any, Any, Any, AbstractVector{Tcl}}} where Tcl","page":"Perceptron","title":"BetaML.Api.predict","text":"predict(x,xtrain,ytrain,α,classes;K)\n\nPredict a multiclass label given the new feature vector and a trained kernel perceptron model.\n\nwarning: Warning\n\n\nThis function is deprecated and will possibly be removed in BetaML 0.9.    Use the predict function with your desired model instead. \n\nParameters:\n\nx:      Feature matrix of the data to predict (n × d)\nxtrain: A vector of the feature matrix used for training each of the one-vs-one class matches (i.e. model.x)\nytrain: A vector of the label vector used for training each of the one-vs-one class matches (i.e. model.y)\nα:      A vector of the errors associated to each record (i.e. model.α)\nclasses: The overal classes encountered in training (i.e. model.classes)\nK:      The kernel function used for the training and to be used for the prediction [def: radial_kernel]\n\nReturn :\n\nŷ: Vector of dictionaries label=>probability (warning: it isn't really a probability, it is just the standardized number of matches \"won\" by this class compared with the other classes)\n\nNotes:\n\nUse mode(ŷ) if you want a single predicted label per record\n\nExample:\n\njulia> model  = kernelPerceptron([1.1 2.1; 5.3 4.2; 1.8 1.7], [-1,1,-1])\njulia> ŷtrain = Perceptron.predict([10 10; 2.2 2.5],model.x,model.y,model.α, model.classes,K=model.K)\n\n\n\n\n\n","category":"method"},{"location":"Perceptron.html#BetaML.Api.predict-Union{Tuple{Tcl}, Tuple{T}, Tuple{Any, AbstractVector{T}, AbstractVector{Float64}, Vector{Tcl}}} where {T<:AbstractVector{Float64}, Tcl}","page":"Perceptron","title":"BetaML.Api.predict","text":"predict(x,θ,θ₀,classes)\n\nPredict a multiclass label given the feature vector, the linear coefficients and the classes vector\n\nwarning: Warning\nThis function is deprecated and will possibly be removed in BetaML 0.9. Use the predict function of your desired model instead. \n\nParameters:\n\nx:       Feature matrix of the training data (n × d)\nθ:       Vector of the trained parameters for each one-vs-all model (i.e. model.θ)\nθ₀:      Vector of the trained bias barameter for each one-vs-all model (i.e. model.θ₀)\nclasses: The overal classes encountered in training (i.e. model.classes)\n\nReturn :\n\nŷ: Vector of dictionaries label=>probability\n\nNotes:\n\nUse mode(ŷ) if you want a single predicted label per record\n\nExample:\n\n```julia julia> model  = perceptron([1.1 2.1; 5.3 4.2; 1.8 1.7], [-1,1,-1]) julia> ŷtrain = predict([10 10; 2.5 2.5],model.θ,model.θ₀, model.classes)\n\n\n\n\n\n","category":"method"},{"location":"Perceptron.html#BetaML.Perceptron.kernelPerceptron-Tuple{Any, Any}","page":"Perceptron","title":"BetaML.Perceptron.kernelPerceptron","text":"kernelPerceptron(x,y;K,T,α,nMsgs,shuffle)\n\nTrain a multiclass kernel classifier \"perceptron\" algorithm based on x and y.\n\nwarning: Warning\nThis function is deprecated and will possibly be removed in BetaML 0.9. Use the model kernelPerceptron() instead. \n\nkernelPerceptron is a (potentially) non-linear perceptron-style classifier employing user-defined kernel funcions. Multiclass is supported using a one-vs-one approach.\n\nParameters:\n\nx:        Feature matrix of the training data (n × d)\ny:        Associated labels of the training data\nK:        Kernel function to employ. See ?radial_kernel or ?polynomial_kernelfor details or check ?BetaML.Utils to verify if other kernels are defined (you can alsways define your own kernel) [def: radial_kernel]\nT:        Maximum number of iterations (aka \"epochs\") across the whole set (if the set is not fully classified earlier) [def: 100]\nα:        Initial distribution of the number of errors errors [def: nothing, i.e. zeros]. If provided, this should be a nModels-lenght vector of nRecords integer values vectors , where nModels is computed as (n_classes  * (n_classes - 1)) / 2\nnMsg:     Maximum number of messages to show if all iterations are done [def: 0]\nshuffle:  Whether to randomly shuffle the data at each iteration [def: false]\nrng:      Random Number Generator (see FIXEDSEED) [deafult: Random.GLOBAL_RNG]\n\nReturn a named tuple with:\n\nx: The x data (eventually shuffled if shuffle=true)\ny: The label\nα: The errors associated to each record\nclasses: The labels classes encountered in the training\n\nNotes:\n\nThe trained model can then be used to make predictions using the function predict().\nThis model is available in the MLJ framework as the KernelPerceptron\n\nExample:\n\njulia> model = kernelPerceptron([1.1 1.1; 5.3 4.2; 1.8 1.7; 7.5 5.2;], [\"a\",\"c\",\"b\",\"c\"])\njulia> ŷtest = Perceptron.predict([10 10; 2.2 2.5; 1 1],model.x,model.y,model.α, model.classes,K=model.K)\n\n\n\n\n\n","category":"method"},{"location":"Perceptron.html#BetaML.Perceptron.kernelPerceptronBinary-Tuple{Any, Any}","page":"Perceptron","title":"BetaML.Perceptron.kernelPerceptronBinary","text":"kernelPerceptronBinary(x,y;K,T,α,nMsgs,shuffle)\n\nTrain a binary kernel classifier \"perceptron\" algorithm based on x and y\n\nwarning: Warning\nThis function is deprecated and will possibly be removed in BetaML 0.9. Use the model KernelPerceptronClassifier() instead. \n\nParameters:\n\nx:        Feature matrix of the training data (n × d)\ny:        Associated labels of the training data, in the format of ⨦ 1\nK:        Kernel function to employ. See ?radial_kernel or ?polynomial_kernelfor details or check ?BetaML.Utils to verify if other kernels are defined (you can alsways define your own kernel) [def: radial_kernel]\nT:        Maximum number of iterations across the whole set (if the set is not fully classified earlier) [def: 1000]\nα:        Initial distribution of the errors [def: zeros(length(y))]\nnMsg:     Maximum number of messages to show if all iterations are done\nshuffle:  Whether to randomly shuffle the data at each iteration [def: false]\nrng:      Random Number Generator (see FIXEDSEED) [deafult: Random.GLOBAL_RNG]\n\nReturn a named tuple with:\n\nx: the x data (eventually shuffled if shuffle=true)\ny: the label\nα: the errors associated to each record\nerrors: the number of errors in the last iteration\nbesterrors: the minimum number of errors in classifying the data ever reached\niterations: the actual number of iterations performed\nseparated: a flag if the data has been successfully separated\n\nNotes:\n\nThe trained data can then be used to make predictions using the function predict(). If the option shuffle has been used, it is important to use there the returned (x,y,α) as these would have been shuffled compared with the original (x,y).\nPlease see @kernelPerceptron for a multi-class version\n\nExample:\n\njulia> model = kernelPerceptronBinary([1.1 2.1; 5.3 4.2; 1.8 1.7], [-1,1,-1])\n\n\n\n\n\n","category":"method"},{"location":"Perceptron.html#BetaML.Perceptron.pegasos-Tuple{Any, Any}","page":"Perceptron","title":"BetaML.Perceptron.pegasos","text":"pegasos(x,y;θ,θ₀,λ,η,T,nMsgs,shuffle,forceorigin,returnmean_hyperplane)\n\nTrain the multiclass classifier \"pegasos\" algorithm according to x (features) and y (labels)\n\nwarning: Warning\nThis function is deprecated and will possibly be removed in BetaML 0.9. Use the model PegasosClassifier instead. \n\nPegasosClassifier is a linear, gradient-based classifier. Multiclass is supported using a one-vs-all approach.\n\nParameters:\n\nx:           Feature matrix of the training data (n × d)\ny:           Associated labels of the training data, can be in any format (string, integers..)\nθ:           Initial value of the weights (parameter) [def: zeros(d)]\nθ₀:          Initial value of the weight (parameter) associated to the constant term [def: 0]\nλ:           Multiplicative term of the learning rate\nη:           Learning rate [def: (t -> 1/sqrt(t))]\nT:           Maximum number of iterations across the whole set (if the set is not fully classified earlier) [def: 1000]\nnMsg:        Maximum number of messages to show if all iterations are done\nshuffle:     Whether to randomly shuffle the data at each iteration [def: false]\nforce_origin: Whether to force θ₀ to remain zero [def: false]\nreturn_mean_hyperplane: Whether to return the average hyperplane coefficients instead of the average ones  [def: false]\nrng:         Random Number Generator (see FIXEDSEED) [deafult: Random.GLOBAL_RNG]\n\nReturn a named tuple with:\n\nθ:          The weights of the classifier\nθ₀:         The weight of the classifier associated to the constant term\nclasses:    The classes (unique values) of y\n\nNotes:\n\nThe trained parameters can then be used to make predictions using the function predict().\nThis model is available in the MLJ framework as the Pegasos\n\nExample:\n\njulia> model = pegasos([1.1 2.1; 5.3 4.2; 1.8 1.7], [-1,1,-1])\njulia> ŷ     = predict([2.1 3.1; 7.3 5.2], model.θ, model.θ₀, model.classes)\n\n\n\n\n\n","category":"method"},{"location":"Perceptron.html#BetaML.Perceptron.pegasosBinary-Tuple{Any, Any}","page":"Perceptron","title":"BetaML.Perceptron.pegasosBinary","text":"pegasosBinary(x,y;θ,θ₀,λ,η,T,nMsgs,shuffle,force_origin)\n\nTrain the peagasos algorithm based on x and y (labels)\n\nwarning: Warning\nThis function is deprecated and will possibly be removed in BetaML 0.9. Use the model PegasosClassifier instead. \n\nParameters:\n\nx:           Feature matrix of the training data (n × d)\ny:           Associated labels of the training data, in the format of ⨦ 1\nθ:           Initial value of the weights (parameter) [def: zeros(d)]\nθ₀:          Initial value of the weight (parameter) associated to the constant term [def: 0]\nλ:           Multiplicative term of the learning rate\nη:           Learning rate [def: (t -> 1/sqrt(t))]\nT:           Maximum number of iterations across the whole set (if the set is not fully classified earlier) [def: 1000]\nnMsg:        Maximum number of messages to show if all iterations are done\nshuffle:    Whether to randomly shuffle the data at each iteration [def: false]\nforce_origin: Whether to force θ₀ to remain zero [def: false]\n\nReturn a named tuple with:\n\nθ:          The final weights of the classifier\nθ₀:         The final weight of the classifier associated to the constant term\navgθ:       The average weights of the classifier\navgθ₀:      The average weight of the classifier associated to the constant term\nerrors:     The number of errors in the last iteration\nbesterrors: The minimum number of errors in classifying the data ever reached\niterations: The actual number of iterations performed\nseparated:  Weather the data has been successfully separated\n\nNotes:\n\nThe trained parameters can then be used to make predictions using the function predict().\n\nExample:\n\njulia> pegasos([1.1 2.1; 5.3 4.2; 1.8 1.7], [-1,1,-1])\n\n\n\n\n\n","category":"method"},{"location":"Perceptron.html#BetaML.Perceptron.perceptron-Tuple{AbstractMatrix{T} where T, AbstractVector{T} where T}","page":"Perceptron","title":"BetaML.Perceptron.perceptron","text":"perceptron(x,y;θ,θ₀,T,nMsgs,shuffle,force_origin,return_mean_hyperplane)\n\nTrain the multiclass classifier \"perceptron\" algorithm  based on x and y (labels).\n\nwarning: Warning\nThis function is deprecated and will possibly be removed in BetaML 0.9. Use the model PerceptronClassifier instead. \n\nThe perceptron is a linear classifier. Multiclass is supported using a one-vs-all approach.\n\nParameters:\n\nx:           Feature matrix of the training data (n × d)\ny:           Associated labels of the training data, can be in any format (string, integers..)\nθ:           Initial value of the weights (parameter) [def: zeros(d)]\nθ₀:          Initial value of the weight (parameter) associated to the constant            term [def: 0]\nT:           Maximum number of iterations across the whole set (if the set            is not fully classified earlier) [def: 1000]\nnMsg:        Maximum number of messages to show if all iterations are done [def: 0]\nshuffle:     Whether to randomly shuffle the data at each iteration [def: false]\nforce_origin: Whether to force θ₀ to remain zero [def: false]\nreturn_mean_hyperplane: Whether to return the average hyperplane coefficients instead of the final ones  [def: false]\nrng:         Random Number Generator (see FIXEDSEED) [deafult: Random.GLOBAL_RNG]\n\nReturn a named tuple with:\n\nθ:          The weights of the classifier\nθ₀:         The weight of the classifier associated to the constant term\nclasses:    The classes (unique values) of y\n\nNotes:\n\nThe trained parameters can then be used to make predictions using the function predict().\nThis model is available in the MLJ framework as the LinearPerceptron\n\nExample:\n\njulia> model = perceptron([1.1 2.1; 5.3 4.2; 1.8 1.7], [-1,1,-1])\njulia> ŷ     = predict([2.1 3.1; 7.3 5.2], model.θ, model.θ₀, model.classes)\n\n\n\n\n\n","category":"method"},{"location":"Perceptron.html#BetaML.Perceptron.perceptronBinary-Tuple{Any, Any}","page":"Perceptron","title":"BetaML.Perceptron.perceptronBinary","text":"perceptronBinary(x,y;θ,θ₀,T,nMsgs,shuffle,force_origin)\n\nwarning: Warning\nThis function is deprecated and will possibly be removed in BetaML 0.9. Use the model PerceptronClassifier() instead. \n\nTrain the binary classifier \"perceptron\" algorithm based on x and y (labels)\n\nParameters:\n\nx:           Feature matrix of the training data (n × d)\ny:           Associated labels of the training data, in the format of ⨦ 1\nθ:           Initial value of the weights (parameter) [def: zeros(d)]\nθ₀:          Initial value of the weight (parameter) associated to the constant            term [def: 0]\nT:           Maximum number of iterations across the whole set (if the set            is not fully classified earlier) [def: 1000]\nnMsg:        Maximum number of messages to show if all iterations are done\nshuffle:     Whether to randomly shuffle the data at each iteration [def: false]\nforce_origin: Whether to force θ₀ to remain zero [def: false]\nrng:         Random Number Generator (see FIXEDSEED) [deafult: Random.GLOBAL_RNG]\n\nReturn a named tuple with:\n\nθ:          The final weights of the classifier\nθ₀:         The final weight of the classifier associated to the constant term\navgθ:       The average weights of the classifier\navgθ₀:      The average weight of the classifier associated to the constant term\nerrors:     The number of errors in the last iteration\nbesterrors: The minimum number of errors in classifying the data ever reached\niterations: The actual number of iterations performed\nseparated:  Weather the data has been successfully separated\n\nNotes:\n\nThe trained parameters can then be used to make predictions using the function predict().\n\nExample:\n\njulia> model = perceptronBinary([1.1 2.1; 5.3 4.2; 1.8 1.7], [-1,1,-1])\n\n\n\n\n\n","category":"method"},{"location":"tutorials/Classification - cars/betaml_tutorial_classification_cars.html","page":"A classification task when labels are known - determining the country of origin of cars given the cars characteristics","title":"A classification task when labels are known - determining the country of origin of cars given the cars characteristics","text":"EditURL = \"https://github.com/sylvaticus/BetaML.jl/blob/master/docs/src/tutorials/Classification - cars/betaml_tutorial_classification_cars.jl\"","category":"page"},{"location":"tutorials/Classification - cars/betaml_tutorial_classification_cars.html#classification_tutorial","page":"A classification task when labels are known - determining the country of origin of cars given the cars characteristics","title":"A classification task when labels are known - determining the country of origin of cars given the cars characteristics","text":"","category":"section"},{"location":"tutorials/Classification - cars/betaml_tutorial_classification_cars.html","page":"A classification task when labels are known - determining the country of origin of cars given the cars characteristics","title":"A classification task when labels are known - determining the country of origin of cars given the cars characteristics","text":"In this exercise we have some car technical characteristics (mpg, horsepower,weight, model year...) and the country of origin and we would like to create a model such that the country of origin can be accurately predicted given the technical characteristics. As the information to predict is a multi-class one, this is a [classification](https://en.wikipedia.org/wiki/Statistical_classification) task. It is a challenging exercise due to the simultaneous presence of three factors: (1) presence of missing data; (2) unbalanced data - 254 out of 406 cars are US made; (3) small dataset.","category":"page"},{"location":"tutorials/Classification - cars/betaml_tutorial_classification_cars.html","page":"A classification task when labels are known - determining the country of origin of cars given the cars characteristics","title":"A classification task when labels are known - determining the country of origin of cars given the cars characteristics","text":"Data origin:","category":"page"},{"location":"tutorials/Classification - cars/betaml_tutorial_classification_cars.html","page":"A classification task when labels are known - determining the country of origin of cars given the cars characteristics","title":"A classification task when labels are known - determining the country of origin of cars given the cars characteristics","text":"dataset description: https://archive.ics.uci.edu/ml/datasets/auto+mpg\ndata source we use here: https://archive.ics.uci.edu/ml/machine-learning-databases/auto-mpg/auto-mpg.data","category":"page"},{"location":"tutorials/Classification - cars/betaml_tutorial_classification_cars.html","page":"A classification task when labels are known - determining the country of origin of cars given the cars characteristics","title":"A classification task when labels are known - determining the country of origin of cars given the cars characteristics","text":"Field description:","category":"page"},{"location":"tutorials/Classification - cars/betaml_tutorial_classification_cars.html","page":"A classification task when labels are known - determining the country of origin of cars given the cars characteristics","title":"A classification task when labels are known - determining the country of origin of cars given the cars characteristics","text":"mpg:           continuous\ncylinders:     multi-valued discrete\ndisplacement:  continuous\nhorsepower:    continuous\nweight:        continuous\nacceleration:  continuous\nmodel year:    multi-valued discrete\norigin:        multi-valued discrete\ncar name:      string (unique for each instance) - not used here","category":"page"},{"location":"tutorials/Classification - cars/betaml_tutorial_classification_cars.html","page":"A classification task when labels are known - determining the country of origin of cars given the cars characteristics","title":"A classification task when labels are known - determining the country of origin of cars given the cars characteristics","text":"!!! warning As the above example is automatically executed by GitHub on every code update, it uses parameters (epoch numbers, parameter space of hyperparameter validation, number of trees,...) that minimise the computation. In real case you will want to use better but more computationally intensive ones. For the same reason benchmarks codes are commented and the pre-run output reported rather than actually being executed.","category":"page"},{"location":"tutorials/Classification - cars/betaml_tutorial_classification_cars.html#Library-and-data-loading","page":"A classification task when labels are known - determining the country of origin of cars given the cars characteristics","title":"Library and data loading","text":"","category":"section"},{"location":"tutorials/Classification - cars/betaml_tutorial_classification_cars.html","page":"A classification task when labels are known - determining the country of origin of cars given the cars characteristics","title":"A classification task when labels are known - determining the country of origin of cars given the cars characteristics","text":"We load a buch of packages that we'll use during this tutorial..","category":"page"},{"location":"tutorials/Classification - cars/betaml_tutorial_classification_cars.html","page":"A classification task when labels are known - determining the country of origin of cars given the cars characteristics","title":"A classification task when labels are known - determining the country of origin of cars given the cars characteristics","text":"using Random, HTTP, CSV, DataFrames, BenchmarkTools, BetaML\nimport DecisionTree, Flux\nimport Pipe: @pipe","category":"page"},{"location":"tutorials/Classification - cars/betaml_tutorial_classification_cars.html","page":"A classification task when labels are known - determining the country of origin of cars given the cars characteristics","title":"A classification task when labels are known - determining the country of origin of cars given the cars characteristics","text":"To load the data from the internet our workflow is (1) Retrieve the data –> (2) Clean it –> (3) Load it –> (4) Output it as a DataFrame.","category":"page"},{"location":"tutorials/Classification - cars/betaml_tutorial_classification_cars.html","page":"A classification task when labels are known - determining the country of origin of cars given the cars characteristics","title":"A classification task when labels are known - determining the country of origin of cars given the cars characteristics","text":"For step (1) we use HTTP.get(), for step (2) we use replace!, for steps (3) and (4) we uses the CSV package, and we use the \"pip\" |> operator to chain these operations:","category":"page"},{"location":"tutorials/Classification - cars/betaml_tutorial_classification_cars.html","page":"A classification task when labels are known - determining the country of origin of cars given the cars characteristics","title":"A classification task when labels are known - determining the country of origin of cars given the cars characteristics","text":"urlDataOriginal = \"https://archive.ics.uci.edu/ml/machine-learning-databases/auto-mpg/auto-mpg.data-original\"\ndata = @pipe HTTP.get(urlDataOriginal).body                                                |>\n             replace!(_, UInt8('\\t') => UInt8(' '))                                        |>\n             CSV.File(_, delim=' ', missingstring=\"NA\", ignorerepeated=true, header=false) |>\n             DataFrame;\nnothing #hide","category":"page"},{"location":"tutorials/Classification - cars/betaml_tutorial_classification_cars.html","page":"A classification task when labels are known - determining the country of origin of cars given the cars characteristics","title":"A classification task when labels are known - determining the country of origin of cars given the cars characteristics","text":"This results in a table where the rows are the observations (the various cars) and the column the fields. All BetaML models expect this layout.","category":"page"},{"location":"tutorials/Classification - cars/betaml_tutorial_classification_cars.html","page":"A classification task when labels are known - determining the country of origin of cars given the cars characteristics","title":"A classification task when labels are known - determining the country of origin of cars given the cars characteristics","text":"As the dataset is ordered, we randomly shuffle the data. Note that we pass to shuffle copy(FIXEDRNG) as the random nuber generator in order to obtain reproducible output ( FIXEDRNG is nothing else than an istance of StableRNG(123) defined in the BetaML.Utils sub-module, but you can choose of course your own \"fixed\" RNG). See the Dealing with stochasticity section in the Getting started tutorial for details.","category":"page"},{"location":"tutorials/Classification - cars/betaml_tutorial_classification_cars.html","page":"A classification task when labels are known - determining the country of origin of cars given the cars characteristics","title":"A classification task when labels are known - determining the country of origin of cars given the cars characteristics","text":"data[shuffle(copy(FIXEDRNG),axes(data, 1)), :]\ndescribe(data)","category":"page"},{"location":"tutorials/Classification - cars/betaml_tutorial_classification_cars.html","page":"A classification task when labels are known - determining the country of origin of cars given the cars characteristics","title":"A classification task when labels are known - determining the country of origin of cars given the cars characteristics","text":"Columns 1 to 7 contain  characteristics of the car, while column 8 encodes the country or origin (\"1\" -> US, \"2\" -> EU, \"3\" -> Japan). That's the variable we want to be able to predict.","category":"page"},{"location":"tutorials/Classification - cars/betaml_tutorial_classification_cars.html","page":"A classification task when labels are known - determining the country of origin of cars given the cars characteristics","title":"A classification task when labels are known - determining the country of origin of cars given the cars characteristics","text":"Columns 9 contains the car name, but we are not going to use this information in this tutorial. Note also that some fields have missing data.","category":"page"},{"location":"tutorials/Classification - cars/betaml_tutorial_classification_cars.html","page":"A classification task when labels are known - determining the country of origin of cars given the cars characteristics","title":"A classification task when labels are known - determining the country of origin of cars given the cars characteristics","text":"Our first step is hence to divide the dataset in features (the x) and the labels (the y) we want to predict. The x is then a Julia standard Matrix of 406 rows by 7 columns and the y is a vector of the 406 observations:","category":"page"},{"location":"tutorials/Classification - cars/betaml_tutorial_classification_cars.html","page":"A classification task when labels are known - determining the country of origin of cars given the cars characteristics","title":"A classification task when labels are known - determining the country of origin of cars given the cars characteristics","text":"x     = Matrix{Union{Missing,Float64}}(data[:,1:7]);\ny     = Vector{Int64}(data[:,8]);\nnothing #hide","category":"page"},{"location":"tutorials/Classification - cars/betaml_tutorial_classification_cars.html","page":"A classification task when labels are known - determining the country of origin of cars given the cars characteristics","title":"A classification task when labels are known - determining the country of origin of cars given the cars characteristics","text":"Some algorithms that we will use today don't work with missing data, so we need to impute them. We use the predictMissing function provided by the BetaML.Clustering sub-module. Internally the function uses a Gaussian Mixture Model to assign to the missing walue of a given record an average of the values of the non-missing records weighted for how close they are to our specific record. Note that the same function (predictMissing) can be used for Collaborative Filtering / recomendation systems. Using GMM has the advantage over traditional algorithms as k-nearest neighbors (KNN) that GMM can \"detect\" the hidden structure of the observed data, where some observation can be similar to a certain pool of other observvations for a certain characteristic, but similar to an other pool of observations for other characteristics.","category":"page"},{"location":"tutorials/Classification - cars/betaml_tutorial_classification_cars.html","page":"A classification task when labels are known - determining the country of origin of cars given the cars characteristics","title":"A classification task when labels are known - determining the country of origin of cars given the cars characteristics","text":"xFull = predictMissing(x,rng=copy(FIXEDRNG)).X̂;\nnothing #hide","category":"page"},{"location":"tutorials/Classification - cars/betaml_tutorial_classification_cars.html","page":"A classification task when labels are known - determining the country of origin of cars given the cars characteristics","title":"A classification task when labels are known - determining the country of origin of cars given the cars characteristics","text":"Further, some models don't work with categorical data as such, so we need to represent our y as a matrix with a separate column for each possible categorical value (the so called \"one-hot\" representation). For example, within a three classes field, the individual value 2 (or \"Europe\" for what it matters) would be represented as the vector [0 1 0], while 3 (or \"Japan\") would become the vector [0 0 1]. To encode as one-hot we use the function onehotencoder in BetaML.Utils","category":"page"},{"location":"tutorials/Classification - cars/betaml_tutorial_classification_cars.html","page":"A classification task when labels are known - determining the country of origin of cars given the cars characteristics","title":"A classification task when labels are known - determining the country of origin of cars given the cars characteristics","text":"y_oh  = onehotencoder(y);\nnothing #hide","category":"page"},{"location":"tutorials/Classification - cars/betaml_tutorial_classification_cars.html","page":"A classification task when labels are known - determining the country of origin of cars given the cars characteristics","title":"A classification task when labels are known - determining the country of origin of cars given the cars characteristics","text":"In supervised machine learning it is good practice to partition the available data in a training, validation, and test subsets, where the first one is used to train the ML algorithm, the second one to train any eventual \"hyper-parameters\" of the algorithm and the test subset is finally used to evaluate the quality of the algorithm. Here, for brevity, we use only the train and the test subsets, implicitly assuming we already know the best hyper-parameters. Please refer to the regression tutorial for examples of how to use the validation subset to train the hyper-parameters, or even better the clustering tutorial for an example of using the cross_validation function.","category":"page"},{"location":"tutorials/Classification - cars/betaml_tutorial_classification_cars.html","page":"A classification task when labels are known - determining the country of origin of cars given the cars characteristics","title":"A classification task when labels are known - determining the country of origin of cars given the cars characteristics","text":"We use then the partition function in BetaML.Utils, where we can specify the different data to partition (each matrix or vector to partition must have the same number of observations) and the shares of observation that we want in each subset. Here we keep 80% of observations for training (xtrain, xTrainFull and ytrain) and we use 20% of them for testing (xtest, xTestFull and ytest):","category":"page"},{"location":"tutorials/Classification - cars/betaml_tutorial_classification_cars.html","page":"A classification task when labels are known - determining the country of origin of cars given the cars characteristics","title":"A classification task when labels are known - determining the country of origin of cars given the cars characteristics","text":"((xtrain,xtest),(xtrainFull,xtestFull),(ytrain,ytest),(ytrain_oh,ytest_oh)) = partition([x,xFull,y,y_oh],[0.8,1-0.8],rng=copy(FIXEDRNG));\nnothing #hide","category":"page"},{"location":"tutorials/Classification - cars/betaml_tutorial_classification_cars.html#Random-Forests","page":"A classification task when labels are known - determining the country of origin of cars given the cars characteristics","title":"Random Forests","text":"","category":"section"},{"location":"tutorials/Classification - cars/betaml_tutorial_classification_cars.html","page":"A classification task when labels are known - determining the country of origin of cars given the cars characteristics","title":"A classification task when labels are known - determining the country of origin of cars given the cars characteristics","text":"We are now ready to use our first model, the Random Forests (in the BetaML.Trees sub-module). Random Forests build a \"forest\" of decision trees models and then average their predictions in order to make an overall prediction out of a feature vector.","category":"page"},{"location":"tutorials/Classification - cars/betaml_tutorial_classification_cars.html","page":"A classification task when labels are known - determining the country of origin of cars given the cars characteristics","title":"A classification task when labels are known - determining the country of origin of cars given the cars characteristics","text":"To \"build\" the forest model (i.e. to \"train\" it) we need to give the model the training feature matrix and the associated \"true\" training labels, and we need to specify the number of trees to employ (this is an example of hyper-parameters). Here we use 30 individual decision trees.","category":"page"},{"location":"tutorials/Classification - cars/betaml_tutorial_classification_cars.html","page":"A classification task when labels are known - determining the country of origin of cars given the cars characteristics","title":"A classification task when labels are known - determining the country of origin of cars given the cars characteristics","text":"As the labels are encoded using integers,  we need also to specify the parameter force_classification=true, otherwise the model would undergo a regression job instead.","category":"page"},{"location":"tutorials/Classification - cars/betaml_tutorial_classification_cars.html","page":"A classification task when labels are known - determining the country of origin of cars given the cars characteristics","title":"A classification task when labels are known - determining the country of origin of cars given the cars characteristics","text":"myForest       = buildForest(xtrain,ytrain,30, rng=copy(FIXEDRNG),force_classification=true);\nnothing #hide","category":"page"},{"location":"tutorials/Classification - cars/betaml_tutorial_classification_cars.html","page":"A classification task when labels are known - determining the country of origin of cars given the cars characteristics","title":"A classification task when labels are known - determining the country of origin of cars given the cars characteristics","text":"To obtain the predicted values, we can simply use the function BetaML.Trees.predict with our myForest model and either the training or the testing data.","category":"page"},{"location":"tutorials/Classification - cars/betaml_tutorial_classification_cars.html","page":"A classification task when labels are known - determining the country of origin of cars given the cars characteristics","title":"A classification task when labels are known - determining the country of origin of cars given the cars characteristics","text":"ŷtrain,ŷtest   = predict.(Ref(myForest), [xtrain,xtest],rng=copy(FIXEDRNG));\nnothing #hide","category":"page"},{"location":"tutorials/Classification - cars/betaml_tutorial_classification_cars.html","page":"A classification task when labels are known - determining the country of origin of cars given the cars characteristics","title":"A classification task when labels are known - determining the country of origin of cars given the cars characteristics","text":"Finally we can measure the accuracy of our predictions with the accuracy function, with the sidenote that we need first to \"parse\" the ŷs as forcing the classification job transformed automatically them to strings (they originally were integers):","category":"page"},{"location":"tutorials/Classification - cars/betaml_tutorial_classification_cars.html","page":"A classification task when labels are known - determining the country of origin of cars given the cars characteristics","title":"A classification task when labels are known - determining the country of origin of cars given the cars characteristics","text":"trainAccuracy,testAccuracy  = accuracy.([parse.(Int64,mode(ŷtrain,rng=copy(FIXEDRNG))),parse.(Int64,mode(ŷtest,rng=copy(FIXEDRNG)))],[ytrain,ytest])","category":"page"},{"location":"tutorials/Classification - cars/betaml_tutorial_classification_cars.html","page":"A classification task when labels are known - determining the country of origin of cars given the cars characteristics","title":"A classification task when labels are known - determining the country of origin of cars given the cars characteristics","text":"The predictions are quite good, for the training set the algoritm predicted almost all cars' origins correctly, while for the testing set (i.e. those records that has not been used to train the algorithm), the correct prediction level is still quite high, at 80%","category":"page"},{"location":"tutorials/Classification - cars/betaml_tutorial_classification_cars.html","page":"A classification task when labels are known - determining the country of origin of cars given the cars characteristics","title":"A classification task when labels are known - determining the country of origin of cars given the cars characteristics","text":"While accuracy can sometimes suffice, we may often want to better understand which categories our model has trouble to predict correctly. We can investigate the output of a multi-class classifier more in-deep with a ConfMatrix where the true values (y) are given in rows and the predicted ones (ŷ) in columns, together to some per-class metrics like the precision (true class i over predicted in class i), the recall (predicted class i over the true class i) and others.","category":"page"},{"location":"tutorials/Classification - cars/betaml_tutorial_classification_cars.html","page":"A classification task when labels are known - determining the country of origin of cars given the cars characteristics","title":"A classification task when labels are known - determining the country of origin of cars given the cars characteristics","text":"We fist build the ConfMatrix object between ŷ and y and then we print it (we do it here for the test subset):","category":"page"},{"location":"tutorials/Classification - cars/betaml_tutorial_classification_cars.html","page":"A classification task when labels are known - determining the country of origin of cars given the cars characteristics","title":"A classification task when labels are known - determining the country of origin of cars given the cars characteristics","text":"cm = ConfMatrix(parse.(Int64,mode(ŷtest,rng=copy(FIXEDRNG))),ytest,classes=[1,2,3],labels=[\"US\",\"EU\",\"Japan\"])\nprint(cm,\"all\")","category":"page"},{"location":"tutorials/Classification - cars/betaml_tutorial_classification_cars.html","page":"A classification task when labels are known - determining the country of origin of cars given the cars characteristics","title":"A classification task when labels are known - determining the country of origin of cars given the cars characteristics","text":"From the report we can see that Japanese cars have more trouble in being correctly classified, and in particular many Japanease cars are classified as US ones. This is likely a result of the class imbalance of the data set, and could be solved by balancing the dataset with various sampling tecniques before training the model.","category":"page"},{"location":"tutorials/Classification - cars/betaml_tutorial_classification_cars.html","page":"A classification task when labels are known - determining the country of origin of cars given the cars characteristics","title":"A classification task when labels are known - determining the country of origin of cars given the cars characteristics","text":"When we benchmark the resourse used (time and memory) we find that Random Forests remain pretty fast, expecially when we compare them with neural networks (see later) @btime buildForest(xtrain,ytrain,30, rng=copy(FIXEDRNG),force_classification=true); 134.096 ms (781027 allocations: 196.30 MiB)","category":"page"},{"location":"tutorials/Classification - cars/betaml_tutorial_classification_cars.html#Comparision-with-DecisionTree.jl","page":"A classification task when labels are known - determining the country of origin of cars given the cars characteristics","title":"Comparision with DecisionTree.jl","text":"","category":"section"},{"location":"tutorials/Classification - cars/betaml_tutorial_classification_cars.html","page":"A classification task when labels are known - determining the country of origin of cars given the cars characteristics","title":"A classification task when labels are known - determining the country of origin of cars given the cars characteristics","text":"DecisionTrees.jl random forests are similar in usage: we first \"build\" (train) the forest and we then make predictions out of the trained model. The main difference is that the model requires data with nonmissing values, so we are going to use the xtrainFull and xtestFull feature labels we created earlier:","category":"page"},{"location":"tutorials/Classification - cars/betaml_tutorial_classification_cars.html","page":"A classification task when labels are known - determining the country of origin of cars given the cars characteristics","title":"A classification task when labels are known - determining the country of origin of cars given the cars characteristics","text":"# We train the model...\nmodel = DecisionTree.build_forest(ytrain, xtrainFull,-1,30,rng=123)\n# ..and we generate predictions and measure their error\n(ŷtrain,ŷtest) = DecisionTree.apply_forest.([model],[xtrainFull,xtestFull]);\n(trainAccuracy,testAccuracy) = accuracy.([ŷtrain,ŷtest],[ytrain,ytest])","category":"page"},{"location":"tutorials/Classification - cars/betaml_tutorial_classification_cars.html","page":"A classification task when labels are known - determining the country of origin of cars given the cars characteristics","title":"A classification task when labels are known - determining the country of origin of cars given the cars characteristics","text":"While the accuracy on the training set is exactly the same as for BetaML random forets, DecisionTree.jl random forests are slighly less accurate in the testing sample. Where however DecisionTrees.jl excell is in the efficiency: they are extremelly fast and memory thrifty, even if to this benchmark we should add the resources needed to impute the missing values.","category":"page"},{"location":"tutorials/Classification - cars/betaml_tutorial_classification_cars.html","page":"A classification task when labels are known - determining the country of origin of cars given the cars characteristics","title":"A classification task when labels are known - determining the country of origin of cars given the cars characteristics","text":"Also, one of the reasons DecisionTrees are such efficient is that internally they sort the data to avoid repeated comparision, but in this way they work only with features that are sortable, while BetaML random forests accept virtually any kind of input without the need of adapt it. @btime  DecisionTree.build_forest(ytrain, xtrainFull,-1,30,rng=123); 1.431 ms (10875 allocations: 1.52 MiB)","category":"page"},{"location":"tutorials/Classification - cars/betaml_tutorial_classification_cars.html#Neural-network","page":"A classification task when labels are known - determining the country of origin of cars given the cars characteristics","title":"Neural network","text":"","category":"section"},{"location":"tutorials/Classification - cars/betaml_tutorial_classification_cars.html","page":"A classification task when labels are known - determining the country of origin of cars given the cars characteristics","title":"A classification task when labels are known - determining the country of origin of cars given the cars characteristics","text":"Neural networks (NN) can be very powerfull, but have two \"inconvenients\" compared with random forests: first, are a bit \"picky\". We need to do a bit of work to provide data in specific format. Note that this is not feature engineering. One of the advantages on neural network is that for the most this is not needed for neural networks. However we still need to \"clean\" the data. One issue is that NN don't like missing data. So we need to provide them with the feature matrix \"clean\" of missing data. Secondly, they work only with numerical data. So we need to use the one-hot encoding we saw earlier. Further, they work best if the features are scaled such that each feature has mean zero and standard deviation 1. We can achieve it with the function scale or, as in this case, get_scalefactors.","category":"page"},{"location":"tutorials/Classification - cars/betaml_tutorial_classification_cars.html","page":"A classification task when labels are known - determining the country of origin of cars given the cars characteristics","title":"A classification task when labels are known - determining the country of origin of cars given the cars characteristics","text":"xScaleFactors   = get_scalefactors(xtrainFull)\nD               = size(xtrainFull,2)\nclasses         = unique(y)\nnCl             = length(classes)","category":"page"},{"location":"tutorials/Classification - cars/betaml_tutorial_classification_cars.html","page":"A classification task when labels are known - determining the country of origin of cars given the cars characteristics","title":"A classification task when labels are known - determining the country of origin of cars given the cars characteristics","text":"The second \"inconvenient\" of NN is that, while not requiring feature engineering, they stil lneed a bit of practice on the way to build the network. It's not as simple as train(model,x,y). We need here to specify how we want our layers, chain the layers together and then decide a loss overall function. Only when we done these steps, we have the model ready for training. Here we define 2 DenseLayer where, for each of them, we specify the number of neurons in input (the first layer being equal to the dimensions of the data), the output layer (for a classification task, the last layer output size beying equal to the number of classes) and an activation function for each layer (default the identity function).","category":"page"},{"location":"tutorials/Classification - cars/betaml_tutorial_classification_cars.html","page":"A classification task when labels are known - determining the country of origin of cars given the cars characteristics","title":"A classification task when labels are known - determining the country of origin of cars given the cars characteristics","text":"ls   = 50\nl1   = DenseLayer(D,ls,f=relu,rng=copy(FIXEDRNG))\nl2   = DenseLayer(ls,nCl,f=relu,rng=copy(FIXEDRNG))","category":"page"},{"location":"tutorials/Classification - cars/betaml_tutorial_classification_cars.html","page":"A classification task when labels are known - determining the country of origin of cars given the cars characteristics","title":"A classification task when labels are known - determining the country of origin of cars given the cars characteristics","text":"For a classification the last layer is a VectorFunctionLayer that has no learnable parameters but whose activation function is applied to the ensemble of the neurons, rather than individually on each neuron. In particular, for classification we pass the BetaML.Utils.softmax function whose output has the same size as the input (and the number of classes to predict), but we can use the VectorFunctionLayer with any function, including the pool1d function to create a \"pooling\" layer (using maximum, mean or whatever other subfunction we pass to pool1d)","category":"page"},{"location":"tutorials/Classification - cars/betaml_tutorial_classification_cars.html","page":"A classification task when labels are known - determining the country of origin of cars given the cars characteristics","title":"A classification task when labels are known - determining the country of origin of cars given the cars characteristics","text":"l3   = VectorFunctionLayer(nCl,f=softmax) ## Add a (parameterless) layer whose activation function (softMax in this case) is defined to all its nodes at once","category":"page"},{"location":"tutorials/Classification - cars/betaml_tutorial_classification_cars.html","page":"A classification task when labels are known - determining the country of origin of cars given the cars characteristics","title":"A classification task when labels are known - determining the country of origin of cars given the cars characteristics","text":"Finally we chain the layers and assign a loss function with buildNetwork:","category":"page"},{"location":"tutorials/Classification - cars/betaml_tutorial_classification_cars.html","page":"A classification task when labels are known - determining the country of origin of cars given the cars characteristics","title":"A classification task when labels are known - determining the country of origin of cars given the cars characteristics","text":"mynn = buildNetwork([l1,l2,l3],squared_cost,name=\"Multinomial logistic regression Model Cars\") ## Build the NN and use the squared cost (aka MSE) as error function (crossentropy could also be used)","category":"page"},{"location":"tutorials/Classification - cars/betaml_tutorial_classification_cars.html","page":"A classification task when labels are known - determining the country of origin of cars given the cars characteristics","title":"A classification task when labels are known - determining the country of origin of cars given the cars characteristics","text":"Now we can train our network using the function train!. It has many options, have a look at the documentation for all the possible arguments. Note that we train the network based on the scaled feature matrix.","category":"page"},{"location":"tutorials/Classification - cars/betaml_tutorial_classification_cars.html","page":"A classification task when labels are known - determining the country of origin of cars given the cars characteristics","title":"A classification task when labels are known - determining the country of origin of cars given the cars characteristics","text":"res  = train!(mynn,scale(xtrainFull,xScaleFactors),ytrain_oh,epochs=500,batch_size=8,opt_alg=ADAM(),rng=copy(FIXEDRNG)) ## Use opt_alg=SGD() to use Stochastic Gradient Descent instead","category":"page"},{"location":"tutorials/Classification - cars/betaml_tutorial_classification_cars.html","page":"A classification task when labels are known - determining the country of origin of cars given the cars characteristics","title":"A classification task when labels are known - determining the country of origin of cars given the cars characteristics","text":"Once trained, we can predict the label. As the trained was based on the scaled feature matrix, so must be for the predictions","category":"page"},{"location":"tutorials/Classification - cars/betaml_tutorial_classification_cars.html","page":"A classification task when labels are known - determining the country of origin of cars given the cars characteristics","title":"A classification task when labels are known - determining the country of origin of cars given the cars characteristics","text":"(ŷtrain,ŷtest)  = predict.(Ref(mynn),[scale(xtrainFull,xScaleFactors),scale(xtestFull,xScaleFactors)])\ntrainAccuracy   = accuracy(ŷtrain,ytrain,rng=copy(FIXEDRNG))","category":"page"},{"location":"tutorials/Classification - cars/betaml_tutorial_classification_cars.html","page":"A classification task when labels are known - determining the country of origin of cars given the cars characteristics","title":"A classification task when labels are known - determining the country of origin of cars given the cars characteristics","text":"testAccuracy    = accuracy(ŷtest,ytest,rng=copy(FIXEDRNG))\n\n\n\ncm = ConfMatrix(ŷtest,ytest,classes=[1,2,3],labels=[\"US\",\"EU\",\"Japan\"],rng=copy(FIXEDRNG))","category":"page"},{"location":"tutorials/Classification - cars/betaml_tutorial_classification_cars.html","page":"A classification task when labels are known - determining the country of origin of cars given the cars characteristics","title":"A classification task when labels are known - determining the country of origin of cars given the cars characteristics","text":"print(cm)","category":"page"},{"location":"tutorials/Classification - cars/betaml_tutorial_classification_cars.html","page":"A classification task when labels are known - determining the country of origin of cars given the cars characteristics","title":"A classification task when labels are known - determining the country of origin of cars given the cars characteristics","text":"4×4 Matrix{Any}:  \"Labels\"    \"US\"    \"EU\"   \"Japan\"  \"US\"      44       0      5  \"EU\"       3      10      3  \"Japan\"    6       2      8 4×4 Matrix{Any}:  \"Labels\"   \"US\"      \"EU\"   \"Japan\"  \"US\"      0.897959  0.0    0.102041  \"EU\"      0.1875    0.625  0.1875  \"Japan\"   0.375     0.125  0.5","category":"page"},{"location":"tutorials/Classification - cars/betaml_tutorial_classification_cars.html","page":"A classification task when labels are known - determining the country of origin of cars given the cars characteristics","title":"A classification task when labels are known - determining the country of origin of cars given the cars characteristics","text":"We see a bit the limits of neural networks in this example. While NN can be extremelly performant in many domains, they also require lot of data and computational power, expecially considering the many possible hyper-parameters and hence its large space in the hyper-parameter tuning. In this example we arrive short to the performance of random forests, yet with a significant numberof neurons.","category":"page"},{"location":"tutorials/Classification - cars/betaml_tutorial_classification_cars.html","page":"A classification task when labels are known - determining the country of origin of cars given the cars characteristics","title":"A classification task when labels are known - determining the country of origin of cars given the cars characteristics","text":"@btime train!(mynn,scale(xtrainFull),ytrainoh,epochs=300,batchsize=8,rng=copy(FIXEDRNG),verbosity=NONE); 11.841 s (62860672 allocations: 4.21 GiB)","category":"page"},{"location":"tutorials/Classification - cars/betaml_tutorial_classification_cars.html#Comparisons-with-Flux","page":"A classification task when labels are known - determining the country of origin of cars given the cars characteristics","title":"Comparisons with Flux","text":"","category":"section"},{"location":"tutorials/Classification - cars/betaml_tutorial_classification_cars.html","page":"A classification task when labels are known - determining the country of origin of cars given the cars characteristics","title":"A classification task when labels are known - determining the country of origin of cars given the cars characteristics","text":"In Flux the input must be in the form (fields, observations), so we transpose our original matrices","category":"page"},{"location":"tutorials/Classification - cars/betaml_tutorial_classification_cars.html","page":"A classification task when labels are known - determining the country of origin of cars given the cars characteristics","title":"A classification task when labels are known - determining the country of origin of cars given the cars characteristics","text":"xtrainT, ytrain_ohT = transpose.([scale(xtrainFull,xScaleFactors), ytrain_oh])\nxtestT, ytest_ohT   = transpose.([scale(xtestFull,xScaleFactors), ytest_oh])","category":"page"},{"location":"tutorials/Classification - cars/betaml_tutorial_classification_cars.html","page":"A classification task when labels are known - determining the country of origin of cars given the cars characteristics","title":"A classification task when labels are known - determining the country of origin of cars given the cars characteristics","text":"We define the Flux neural network model in a similar way than BetaML and load it with data, we train it, predict and measure the accuracies on the training and the test sets:","category":"page"},{"location":"tutorials/Classification - cars/betaml_tutorial_classification_cars.html","page":"A classification task when labels are known - determining the country of origin of cars given the cars characteristics","title":"A classification task when labels are known - determining the country of origin of cars given the cars characteristics","text":"Random.seed!(123)\n\nl1         = Flux.Dense(D,ls,Flux.relu)\n#l2         = Flux.Dense(ls,ls,Flux.relu)\nl3         = Flux.Dense(ls,nCl,Flux.relu)\nFlux_nn    = Flux.Chain(l1,l3)\nloss(x, y) = Flux.logitcrossentropy(Flux_nn(x), y)\nps         = Flux.params(Flux_nn)\nnndata     = Flux.Data.DataLoader((xtrainT, ytrain_ohT), batchsize=8,shuffle=true)\nbegin for i in 1:500  Flux.train!(loss, ps, nndata, Flux.ADAM()) end end\nŷtrain     = Flux.onecold(Flux_nn(xtrainT),1:3)\nŷtest      = Flux.onecold(Flux_nn(xtestT),1:3)\ntrainAccuracy =  accuracy(ŷtrain,ytrain)","category":"page"},{"location":"tutorials/Classification - cars/betaml_tutorial_classification_cars.html","page":"A classification task when labels are known - determining the country of origin of cars given the cars characteristics","title":"A classification task when labels are known - determining the country of origin of cars given the cars characteristics","text":"testAccuracy  = accuracy(ŷtest,ytest)","category":"page"},{"location":"tutorials/Classification - cars/betaml_tutorial_classification_cars.html","page":"A classification task when labels are known - determining the country of origin of cars given the cars characteristics","title":"A classification task when labels are known - determining the country of origin of cars given the cars characteristics","text":"While the train accuracy is little bit higher that BetaML, the test accuracy remains comparable","category":"page"},{"location":"tutorials/Classification - cars/betaml_tutorial_classification_cars.html","page":"A classification task when labels are known - determining the country of origin of cars given the cars characteristics","title":"A classification task when labels are known - determining the country of origin of cars given the cars characteristics","text":"However the time is again lower than BetaML, even if here for \"just\" a factor 2 @btime begin for i in 1:500 Flux.train!(loss, ps, nndata, Flux.ADAM()) end end; 5.665 s (8943640 allocations: 1.07 GiB)","category":"page"},{"location":"tutorials/Classification - cars/betaml_tutorial_classification_cars.html#Summary","page":"A classification task when labels are known - determining the country of origin of cars given the cars characteristics","title":"Summary","text":"","category":"section"},{"location":"tutorials/Classification - cars/betaml_tutorial_classification_cars.html","page":"A classification task when labels are known - determining the country of origin of cars given the cars characteristics","title":"A classification task when labels are known - determining the country of origin of cars given the cars characteristics","text":"This is the summary of the results we had trying to predict the country of origin of the cars, based on their technical characteristics:","category":"page"},{"location":"tutorials/Classification - cars/betaml_tutorial_classification_cars.html","page":"A classification task when labels are known - determining the country of origin of cars given the cars characteristics","title":"A classification task when labels are known - determining the country of origin of cars given the cars characteristics","text":"Model Train acc Test Acc Training time (ms)* Training mem (MB) *\nRF 0.9969 0.8025 134 196\nRF (DecisionTree.jl) 0.9969 0.7531 1.43 1.5\nNN 0.895 0.765 11841 4311\nNN (Flux.jl) 0.938 0.741 5665 1096","category":"page"},{"location":"tutorials/Classification - cars/betaml_tutorial_classification_cars.html","page":"A classification task when labels are known - determining the country of origin of cars given the cars characteristics","title":"A classification task when labels are known - determining the country of origin of cars given the cars characteristics","text":"on a Intel Core i5-8350U laptop","category":"page"},{"location":"tutorials/Classification - cars/betaml_tutorial_classification_cars.html","page":"A classification task when labels are known - determining the country of origin of cars given the cars characteristics","title":"A classification task when labels are known - determining the country of origin of cars given the cars characteristics","text":"We warn that this table just provides a rought idea of the various algorithms performances. Indeed there is a large amount of stochasticity both in the sampling of the data used for training/testing and in the initial settings of the parameters of the algorithm. For a statistically significant comparision we would have to repeat the analysis with multiple sampling (e.g. by cross-validation, see the clustering tutorial for an example) and initial random parameters.","category":"page"},{"location":"tutorials/Classification - cars/betaml_tutorial_classification_cars.html","page":"A classification task when labels are known - determining the country of origin of cars given the cars characteristics","title":"A classification task when labels are known - determining the country of origin of cars given the cars characteristics","text":"Neverthless the table above shows that, when we compare BetaML with the algorithm-specific leading packages, we found similar results in terms of accuracy, but often the leading packages are better optimised and run more efficiently (but sometimes at the cost of being less verstatile). Also, for this dataset, Random Forests seems to remain marginally more accurate than Neural Network, altought of course this depends on the hyper-parameters and, with a single run of the models, we don't know if this difference is significant.","category":"page"},{"location":"tutorials/Classification - cars/betaml_tutorial_classification_cars.html","page":"A classification task when labels are known - determining the country of origin of cars given the cars characteristics","title":"A classification task when labels are known - determining the country of origin of cars given the cars characteristics","text":"View this file on Github.","category":"page"},{"location":"tutorials/Classification - cars/betaml_tutorial_classification_cars.html","page":"A classification task when labels are known - determining the country of origin of cars given the cars characteristics","title":"A classification task when labels are known - determining the country of origin of cars given the cars characteristics","text":"","category":"page"},{"location":"tutorials/Classification - cars/betaml_tutorial_classification_cars.html","page":"A classification task when labels are known - determining the country of origin of cars given the cars characteristics","title":"A classification task when labels are known - determining the country of origin of cars given the cars characteristics","text":"This page was generated using Literate.jl.","category":"page"},{"location":"tutorials/Betaml_tutorial_getting_started.html#getting_started","page":"Getting started","title":"Getting started","text":"","category":"section"},{"location":"tutorials/Betaml_tutorial_getting_started.html#Introduction","page":"Getting started","title":"Introduction","text":"","category":"section"},{"location":"tutorials/Betaml_tutorial_getting_started.html","page":"Getting started","title":"Getting started","text":"This \"tutorial\" part of the documentation presents a step-by-step guide to the main algorithms and utility functions provided by BetaML and comparisons with the leading packages in each field. Aside this page, the tutorial is divided in the following sections:","category":"page"},{"location":"tutorials/Betaml_tutorial_getting_started.html","page":"Getting started","title":"Getting started","text":"Regression tutorial - Arguments: Decision trees, Random forests, neural networks, hyper-parameter tuning, continuous error measures\nClassification tutorial - Arguments: Decision trees and random forests, neural networks (softmax), pre-processing workflow, confusion matrix\nClustering tutorial - Arguments: k-means, kmedoids, generative (gaussian) mixture models (gmm), cross-validation","category":"page"},{"location":"tutorials/Betaml_tutorial_getting_started.html","page":"Getting started","title":"Getting started","text":"Detailed information on the algorithms can be instead found in the API (Reference manual) of the individual modules. The following modules are currently implemented: Perceptron (linear and kernel-based classifiers), Trees (Decision Trees and Random Forests), Nn (Neural Networks), Clustering (Kmean, Kmenoids, Expectation-Maximisation, Missing value imputation, ...) and Utils.","category":"page"},{"location":"tutorials/Betaml_tutorial_getting_started.html","page":"Getting started","title":"Getting started","text":"Finally, theoretical notes describing most of these algorithms can be found at the companion repository https://github.com/sylvaticus/MITx_6.86x.","category":"page"},{"location":"tutorials/Betaml_tutorial_getting_started.html","page":"Getting started","title":"Getting started","text":"The overall \"philosophy\" of BetaML is to support simple machine learning tasks easily and make complex tasks possible. An the most basic level, the majority of  algorithms have default parameters suitable for a basic analysis. A great level of flexibility can be already achieved by just employing the full set of model parameters, for example changing the distance function in kmedoids to l1_distance (aka \"Manhattan distance\"). Finally, the greatest flexibility can be obtained by customising BetaML and writing, for example, its own neural network layer type (by subclassing AbstractLayer), its own sampler (by subclassing AbstractDataSampler) or its own mixture component (by subclassing AbstractMixture), In such a case, while not required by any means, please consider to give it back to the community and open a pull request to integrate your types in BetaML.","category":"page"},{"location":"tutorials/Betaml_tutorial_getting_started.html","page":"Getting started","title":"Getting started","text":"If you are looking for an introductory book on Julia, you could have a look on \"Julia Quick Syntax Reference\" (Apress,2019).","category":"page"},{"location":"tutorials/Betaml_tutorial_getting_started.html","page":"Getting started","title":"Getting started","text":"A few miscellaneous notes:","category":"page"},{"location":"tutorials/Betaml_tutorial_getting_started.html","page":"Getting started","title":"Getting started","text":"Functions and type names use the so-called \"CamelCase\" convention, where the words are separated by a capital letter rather than _;\nWhile some functions provide a dims parameter, most BetaML algorithms expect the input data layout with observations organised by rows and fields/features by columns. Almost everywhere we call N the number of observations/records, and D the number of dimensions;\nWhile some algorithms accept as input DataFrames, the usage of standard arrays is encourages (if the data is passed to the function as dataframe, it may be converted to standard arrays somewhere inside inner loops, leading to great inefficiencies).","category":"page"},{"location":"tutorials/Betaml_tutorial_getting_started.html#using_betaml_from_other_languages","page":"Getting started","title":"Using BetaML from other programming languages","text":"","category":"section"},{"location":"tutorials/Betaml_tutorial_getting_started.html","page":"Getting started","title":"Getting started","text":"Thanks to respectively PyJulia and JuliaCall, using BetaML in Python or R is almost as simple as using a native library. In both cases we need first to download and install the Julia binaries for our operating system from JuliaLang.org. Be sure that Julia is working by opening the Julia terminal and e.g. typing println(\"hello world\") (JuliaCall has an option to install a private-to-R version of Julia from within R). Also, in both case we do not need to think to converting Python/R objects to Julia objects when calling a Julia function and converting back the result from the Julia object to a Pytoh or R object, as this is handled automatically by PyJulia and JuliaCall, at least for simple types (arrays, strings,...)","category":"page"},{"location":"tutorials/Betaml_tutorial_getting_started.html#Use-BetaML-in-Python","page":"Getting started","title":"Use BetaML in Python","text":"","category":"section"},{"location":"tutorials/Betaml_tutorial_getting_started.html","page":"Getting started","title":"Getting started","text":"$ python3 -m pip install --user julia   # the name of the package in `pip` is `julia`, not `PyJulia`","category":"page"},{"location":"tutorials/Betaml_tutorial_getting_started.html","page":"Getting started","title":"Getting started","text":"For the sake of this tutorial, let's also install in Python a package that contains the dataset that we will use:","category":"page"},{"location":"tutorials/Betaml_tutorial_getting_started.html","page":"Getting started","title":"Getting started","text":"$ python3 -m pip install --user sklearn # only for retrieving the dataset in the python way","category":"page"},{"location":"tutorials/Betaml_tutorial_getting_started.html","page":"Getting started","title":"Getting started","text":"We can now open a Python terminal and, to obtain an interface to Julia, just run:","category":"page"},{"location":"tutorials/Betaml_tutorial_getting_started.html","page":"Getting started","title":"Getting started","text":">>> import julia\n>>> julia.install() # Only once to set-up in julia the julia packages required by PyJulia\n>>> jl = julia.Julia(compiled_modules=False)","category":"page"},{"location":"tutorials/Betaml_tutorial_getting_started.html","page":"Getting started","title":"Getting started","text":"If we have multiple Julia versions, we can specify the one to use in Python passing julia=\"/path/to/julia/binary/executable\" (e.g. julia = \"/home/myUser/lib/julia-1.1.0/bin/julia\") to the install() function.","category":"page"},{"location":"tutorials/Betaml_tutorial_getting_started.html","page":"Getting started","title":"Getting started","text":"The compiled_module=False in the Julia constructor is a workaround to the common situation when the Python interpreter is statically linked to libpython, but it will slow down the interactive experience, as it will disable Julia packages pre-compilation, and every time we will use a module for the first time, this will need to be compiled first. Other, more efficient but also more complicate, workarounds are given in the package documentation, under the https://pyjulia.readthedocs.io/en/stable/troubleshooting.html[Troubleshooting section].","category":"page"},{"location":"tutorials/Betaml_tutorial_getting_started.html","page":"Getting started","title":"Getting started","text":"Let's now add to Julia the BetaML package. We can surely do it from within Julia, but we can also do it while remaining in Python:","category":"page"},{"location":"tutorials/Betaml_tutorial_getting_started.html","page":"Getting started","title":"Getting started","text":">>> jl.eval('using Pkg; Pkg.add(\"BetaML\")') # Only once to install BetaML","category":"page"},{"location":"tutorials/Betaml_tutorial_getting_started.html","page":"Getting started","title":"Getting started","text":"While jl.eval('some Julia code') evaluates any arbitrary Julia code (see below), most of the time we can use Julia in a more direct way. Let's start by importing the BetaML Julia package as a submodule of the Python Julia module:","category":"page"},{"location":"tutorials/Betaml_tutorial_getting_started.html","page":"Getting started","title":"Getting started","text":">>> from julia import BetaML","category":"page"},{"location":"tutorials/Betaml_tutorial_getting_started.html","page":"Getting started","title":"Getting started","text":"As you can see, it is no different than importing any other Python module.","category":"page"},{"location":"tutorials/Betaml_tutorial_getting_started.html","page":"Getting started","title":"Getting started","text":"For the data, let's load it \"Python side\":","category":"page"},{"location":"tutorials/Betaml_tutorial_getting_started.html","page":"Getting started","title":"Getting started","text":">>> from sklearn import datasets\n>>> iris = datasets.load_iris()\n>>> X = iris.data[:, :4]\n>>> y = iris.target + 1 # Julia arrays start from 1 not 0","category":"page"},{"location":"tutorials/Betaml_tutorial_getting_started.html","page":"Getting started","title":"Getting started","text":"Note that X and y are Numpy arrays.","category":"page"},{"location":"tutorials/Betaml_tutorial_getting_started.html","page":"Getting started","title":"Getting started","text":"We can now call BetaML functions as we would do for any other Python library functions. In particular, we can pass to the functions (and retrieve) complex data types without worrying too much about the conversion between Python and Julia types, as these are converted automatically:","category":"page"},{"location":"tutorials/Betaml_tutorial_getting_started.html","page":"Getting started","title":"Getting started","text":" >>> (Xs,ys) = BetaML.shuffle([X,y]) # X and y are first converted to julia arrays and then the returned julia arrays are converted back to python Numpy arrays\n >>> cOut    = BetaML.kmeans(Xs,3)\n >>> y_hat   = cOut[0]\n >>> acc     = BetaML.accuracy(y_hat,ys)\n >>> acc\n 0.8933333333333333","category":"page"},{"location":"tutorials/Betaml_tutorial_getting_started.html","page":"Getting started","title":"Getting started","text":"Note: If we are using the jl.eval() interface, the objects we use must be already known to julia. To pass objects from Python to Julia, import the julia Main module (the root module in julia) and assign the needed variables, e.g.","category":"page"},{"location":"tutorials/Betaml_tutorial_getting_started.html","page":"Getting started","title":"Getting started","text":">>> X_python = [1,2,3,2,4]\n>>> from julia import Main\n>>> Main.X_julia = X_python\n>>> jl.eval('BetaML.gini(X_julia)')\n0.7199999999999999","category":"page"},{"location":"tutorials/Betaml_tutorial_getting_started.html#Use-BetaML-in-R","page":"Getting started","title":"Use BetaML in R","text":"","category":"section"},{"location":"tutorials/Betaml_tutorial_getting_started.html","page":"Getting started","title":"Getting started","text":"We start by installing the JuliaCall R package:","category":"page"},{"location":"tutorials/Betaml_tutorial_getting_started.html","page":"Getting started","title":"Getting started","text":"> install.packages(\"JuliaCall\")\n> library(JuliaCall)\n> julia_setup(installJulia = FALSE) # use installJulia = FALSE to let R download and install a private copy of julia","category":"page"},{"location":"tutorials/Betaml_tutorial_getting_started.html","page":"Getting started","title":"Getting started","text":"Note that, differently than PyJulia, the \"setup\" function needs to be called every time we start a new R section, not just when we install the JuliaCall package. If we don't have julia in the path of our system, or if we have multiple versions and we want to specify the one to work with, we can pass the JULIA_HOME = \"/path/to/julia/binary/executable/directory\" (e.g. JULIA_HOME = \"/home/myUser/lib/julia-1.1.0/bin\") parameter to the julia_setup call. Or just let JuliaCall automatically download and install a private copy of julia.","category":"page"},{"location":"tutorials/Betaml_tutorial_getting_started.html","page":"Getting started","title":"Getting started","text":"JuliaCall depends for some things (like object conversion between Julia and R) from the Julia RCall package. If we don't already have it installed in Julia, it will try to install it automatically.","category":"page"},{"location":"tutorials/Betaml_tutorial_getting_started.html","page":"Getting started","title":"Getting started","text":"As in Python, let's start from the data loaded from R and do some work with them in Julia:","category":"page"},{"location":"tutorials/Betaml_tutorial_getting_started.html","page":"Getting started","title":"Getting started","text":"> library(datasets)\n> X <- as.matrix(sapply(iris[,1:4], as.numeric))\n> y <- sapply(iris[,5], as.integer)","category":"page"},{"location":"tutorials/Betaml_tutorial_getting_started.html","page":"Getting started","title":"Getting started","text":"Let's install BetaML. As we did in Python, we can install a Julia package from Julia itself or from within R:","category":"page"},{"location":"tutorials/Betaml_tutorial_getting_started.html","page":"Getting started","title":"Getting started","text":"> julia_eval('using Pkg; Pkg.add(\"BetaML\")')","category":"page"},{"location":"tutorials/Betaml_tutorial_getting_started.html","page":"Getting started","title":"Getting started","text":"We can now \"import\" the BetaML julia package (in julia a \"Package\" is basically a module plus some metadata that facilitate its discovery and integration with other packages, like the reuired set) and call its functions with the julia_call(\"juliaFunction\",args) R function:","category":"page"},{"location":"tutorials/Betaml_tutorial_getting_started.html","page":"Getting started","title":"Getting started","text":"> julia_eval(\"using BetaML\")\n> yencoded <- julia_call(\"integerencoder\",y)\n> ids      <- julia_call(\"shuffle\",1:length(y))\n> Xs       <- X[ids,]\n> ys       <- yencoded[ids]\n> cOut     <- julia_call(\"kmeans\",Xs,3L)    # kmeans expects K to be an integer\n> y_hat    <- sapply(cOut[1],as.integer)[,] # We need a vector, not a matrix\n> acc      <- julia_call(\"accuracy\",y_hat,ys)\n> acc\n[1] 0.8933333","category":"page"},{"location":"tutorials/Betaml_tutorial_getting_started.html","page":"Getting started","title":"Getting started","text":"As alternative, we can embed Julia code directly in R using the julia_eval() function:","category":"page"},{"location":"tutorials/Betaml_tutorial_getting_started.html","page":"Getting started","title":"Getting started","text":"kMeansR  <- julia_eval('\n    function accFromKmeans(x,k,y_true)\n      cOut = kmeans(x,Int(k))\n      acc = accuracy(cOut[1],y_true)\n      return acc\n    end\n')","category":"page"},{"location":"tutorials/Betaml_tutorial_getting_started.html","page":"Getting started","title":"Getting started","text":"We can then call the above function in R in one of the following three ways:","category":"page"},{"location":"tutorials/Betaml_tutorial_getting_started.html","page":"Getting started","title":"Getting started","text":"kMeansR(Xs,3,ys)\njulia_assign(\"Xs_julia\", Xs); julia_assign(\"ys_julia\", ys); julia_eval(\"accFromKmeans(Xs_julia,3,ys_julia)\")\njulia_call(\"accFromKmeans\",Xs,3,ys).","category":"page"},{"location":"tutorials/Betaml_tutorial_getting_started.html","page":"Getting started","title":"Getting started","text":"While other \"convenience\" functions are provided by the package, using  julia_call or julia_assign followed by julia_eval should suffix to accomplish any task we may need with BetaML.","category":"page"},{"location":"tutorials/Betaml_tutorial_getting_started.html#dealing_with_stochasticity","page":"Getting started","title":"Dealing with stochasticity","text":"","category":"section"},{"location":"tutorials/Betaml_tutorial_getting_started.html","page":"Getting started","title":"Getting started","text":"All BetaML models with a stochastic components support a rng parameter, standing for Random Number Generator. A RNG is a \"machine\" that streams a flow of random numbers. The flow itself however is deterministically determined for each \"seed\" (an integer number) that the RNG has been told to use. Normally this seed changes at each running of the script/model, so that stochastic models are indeed stochastic and their output differs at each run.","category":"page"},{"location":"tutorials/Betaml_tutorial_getting_started.html","page":"Getting started","title":"Getting started","text":"If we want to obtain reproductible results we can fix the seed at the very beginning of our model with Random.seed!([AnInteger]). Now our model or script will pick up a specific flow of random numbers, but this flow will always be the same, so that its results will always be the same.","category":"page"},{"location":"tutorials/Betaml_tutorial_getting_started.html","page":"Getting started","title":"Getting started","text":"However the default Julia RNG guarantee to provide the same flow of random numbers, conditional to the seed, only within minor versions of Julia. If we want to \"guarantee\" reproducibility of the results with different versions of Julia, or \"fix\" only some parts of our script, we can call the individual functions passing FIXEDRNG, an instance of StableRNG(FIXEDSEED) provided by BetaML, to the rng parameter. Use it with:","category":"page"},{"location":"tutorials/Betaml_tutorial_getting_started.html","page":"Getting started","title":"Getting started","text":"myAlgorithm(;rng=FIXEDRNG)               : always produce the same sequence of results on each run of the script (\"pulling\" from the same rng object on different calls)\nmyAlgorithm(;rng=StableRNG(SOMEINTEGER)) : always produce the same result (new rng object on each call)","category":"page"},{"location":"tutorials/Betaml_tutorial_getting_started.html","page":"Getting started","title":"Getting started","text":"In particular, use rng=StableRNG(FIXEDSEED) or rng=copy(FIXEDRNG) with FIXEDSEED  to retrieve the exact output as in the documentation or in the unit tests.","category":"page"},{"location":"tutorials/Betaml_tutorial_getting_started.html","page":"Getting started","title":"Getting started","text":"Most of the stochasticity appears in training a model. However in few cases (e.g. decision trees with missing values) some stochasticity appears also in predicting new data using a trained model. In such cases the model doesn't restrict the random seed, so that you can choose at predict time to use a fixed or a variable random seed.","category":"page"},{"location":"GMM.html#gmm_module","page":"GMM","title":"The BetaML.GMM Module","text":"","category":"section"},{"location":"GMM.html","page":"GMM","title":"GMM","text":"GMM","category":"page"},{"location":"GMM.html#BetaML.GMM","page":"GMM","title":"BetaML.GMM","text":"GMM module\n\nGenerative (Gaussian) Mixed Model learners (supervised/unsupervised)\n\nProvides clustering and regressors using  (Generative) Gaussiam Mixture Model (probabilistic).\n\nCollaborative filtering / missing values imputation / reccomendation systems based on GMM is available in the Imputation module.\n\nThe module provides the following models. Use ?[model] to access their documentation:\n\nGMMClusterMode: soft-clustering using GMM\nGMMRegressor1: regressor using GMM as back-end (first algorithm)\nGMMRegressor1: regressor using GMM as back-end (second algorithm)\n\nAll the algorithms works with arbitrary mixture distribution, altought only {Spherical|Diagonal|Full} Gaussian mixtures has been implemented. User defined mixtures can be used defining a struct as subtype of AbstractMixture and implementing for that mixture the following functions:\n\ninit_mixtures!(mixtures, X; minimum_variance, minimum_covariance, initialisation_strategy)\nlpdf(m,x,mask) (for the e-step)\nupdate_parameters!(mixtures, X, pₙₖ; minimum_variance, minimum_covariance) (the m-step)\n\nAll the GMM-based algorithms works only with numerical data, but accepts also Missing one.\n\n\n\n\n\n","category":"module"},{"location":"GMM.html#Module-Index","page":"GMM","title":"Module Index","text":"","category":"section"},{"location":"GMM.html","page":"GMM","title":"GMM","text":"Modules = [GMM]\nOrder   = [:constant, :type, :function, :macro]\nPrivate = false","category":"page"},{"location":"GMM.html#Detailed-API","page":"GMM","title":"Detailed API","text":"","category":"section"},{"location":"GMM.html","page":"GMM","title":"GMM","text":"Modules = [GMM]\nPrivate = false","category":"page"},{"location":"GMM.html#BetaML.GMM.GMMClusterer","page":"GMM","title":"BetaML.GMM.GMMClusterer","text":"mutable struct GMMClusterer <: BetaMLUnsupervisedModel\n\nAssign class probabilities to records (i.e. soft clustering) assuming a probabilistic generative model of observed data using mixtures.\n\nFor the parameters see ?GMMHyperParametersSet and ?BetaMLDefaultOptionsSet.\n\nNotes:\n\nData must be numerical\nMixtures can be user defined: see the ?GMM module documentation for a discussion on provided vs custom mixtures.\nOnline fitting (re-fitting with new data) is supported by setting the old learned mixtrures as the starting values\nThe model is fitted using an Expectation-Minimisation (EM) algorithm that supports Missing data and is implemented in the log-domain for better numerical accuracy with many dimensions\n\n\n\n\n\n","category":"type"},{"location":"GMM.html#BetaML.GMM.GMMHyperParametersSet","page":"GMM","title":"BetaML.GMM.GMMHyperParametersSet","text":"mutable struct GMMHyperParametersSet <: BetaMLHyperParametersSet\n\nHyperparameters for GMM clusters and other GMM-based algorithms\n\nParameters:\n\nn_classes\nNumber of mixtures (latent classes) to consider [def: 3]\ninitial_probmixtures\nInitial probabilities of the categorical distribution (n_classes x 1) [default: []]\nmixtures\nAn array (of length n_classes) of the mixtures to employ (see the [?GMM](@ref GMM) module). Each mixture object can be provided with or without its parameters (e.g. mean and variance for the gaussian ones). Fully qualified mixtures are useful only if theinitialisationstrategyparameter is  set to \"gived\" [def: `[DiagonalGaussian() for i in 1:nclasses]`]\ntol\nTolerance to stop the algorithm [default: 10^(-6)]\nminimum_variance\nMinimum variance for the mixtures [default: 0.05]\nminimum_covariance\nMinimum covariance for the mixtures with full covariance matrix [default: 0]. This should be set different than minimum_variance (see notes).\ninitialisation_strategy\nThe computation method of the vector of the initial mixtures. One of the following:\n\"grid\": using a grid approach\n\"given\": using the mixture provided in the fully qualified mixtures parameter\n\"kmeans\": use first kmeans (itself initialised with a \"grid\" strategy) to set the initial mixture centers [default]\nNote that currently \"random\" and \"shuffle\" initialisations are not supported in gmm-based algorithms.\n\nmaximum_iterations\nMaximum number of iterations [def: typemax(Int64), i.e. ∞]\n\n\n\n\n\n","category":"type"},{"location":"GMM.html#BetaML.GMM.GMMRegressor1","page":"GMM","title":"BetaML.GMM.GMMRegressor1","text":"mutable struct GMMRegressor1 <: BetaMLUnsupervisedModel\n\nA multi-dimensional, missing data friendly non-linear regressor based on Generative (Gaussian) Mixture Model (strategy \"1\").\n\nThe training data is used to fit a probabilistic model with latent mixtures (Gaussian distributions with different covariances are already implemented) and then predictions of new data is obtained by fitting the new data to the mixtures.\n\nFor hyperparameters see GMMHyperParametersSet and BetaMLDefaultOptionsSet.\n\nThis strategy (GMMRegressor1) works by fitting the EM algorithm on the feature matrix X. Once the data has been probabilistically assigned to the various classes, a mean value of fitting values Y is computed for each cluster (using the probabilities as weigths). At predict time, the new data is first fitted to the learned mixtures using the e-step part of the EM algorithm to obtain the probabilistic assignment of each record to the various mixtures. Then these probabilities are multiplied to the mixture averages for the Y dimensions learned at training time to obtain the predicted value(s) for each record. \n\nNotes:\n\nthe predicted values are always a matrix, even when a single variable is predicted (use dropdims(ŷ,dims=2) to get a single vector).\n\n\n\n\n\n","category":"type"},{"location":"GMM.html#BetaML.GMM.GMMRegressor2","page":"GMM","title":"BetaML.GMM.GMMRegressor2","text":"mutable struct GMMRegressor2 <: BetaMLUnsupervisedModel\n\nA multi-dimensional, missing data friendly non-linear regressor based on Generative (Gaussian) Mixture Model.\n\nThe training data is used to fit a probabilistic model with latent mixtures (Gaussian distributions with different covariances are already implemented) and then predictions of new data is obtained by fitting the new data to the mixtures.\n\nFor hyperparameters see GMMHyperParametersSet and GMMClusterOptionsSet.\n\nThsi strategy (GMMRegressor2) works by training the EM algorithm on a combined (hcat) matrix of X and Y. At predict time, the new data is first fitted to the learned mixtures using the e-step part of the EM algorithm (and using missing values for the dimensions belonging to Y) to obtain the probabilistic assignment of each record to the various mixtures. Then these probabilities are multiplied to the mixture averages for the Y dimensions to obtain the predicted value(s) for each record. \n\n\n\n\n\n","category":"type"},{"location":"GMM.html#BetaML.GMM.GaussianMixtureClusterer","page":"GMM","title":"BetaML.GMM.GaussianMixtureClusterer","text":"mutable struct GaussianMixtureClusterer <: MLJModelInterface.Unsupervised\n\nA Expectation-Maximisation clustering algorithm with customisable mixtures, from the Beta Machine Learning Toolkit (BetaML).\n\nHyperparameters:\n\nn_classes::Int64\nNumber of mixtures (latent classes) to consider [def: 3]\ninitial_probmixtures::AbstractVector{Float64}\nInitial probabilities of the categorical distribution (n_classes x 1) [default: []]\nmixtures::Vector{AbstractMixture}\nAn array (of length n_classes) of the mixtures to employ (see the [?GMM](@ref GMM) module of BetaML for available mixtures). Each mixture object can be provided with or without its parameters (e.g. mean and variance for the gaussian ones). Fully qualified mixtures are useful only if theinitialisationstrategyparameter is  set to \"given\" [def: `[DiagonalGaussian() for i in 1:nclasses]`]\ntol::Float64\nTolerance to stop the algorithm [default: 10^(-6)]\nminimum_variance::Float64\nMinimum variance for the mixtures [default: 0.05]\nminimum_covariance::Float64\nMinimum covariance for the mixtures with full covariance matrix [default: 0]. This should be set different than minimum_variance (see notes).\ninitialisation_strategy::String\nThe computation method of the vector of the initial mixtures. One of the following:\n\"grid\": using a grid approach\n\"given\": using the mixture provided in the fully qualified mixtures parameter\n\"kmeans\": use first kmeans (itself initialised with a \"grid\" strategy) to set the initial mixture centers [default]\nNote that currently \"random\" and \"shuffle\" initialisations are not supported in gmm-based algorithms.\nmaximum_iterations::Int64\nMaximum number of iterations [def: typemax(Int64), i.e. ∞]\nrng::Random.AbstractRNG\nRandom Number Generator [deafult: Random.GLOBAL_RNG]\n\n\n\n\n\n","category":"type"},{"location":"GMM.html#BetaML.GMM.GaussianMixtureRegressor","page":"GMM","title":"BetaML.GMM.GaussianMixtureRegressor","text":"mutable struct GaussianMixtureRegressor <: MLJModelInterface.Deterministic\n\nA non-linear regressor derived from fitting the data on a probabilistic model (Gaussian Mixture Model). Relatively fast.\n\nThis is the single-target version of the model. If you want to predict several labels (y) at once, use the MLJ model MultitargetGaussianMixtureRegressor.\n\nHyperparameters:\n\nn_classes::Int64\nNumber of mixtures (latent classes) to consider [def: 3]\ninitial_probmixtures::Vector{Float64}\nInitial probabilities of the categorical distribution (n_classes x 1) [default: []]\nmixtures::Vector{AbstractMixture}\nAn array (of length n_classes) of the mixtures to employ (see the [?GMM](@ref GMM) module). Each mixture object can be provided with or without its parameters (e.g. mean and variance for the gaussian ones). Fully qualified mixtures are useful only if theinitialisationstrategyparameter is  set to \"given\" [def: `[DiagonalGaussian() for i in 1:nclasses]`]\ntol::Float64\nTolerance to stop the algorithm [default: 10^(-6)]\nminimum_variance::Float64\nMinimum variance for the mixtures [default: 0.05]\nminimum_covariance::Float64\nMinimum covariance for the mixtures with full covariance matrix [default: 0]. This should be set different than minimum_variance (see notes).\ninitialisation_strategy::String\nThe computation method of the vector of the initial mixtures. One of the following:\n\"grid\": using a grid approach\n\"given\": using the mixture provided in the fully qualified mixtures parameter\n\"kmeans\": use first kmeans (itself initialised with a \"grid\" strategy) to set the initial mixture centers [default]\nNote that currently \"random\" and \"shuffle\" initialisations are not supported in gmm-based algorithms.\n\nmaximum_iterations::Int64\nMaximum number of iterations [def: typemax(Int64), i.e. ∞]\nrng::Random.AbstractRNG\nRandom Number Generator [deafult: Random.GLOBAL_RNG]\n\n\n\n\n\n","category":"type"},{"location":"GMM.html#BetaML.GMM.MultitargetGaussianMixtureRegressor","page":"GMM","title":"BetaML.GMM.MultitargetGaussianMixtureRegressor","text":"mutable struct MultitargetGaussianMixtureRegressor <: MLJModelInterface.Deterministic\n\nA non-linear regressor derived from fitting the data on a probabilistic model (Gaussian Mixture Model). Relatively fast.\n\nThis is the multi-target version of the model. If you want to predict a single label (y), use the MLJ model GaussianMixtureRegressor.\n\nHyperparameters:\n\nn_classes::Int64\nNumber of mixtures (latent classes) to consider [def: 3]\ninitial_probmixtures::Vector{Float64}\nInitial probabilities of the categorical distribution (n_classes x 1) [default: []]\nmixtures::Vector{AbstractMixture}\nAn array (of length n_classes) of the mixtures to employ (see the [?GMM](@ref GMM) module). Each mixture object can be provided with or without its parameters (e.g. mean and variance for the gaussian ones). Fully qualified mixtures are useful only if theinitialisationstrategyparameter is  set to \"given\" [def: `[DiagonalGaussian() for i in 1:nclasses]`]\ntol::Float64\nTolerance to stop the algorithm [default: 10^(-6)]\nminimum_variance::Float64\nMinimum variance for the mixtures [default: 0.05]\nminimum_covariance::Float64\nMinimum covariance for the mixtures with full covariance matrix [default: 0]. This should be set different than minimum_variance (see notes).\ninitialisation_strategy::String\nThe computation method of the vector of the initial mixtures. One of the following:\n\"grid\": using a grid approach\n\"given\": using the mixture provided in the fully qualified mixtures parameter\n\"kmeans\": use first kmeans (itself initialised with a \"grid\" strategy) to set the initial mixture centers [default]\nNote that currently \"random\" and \"shuffle\" initialisations are not supported in gmm-based algorithms.\n\nmaximum_iterations::Int64\nMaximum number of iterations [def: typemax(Int64), i.e. ∞]\nrng::Random.AbstractRNG\nRandom Number Generator [deafult: Random.GLOBAL_RNG]\n\n\n\n\n\n","category":"type"},{"location":"GMM.html#BetaML.GMM.gmm-Tuple{Any, Any}","page":"GMM","title":"BetaML.GMM.gmm","text":"gmm(X,K;initialprobmixtures,mixtures,tol,verbosity,minimumvariance,minimumcovariance,initialisationstrategy)\n\nCompute Expectation-Maximisation algorithm to identify K clusters of X data, i.e. employ a Generative Mixture Model as the underlying probabilistic model.\n\nwarning: Warning\nThis function is deprecated and will possibly be removed in BetaML 0.9. Use one of the various models that use GMM as backend instead.\n\nX can contain missing values in some or all of its dimensions. In such case the learning is done only with the available data. Implemented in the log-domain for better numerical accuracy with many dimensions.\n\nParameters:\n\nX  :           A (n x d) data to clusterise\nK  :           Number of cluster wanted\ninitial_probmixtures :           Initial probabilities of the categorical distribution (K x 1) [default: []]\nmixtures:      An array (of length K) of the mixture to employ (see notes) [def: [DiagonalGaussian() for i in 1:K]]\ntol:           Tolerance to stop the algorithm [default: 10^(-6)]\nverbosity:     A verbosity parameter regulating the information messages frequency [def: STD]\nminimum_variance:   Minimum variance for the mixtures [default: 0.05]\nminimum_covariance: Minimum covariance for the mixtures with full covariance matrix [default: 0]. This should be set different than minimum_variance (see notes).\ninitialisation_strategy:  Mixture initialisation algorithm [def: kmeans]\nmaximum_iterations:       Maximum number of iterations [def: typemax(Int64), i.e. ∞]\nrng:           Random Number Generator (see FIXEDSEED) [deafult: Random.GLOBAL_RNG]\n\nReturns:\n\nA named touple of:\npₙₖ:      Matrix of size (N x K) of the probabilities of each point i to belong to cluster j\npₖ:       Probabilities of the categorical distribution (K x 1)\nmixtures: Vector (K x 1) of the estimated underlying distributions\nϵ:        Vector of the discrepancy (matrix norm) between pⱼₓ and the lagged pⱼₓ at each iteration\nlL:       The log-likelihood (without considering the last mixture optimisation)\nBIC:      The Bayesian Information Criterion (lower is better)\nAIC:      The Akaike Information Criterion (lower is better)\n\nNotes:\n\nThe mixtures currently implemented are SphericalGaussian(μ,σ²),DiagonalGaussian(μ,σ²) and FullGaussian(μ,σ²)\nReasonable choices for the minimum_variance/Covariance depends on the mixture. For example 0.25 seems a reasonable value for the SphericalGaussian, 0.05 seems better for the DiagonalGaussian, and FullGaussian seems to prefer either very low values of variance/covariance (e.g. (0.05,0.05) ) or very big but similar ones (e.g. (100,100) ).\nFor initialisation_strategy, look at the documentation of init_mixtures! for the mixture you want. The provided gaussian mixtures support grid, kmeans or given. grid is faster (expecially if X contains missing values), but kmeans often provides better results.\n\nResources:\n\nPaper describing gmm with missing values\nClass notes from MITx 6.86x (Sec 15.9)\nLimitations of gmm\n\nExample:\n\njulia> clusters = gmm([1 10.5;1.5 0; 1.8 8; 1.7 15; 3.2 40; 0 0; 3.3 38; 0 -2.3; 5.2 -2.4],3,verbosity=HIGH)\n\n\n\n\n\n","category":"method"},{"location":"GMM.html#BetaML.GMM.init_mixtures!-Union{Tuple{T}, Tuple{Vector{T}, Any}} where T<:BetaML.GMM.AbstractGaussian","page":"GMM","title":"BetaML.GMM.init_mixtures!","text":"init_mixtures!(mixtures::Array{T,1}, X; minimum_variance=0.25, minimum_covariance=0.0, initialisation_strategy=\"grid\",rng=Random.GLOBAL_RNG)\n\nThe parameter initialisation_strategy can be grid, kmeans or given:\n\ngrid: Uniformly cover the space observed by the data\nkmeans: Use the kmeans algorithm. If the data contains missing values, a first run of predictMissing is done under init=grid to impute the missing values just to allow the kmeans algorithm. Then the em algorithm is used with the output of kmean as init values.\ngiven: Leave the provided set of initial mixtures\n\n\n\n\n\n","category":"method"},{"location":"GMM.html#BetaML.GMM.lpdf-Tuple{DiagonalGaussian, Any, Any}","page":"GMM","title":"BetaML.GMM.lpdf","text":"lpdf(m::DiagonalGaussian,x,mask) - Log PDF of the mixture given the observation x\n\n\n\n\n\n","category":"method"},{"location":"GMM.html#BetaML.GMM.lpdf-Tuple{FullGaussian, Any, Any}","page":"GMM","title":"BetaML.GMM.lpdf","text":"lpdf(m::FullGaussian,x,mask) - Log PDF of the mixture given the observation x\n\n\n\n\n\n","category":"method"},{"location":"GMM.html#BetaML.GMM.lpdf-Tuple{SphericalGaussian, Any, Any}","page":"GMM","title":"BetaML.GMM.lpdf","text":"lpdf(m::SphericalGaussian,x,mask) - Log PDF of the mixture given the observation x\n\n\n\n\n\n","category":"method"},{"location":"StyleGuide_templates.html#Style-guide-and-template-for-BetaML-developers","page":"Style guide","title":"Style guide and template for BetaML developers","text":"","category":"section"},{"location":"StyleGuide_templates.html#Master-Style-guide","page":"Style guide","title":"Master Style guide","text":"","category":"section"},{"location":"StyleGuide_templates.html","page":"Style guide","title":"Style guide","text":"The Style guide should follow the official Julia Style Guide: https://docs.julialang.org/en/v1/manual/style-guide/","category":"page"},{"location":"StyleGuide_templates.html#File-management","page":"Style guide","title":"File management","text":"","category":"section"},{"location":"StyleGuide_templates.html","page":"Style guide","title":"Style guide","text":"Each file name shoudl start with a capital letter, no spaces allowed, and each file content should start with:","category":"page"},{"location":"StyleGuide_templates.html","page":"Style guide","title":"Style guide","text":"\"Part of [BetaML](https://github.com/sylvaticus/BetaML.jl). Licence is MIT.\"","category":"page"},{"location":"StyleGuide_templates.html#Docstrings","page":"Style guide","title":"Docstrings","text":"","category":"section"},{"location":"StyleGuide_templates.html","page":"Style guide","title":"Style guide","text":"Please apply the following templates when writing a docstring for BetaML:","category":"page"},{"location":"StyleGuide_templates.html","page":"Style guide","title":"Style guide","text":"Functions","category":"page"},{"location":"StyleGuide_templates.html","page":"Style guide","title":"Style guide","text":"\"\"\"\n$(TYPEDSIGNATURES)\n\nOne line description\n\n[Further description]\n\n# Parameters:\n\n$(TYPEDFIELDS)\n\n# Returns:\n- Elements the funtion need\n\n# Notes:\n- notes\n\n# Example:\n` ` `julia\njulia> [code]\n[output]\n` ` `\n\"\"\"","category":"page"},{"location":"StyleGuide_templates.html","page":"Style guide","title":"Style guide","text":"Structs","category":"page"},{"location":"StyleGuide_templates.html","page":"Style guide","title":"Style guide","text":"\"\"\"\n\n\nOne line description\n\n[Further description]\n\n# Fields: (if relevant)\n$(TYPEDFIELDS)\n\n# Notes:\n\n# Example:\n` ` `julia\njulia> [code]\n[output]\n` ` `\n\"\"\"","category":"page"},{"location":"StyleGuide_templates.html","page":"Style guide","title":"Style guide","text":"Enums:","category":"page"},{"location":"StyleGuide_templates.html","page":"Style guide","title":"Style guide","text":"\"\"\"\n$(TYPEDEF)\n\nOne line description\n\n[Further description]\n\n\n# Notes:\n\n\"\"\"","category":"page"},{"location":"StyleGuide_templates.html","page":"Style guide","title":"Style guide","text":"Constants","category":"page"},{"location":"StyleGuide_templates.html","page":"Style guide","title":"Style guide","text":"\"\"\"\n[4 spaces] [Constant name]\n\nOne line description\n\n[Further description]\n\n\n# Notes:\n\n\"\"\"","category":"page"},{"location":"StyleGuide_templates.html","page":"Style guide","title":"Style guide","text":"Modules","category":"page"},{"location":"StyleGuide_templates.html","page":"Style guide","title":"Style guide","text":"\"\"\"\n[4 spaces] [Module name]\n\nOne line description\n\nDetailed description on the module objectives, content and organisation\n\n\"\"\"","category":"page"},{"location":"StyleGuide_templates.html#Internal-links","page":"Style guide","title":"Internal links","text":"","category":"section"},{"location":"StyleGuide_templates.html","page":"Style guide","title":"Style guide","text":"To refer to a documented object: [\\NAME`](@ref)or`NAME`. In particular for internal links use`?NAME``","category":"page"},{"location":"StyleGuide_templates.html","page":"Style guide","title":"Style guide","text":"To create a id manually: [Title](@id manual_id).","category":"page"},{"location":"Api_v2_user.html#api_usage","page":"Introduction for user","title":"BetaML Api v2","text":"","category":"section"},{"location":"Api_v2_user.html","page":"Introduction for user","title":"Introduction for user","text":"note: Note\nThe API described below is the default one starting from BetaML v0.8, with most of low-level functions now deprecated and possibly removed in further versions.","category":"page"},{"location":"Api_v2_user.html","page":"Introduction for user","title":"Introduction for user","text":"The following API is designed to further simply the usage of the various ML models provided by BetaML introducing a common workflow. This is the user documentation. Refer to the developer documentation to learn how the API is implemented. ","category":"page"},{"location":"Api_v2_user.html#Supervised-,-unsupervised-and-transformed-models","page":"Introduction for user","title":"Supervised , unsupervised and transformed models","text":"","category":"section"},{"location":"Api_v2_user.html","page":"Introduction for user","title":"Introduction for user","text":"Supervised refers to models designed to learn a relation between some features (often noted with X) and some labels (often noted with Y) in order to predict the label of new data given the observed features alone. Perceptron, decision trees or neural networks are common examples. Unsupervised and transformer models relate to models that learn a \"structure\" from the data itself (without any label attached from which to learn) and report either some new information using this learned structure (e.g. a cluster class) or directly process a transformation of the data itself, like PCA or missing imputers. There is no difference in BetaML about these kind of models, aside that the fitting (aka training) function for the former takes both the features and the labels. In particular there isn't a separate transform function as in other frameworks, but any information we need to learn using the model, wheter a label or some transformation of the original data, is provided by the predict function. ","category":"page"},{"location":"Api_v2_user.html#Model-constructor","page":"Introduction for user","title":"Model constructor","text":"","category":"section"},{"location":"Api_v2_user.html","page":"Introduction for user","title":"Introduction for user","text":"The first step is to build the model constructor by passing (using keyword arguments) the agorithm hyperparameters and various options (cache results flag, debug levels, random number generators, ...):","category":"page"},{"location":"Api_v2_user.html","page":"Introduction for user","title":"Introduction for user","text":"mod = ModelName(par1=X,par2=Y,...)","category":"page"},{"location":"Api_v2_user.html","page":"Introduction for user","title":"Introduction for user","text":"Sometimes a parameter is itself another model, in such case we would have:","category":"page"},{"location":"Api_v2_user.html","page":"Introduction for user","title":"Introduction for user","text":"mod = ModelName(par1=OtherModel(a_par_of_OtherModel=X,...),par2=Y,...)","category":"page"},{"location":"Api_v2_user.html#Training-of-the-model","page":"Introduction for user","title":"Training of the model","text":"","category":"section"},{"location":"Api_v2_user.html","page":"Introduction for user","title":"Introduction for user","text":"The second step is to fit (aka train) the model:","category":"page"},{"location":"Api_v2_user.html","page":"Introduction for user","title":"Introduction for user","text":"fit!(m,X,[Y])","category":"page"},{"location":"Api_v2_user.html","page":"Introduction for user","title":"Introduction for user","text":"where Y is present only for supervised models.","category":"page"},{"location":"Api_v2_user.html","page":"Introduction for user","title":"Introduction for user","text":"For online algorithms, i.e. models that support updating of the learned parameters with new data, fit! can be repeated as new data arrive, altought not all algorithms guarantee that training each record at the time is equivalent to train all the records at once. In some algorithms the \"old training\" could be used as initial conditions, without consideration if these has been achieved with hundread or millions of records, and the new data we use for training become much more important than the old one for the determination of the learned parameters.","category":"page"},{"location":"Api_v2_user.html#Prediction","page":"Introduction for user","title":"Prediction","text":"","category":"section"},{"location":"Api_v2_user.html","page":"Introduction for user","title":"Introduction for user","text":"Fitted models can be used to predict y (wheter the label, some desired new information or a transformation) given new X:","category":"page"},{"location":"Api_v2_user.html","page":"Introduction for user","title":"Introduction for user","text":"ŷ = predict(mod,X)","category":"page"},{"location":"Api_v2_user.html","page":"Introduction for user","title":"Introduction for user","text":"As a convenience, if the model has been trained while having the cache option set on true (by default) the ŷ of the last training is retained in the  model object and it can be retrieved simply with predict(mod). Also in such case the fit! function returns ŷ instead of nothing effectively making it to behave like a fit-and-transform function.  The 3 expressions below are hence equivalent :","category":"page"},{"location":"Api_v2_user.html","page":"Introduction for user","title":"Introduction for user","text":"ŷ  = fit!(mod,xtrain)    # only with `cache=true` in the model constructor (default)\nŷ1 = predict(mod)        # only with `cache=true` in the model constructor (default)\nŷ2 = predict(mod,xtrain) ","category":"page"},{"location":"Api_v2_user.html#Other-functions","page":"Introduction for user","title":"Other functions","text":"","category":"section"},{"location":"Api_v2_user.html","page":"Introduction for user","title":"Introduction for user","text":"Models can be resetted to lose the learned information with reset!(mod) and training information (other than the algorithm learned parameters) can be retrieved with info(mod).","category":"page"},{"location":"Api_v2_user.html","page":"Introduction for user","title":"Introduction for user","text":"Hyperparameters, options and learned parameters can be retrieved with the functions hyperparameters, parameters and options respectively. Note that they can be used also to set new values to the model as they return a reference to the required objects.","category":"page"},{"location":"Api_v2_user.html","page":"Introduction for user","title":"Introduction for user","text":"Some models allow an inverse transformation, that using the parameters learned at trainign time (e.g. the scale factors) perform an inverse tranformation of new data to the space of the training data (e.g. the unscaled space). Use inverse_predict(mod,xnew).","category":"page"},{"location":"Api_v2_user.html#Available-models","page":"Introduction for user","title":"Available models","text":"","category":"section"},{"location":"Api_v2_user.html","page":"Introduction for user","title":"Introduction for user","text":"Currently the following models are available:","category":"page"},{"location":"Api_v2_user.html","page":"Introduction for user","title":"Introduction for user","text":"BetaML name MLJ Interface Typology*\nPerceptronClassifier LinearPerceptron Supervised regressor\nKernelPerceptronClassifier KernelPerceptron Supervised regressor\nPegasosClassifier Pegasos Supervised classifier\nDecisionTreeEstimator DecisionTreeClassifier, DecisionTreeRegressor Supervised regressor and classifier\nRandomForestEstimator RandomForestClassifier, RandomForestRegressor Supervised regressor and classifier\nNeuralNetworkEstimator MultitargetNeuralNetworkRegressor, NeuralNetworkClassifier Supervised regressor and classifier\nGMMRegressor1  Supervised regressor\nGMMRegressor2 GaussianMixtureRegressor Supervised regressor\nKMeansClusterer KMeans Unsupervised hard clusterer\nKMedoidsClusterer KMedoids Unsupervised hard clusterer\nGMMClusterer GaussianMixtureClusterer Unsupervised soft clusterer\nFeatureBasedImputer SimpleImputer Unsupervised missing data imputer\nGMMImputer GaussianMixtureImputer Unsupervised missing data imputer\nRFImputer RandomForestImputer Unsupervised missing data imputer\nUniversalImputer GeneralImputer Unsupervised missing data imputer\nMinMaxScaler  Data transformer\nStandardScaler  Data transformer\nScaler  Data transformer\nPCA  Data transformer\nOneHotEncoder  Data transformer\nOrdinalEncoder  Data transformer\nConfusionMatrix  Predictions assessment","category":"page"},{"location":"Api_v2_user.html","page":"Introduction for user","title":"Introduction for user","text":"* There is no formal distinction in BetaML between a transformer, or also a prediction assessment model, and a unsupervised model. They are all treated as unsupervised models that given some data they lern how to return some useful information, wheter a class grouping, a specific tranformation or a quality evaluation..","category":"page"},{"location":"Api_v2_developer.html#api_implementation","page":"API implementation","title":"Api v2 - developer documentation (API implementation)","text":"","category":"section"},{"location":"Api_v2_developer.html","page":"API implementation","title":"API implementation","text":"Each model is a child of either BetaMLSuperVisedModel or BetaMLSuperVisedModel, both in turn child of BetaMLModel:","category":"page"},{"location":"Api_v2_developer.html","page":"API implementation","title":"API implementation","text":"BetaMLSuperVisedModel   <: BetaMLModel\nBetaMLUnsupervisedModel <: BetaMLModel\nRandomForestEstimator                 <: BetaMLSuperVisedModel","category":"page"},{"location":"Api_v2_developer.html","page":"API implementation","title":"API implementation","text":"The model struct is composed of the following elements:","category":"page"},{"location":"Api_v2_developer.html","page":"API implementation","title":"API implementation","text":"mutable struct DecisionTreeEstimator <: BetaMLSupervisedModel\n    hpar::DTHyperParametersSet   # Hyper-pharameters\n    opt::BetaMLDefaultOptionsSet # Option sets, default or a specific one for the model\n    par::DTLearnableParameters   # Model learnable parameters (needed for predictions)\n    cres::T                      # Cached results\n    trained::Bool                # Trained flag\n    info                         # Complementary information, but not needed to make predictions\nend","category":"page"},{"location":"Api_v2_developer.html","page":"API implementation","title":"API implementation","text":"Each specific model hyperparameter set and learnable parameter set are childs of BetaMLHyperParametersSet and BetaMLLearnedParametersSet and, if a specific model option set is used, this would be child of BetaMLOptionsSet.","category":"page"},{"location":"Api_v2_developer.html","page":"API implementation","title":"API implementation","text":"While hyperparameters are elements that control the learning process, i.e. would influence the model training and prediction, the options have a more general meaning and do not directly affect the training (they can do indirectly, like the rng). The default option set is implemented as:","category":"page"},{"location":"Api_v2_developer.html","page":"API implementation","title":"API implementation","text":"Base.@kwdef mutable struct BetaMLDefaultOptionsSet\n   \"Cache the results of the fitting stage, as to allow predict(mod) [default: `true`]. Set it to `false` to save memory for large data.\"\n   cache::Bool = true\n   \"An optional title and/or description for this model\"\n   descr::String = \"\" \n   \"The verbosity level to be used in training or prediction (see [`Verbosity`](@ref)) [deafult: `STD`]\n   \"\n   verbosity::Verbosity = STD\n   \"Random Number Generator (see [`FIXEDSEED`](@ref)) [deafult: `Random.GLOBAL_RNG`]\n   \"\n   rng::AbstractRNG = Random.GLOBAL_RNG\nend","category":"page"},{"location":"Api_v2_developer.html","page":"API implementation","title":"API implementation","text":"Note that the user doesn't generally need to make a difference between an hyperparameter and an option, as both are provided as keyword arguments to the model constructor thanks to a model constructor like the following one:","category":"page"},{"location":"Api_v2_developer.html","page":"API implementation","title":"API implementation","text":"function KMedoidsClusterer(;kwargs...)\n    m = KMedoidsClusterer(KMeansMedoidsHyperParametersSet(),BetaMLDefaultOptionsSet(),KMeansMedoidsLearnableParameters(),nothing,false,Dict{Symbol,Any}())\n    thisobjfields  = fieldnames(nonmissingtype(typeof(m)))\n    for (kw,kwv) in kwargs\n       found = false\n       for f in thisobjfields\n          fobj = getproperty(m,f)\n          if kw in fieldnames(typeof(fobj))\n              setproperty!(fobj,kw,kwv)\n              found = true\n          end\n        end\n        found || error(\"Keyword \\\"$kw\\\" is not part of this model.\")\n    end\n    return m\nend","category":"page"},{"location":"Api_v2_developer.html","page":"API implementation","title":"API implementation","text":"So, in order to implement a new model we need to:","category":"page"},{"location":"Api_v2_developer.html","page":"API implementation","title":"API implementation","text":"implement its struct and constructor\nimplement the relative ModelHyperParametersSet, ModelLearnedParametersSet and eventually ModelOptionsSet.\ndefine fit!(model, X, [y]), predict(model,X) and eventually inverse_predict(model,X).","category":"page"},{"location":"index.html#![BLogos](assets/BetaML_logo_30x30.png)-BetaML.jl-Documentation","page":"Index","title":"(Image: BLogos) BetaML.jl Documentation","text":"","category":"section"},{"location":"index.html","page":"Index","title":"Index","text":"Welcome to the documentation of the Beta Machine Learning toolkit.","category":"page"},{"location":"index.html#About","page":"Index","title":"About","text":"","category":"section"},{"location":"index.html","page":"Index","title":"Index","text":"The BetaML toolkit provides classical algorithms written in the Julia programming language useful to \"learn\" the relationship between some inputs and some outputs, with the objective to make accurate predictions of the output given new inputs (\"supervised machine learning\") or to better understand the structure of the data, perhaps hidden because of the high dimensionality (\"unsupervised machine learning\").","category":"page"},{"location":"index.html","page":"Index","title":"Index","text":"While specific packages exist for state-of-the art implementations of these algorithms (see the section \"Alternative Packages\"), thanks to the Just-In-Time compilation nature of Julia, BetaML is reasonably fast for datasets that fit in memory while keeping both the code and the usage as simple as possible.","category":"page"},{"location":"index.html","page":"Index","title":"Index","text":"Aside the algorithms themselves, BetaML provides many \"utility\" functions. Because algorithms are all self-contained in the library itself (you are invited to explore their source code by typing @edit functionOfInterest(par1,par2,...)), the utility functions have APIs that are coordinated with the algorithms, facilitating the \"preparation\" of the data for the analysis, the evaluation of the models or the implementation of several models in chains (pipelines). While BetaML doesn't provide itself tools for hyper-parameters optimisation or complex pipeline building tools, most models have an interface for the MLJ framework that allows it.","category":"page"},{"location":"index.html","page":"Index","title":"Index","text":"Aside Julia, BetaML can be accessed in R or Python using respectively JuliaCall and PyJulia. See the tutorial for details.","category":"page"},{"location":"index.html#Installation","page":"Index","title":"Installation","text":"","category":"section"},{"location":"index.html","page":"Index","title":"Index","text":"The BetaML package is included in the standard Julia register, install it with:","category":"page"},{"location":"index.html","page":"Index","title":"Index","text":"] add BetaML","category":"page"},{"location":"index.html#Loading-the-module(s)","page":"Index","title":"Loading the module(s)","text":"","category":"section"},{"location":"index.html","page":"Index","title":"Index","text":"This package is split in several submodules, but all modules are re-exported at the root module level. This means that you can access their functionality by simply using BetaML.","category":"page"},{"location":"index.html","page":"Index","title":"Index","text":"using BetaML\nmyLayer = DenseLayer(2,3) # DenseLayer is defined in the Nn submodule\nres     = KernelPerceptronClassifier([1.1 2.1; 5.3 4.2; 1.8 1.7], [-1,1,-1]) # KernelPerceptronClassifier is defined in the Perceptron module\n@edit DenseLayer(2,3)     # Open a text editor with to the relevant source code","category":"page"},{"location":"index.html#Usage","page":"Index","title":"Usage","text":"","category":"section"},{"location":"index.html","page":"Index","title":"Index","text":"New to BetaML or even to Julia / Machine Learning altogether? Start from the tutorial!","category":"page"},{"location":"index.html","page":"Index","title":"Index","text":"Detailed documentation for most algorithms can be retrieved using the inline Julia help system (just press the question mark ? and then, on the special help prompt help?>, type the function name) or on these pages under the section \"Api (Reference Manual)\" for the individual modules:","category":"page"},{"location":"index.html","page":"Index","title":"Index","text":"BetaML.Perceptron: The Perceptron, Kernel Perceptron and PegasosClassifier classification algorithms;\nBetaML.Trees: The Decision Trees and Random Forests algorithms for classification or regression (with missing values supported);\nBetaML.Nn: Implementation of Artificial Neural Networks;\nBetaML.Clustering: (hard) Clustering algorithms (Kmeans, Mdedoids\nBetaML.GMM: Various algorithms (Clustering, regressor, missing imputation / collaborative filtering / recommandation systems) that use a Generative (Gaussian) mixture models (probabilistic) fitter fitted using a EM algorithm;\nBetaML.Imputation: Imputation algorithms;\nBetaML.Utils`: Various utility functions (scale, one-hot, distances, kernels, pca, accuracy/error measures..).","category":"page"},{"location":"index.html#MLJ-interface","page":"Index","title":"MLJ interface","text":"","category":"section"},{"location":"index.html","page":"Index","title":"Index","text":"BetaML exports the following modules for usage with the MLJ toolkit:","category":"page"},{"location":"index.html","page":"Index","title":"Index","text":"Perceptron models: LinearPerceptron, KernelPerceptron, Pegasos\nDecision trees/Random forest models:  DecisionTreeClassifier, DecisionTreeRegressor, RandomForestClassifier, RandomForestRegressor\nClustering models and derived models: KMeans, KMedoids, GaussianMixtures, MissingImputator","category":"page"},{"location":"index.html","page":"Index","title":"Index","text":"Currently BetaML neural network models are not available in MLJ.","category":"page"},{"location":"index.html#Quick-examples","page":"Index","title":"Quick examples","text":"","category":"section"},{"location":"index.html","page":"Index","title":"Index","text":"(see the tutorial for a more step-by-step guide to the examples below and to other examples)","category":"page"},{"location":"index.html","page":"Index","title":"Index","text":"A \"V2\" API that uses a more uniform fit!(model,X,[Y]), predict(model,X) workflow is currently worked on.","category":"page"},{"location":"index.html","page":"Index","title":"Index","text":"Using an Artificial Neural Network for multinomial categorisation","category":"page"},{"location":"index.html","page":"Index","title":"Index","text":"# Load Modules\nusing BetaML, DelimitedFiles, Random, StatsPlots # Load the main module and ausiliary modules\nRandom.seed!(123); # Fix the random seed (to obtain reproducible results)\n\n# Load the data\niris     = readdlm(joinpath(dirname(pathof(BetaML)),\"..\",\"test\",\"data\",\"iris.csv\"),',',skipstart=1)\niris     = iris[shuffle(axes(iris, 1)), :] # Shuffle the records, as they aren't by default\nx        = convert(Array{Float64,2}, iris[:,1:4])\ny        = map(x->Dict(\"setosa\" => 1, \"versicolor\" => 2, \"virginica\" =>3)[x],iris[:, 5]) # Convert the target column to numbers\ny_oh     = onehotencoder(y) # Convert to One-hot representation (e.g. 2 => [0 1 0], 3 => [0 0 1])\n\n# Split the data in training/testing sets\n((xtrain,xtest),(ytrain,ytest),(ytrain_oh,ytest_oh)) = partition([x,y,y_oh],[0.8,0.2],shuffle=false)\n(ntrain, ntest) = size.([xtrain,xtest],1)\n\n# Define the Artificial Neural Network model\nl1   = DenseLayer(4,10,f=relu) # Activation function is ReLU\nl2   = DenseLayer(10,3)        # Activation function is identity by default\nl3   = VectorFunctionLayer(3,3,f=softMax) # Add a (parameterless) layer whose activation function (softMax in this case) is defined to all its nodes at once\nmynn = buildNetwork([l1,l2,l3],squared_cost,name=\"Multinomial logistic regression Model Sepal\") # Build the NN and use the squared cost (aka MSE) as error function\n\n# Training it (default to ADAM)\nres = train!(mynn,scale(xtrain),ytrain_oh,epochs=100,batch_size=6) # Use opt_alg=SGD (Stochastic Gradient Descent) by default\n\n# Test it\nŷtrain        = predict(mynn,scale(xtrain))   # Note the scaling function\nŷtest         = predict(mynn,scale(xtest))\ntrainAccuracy = accuracy(ŷtrain,ytrain,tol=1) # 0.983\ntestAccuracy  = accuracy(ŷtest,ytest,tol=1)   # 1.0\n\n# Visualise results\ntestSize    = size(ŷtest,1)\nŷtestChosen =  [argmax(ŷtest[i,:]) for i in 1:testSize]\ngroupedbar([ytest ŷtestChosen], label=[\"ytest\" \"ŷtest (est)\"], title=\"True vs estimated categories\") # All records correctly labelled !\nplot(0:res.epochs,res.ϵ_epochs, ylabel=\"epochs\",xlabel=\"error\",legend=nothing,title=\"Avg. error per epoch on the Sepal dataset\")","category":"page"},{"location":"index.html","page":"Index","title":"Index","text":"(Image: results)","category":"page"},{"location":"index.html","page":"Index","title":"Index","text":"(Image: results)","category":"page"},{"location":"index.html","page":"Index","title":"Index","text":"Using the Expectation-Maximisation algorithm for clustering","category":"page"},{"location":"index.html","page":"Index","title":"Index","text":"using BetaML, DelimitedFiles, Random, StatsPlots # Load the main module and ausiliary modules\nRandom.seed!(123); # Fix the random seed (to obtain reproducible results)\n\n# Load the data\niris     = readdlm(joinpath(dirname(pathof(BetaML)),\"..\",\"test\",\"data\",\"iris.csv\"),',',skipstart=1)\niris     = iris[shuffle(axes(iris, 1)), :] # Shuffle the records, as they aren't by default\nx        = convert(Array{Float64,2}, iris[:,1:4])\nx        = scale(x) # normalise all dimensions to (μ=0, σ=1)\ny        = map(x->Dict(\"setosa\" => 1, \"versicolor\" => 2, \"virginica\" =>3)[x],iris[:, 5]) # Convert the target column to numbers\n\n# Get some ranges of minimum_variance and minimum_covariance to test\nminVarRange   = collect(0.04:0.05:1.5)\nminCovarRange = collect(0:0.05:1.45)\n\n# Run the gmm(em) algorithm for the various cases...\nsphOut  = [gmm(x,3,mixtures=[SphericalGaussian() for i in 1:3],minimum_variance=v, minimum_covariance=cv, verbosity=NONE) for v in minVarRange, cv in minCovarRange[1:1]]\ndiagOut  = [gmm(x,3,mixtures=[DiagonalGaussian() for i in 1:3],minimum_variance=v, minimum_covariance=cv, verbosity=NONE)  for v in minVarRange, cv in minCovarRange[1:1]]\nfullOut = [gmm(x,3,mixtures=[FullGaussian() for i in 1:3],minimum_variance=v, minimum_covariance=cv, verbosity=NONE)  for v in minVarRange, cv in minCovarRange]\n\n# Get the Bayesian information criterion (AIC is also available)\nsphBIC = [sphOut[v,cv].BIC for v in 1:length(minVarRange), cv in 1:1]\ndiagBIC = [diagOut[v,cv].BIC for v in 1:length(minVarRange), cv in 1:1]\nfullBIC = [fullOut[v,cv].BIC for v in 1:length(minVarRange), cv in 1:length(minCovarRange)]\n\n# Compare the accuracy with true categories\nsphAcc  = [accuracy(sphOut[v,cv].pₙₖ,y,ignorelabels=true) for v in 1:length(minVarRange), cv in 1:1]\ndiagAcc = [accuracy(diagOut[v,cv].pₙₖ,y,ignorelabels=true) for v in 1:length(minVarRange), cv in 1:1]\nfullAcc = [accuracy(fullOut[v,cv].pₙₖ,y,ignorelabels=true) for v in 1:length(minVarRange), cv in 1:length(minCovarRange)]\n\nplot(minVarRange,[sphBIC diagBIC fullBIC[:,1] fullBIC[:,15] fullBIC[:,30]], markershape=:circle, label=[\"sph\" \"diag\" \"full (cov=0)\" \"full (cov=0.7)\" \"full (cov=1.45)\"], title=\"BIC\", xlabel=\"minimum_variance\")\nplot(minVarRange,[sphAcc diagAcc fullAcc[:,1] fullAcc[:,15] fullAcc[:,30]], markershape=:circle, label=[\"sph\" \"diag\" \"full (cov=0)\" \"full (cov=0.7)\" \"full (cov=1.45)\"], title=\"Accuracies\", xlabel=\"minimum_variance\")","category":"page"},{"location":"index.html","page":"Index","title":"Index","text":"(Image: results)","category":"page"},{"location":"index.html","page":"Index","title":"Index","text":"(Image: results)","category":"page"},{"location":"index.html","page":"Index","title":"Index","text":"Further examples","category":"page"},{"location":"index.html","page":"Index","title":"Index","text":"Finally, you may want to give a look at the \"test\" folder. While the primary objective of the scripts under the \"test\" folder is to provide automatic testing of the BetaML toolkit, they can also be used to see how functions should be called, as virtually all functions provided by BetaML are tested there.","category":"page"},{"location":"index.html#Acknowledgements","page":"Index","title":"Acknowledgements","text":"","category":"section"},{"location":"index.html","page":"Index","title":"Index","text":"The development of this package at the Bureau d'Economie Théorique et Appliquée (BETA, Nancy) was supported by the French National Research Agency through the Laboratory of Excellence ARBRE, a part of the “Investissements d'Avenir” Program (ANR 11 – LABX-0002-01).","category":"page"},{"location":"index.html","page":"Index","title":"Index","text":"(Image: BLogos)","category":"page"}]
}
