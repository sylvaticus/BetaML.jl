<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>A regression task: the prediction of  bike  sharing demand · BetaML.jl Documentation</title><meta name="title" content="A regression task: the prediction of  bike  sharing demand · BetaML.jl Documentation"/><meta property="og:title" content="A regression task: the prediction of  bike  sharing demand · BetaML.jl Documentation"/><meta property="twitter:title" content="A regression task: the prediction of  bike  sharing demand · BetaML.jl Documentation"/><meta name="description" content="Documentation for BetaML.jl Documentation."/><meta property="og:description" content="Documentation for BetaML.jl Documentation."/><meta property="twitter:description" content="Documentation for BetaML.jl Documentation."/><script async src="https://www.googletagmanager.com/gtag/js?id=G-JYKX8QY5JW"></script><script>  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());
  gtag('config', 'G-JYKX8QY5JW', {'page_path': location.pathname + location.search + location.hash});
</script><script data-outdated-warner src="../../assets/warner.js"></script><link href="https://cdnjs.cloudflare.com/ajax/libs/lato-font/3.0.0/css/lato-font.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/juliamono/0.050/juliamono.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/fontawesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/solid.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/brands.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.16.8/katex.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL="../.."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" data-main="../../assets/documenter.js"></script><script src="../../search_index.js"></script><script src="../../siteinfo.js"></script><script src="../../../versions.js"></script><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/documenter-dark.css" data-theme-name="documenter-dark" data-theme-primary-dark/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/documenter-light.css" data-theme-name="documenter-light" data-theme-primary/><script src="../../assets/themeswap.js"></script></head><body><div id="documenter"><nav class="docs-sidebar"><div class="docs-package-name"><span class="docs-autofit"><a href="../../index.html">BetaML.jl Documentation</a></span></div><button class="docs-search-query input is-rounded is-small is-clickable my-2 mx-auto py-1 px-2" id="documenter-search-query">Search docs (Ctrl + /)</button><ul class="docs-menu"><li><a class="tocitem" href="../../index.html">Index</a></li><li><span class="tocitem">Tutorial</span><ul><li><a class="tocitem" href="../Betaml_tutorial_getting_started.html">Getting started</a></li><li><input class="collapse-toggle" id="menuitem-2-2" type="checkbox"/><label class="tocitem" for="menuitem-2-2"><span class="docs-label">Classification - cars</span><i class="docs-chevron"></i></label><ul class="collapsed"><li><a class="tocitem" href="../Classification - cars/betaml_tutorial_classification_cars.html">A classification task when labels are known - determining the country of origin of cars given the cars characteristics</a></li></ul></li><li><input class="collapse-toggle" id="menuitem-2-3" type="checkbox" checked/><label class="tocitem" for="menuitem-2-3"><span class="docs-label">Regression - bike sharing</span><i class="docs-chevron"></i></label><ul class="collapsed"><li class="is-active"><a class="tocitem" href="betaml_tutorial_regression_sharingBikes.html">A regression task: the prediction of  bike  sharing demand</a><ul class="internal"><li><a class="tocitem" href="#Library-and-data-loading"><span>Library and data loading</span></a></li><li><a class="tocitem" href="#Decision-Trees"><span>Decision Trees</span></a></li><li><a class="tocitem" href="#Random-Forests"><span>Random Forests</span></a></li><li><a class="tocitem" href="#Neural-Networks"><span>Neural Networks</span></a></li><li><a class="tocitem" href="#GMM-based-regressors"><span>GMM-based regressors</span></a></li><li><a class="tocitem" href="#Summary"><span>Summary</span></a></li></ul></li></ul></li><li><input class="collapse-toggle" id="menuitem-2-4" type="checkbox"/><label class="tocitem" for="menuitem-2-4"><span class="docs-label">Clustering - Iris</span><i class="docs-chevron"></i></label><ul class="collapsed"><li><a class="tocitem" href="../Clustering - Iris/betaml_tutorial_cluster_iris.html">A clustering task: the prediction of  plant species from floreal measures (the iris dataset)</a></li></ul></li><li><input class="collapse-toggle" id="menuitem-2-5" type="checkbox"/><label class="tocitem" for="menuitem-2-5"><span class="docs-label">Multi-branch neural network</span><i class="docs-chevron"></i></label><ul class="collapsed"><li><a class="tocitem" href="../Multi-branch neural network/betaml_tutorial_multibranch_nn.html">A deep neural network with multi-branch architecture</a></li></ul></li></ul></li><li><span class="tocitem">API (Reference manual)</span><ul><li><input class="collapse-toggle" id="menuitem-3-1" type="checkbox"/><label class="tocitem" for="menuitem-3-1"><span class="docs-label">API V2 (current)</span><i class="docs-chevron"></i></label><ul class="collapsed"><li><a class="tocitem" href="../../Api_v2_user.html">Introduction for user</a></li><li><input class="collapse-toggle" id="menuitem-3-1-2" type="checkbox"/><label class="tocitem" for="menuitem-3-1-2"><span class="docs-label">For developers</span><i class="docs-chevron"></i></label><ul class="collapsed"><li><a class="tocitem" href="../../Api_v2_developer.html">API implementation</a></li><li><a class="tocitem" href="../../StyleGuide_templates.html">Style guide</a></li></ul></li><li><a class="tocitem" href="../../Api.html">The Api module</a></li></ul></li><li><a class="tocitem" href="../../Perceptron.html">Perceptron</a></li><li><a class="tocitem" href="../../Trees.html">Trees</a></li><li><a class="tocitem" href="../../Nn.html">Nn</a></li><li><a class="tocitem" href="../../Clustering.html">Clustering</a></li><li><a class="tocitem" href="../../GMM.html">GMM</a></li><li><a class="tocitem" href="../../Imputation.html">Imputation</a></li><li><a class="tocitem" href="../../Utils.html">Utils</a></li></ul></li><li><a class="tocitem" href="../../MLJ_interface.html">MLJ interface</a></li><li><a class="tocitem" href="../../Benchmarks.html">Benchmarks</a></li></ul><div class="docs-version-selector field has-addons"><div class="control"><span class="docs-label button is-static is-size-7">Version</span></div><div class="docs-selector control is-expanded"><div class="select is-fullwidth is-size-7"><select id="documenter-version-selector"></select></div></div></div></nav><div class="docs-main"><header class="docs-navbar"><a class="docs-sidebar-button docs-navbar-link fa-solid fa-bars is-hidden-desktop" id="documenter-sidebar-button" href="#"></a><nav class="breadcrumb"><ul class="is-hidden-mobile"><li><a class="is-disabled">Tutorial</a></li><li><a class="is-disabled">Regression - bike sharing</a></li><li class="is-active"><a href="betaml_tutorial_regression_sharingBikes.html">A regression task: the prediction of  bike  sharing demand</a></li></ul><ul class="is-hidden-tablet"><li class="is-active"><a href="betaml_tutorial_regression_sharingBikes.html">A regression task: the prediction of  bike  sharing demand</a></li></ul></nav><div class="docs-right"><a class="docs-navbar-link" href="https://github.com/sylvaticus/BetaML.jl" title="View the repository on GitHub"><span class="docs-icon fa-brands"></span><span class="docs-label is-hidden-touch">GitHub</span></a><a class="docs-navbar-link" href="https://github.com/sylvaticus/BetaML.jl/blob/master/docs/src/tutorials/Regression - bike sharing/betaml_tutorial_regression_sharingBikes.jl" title="Edit source on GitHub"><span class="docs-icon fa-solid"></span></a><a class="docs-settings-button docs-navbar-link fa-solid fa-gear" id="documenter-settings-button" href="#" title="Settings"></a><a class="docs-article-toggle-button fa-solid fa-chevron-up" id="documenter-article-toggle-button" href="javascript:;" title="Collapse all docstrings"></a></div></header><article class="content" id="documenter-page"><h1 id="regression_tutorial"><a class="docs-heading-anchor" href="#regression_tutorial">A regression task: the prediction of  bike  sharing demand</a><a id="regression_tutorial-1"></a><a class="docs-heading-anchor-permalink" href="#regression_tutorial" title="Permalink"></a></h1><p>The task is to estimate the influence of several variables (like the weather, the season, the day of the week..) on the demand of shared bicycles, so that the authority in charge of the service can organise the service in the best way.</p><p>Data origin:</p><ul><li>original full dataset (by hour, not used here): <a href="https://archive.ics.uci.edu/ml/datasets/Bike+Sharing+Dataset">https://archive.ics.uci.edu/ml/datasets/Bike+Sharing+Dataset</a></li><li>simplified dataset (by day, with some simple scaling): <a href="https://www.hds.utc.fr/~tdenoeux/dokuwiki/en/aec">https://www.hds.utc.fr/~tdenoeux/dokuwiki/en/aec</a></li><li>description: <a href="https://www.hds.utc.fr/~tdenoeux/dokuwiki/_media/en/exam_2019_ace_.pdf">https://www.hds.utc.fr/~tdenoeux/dokuwiki/<em>media/en/exam</em>2019<em>ace</em>.pdf</a></li><li>data: <a href="https://www.hds.utc.fr/~tdenoeux/dokuwiki/_media/en/bike_sharing_day.csv.zip">https://www.hds.utc.fr/~tdenoeux/dokuwiki/<em>media/en/bike</em>sharing_day.csv.zip</a></li></ul><p>Note that even if we are estimating a time serie, we are not using here a recurrent neural network as we assume the temporal dependence to be negligible (i.e. <span>$Y_t = f(X_t)$</span> alone).</p><h2 id="Library-and-data-loading"><a class="docs-heading-anchor" href="#Library-and-data-loading">Library and data loading</a><a id="Library-and-data-loading-1"></a><a class="docs-heading-anchor-permalink" href="#Library-and-data-loading" title="Permalink"></a></h2><p>Activating the local environment specific to</p><pre><code class="language-julia hljs">using Pkg
Pkg.activate(joinpath(@__DIR__,&quot;..&quot;,&quot;..&quot;,&quot;..&quot;))</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">  Activating environment at `~/work/BetaML.jl/BetaML.jl/docs/Project.toml`</code></pre><p>We first load all the packages we are going to use</p><pre><code class="language-julia hljs">using  LinearAlgebra, Random, Statistics, StableRNGs, DataFrames, CSV, Plots, Pipe, BenchmarkTools, BetaML
import Distributions: Uniform, DiscreteUniform
import DecisionTree, Flux ## For comparisions</code></pre><p>Here we are explicit and we use our own fixed RNG:</p><pre><code class="language-julia hljs">seed = 123 # The table at the end of this tutorial has been obtained with seeds 123, 1000 and 10000
AFIXEDRNG = StableRNG(seed)</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">StableRNGs.LehmerRNG(state=0x000000000000000000000000000000f7)</code></pre><p>Here we load the data from a csv provided by the BataML package</p><pre><code class="language-julia hljs">basedir = joinpath(dirname(pathof(BetaML)),&quot;..&quot;,&quot;docs&quot;,&quot;src&quot;,&quot;tutorials&quot;,&quot;Regression - bike sharing&quot;)
data    = CSV.File(joinpath(basedir,&quot;data&quot;,&quot;bike_sharing_day.csv&quot;),delim=&#39;,&#39;) |&gt; DataFrame
describe(data)</code></pre><div class="data-frame"><p>16 rows × 7 columns</p><table class="data-frame"><thead><tr><th></th><th>variable</th><th>mean</th><th>min</th><th>median</th><th>max</th><th>nmissing</th><th>eltype</th></tr><tr><th></th><th title="Symbol">Symbol</th><th title="Union{Nothing, Float64}">Union…</th><th title="Any">Any</th><th title="Union{Nothing, Float64}">Union…</th><th title="Any">Any</th><th title="Int64">Int64</th><th title="DataType">DataType</th></tr></thead><tbody><tr><th>1</th><td>instant</td><td>366.0</td><td>1</td><td>366.0</td><td>731</td><td>0</td><td>Int64</td></tr><tr><th>2</th><td>dteday</td><td></td><td>2011-01-01</td><td></td><td>2012-12-31</td><td>0</td><td>Date</td></tr><tr><th>3</th><td>season</td><td>2.49658</td><td>1</td><td>3.0</td><td>4</td><td>0</td><td>Int64</td></tr><tr><th>4</th><td>yr</td><td>0.500684</td><td>0</td><td>1.0</td><td>1</td><td>0</td><td>Int64</td></tr><tr><th>5</th><td>mnth</td><td>6.51984</td><td>1</td><td>7.0</td><td>12</td><td>0</td><td>Int64</td></tr><tr><th>6</th><td>holiday</td><td>0.0287278</td><td>0</td><td>0.0</td><td>1</td><td>0</td><td>Int64</td></tr><tr><th>7</th><td>weekday</td><td>2.99726</td><td>0</td><td>3.0</td><td>6</td><td>0</td><td>Int64</td></tr><tr><th>8</th><td>workingday</td><td>0.683995</td><td>0</td><td>1.0</td><td>1</td><td>0</td><td>Int64</td></tr><tr><th>9</th><td>weathersit</td><td>1.39535</td><td>1</td><td>1.0</td><td>3</td><td>0</td><td>Int64</td></tr><tr><th>10</th><td>temp</td><td>0.495385</td><td>0.0591304</td><td>0.498333</td><td>0.861667</td><td>0</td><td>Float64</td></tr><tr><th>11</th><td>atemp</td><td>0.474354</td><td>0.0790696</td><td>0.486733</td><td>0.840896</td><td>0</td><td>Float64</td></tr><tr><th>12</th><td>hum</td><td>0.627894</td><td>0.0</td><td>0.626667</td><td>0.9725</td><td>0</td><td>Float64</td></tr><tr><th>13</th><td>windspeed</td><td>0.190486</td><td>0.0223917</td><td>0.180975</td><td>0.507463</td><td>0</td><td>Float64</td></tr><tr><th>14</th><td>casual</td><td>848.176</td><td>2</td><td>713.0</td><td>3410</td><td>0</td><td>Int64</td></tr><tr><th>15</th><td>registered</td><td>3656.17</td><td>20</td><td>3662.0</td><td>6946</td><td>0</td><td>Int64</td></tr><tr><th>16</th><td>cnt</td><td>4504.35</td><td>22</td><td>4548.0</td><td>8714</td><td>0</td><td>Int64</td></tr></tbody></table></div><p>The variable we want to learn to predict is <code>cnt</code>, the total demand of bikes for a given day. Even if it is indeed an integer, we treat it as a continuous variable, so each single prediction will be a scalar <span>$Y \in \mathbb{R}$</span>.</p><pre><code class="language-julia hljs">plot(data.cnt, title=&quot;Daily bike sharing rents (2Y)&quot;, label=nothing)</code></pre><img src="betaml_tutorial_regression_sharingBikes-68cb64a2.svg" alt="Example block output"/><h2 id="Decision-Trees"><a class="docs-heading-anchor" href="#Decision-Trees">Decision Trees</a><a id="Decision-Trees-1"></a><a class="docs-heading-anchor-permalink" href="#Decision-Trees" title="Permalink"></a></h2><p>We start our regression task with Decision Trees.</p><p>Decision trees training consist in choosing the set of questions (in a hierarcical way, so to form indeed a &quot;decision tree&quot;) that &quot;best&quot; split the dataset given for training, in the sense that the split generate the sub-samples (always 2 subsamples in the BetaML implementation) that are, for the characteristic we want to predict, the most homogeneous possible. Decision trees are one of the few ML algorithms that has an intuitive interpretation and can be used for both regression or classification tasks.</p><h3 id="Data-preparation"><a class="docs-heading-anchor" href="#Data-preparation">Data preparation</a><a id="Data-preparation-1"></a><a class="docs-heading-anchor-permalink" href="#Data-preparation" title="Permalink"></a></h3><p>The first step is to prepare the data for the analysis. This indeed depends already on the model we want to employ, as some models &quot;accept&quot; almost everything as input, no matter if the data is numerical or categorical, if it has missing values or not... while other models are instead much more exigents, and require more work to &quot;clean up&quot; our dataset.</p><p>The tutorial starts using  Decision Tree and Random Forest models that definitly belong to the first group, so the only thing we have to do is to select the variables in input (the &quot;feature matrix&quot;, that we will indicate with &quot;X&quot;) and the variable representing our output (the information we want to learn to predict, we call it &quot;y&quot;):</p><pre><code class="language-julia hljs">x    = Matrix{Float64}(data[:,[:instant,:season,:yr,:mnth,:holiday,:weekday,:workingday,:weathersit,:temp,:atemp,:hum,:windspeed]])
y    = data[:,16];</code></pre><p>We finally set up a dataframe to store the relative mean errors of the various models we&#39;ll use.</p><pre><code class="language-julia hljs">results = DataFrame(model=String[],train_rme=Float64[],test_rme=Float64[])</code></pre><div class="data-frame"><p>0 rows × 3 columns</p><table class="data-frame"><thead><tr><th></th><th>model</th><th>train_rme</th><th>test_rme</th></tr><tr><th></th><th title="String">String</th><th title="Float64">Float64</th><th title="Float64">Float64</th></tr></thead><tbody></tbody></table></div><h3 id="Model-selection"><a class="docs-heading-anchor" href="#Model-selection">Model selection</a><a id="Model-selection-1"></a><a class="docs-heading-anchor-permalink" href="#Model-selection" title="Permalink"></a></h3><p>We can now split the dataset between the data that we will use for training the algorithm and selecting the hyperparameters (<code>xtrain</code>/<code>ytrain</code>) and those for testing the quality of the algoritm with the optimal hyperparameters (<code>xtest</code>/<code>ytest</code>). We use the <code>partition</code> function specifying the share we want to use for these two different subsets, here 80%, and 20% respectively. As our data represents indeed a time serie, we want our model to be able to predict <em>future</em> demand of bike sharing from <em>past</em>, observed rented bikes, so we do not shuffle the datasets as it would be the default.</p><pre><code class="language-julia hljs">((xtrain,xtest),(ytrain,ytest)) = partition([x,y],[0.75,1-0.75],shuffle=false)
(ntrain, ntest) = size.([ytrain,ytest],1)</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">2-element Vector{Int64}:
 548
 183</code></pre><p>Then we define the model we want to use, <a href="../../Trees.html#BetaML.Trees.DecisionTreeEstimator"><code>DecisionTreeEstimator</code></a> in this case, and we create an instance of the model:</p><pre><code class="language-julia hljs">m = DecisionTreeEstimator(autotune=true, rng=copy(AFIXEDRNG))</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">DecisionTreeEstimator - A Decision Tree model (unfitted)</code></pre><p>Passing a fixed Random Number Generator (RNG) to the <code>rng</code> parameter guarantees that everytime we use the model with the same data (from the model creation downward to value prediciton) we obtain the same results. In particular BetaML provide <code>FIXEDRNG</code>, an istance of <code>StableRNG</code> that guarantees reproducibility even across different Julia versions. See the section <a href="../Betaml_tutorial_getting_started.html#stochasticity_reproducibility">&quot;Dealing with stochasticity&quot;</a> for details. Note the <code>autotune</code> parameter. BetaML has perhaps what is the easiest method for automatically tuning the model hyperparameters (thus becoming in this way <em>learned</em> parameters). Indeed, in most cases it is enought to pass the attribute <code>autotune=true</code> on the model constructor and hyperparameters search will be automatically performed on the first <code>fit!</code> call. If needed we can customise hyperparameter tuning, chosing the tuning method on the parameter <code>tunemethod</code>. The single-line above is equivalent to:</p><pre><code class="language-julia hljs">tuning_method = SuccessiveHalvingSearch(
                   hpranges     = Dict(&quot;max_depth&quot; =&gt;[5,10,nothing], &quot;min_gain&quot;=&gt;[0.0, 0.1, 0.5], &quot;min_records&quot;=&gt;[2,3,5],&quot;max_features&quot;=&gt;[nothing,5,10,30]),
                   loss         = l2loss_by_cv,
                   res_shares   = [0.05, 0.2, 0.3],
                   multithreads = true
                )
m_dt = DecisionTreeEstimator(autotune=true, rng=copy(AFIXEDRNG), tunemethod=tuning_method)</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">DecisionTreeEstimator - A Decision Tree model (unfitted)</code></pre><p>Note that the defaults change according to the specific model, for example <code>RandomForestEstimator</code>](@ref) autotuning default to not being multithreaded, as the individual model is already multithreaded.</p><div class="admonition is-success"><header class="admonition-header">Tip</header><div class="admonition-body"><p>Refer to the versions of this tutorial for BetaML &lt;= 0.6 for a good exercise on how to perform model selection using the <a href="../../Utils.html#BetaML.Utils.cross_validation"><code>cross_validation</code></a> function, or even by custom grid search.</p></div></div><p>We can now fit the model, that is learn the model parameters that lead to the best predictions from the data. By default (unless we use <code>cache=false</code> in the model constructor) the model stores also the training predictions, so we can just use <code>fit!()</code> instead of <code>fit!()</code> followed by <code>predict(model,xtrain)</code></p><pre><code class="language-julia hljs">ŷtrain = fit!(m_dt,xtrain,ytrain)</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">548-element Vector{Float64}:
  988.6666666666666
  860.6666666666666
 1349.0
 1551.6666666666667
 1577.3333333333333
 1577.3333333333333
 1505.5
  860.6666666666666
  860.6666666666666
 1321.0
    ⋮
 7478.0
 6849.666666666667
 6849.666666666667
 7442.0
 7336.5
 6849.666666666667
 5560.333333333333
 5560.333333333333
 5560.333333333333</code></pre><p>The above code produces a fitted <code>DecisionTreeEstimator</code> object that can be used to make predictions given some new features, i.e. given a new X matrix of (number of observations x dimensions), predict the corresponding Y vector of scalars in R.</p><pre><code class="language-julia hljs">ŷtest  = predict(m_dt, xtest)</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">183-element Vector{Float64}:
 5560.333333333333
 5560.333333333333
 5560.333333333333
 5560.333333333333
 5560.333333333333
 5560.333333333333
 5560.333333333333
 6804.666666666667
 6804.666666666667
 6804.666666666667
    ⋮
 5459.0
 2120.5
 5459.0
 2120.5
 4896.333333333333
 5459.0
 4169.0
 4896.333333333333
 5459.0</code></pre><p>We now compute the mean relative error for the training and the test set. The <a href="../../Utils.html#BetaML.Utils.relative_mean_error-Tuple{Any, Any}"><code>relative_mean_error</code></a> is a very flexible error function. Without additional parameter, it computes, as the name says, the <em>relative mean error</em>, between an estimated and a true vector. However it can also compute the <em>mean relative error</em>, also known as the &quot;mean absolute percentage error&quot; (<a href="https://en.wikipedia.org/wiki/Mean_absolute_percentage_error">MAPE</a>), or use a p-norm higher than 1. The <em>mean relative error</em> enfatises the relativeness of the error, i.e. all observations and dimensions weigth the same, wether large or small. Conversly, in the <em>relative mean error</em> the same relative error on larger observations (or dimensions) weights more. In this tutorial we use the later, as our data has clearly some outlier days with very small rents, and we care more of avoiding our customers finding empty bike racks than having unrented bikes on the rack. Targeting a low mean average error would push all our predicitons down to try accomodate the low-level predicitons (to avoid a large relative error), and that&#39;s not what we want.</p><p>We can then compute the relative mean error for the decision tree</p><pre><code class="language-julia hljs">rme_train = relative_mean_error(ytrain,ŷtrain) # 0.1367
rme_test  = relative_mean_error(ytest,ŷtest) # 0.1547</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">0.1728808325123301</code></pre><p>And we save the real mean accuracies in the <code>results</code> dataframe:</p><pre><code class="language-julia hljs">push!(results,[&quot;DT&quot;,rme_train,rme_test]);</code></pre><p>We can plot the true labels vs the estimated one for the three subsets...</p><pre><code class="language-julia hljs">scatter(ytrain,ŷtrain,xlabel=&quot;daily rides&quot;,ylabel=&quot;est. daily rides&quot;,label=nothing,title=&quot;Est vs. obs in training period (DT)&quot;)</code></pre><img src="betaml_tutorial_regression_sharingBikes-1422c25e.svg" alt="Example block output"/><pre><code class="language-julia hljs">scatter(ytest,ŷtest,xlabel=&quot;daily rides&quot;,ylabel=&quot;est. daily rides&quot;,label=nothing,title=&quot;Est vs. obs in testing period (DT)&quot;)</code></pre><img src="betaml_tutorial_regression_sharingBikes-15bedfcb.svg" alt="Example block output"/><p>Or we can visualise the true vs estimated bike shared on a temporal base. First on the full period (2 years) ...</p><pre><code class="language-julia hljs">ŷtrainfull = vcat(ŷtrain,fill(missing,ntest))
ŷtestfull  = vcat(fill(missing,ntrain), ŷtest)
plot(data[:,:dteday],[data[:,:cnt] ŷtrainfull ŷtestfull], label=[&quot;obs&quot; &quot;train&quot; &quot;test&quot;], legend=:topleft, ylabel=&quot;daily rides&quot;, title=&quot;Daily bike sharing demand observed/estimated across the\n whole 2-years period (DT)&quot;)</code></pre><img src="betaml_tutorial_regression_sharingBikes-9ca916da.svg" alt="Example block output"/><p>..and then focusing on the testing period</p><pre><code class="language-julia hljs">stc = ntrain
endc = size(x,1)
plot(data[stc:endc,:dteday],[data[stc:endc,:cnt] ŷtestfull[stc:endc]], label=[&quot;obs&quot; &quot;test&quot;], legend=:bottomleft, ylabel=&quot;Daily rides&quot;, title=&quot;Focus on the testing period (DT)&quot;)</code></pre><img src="betaml_tutorial_regression_sharingBikes-d2c080b3.svg" alt="Example block output"/><p>The predictions aren&#39;t so bad in this case, however decision trees are highly instable, and the output could have depended just from the specific initial random seed.</p><h2 id="Random-Forests"><a class="docs-heading-anchor" href="#Random-Forests">Random Forests</a><a id="Random-Forests-1"></a><a class="docs-heading-anchor-permalink" href="#Random-Forests" title="Permalink"></a></h2><p>Rather than trying to solve this problem using a single Decision Tree model, let&#39;s not try to use a <em>Random Forest</em> model. Random forests average the results of many different decision trees and provide a more &quot;stable&quot; result. Being made of many decision trees, random forests are hovever more computationally expensive to train.</p><pre><code class="language-julia hljs">m_rf      = RandomForestEstimator(autotune=true, oob=true, rng=copy(AFIXEDRNG))
ŷtrain    = fit!(m_rf,xtrain,ytrain);
ŷtest     = predict(m_rf,xtest);
rme_train = relative_mean_error(ytrain,ŷtrain) # 0.056
rme_test  = relative_mean_error(ytest,ŷtest)   # 0.161
push!(results,[&quot;RF&quot;,rme_train,rme_test]);</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">Starting hyper-parameters autotuning (this could take a while..)
(e 1 / 7) N data / n candidates / n candidates to retain : 43.84 	 1296 466
(e 2 / 7) N data / n candidates / n candidates to retain : 54.800000000000004 	 466 167
(e 3 / 7) N data / n candidates / n candidates to retain : 71.24000000000001 	 167 60
(e 4 / 7) N data / n candidates / n candidates to retain : 82.2 	 60 22
(e 5 / 7) N data / n candidates / n candidates to retain : 109.60000000000001 	 22 8
(e 6 / 7) N data / n candidates / n candidates to retain : 164.4 	 8 3
(e 7 / 7) N data / n candidates / n candidates to retain : 219.20000000000002 	 3 1</code></pre><p>While slower than individual decision trees, random forests remain relativly fast. We should also consider that they are by default efficiently parallelised, so their speed increases with the number of available cores (in building this documentation page, GitHub CI servers allow for a single core, so all the bechmark you see in this tutorial are run with a single core available).</p><p>Random forests support the so-called &quot;out-of-bag&quot; error, an estimation of the error that we would have when the model is applied on a testing sample. However in this case the oob reported is much smaller than the testing error we will actually find. This is due to the fact that the division between training/validation and testing in this exercise is not random, but has a temporal basis. It seems that in this example the data in validation/testing follows a different pattern/variance than those in training (in probabilistic terms, the daily observations are not i.i.d.).</p><pre><code class="language-julia hljs">info(m_rf)
oob_error, rme_test  = info(m_rf)[&quot;oob_errors&quot;],relative_mean_error(ytest,ŷtest)</code></pre><p>In this case we found an error very similar to the one employing a single decision tree. Let&#39;s print the observed data vs the estimated one using the random forest and then along the temporal axis:</p><pre><code class="language-julia hljs">scatter(ytrain,ŷtrain,xlabel=&quot;daily rides&quot;,ylabel=&quot;est. daily rides&quot;,label=nothing,title=&quot;Est vs. obs in training period (RF)&quot;)</code></pre><img src="betaml_tutorial_regression_sharingBikes-cbe4ed4d.svg" alt="Example block output"/><pre><code class="language-julia hljs">scatter(ytest,ŷtest,xlabel=&quot;daily rides&quot;,ylabel=&quot;est. daily rides&quot;,label=nothing,title=&quot;Est vs. obs in testing period (RF)&quot;)</code></pre><img src="betaml_tutorial_regression_sharingBikes-b0e768fe.svg" alt="Example block output"/><p>Full period plot (2 years):</p><pre><code class="language-julia hljs">ŷtrainfull = vcat(ŷtrain,fill(missing,ntest))
ŷtestfull  = vcat(fill(missing,ntrain), ŷtest)
plot(data[:,:dteday],[data[:,:cnt] ŷtrainfull ŷtestfull], label=[&quot;obs&quot; &quot;train&quot; &quot;test&quot;], legend=:topleft, ylabel=&quot;daily rides&quot;, title=&quot;Daily bike sharing demand observed/estimated across the\n whole 2-years period (RF)&quot;)</code></pre><img src="betaml_tutorial_regression_sharingBikes-bf249a3c.svg" alt="Example block output"/><p>Focus on the testing period:</p><pre><code class="language-julia hljs">stc = 620
endc = size(x,1)
plot(data[stc:endc,:dteday],[data[stc:endc,:cnt] ŷtrainfull[stc:endc] ŷtestfull[stc:endc]], label=[&quot;obs&quot; &quot;val&quot; &quot;test&quot;], legend=:bottomleft, ylabel=&quot;Daily rides&quot;, title=&quot;Focus on the testing period (RF)&quot;)</code></pre><img src="betaml_tutorial_regression_sharingBikes-ed3f97f1.svg" alt="Example block output"/><h3 id="Comparison-with-DecisionTree.jl-random-forest"><a class="docs-heading-anchor" href="#Comparison-with-DecisionTree.jl-random-forest">Comparison with DecisionTree.jl random forest</a><a id="Comparison-with-DecisionTree.jl-random-forest-1"></a><a class="docs-heading-anchor-permalink" href="#Comparison-with-DecisionTree.jl-random-forest" title="Permalink"></a></h3><p>We now compare our results with those obtained employing the same model in the <a href="https://github.com/bensadeghi/DecisionTree.jl">DecisionTree package</a>, using the hyperparameters of the obtimal BetaML Random forest model:</p><pre><code class="language-julia hljs">best_rf_hp = hyperparameters(m_rf)</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">RandomForestE_hp (a BetaMLHyperParametersSet struct)
- n_trees: 30
- max_depth: nothing
- min_gain: 0.0
- min_records: 5
- max_features: 5
- force_classification: false
- splitting_criterion: nothing
- fast_algorithm: false
- integer_encoded_cols: nothing
- beta: 0.01
- oob: true
- tunemethod: SuccessiveHalvingSearch(BetaML.Utils.l2loss_by_cv, [0.08, 0.1, 0.13, 0.15, 0.2, 0.3, 0.4], Dict{String, Any}(&quot;max_features&quot; =&gt; Union{Nothing, Int64}[nothing, 5, 10, 30], &quot;max_depth&quot; =&gt; Union{Nothing, Int64}[5, 10, nothing], &quot;n_trees&quot; =&gt; [10, 20, 30, 40], &quot;min_records&quot; =&gt; [2, 3, 5], &quot;min_gain&quot; =&gt; [0.0, 0.1, 0.5], &quot;beta&quot; =&gt; [0.0, 0.01, 0.1]), false)
</code></pre><p>Hyperparameters of the DecisionTree.jl random forest model</p><pre><code class="language-julia hljs">n_subfeatures=isnothing(best_rf_hp.max_features) ? -1 : best_rf_hp.max_features; n_trees=best_rf_hp.n_trees; partial_sampling=0.7; max_depth=isnothing(best_rf_hp.max_depth) ? typemax(Int64) : best_rf_hp.max_depth;
min_samples_leaf=best_rf_hp.min_records; min_samples_split=best_rf_hp.min_records; min_purity_increase=best_rf_hp.min_gain;</code></pre><p>We train the model..</p><pre><code class="language-julia hljs">model = DecisionTree.build_forest(ytrain, convert(Matrix,xtrain),
                     n_subfeatures,
                     n_trees,
                     partial_sampling,
                     max_depth,
                     min_samples_leaf,
                     min_samples_split,
                     min_purity_increase;
                     rng = seed)</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">Ensemble of Decision Trees
Trees:      30
Avg Leaves: 63.43333333333333
Avg Depth:  6.166666666666667</code></pre><p>And we generate predictions and measure their error</p><pre><code class="language-julia hljs">(ŷtrain,ŷtest) = DecisionTree.apply_forest.([model],[xtrain,xtest]);


(rme_train, rme_test) = relative_mean_error.([ytrain,ytest],[ŷtrain,ŷtest]) # 0.022 and 0.304
push!(results,[&quot;RF (DecisionTree.jl)&quot;,rme_train,rme_test]);</code></pre><p>While the train error is very small, the error on the test set remains relativly high. The very low error level on the training set is a sign that it overspecialised on the training set, and we should have better ran a dedicated hyper-parameter tuning function for the DecisionTree.jl model (we did try using the default <code>DecisionTrees.jl</code> parameters, but we obtained roughtly the same results).</p><p>Finally we plot the DecisionTree.jl predictions alongside the observed value:</p><pre><code class="language-julia hljs">ŷtrainfull = vcat(ŷtrain,fill(missing,ntest))
ŷtestfull  = vcat(fill(missing,ntrain), ŷtest)
plot(data[:,:dteday],[data[:,:cnt] ŷtrainfull ŷtestfull], label=[&quot;obs&quot; &quot;train&quot; &quot;test&quot;], legend=:topleft, ylabel=&quot;daily rides&quot;, title=&quot;Daily bike sharing demand observed/estimated across the\n whole 2-years period (DT.jl RF)&quot;)</code></pre><img src="betaml_tutorial_regression_sharingBikes-012be0da.svg" alt="Example block output"/><p>Again, focusing on the testing data:</p><pre><code class="language-julia hljs">stc  = ntrain
endc = size(x,1)
plot(data[stc:endc,:dteday],[data[stc:endc,:cnt] ŷtestfull[stc:endc]], label=[&quot;obs&quot; &quot;test&quot;], legend=:bottomleft, ylabel=&quot;Daily rides&quot;, title=&quot;Focus on the testing period (DT.jl RF)&quot;)</code></pre><img src="betaml_tutorial_regression_sharingBikes-0c2f88d4.svg" alt="Example block output"/><h3 id="Conclusions-of-Decision-Trees-/-Random-Forests-methods"><a class="docs-heading-anchor" href="#Conclusions-of-Decision-Trees-/-Random-Forests-methods">Conclusions of Decision Trees / Random Forests methods</a><a id="Conclusions-of-Decision-Trees-/-Random-Forests-methods-1"></a><a class="docs-heading-anchor-permalink" href="#Conclusions-of-Decision-Trees-/-Random-Forests-methods" title="Permalink"></a></h3><p>The error obtained employing DecisionTree.jl is significantly larger than those obtained using a BetaML random forest model, altought to be fair with DecisionTrees.jl we didn&#39;t tuned its hyper-parameters. Also, the DecisionTree.jl random forest model is much faster. This is partially due by the fact that, internally, DecisionTree.jl models optimise the algorithm by sorting the observations. BetaML trees/forests don&#39;t employ this optimisation and hence they can work with true categorical data for which ordering is not defined. An other explanation of this difference in speed is that BetaML Random Forest models accept <code>missing</code> values within the feature matrix. To sum up, BetaML random forests are ideal algorithms when we want to obtain good predictions in the most simpler way, even without manually tuning the hyper-parameters, and without spending time in cleaning (&quot;munging&quot;) the feature matrix, as they accept almost &quot;any kind&quot; of data as it is.</p><h2 id="Neural-Networks"><a class="docs-heading-anchor" href="#Neural-Networks">Neural Networks</a><a id="Neural-Networks-1"></a><a class="docs-heading-anchor-permalink" href="#Neural-Networks" title="Permalink"></a></h2><p>BetaML provides only <em>deep forward neural networks</em>, artificial neural network units where the individual &quot;nodes&quot; are arranged in <em>layers</em>, from the <em>input layer</em>, where each unit holds the input coordinate, through various <em>hidden layer</em> transformations, until the actual <em>output</em> of the model:</p><p><img src="imgs/nn_scheme.png" alt="Neural Networks"/></p><p>In this layerwise computation, each unit in a particular layer takes input from <em>all</em> the preceding layer units and it has its own parameters that are adjusted to perform the overall computation. The <em>training</em> of the network consists in retrieving the coefficients that minimise a <em>loss</em> function between the output of the model and the known data. In particular, a <em>deep</em> (feedforward) neural network refers to a neural network that contains not only the input and output layers, but also (a variable number of) hidden layers in between.</p><p>Neural networks accept only numerical inputs. We hence need to convert all categorical data in numerical units. A common approach is to use the so-called &quot;one-hot-encoding&quot; where the catagorical values are converted into indicator variables (0/1), one for each possible value. This can be done in BetaML using the <a href="../../Utils.html#BetaML.Utils.OneHotEncoder"><code>OneHotEncoder</code></a> function:</p><pre><code class="language-julia hljs">seasonDummies  = fit!(OneHotEncoder(),data.season)
weatherDummies = fit!(OneHotEncoder(),data.weathersit)
wdayDummies    = fit!(OneHotEncoder(),data.weekday .+ 1)


# We compose the feature matrix with the new dimensions obtained from the onehotencoder functions
x = hcat(Matrix{Float64}(data[:,[:instant,:yr,:mnth,:holiday,:workingday,:temp,:atemp,:hum,:windspeed]]),
         seasonDummies,
         weatherDummies,
         wdayDummies)
y = data[:,16];</code></pre><p>As we did for decision trees/ random forests, we split the data in training, validation and testing sets</p><pre><code class="language-julia hljs">((xtrain,xtest),(ytrain,ytest)) = partition([x,y],[0.75,1-0.75],shuffle=false)
(ntrain, ntest) = size.([ytrain,ytest],1)</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">2-element Vector{Int64}:
 548
 183</code></pre><p>An other common operation with neural networks is to scale the feature vectors (X) and the labels (Y). The BetaML <a href="../../Utils.html#BetaML.Utils.Scaler"><code>Scaler</code></a> model, by default, scales the data such that each dimension has mean 0 and variance 1.</p><p>Note that we can provide the <code>Scaler</code>` model with different scale factors or specify the columns that shoudn&#39;t be scaled (e.g. those resulting from the one-hot encoding). Finally we can reverse the scaling (this is useful to retrieve the unscaled features from a model trained with scaled ones).</p><pre><code class="language-julia hljs">cols_nottoscale = [2;4;5;10:23]
xsm             = Scaler(skip=cols_nottoscale)
xtrain_scaled   = fit!(xsm,xtrain)
xtest_scaled    = predict(xsm,xtest)
ytrain_scaled   = ytrain ./ 1000 # We just divide Y by 1000, as using full scaling of Y we may get negative demand.
ytest_scaled    = ytest ./ 1000
D               = size(xtrain,2)</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">23</code></pre><p>We can now build our feed-forward neaural network. We create three layers, the first layers will always have a input size equal to the dimensions of our data (the number of columns), and the output layer, for a simple regression where the predictions are scalars, it will always be one. We will tune the size of the middle layer size.</p><p>There are already several kind of layers available (and you can build your own kind by defining a new <code>struct</code> and implementing a few functions. See the <a href="../../Nn.html#nn_module"><code>Nn</code></a> module documentation for details). Here we use only <em>dense</em> layers, those found in typycal feed-fordward neural networks.</p><p>For each layer, on top of its size (in &quot;neurons&quot;) we can specify an <em>activation function</em>. Here we use the <a href="../../Utils.html#BetaML.Utils.relu-Tuple{Any}"><code>relu</code></a> for the terminal layer (this will guarantee that our predictions are always positive) and <code>identity</code> for the hidden layer. Again, consult the <code>Nn</code> module documentation for other activation layers already defined, or use any function of your choice.</p><p>Initial weight parameters can also be specified if needed. By default <a href="../../Nn.html#BetaML.Nn.DenseLayer"><code>DenseLayer</code></a> use the so-called <em>Xavier initialisation</em>.</p><p>Let&#39;s hence build our candidate neural network structures, choosing between 5 and 10 nodes in the hidden layers:</p><pre><code class="language-julia hljs">candidate_structures = [
        [DenseLayer(D,k,f=relu,df=drelu,rng=copy(AFIXEDRNG)),     # Activation function is ReLU, it&#39;s derivative is drelu
         DenseLayer(k,k,f=identity,df=identity,rng=copy(AFIXEDRNG)), # This is the hidden layer we vant to test various sizes
         DenseLayer(k,1,f=relu,df=didentity,rng=copy(AFIXEDRNG))] for k in 5:2:10]</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">3-element Vector{Vector{DenseLayer{TF, TDF, Float64} where {TF&lt;:Function, TDF&lt;:Union{Nothing, Function}}}}:
 [DenseLayer{typeof(relu), typeof(drelu), Float64}([-0.29531296438620935 -0.015470333744021791 … 0.22698611148327613 0.2904274420874035; -0.12268047123900877 0.24439218426433357 … 0.36149589819305356 0.25651053821356756; … ; -0.4233492331828929 0.013929076127145168 … 0.12727781451876424 -0.3220056051786648; -0.05740921476706312 -0.004590477627889222 … 0.33006927322971974 -0.3229902182657409], [-0.16275699771952834, 0.2715510654801558, -0.3773528046642076, -0.15805165866556825, -0.45768862798217264], BetaML.Utils.relu, BetaML.Utils.drelu), DenseLayer{typeof(identity), typeof(identity), Float64}([-0.49415310523844497 -0.025886819681528617 … 0.09224620317870791 -0.2298725180847525; -0.20528369264408397 0.40894634274263597 … 0.01065168104625569 0.04243100148415224; … ; -0.7083987613359595 0.023307802404264888 … -0.3619336136169306 -0.4825559053138102; -0.09606399030062296 -0.0076813382679077336 … -0.3824319138128976 0.21378287379211514], [-0.11178452751598777, -0.2115472973462721, 0.12029035453379433, -0.06939594013486328, -0.5177091839997635], identity, identity), DenseLayer{typeof(relu), typeof(didentity), Float64}([-0.6379489156883928 -0.2650201076194998 … -0.9145388683760447 -0.12401807820151456], [-0.033419740504278206], BetaML.Utils.relu, BetaML.Utils.didentity)]
 [DenseLayer{typeof(relu), typeof(drelu), Float64}([-0.28529942833030564 0.379669223140765 … 0.28591139889714334 0.2606243753947177; -0.11852059520830227 0.01345676599232093 … -0.29706242768791313 0.23712909181525033; … ; -0.014945762311593835 0.3365486213316541 … 0.38625836286183857 -0.3815582599599021; 0.2361052810665739 0.39732182872463784 … -0.16904473533198983 0.4058193222986791], [-0.2941231824618411, 0.22676070936994097, 0.36548939215338183, 0.2223280912744705, 0.14878832348318172, 0.14237054395672044, 0.12194600561371483], BetaML.Utils.relu, BetaML.Utils.drelu), DenseLayer{typeof(identity), typeof(identity), Float64}([-0.41763559937958017 0.5557788338390783 … 0.4559073525474535 -0.36261818505175825; -0.17349638626452868 0.01969868837033595 … 0.14788575447060037 0.12932806923843965; … ; -0.021878355795233784 0.4926567361624343 … 0.053199618796913706 -0.06119821157571559; 0.34562274152460515 0.5816196024546281 … 0.41959953754300483 -0.0549751039594103], [0.4400179121658523, 0.24489574852922413, -0.19121243996167125, -0.4563330828793923, 0.2666684007070268, 0.37420770408480153, 0.06760655291977202], identity, identity), DenseLayer{typeof(relu), typeof(didentity), Float64}([-0.5524799673028851 -0.22951414571217266 … -0.028942344264588638 0.4572159107612309], [0.7352262891458451], BetaML.Utils.relu, BetaML.Utils.didentity)]
 [DenseLayer{typeof(relu), typeof(drelu), Float64}([-0.27623998365144253 -0.0042939986313030865 … 0.03954785061772442 -0.08082404539953181; -0.11475707285608633 0.17436554863663706 … -0.05561917436604025 -0.21782018027072575; … ; 0.36761314457292255 0.005954470723518457 … -0.3837383360631904 0.3467117277857586; 0.013029457645517328 0.12646036547257528 … 0.22589917353995687 -0.05220438901739144], [0.38096922973233555, -0.17198944163464452, 0.3231333343145583, 0.07247817545621887, 0.2610547851810732, 0.32440076793779044, -0.36628335178017296, -0.09725122313424439, 0.005944201402634575], BetaML.Utils.relu, BetaML.Utils.drelu), DenseLayer{typeof(identity), typeof(identity), Float64}([-0.36831997820192347 -0.005725331508404152 … -0.3958593058455864 -0.009396020856098364; -0.15300943047478177 0.23248739818218278 … -0.5034837638317298 -0.42198614060305667; … ; 0.49015085943056347 0.007939294298024535 … -0.08579127354734356 0.12048358271778759; 0.01737261019402303 0.16861382063010033 … 0.4925234679180297 -0.18459818635959535], [-0.4747622464385455, -0.2650918386962326, -0.2416897178346205, -0.4434088787999071, 0.40076564807210224, 0.36252051789995865, -0.5078601012084061, -0.09079195063298717, 0.2076336496753396], identity, identity), DenseLayer{typeof(relu), typeof(didentity), Float64}([-0.49415310523844497 -0.20528369264408397 … 0.6576063845500103 0.023307802404264888], [-0.0076813382679077336], BetaML.Utils.relu, BetaML.Utils.didentity)]</code></pre><p>Note that specify the derivatives of the activation functions (and of the loss function that we&#39;ll see in a moment) it totally optional, as without them BetaML will use [<code>Zygote.jl</code>](https://github.com/FluxML/Zygote.jl for automatic differentiation.</p><p>We do also set a few other parameters as &quot;turnable&quot;: the number of &quot;epochs&quot; to train the model (the number of iterations trough the whole dataset), the sample size at each batch and the optimisation algorithm to use. Several optimisation algorithms are indeed available, and each accepts different parameters, like the <em>learning rate</em> for the Stochastic Gradient Descent algorithm (<a href="../../Nn.html#BetaML.Nn.SGD"><code>SGD</code></a>, used by default) or the exponential decay rates for the  moments estimates for the <a href="../../Nn.html#BetaML.Nn.ADAM"><code>ADAM</code></a> algorithm (that we use here, with the default parameters).</p><p>The hyperparameter ranges will then look as follow:</p><pre><code class="language-julia hljs">hpranges = Dict(&quot;layers&quot;     =&gt; candidate_structures,
                &quot;epochs&quot;     =&gt; rand(copy(AFIXEDRNG),DiscreteUniform(50,100),3), # 3 values sampled at random between 50 and 100
                &quot;batch_size&quot; =&gt; [4,8,16],
                &quot;opt_alg&quot;    =&gt; [SGD(λ=2),SGD(λ=1),SGD(λ=3),ADAM(λ=0.5),ADAM(λ=1),ADAM(λ=0.25)])</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">Dict{String, Vector{T} where T} with 4 entries:
  &quot;epochs&quot;     =&gt; [93, 64, 81]
  &quot;batch_size&quot; =&gt; [4, 8, 16]
  &quot;layers&quot;     =&gt; Vector{DenseLayer{TF, TDF, Float64} where {TF&lt;:Function, TDF&lt;…
  &quot;opt_alg&quot;    =&gt; BetaML.Nn.OptimisationAlgorithm[SGD(#110, 2.0), SGD(#110, 1.0…</code></pre><p>Finally we can build &quot;neural network&quot; <a href="../../Nn.html#BetaML.Nn.NeuralNetworkEstimator"><code>NeuralNetworkEstimator</code></a> model where we &quot;chain&quot; the layers together and we assign a final loss function (again, you can provide your own loss function, if those available in BetaML don&#39;t suit your needs):</p><pre><code class="language-julia hljs">nnm = NeuralNetworkEstimator(loss=squared_cost, descr=&quot;Bike sharing regression model&quot;, tunemethod=SuccessiveHalvingSearch(hpranges = hpranges), autotune=true,rng=copy(AFIXEDRNG)) # Build the NN model and use the squared cost (aka MSE) as error function by default</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">NeuralNetworkEstimator - A Feed-forward neural network (unfitted)</code></pre><p>We can now fit and autotune the model:</p><pre><code class="language-julia hljs">ŷtrain_scaled = fit!(nnm,xtrain_scaled,ytrain_scaled)</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">548-element Vector{Float64}:
 1.6276179640617545
 1.6262044395781123
 2.0761172765792026
 1.8405431771565322
 1.9356124405265465
 1.5636071679418635
 1.9302085633254327
 2.0974152546824802
 2.173954358117645
 2.0608896521431084
 ⋮
 6.401749823084939
 6.136690161454254
 5.85391803215625
 5.881135603594056
 6.001419626157915
 6.156786914040388
 6.321342640942574
 6.4412702727723845
 6.2924193921933265</code></pre><p>The model training is one order of magnitude slower than random forests, altought the memory requirement is approximatly the same.</p><p>To obtain the neural network predictions we apply the function <code>predict</code> to the feature matrix X for which we want to generate previsions, and then we rescale y. Normally we would apply here the <code>inverse_predict</code> function, but as we simple divided by 1000, we multiply ŷ by the same amount:</p><pre><code class="language-julia hljs">ŷtrain = ŷtrain_scaled .* 1000
ŷtest  = predict(nnm,xtest_scaled) .* 1000</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">183-element Vector{Float64}:
 6071.248677131911
 6162.5795662457485
 5866.734380139826
 6329.305353565087
 6378.723890849175
 6793.329501664083
 6494.632970792904
 5484.620719230335
 5603.586480928075
 5978.90400942508
    ⋮
 4627.658649482943
 3628.785532010524
 2872.6572274644486
 1642.0360089958003
 2925.412202273377
 4233.449896021085
 3614.919209226012
 3534.3393968342434
 3449.2829592663797</code></pre><pre><code class="language-julia hljs">(rme_train, rme_test) = relative_mean_error.([ŷtrain,ŷtest],[ytrain,ytest])
push!(results,[&quot;NN&quot;,rme_train,rme_test]);</code></pre><p>The error is much lower. Let&#39;s plot our predictions:</p><p>Again, we can start by plotting the estimated vs the observed value:</p><pre><code class="language-julia hljs">scatter(ytrain,ŷtrain,xlabel=&quot;daily rides&quot;,ylabel=&quot;est. daily rides&quot;,label=nothing,title=&quot;Est vs. obs in training period (NN)&quot;)</code></pre><img src="betaml_tutorial_regression_sharingBikes-68c01936.svg" alt="Example block output"/><pre><code class="language-julia hljs">scatter(ytest,ŷtest,xlabel=&quot;daily rides&quot;,ylabel=&quot;est. daily rides&quot;,label=nothing,title=&quot;Est vs. obs in testing period (NN)&quot;)</code></pre><img src="betaml_tutorial_regression_sharingBikes-8af4fc89.svg" alt="Example block output"/><p>We now plot across the time dimension, first plotting the whole period (2 years):</p><pre><code class="language-julia hljs">ŷtrainfull = vcat(ŷtrain,fill(missing,ntest))
ŷtestfull  = vcat(fill(missing,ntrain), ŷtest)
plot(data[:,:dteday],[data[:,:cnt] ŷtrainfull ŷtestfull], label=[&quot;obs&quot; &quot;train&quot; &quot;test&quot;], legend=:topleft, ylabel=&quot;daily rides&quot;, title=&quot;Daily bike sharing demand observed/estimated across the\n whole 2-years period  (NN)&quot;)</code></pre><img src="betaml_tutorial_regression_sharingBikes-f1cce7bb.svg" alt="Example block output"/><p>...and then focusing on the testing data</p><pre><code class="language-julia hljs">stc  = 620
endc = size(x,1)
plot(data[stc:endc,:dteday],[data[stc:endc,:cnt] ŷtestfull[stc:endc]], label=[&quot;obs&quot; &quot;val&quot; &quot;test&quot;], legend=:bottomleft, ylabel=&quot;Daily rides&quot;, title=&quot;Focus on the testing period (NN)&quot;)</code></pre><img src="betaml_tutorial_regression_sharingBikes-b6de894f.svg" alt="Example block output"/><h3 id="Comparison-with-Flux.jl"><a class="docs-heading-anchor" href="#Comparison-with-Flux.jl">Comparison with Flux.jl</a><a id="Comparison-with-Flux.jl-1"></a><a class="docs-heading-anchor-permalink" href="#Comparison-with-Flux.jl" title="Permalink"></a></h3><p>We now apply the same Neural Network model using the <a href="https://fluxml.ai/">Flux</a> framework, a dedicated neural network library, reusing the optimal parameters that we did learn from tuning <code>NeuralNetworkEstimator</code>:</p><pre><code class="language-julia hljs">hp_opt         = hyperparameters(nnm)
opt_size       = size(hp_opt.layers[1])[2][1]
opt_batch_size = hp_opt.batch_size
opt_epochs     = hp_opt.epochs</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">93</code></pre><p>We fix the default random number generator so that the Flux example gives a reproducible output</p><pre><code class="language-julia hljs">Random.seed!(seed)</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">MersenneTwister(123)</code></pre><p>We define the Flux neural network model and load it with data...</p><pre><code class="language-julia hljs">l1         = Flux.Dense(D,opt_size,Flux.relu)
l2         = Flux.Dense(opt_size,opt_size,identity)
l3         = Flux.Dense(opt_size,1,Flux.relu)
Flux_nn    = Flux.Chain(l1,l2,l3)
fluxloss(x, y) = Flux.mse(Flux_nn(x), y)
ps         = Flux.params(Flux_nn)
nndata     = Flux.Data.DataLoader((xtrain_scaled&#39;, ytrain_scaled&#39;), batchsize=opt_batch_size,shuffle=true)</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">137-element DataLoader(::Tuple{LinearAlgebra.Adjoint{Float64, Matrix{Float64}}, LinearAlgebra.Adjoint{Float64, Vector{Float64}}}, shuffle=true, batchsize=4)
  with first element:
  (23×4 Matrix{Float64}, 1×4 adjoint(::Vector{Float64}) with eltype Float64,)</code></pre><p>We do the training of the Flux model...</p><pre><code class="language-julia hljs">[Flux.train!(fluxloss, ps, nndata, Flux.ADAM(0.001, (0.9, 0.8))) for i in 1:opt_epochs]</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">93-element Vector{Nothing}:
 nothing
 nothing
 nothing
 nothing
 nothing
 nothing
 nothing
 nothing
 nothing
 nothing
 ⋮
 nothing
 nothing
 nothing
 nothing
 nothing
 nothing
 nothing
 nothing
 nothing</code></pre><p>We obtain the predicitons...</p><pre><code class="language-julia hljs">ŷtrainf = @pipe Flux_nn(xtrain_scaled&#39;)&#39; .* 1000;
ŷtestf  = @pipe Flux_nn(xtest_scaled&#39;)&#39;  .* 1000;</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">┌ Warning: Layer with Float32 parameters got Float64 input.
│   The input will be converted, but any earlier layers may be very slow.
│   layer = Dense(23 =&gt; 9, relu)  # 216 parameters
│   summary(x) = &quot;23×548 adjoint(::Matrix{Float64}) with eltype Float64&quot;
└ @ Flux ~/.julia/packages/Flux/uCLgc/src/layers/stateless.jl:50
┌ Warning: Layer with Float32 parameters got Float64 input.
│   The input will be converted, but any earlier layers may be very slow.
│   layer = Dense(23 =&gt; 9, relu)  # 216 parameters
│   summary(x) = &quot;23×183 adjoint(::Matrix{Float64}) with eltype Float64&quot;
└ @ Flux ~/.julia/packages/Flux/uCLgc/src/layers/stateless.jl:50</code></pre><p>..and we compute the mean relative errors..</p><pre><code class="language-julia hljs">(rme_train, rme_test) = relative_mean_error.([ŷtrainf,ŷtestf],[ytrain,ytest])
push!(results,[&quot;NN (Flux.jl)&quot;,rme_train,rme_test]);</code></pre><p>.. finding an error not significantly different than the one obtained from BetaML.Nn.</p><p>Plots:</p><pre><code class="language-julia hljs">scatter(ytrain,ŷtrainf,xlabel=&quot;daily rides&quot;,ylabel=&quot;est. daily rides&quot;,label=nothing,title=&quot;Est vs. obs in training period (Flux.NN)&quot;)</code></pre><img src="betaml_tutorial_regression_sharingBikes-44c61538.svg" alt="Example block output"/><pre><code class="language-julia hljs">scatter(ytest,ŷtestf,xlabel=&quot;daily rides&quot;,ylabel=&quot;est. daily rides&quot;,label=nothing,title=&quot;Est vs. obs in testing period (Flux.NN)&quot;)</code></pre><img src="betaml_tutorial_regression_sharingBikes-10cb330e.svg" alt="Example block output"/><pre><code class="language-julia hljs">ŷtrainfullf = vcat(ŷtrainf,fill(missing,ntest))
ŷtestfullf  = vcat(fill(missing,ntrain), ŷtestf)
plot(data[:,:dteday],[data[:,:cnt] ŷtrainfullf ŷtestfullf], label=[&quot;obs&quot; &quot;train&quot; &quot;test&quot;], legend=:topleft, ylabel=&quot;daily rides&quot;, title=&quot;Daily bike sharing demand observed/estimated across the\n whole 2-years period (Flux.NN)&quot;)</code></pre><img src="betaml_tutorial_regression_sharingBikes-d73d2901.svg" alt="Example block output"/><pre><code class="language-julia hljs">stc = 620
endc = size(x,1)
plot(data[stc:endc,:dteday],[data[stc:endc,:cnt] ŷtestfullf[stc:endc]], label=[&quot;obs&quot; &quot;val&quot; &quot;test&quot;], legend=:bottomleft, ylabel=&quot;Daily rides&quot;, title=&quot;Focus on the testing period (Flux.NN)&quot;)</code></pre><img src="betaml_tutorial_regression_sharingBikes-215e3d68.svg" alt="Example block output"/><h3 id="Conclusions-of-Neural-Network-models"><a class="docs-heading-anchor" href="#Conclusions-of-Neural-Network-models">Conclusions of Neural Network models</a><a id="Conclusions-of-Neural-Network-models-1"></a><a class="docs-heading-anchor-permalink" href="#Conclusions-of-Neural-Network-models" title="Permalink"></a></h3><p>If we strive for the most accurate predictions, deep neural networks are usually the best choice. However they are computationally expensive, so with limited resourses we may get better results by fine tuning and running many repetitions of &quot;simpler&quot; decision trees or even random forest models than a large naural network with insufficient hyper-parameter tuning. Also, we shoudl consider that decision trees/random forests are much simpler to work with.</p><p>That said, specialised neural network libraries, like Flux, allow to use GPU and specialised hardware letting neural networks to scale with very large datasets.</p><p>Still, for small and medium datasets, BetaML provides simpler yet customisable solutions that are accurate and fast.</p><h2 id="GMM-based-regressors"><a class="docs-heading-anchor" href="#GMM-based-regressors">GMM-based regressors</a><a id="GMM-based-regressors-1"></a><a class="docs-heading-anchor-permalink" href="#GMM-based-regressors" title="Permalink"></a></h2><p>BetaML 0.8 introduces new regression algorithms based on Gaussian Mixture Model. Specifically, there are two variants available, <code>GaussianMixtureRegressor2</code> and <code>GaussianMixtureRegressor</code>, and this example uses  <code>GaussianMixtureRegressor</code> As for neural networks, they work on numerical data only, so we reuse the datasets we prepared for the neural networks.</p><p>As usual we first define the model.</p><pre><code class="language-julia hljs">m = GaussianMixtureRegressor(rng=copy(AFIXEDRNG),verbosity=NONE)</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">GaussianMixtureRegressor - A regressor based on Generative Mixture Model (unfitted)</code></pre><div class="admonition is-info"><header class="admonition-header">Info</header><div class="admonition-body"><p>We disabled autotune here, as this code is run by GitHub continuous_integration servers on each code update, and GitHub servers seem to have some strange problem with it, taking almost 4 hours instead of a few seconds on my machine.</p></div></div><p>We then fit the model to the training data..</p><pre><code class="language-julia hljs">ŷtrainGMM_unscaled = fit!(m,xtrain_scaled,ytrain_scaled)</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">548×1 Matrix{Float64}:
 2.05456959514988
 2.0545695951498795
 2.0545695951498795
 2.0545695951498795
 2.0545695951498795
 2.0545695951498795
 2.0545695951498795
 2.0545695951498795
 2.0545695951498795
 2.0545695951498795
 ⋮
 5.525359181472882
 5.525359181472882
 5.525359181472882
 5.525359181472882
 5.525359181472882
 5.525359181472882
 5.525359181472882
 5.525359181472882
 5.525359181472882</code></pre><p>And we predict...</p><pre><code class="language-julia hljs">ŷtrainGMM = ŷtrainGMM_unscaled .* 1000;
ŷtestGMM  = predict(m,xtest_scaled)  .* 1000;

(rme_train, rme_test) = relative_mean_error.([ŷtrainGMM,ŷtestGMM],[ytrain,ytest])
push!(results,[&quot;GMM&quot;,rme_train,rme_test]);</code></pre><h2 id="Summary"><a class="docs-heading-anchor" href="#Summary">Summary</a><a id="Summary-1"></a><a class="docs-heading-anchor-permalink" href="#Summary" title="Permalink"></a></h2><p>This is the summary of the results (train and test relative mean error) we had trying to predict the daily bike sharing demand, given weather and calendar information:</p><pre><code class="language-julia hljs">println(results)</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">6×3 DataFrame
 Row │ model                 train_rme  test_rme
     │ String                Float64    Float64
─────┼───────────────────────────────────────────
   1 │ DT                    0.0213416  0.172881
   2 │ RF                    0.0469041  0.166364
   3 │ RF (DecisionTree.jl)  0.0987308  0.286927
   4 │ NN                    0.149393   0.166221
   5 │ NN (Flux.jl)          0.08599    0.172753
   6 │ GMM                   0.216144   0.26681</code></pre><p>You may ask how stable are these results? How much do they depend from the specific RNG seed ? We re-evaluated a couple of times the whole script but changing random seeds (to <code>1000</code> and <code>10000</code>):</p><table><tr><th style="text-align: left">Model</th><th style="text-align: center">Train rme1</th><th style="text-align: center">Test rme1</th><th style="text-align: center">Train rme2</th><th style="text-align: center">Test rme2</th><th style="text-align: center">Train rme3</th><th style="text-align: center">Test rme3</th></tr><tr><td style="text-align: left">DT</td><td style="text-align: center">0.1366960</td><td style="text-align: center">0.154720</td><td style="text-align: center">0.0233044</td><td style="text-align: center">0.249329</td><td style="text-align: center">0.0621571</td><td style="text-align: center">0.161657</td></tr><tr><td style="text-align: left">RF</td><td style="text-align: center">0.0421267</td><td style="text-align: center">0.180186</td><td style="text-align: center">0.0535776</td><td style="text-align: center">0.136920</td><td style="text-align: center">0.0386144</td><td style="text-align: center">0.141606</td></tr><tr><td style="text-align: left">RF (DecisionTree.jl)</td><td style="text-align: center">0.0230439</td><td style="text-align: center">0.235823</td><td style="text-align: center">0.0801040</td><td style="text-align: center">0.243822</td><td style="text-align: center">0.0168764</td><td style="text-align: center">0.219011</td></tr><tr><td style="text-align: left">NN</td><td style="text-align: center">0.1604000</td><td style="text-align: center">0.169952</td><td style="text-align: center">0.1091330</td><td style="text-align: center">0.121496</td><td style="text-align: center">0.1481440</td><td style="text-align: center">0.150458</td></tr><tr><td style="text-align: left">NN (Flux.jl)</td><td style="text-align: center">0.0931161</td><td style="text-align: center">0.166228</td><td style="text-align: center">0.0920796</td><td style="text-align: center">0.167047</td><td style="text-align: center">0.0907810</td><td style="text-align: center">0.122469</td></tr><tr><td style="text-align: left">GaussianMixtureRegressor*</td><td style="text-align: center">0.1432800</td><td style="text-align: center">0.293891</td><td style="text-align: center">0.1380340</td><td style="text-align: center">0.295470</td><td style="text-align: center">0.1477570</td><td style="text-align: center">0.284567</td></tr></table><ul><li>GMM is a deterministic model, the variations are due to the different random sampling in choosing the best hyperparameters</li></ul><p>Neural networks can be more precise than random forests models, but are more computationally expensive (and tricky to set up). When we compare BetaML with the algorithm-specific leading packages, we found similar results in terms of accuracy, but often the leading packages are better optimised and run more efficiently (but sometimes at the cost of being less versatile). GMM_based regressors are very computationally cheap and a good compromise if accuracy can be traded off for performances.</p><p><a href="betaml_tutorial_regression_sharingBikes.jl">View this file on Github</a>.</p><hr/><p><em>This page was generated using <a href="https://github.com/fredrikekre/Literate.jl">Literate.jl</a>.</em></p></article><nav class="docs-footer"><a class="docs-footer-prevpage" href="../Classification - cars/betaml_tutorial_classification_cars.html">« A classification task when labels are known - determining the country of origin of cars given the cars characteristics</a><a class="docs-footer-nextpage" href="../Clustering - Iris/betaml_tutorial_cluster_iris.html">A clustering task: the prediction of  plant species from floreal measures (the iris dataset) »</a><div class="flexbox-break"></div><p class="footer-message">Powered by <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> and the <a href="https://julialang.org/">Julia Programming Language</a>.</p></nav></div><div class="modal" id="documenter-settings"><div class="modal-background"></div><div class="modal-card"><header class="modal-card-head"><p class="modal-card-title">Settings</p><button class="delete"></button></header><section class="modal-card-body"><p><label class="label">Theme</label><div class="select"><select id="documenter-themepicker"><option value="auto">Automatic (OS)</option><option value="documenter-light">documenter-light</option><option value="documenter-dark">documenter-dark</option></select></div></p><hr/><p>This document was generated with <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> version 1.3.0 on <span class="colophon-date" title="Wednesday 20 March 2024 12:48">Wednesday 20 March 2024</span>. Using Julia version 1.6.7.</p></section><footer class="modal-card-foot"></footer></div></div></div></body></html>
