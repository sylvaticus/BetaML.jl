<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>A classification task when labels are known - determining the country of origin of cars given the cars characteristics · BetaML.jl Documentation</title><script async src="https://www.googletagmanager.com/gtag/js?id=G-JYKX8QY5JW"></script><script>  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());
  gtag('config', 'G-JYKX8QY5JW', {'page_path': location.pathname + location.search + location.hash});
</script><script data-outdated-warner src="../../assets/warner.js"></script><link href="https://cdnjs.cloudflare.com/ajax/libs/lato-font/3.0.0/css/lato-font.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/juliamono/0.045/juliamono.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.4/css/fontawesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.4/css/solid.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.4/css/brands.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.24/katex.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL="../.."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" data-main="../../assets/documenter.js"></script><script src="../../siteinfo.js"></script><script src="../../../versions.js"></script><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/documenter-dark.css" data-theme-name="documenter-dark" data-theme-primary-dark/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/documenter-light.css" data-theme-name="documenter-light" data-theme-primary/><script src="../../assets/themeswap.js"></script></head><body><div id="documenter"><nav class="docs-sidebar"><div class="docs-package-name"><span class="docs-autofit"><a href="../../index.html">BetaML.jl Documentation</a></span></div><form class="docs-search" action="../../search.html"><input class="docs-search-query" id="documenter-search-query" name="q" type="text" placeholder="Search docs"/></form><ul class="docs-menu"><li><a class="tocitem" href="../../index.html">Index</a></li><li><span class="tocitem">Tutorial</span><ul><li><a class="tocitem" href="../Betaml_tutorial_getting_started.html">Getting started</a></li><li><input class="collapse-toggle" id="menuitem-2-2" type="checkbox" checked/><label class="tocitem" for="menuitem-2-2"><span class="docs-label">Classification - cars</span><i class="docs-chevron"></i></label><ul class="collapsed"><li class="is-active"><a class="tocitem" href="betaml_tutorial_classification_cars.html">A classification task when labels are known - determining the country of origin of cars given the cars characteristics</a><ul class="internal"><li><a class="tocitem" href="#Library-loading-and-initialisation"><span>Library loading and initialisation</span></a></li><li><a class="tocitem" href="#Data-loading-and-preparation"><span>Data loading and preparation</span></a></li><li><a class="tocitem" href="#Random-Forests"><span>Random Forests</span></a></li><li><a class="tocitem" href="#Perceptron-like-classifiers."><span>Perceptron-like classifiers.</span></a></li><li><a class="tocitem" href="#Summary"><span>Summary</span></a></li></ul></li></ul></li><li><input class="collapse-toggle" id="menuitem-2-3" type="checkbox"/><label class="tocitem" for="menuitem-2-3"><span class="docs-label">Regression - bike sharing</span><i class="docs-chevron"></i></label><ul class="collapsed"><li><a class="tocitem" href="../Regression - bike sharing/betaml_tutorial_regression_sharingBikes.html">A regression task: the prediction of  bike  sharing demand</a></li></ul></li><li><input class="collapse-toggle" id="menuitem-2-4" type="checkbox"/><label class="tocitem" for="menuitem-2-4"><span class="docs-label">Clustering - Iris</span><i class="docs-chevron"></i></label><ul class="collapsed"><li><a class="tocitem" href="../Clustering - Iris/betaml_tutorial_cluster_iris.html">A clustering task: the prediction of  plant species from floreal measures (the iris dataset)</a></li></ul></li></ul></li><li><span class="tocitem">API (Reference manual)</span><ul><li><input class="collapse-toggle" id="menuitem-3-1" type="checkbox"/><label class="tocitem" for="menuitem-3-1"><span class="docs-label">API V2 (current)</span><i class="docs-chevron"></i></label><ul class="collapsed"><li><a class="tocitem" href="../../Api_v2_user.html">Introduction for user</a></li><li><input class="collapse-toggle" id="menuitem-3-1-2" type="checkbox"/><label class="tocitem" for="menuitem-3-1-2"><span class="docs-label">For developers</span><i class="docs-chevron"></i></label><ul class="collapsed"><li><a class="tocitem" href="../../Api_v2_developer.html">API implementation</a></li><li><a class="tocitem" href="../../StyleGuide_templates.html">Style guide</a></li></ul></li><li><a class="tocitem" href="../../Api.html">The Api module</a></li></ul></li><li><a class="tocitem" href="../../Perceptron.html">Perceptron</a></li><li><a class="tocitem" href="../../Trees.html">Trees</a></li><li><a class="tocitem" href="../../Nn.html">Nn</a></li><li><a class="tocitem" href="../../Clustering.html">Clustering</a></li><li><a class="tocitem" href="../../GMM.html">GMM</a></li><li><a class="tocitem" href="../../Imputation.html">Imputation</a></li><li><a class="tocitem" href="../../Utils.html">Utils</a></li></ul></li></ul><div class="docs-version-selector field has-addons"><div class="control"><span class="docs-label button is-static is-size-7">Version</span></div><div class="docs-selector control is-expanded"><div class="select is-fullwidth is-size-7"><select id="documenter-version-selector"></select></div></div></div></nav><div class="docs-main"><header class="docs-navbar"><nav class="breadcrumb"><ul class="is-hidden-mobile"><li><a class="is-disabled">Tutorial</a></li><li><a class="is-disabled">Classification - cars</a></li><li class="is-active"><a href="betaml_tutorial_classification_cars.html">A classification task when labels are known - determining the country of origin of cars given the cars characteristics</a></li></ul><ul class="is-hidden-tablet"><li class="is-active"><a href="betaml_tutorial_classification_cars.html">A classification task when labels are known - determining the country of origin of cars given the cars characteristics</a></li></ul></nav><div class="docs-right"><a class="docs-edit-link" href="https://github.com/sylvaticus/BetaML.jl/blob/master/docs/src/tutorials/Classification - cars/betaml_tutorial_classification_cars.jl" title="Edit on GitHub"><span class="docs-icon fab"></span><span class="docs-label is-hidden-touch">Edit on GitHub</span></a><a class="docs-settings-button fas fa-cog" id="documenter-settings-button" href="#" title="Settings"></a><a class="docs-sidebar-button fa fa-bars is-hidden-desktop" id="documenter-sidebar-button" href="#"></a></div></header><article class="content" id="documenter-page"><h1 id="classification_tutorial"><a class="docs-heading-anchor" href="#classification_tutorial">A classification task when labels are known - determining the country of origin of cars given the cars characteristics</a><a id="classification_tutorial-1"></a><a class="docs-heading-anchor-permalink" href="#classification_tutorial" title="Permalink"></a></h1><p>In this exercise we are provided with several technical characteristics (mpg, horsepower,weight, model year...) for several car&#39;s models, together with the country of origin of such models, and we would like to create a machine learning model such that the country of origin can be accurately predicted given the technical characteristics. As the information to predict is a multi-class one, this is a <em>[classification]</em>(https://en.wikipedia.org/wiki/Statistical_classification) task. It is a challenging exercise due to the simultaneous presence of three factors: (1) presence of missing data; (2) unbalanced data - 254 out of 406 cars are US made; (3) small dataset.</p><p>Data origin:</p><ul><li>dataset description: <a href="https://archive.ics.uci.edu/ml/datasets/auto+mpg">https://archive.ics.uci.edu/ml/datasets/auto+mpg</a></li><li>data source we use here: <a href="https://archive.ics.uci.edu/ml/machine-learning-databases/auto-mpg/auto-mpg.data-original">https://archive.ics.uci.edu/ml/machine-learning-databases/auto-mpg/auto-mpg.data</a></li></ul><p>Field description:</p><ol><li>mpg:           <em>continuous</em></li><li>cylinders:     <em>multi-valued discrete</em></li><li>displacement:  <em>continuous</em></li><li>horsepower:    <em>continuous</em></li><li>weight:        <em>continuous</em></li><li>acceleration:  <em>continuous</em></li><li>model year:    <em>multi-valued discrete</em></li><li>origin:        <em>multi-valued discrete</em></li><li>car name:      <em>string (unique for each instance)</em></li></ol><p>The car name is not used in this tutorial, so that the country is inferred only from technical data. As this field includes also the car maker, and there are several car&#39;s models from the same car maker, a more sophisticated machine learnign model could exploit this information e.g. using a bag of word encoding.</p><h2 id="Library-loading-and-initialisation"><a class="docs-heading-anchor" href="#Library-loading-and-initialisation">Library loading and initialisation</a><a id="Library-loading-and-initialisation-1"></a><a class="docs-heading-anchor-permalink" href="#Library-loading-and-initialisation" title="Permalink"></a></h2><p>Activating the local environment specific to BetaML documentation</p><pre><code class="language-julia hljs">using Pkg
Pkg.activate(joinpath(@__DIR__,&quot;..&quot;,&quot;..&quot;,&quot;..&quot;))</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">  Activating environment at `~/work/BetaML.jl/BetaML.jl/docs/Project.toml`</code></pre><p>We load a buch of packages that we&#39;ll use during this tutorial..</p><pre><code class="language-julia hljs">using Random, HTTP, Plots, CSV, DataFrames, BenchmarkTools, StableRNGs, BetaML
import DecisionTree, Flux
import Pipe: @pipe</code></pre><p>Machine Learning workflows include stochastic components in several steps: in the data sampling, in the model initialisation and often in the models&#39;s own algorithms (and sometimes also in the prediciton step). BetaML provides a random nuber generator  (RNG) in order to simplify reproducibility ( <a href="../../Api.html#BetaML.Api.FIXEDRNG"><code>FIXEDRNG</code></a>. This is nothing else than an istance of <code>StableRNG(123)</code> defined in the <a href="../../Utils.html#utils_module"><code>BetaML.Utils</code></a> sub-module, but you can choose of course your own &quot;fixed&quot; RNG). See the <a href="../Betaml_tutorial_getting_started.html#dealing_with_stochasticity">Dealing with stochasticity</a> section in the <a href="../Betaml_tutorial_getting_started.html#getting_started">Getting started</a> tutorial for details.</p><p>Here we are explicit and we use our own fixed RNG:</p><pre><code class="language-julia hljs">seed = 123 # The table at the end of this tutorial has been obtained with seeds 123, 1000 and 10000
AFIXEDRNG = StableRNG(seed)</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">StableRNGs.LehmerRNG(state=0x000000000000000000000000000000f7)</code></pre><h2 id="Data-loading-and-preparation"><a class="docs-heading-anchor" href="#Data-loading-and-preparation">Data loading and preparation</a><a id="Data-loading-and-preparation-1"></a><a class="docs-heading-anchor-permalink" href="#Data-loading-and-preparation" title="Permalink"></a></h2><p>To load the data from the internet our workflow is (1) Retrieve the data –&gt; (2) Clean it –&gt; (3) Load it –&gt; (4) Output it as a DataFrame.</p><p>For step (1) we use <code>HTTP.get()</code>, for step (2) we use <code>replace!</code>, for steps (3) and (4) we uses the <code>CSV</code> package, and we use the &quot;pip&quot; <code>|&gt;</code> operator to chain these operations, so that no file is ever saved on disk:</p><pre><code class="language-julia hljs">urlDataOriginal = &quot;https://archive.ics.uci.edu/ml/machine-learning-databases/auto-mpg/auto-mpg.data-original&quot;
data = @pipe HTTP.get(urlDataOriginal).body                                                |&gt;
             replace!(_, UInt8(&#39;\t&#39;) =&gt; UInt8(&#39; &#39;))                                        |&gt; # the original dataset has mixed field delimiters !
             CSV.File(_, delim=&#39; &#39;, missingstring=&quot;NA&quot;, ignorerepeated=true, header=false) |&gt;
             DataFrame;</code></pre><p>This results in a table where the rows are the observations (the various cars&#39; models) and the column the fields. All BetaML models expect this layout.</p><p>As the dataset is ordered, we randomly shuffle the data.</p><pre><code class="language-julia hljs">data[shuffle(copy(AFIXEDRNG),axes(data, 1)), :]
describe(data)</code></pre><div class="data-frame"><p>9 rows × 7 columns</p><table class="data-frame"><thead><tr><th></th><th>variable</th><th>mean</th><th>min</th><th>median</th><th>max</th><th>nmissing</th><th>eltype</th></tr><tr><th></th><th title="Symbol">Symbol</th><th title="Union{Nothing, Float64}">Union…</th><th title="Any">Any</th><th title="Union{Nothing, Float64}">Union…</th><th title="Any">Any</th><th title="Int64">Int64</th><th title="Type">Type</th></tr></thead><tbody><tr><th>1</th><td>Column1</td><td>23.5146</td><td>9.0</td><td>23.0</td><td>46.6</td><td>8</td><td>Union{Missing, Float64}</td></tr><tr><th>2</th><td>Column2</td><td>5.47537</td><td>3.0</td><td>4.0</td><td>8.0</td><td>0</td><td>Float64</td></tr><tr><th>3</th><td>Column3</td><td>194.78</td><td>68.0</td><td>151.0</td><td>455.0</td><td>0</td><td>Float64</td></tr><tr><th>4</th><td>Column4</td><td>105.082</td><td>46.0</td><td>95.0</td><td>230.0</td><td>6</td><td>Union{Missing, Float64}</td></tr><tr><th>5</th><td>Column5</td><td>2979.41</td><td>1613.0</td><td>2822.5</td><td>5140.0</td><td>0</td><td>Float64</td></tr><tr><th>6</th><td>Column6</td><td>15.5197</td><td>8.0</td><td>15.5</td><td>24.8</td><td>0</td><td>Float64</td></tr><tr><th>7</th><td>Column7</td><td>75.9212</td><td>70.0</td><td>76.0</td><td>82.0</td><td>0</td><td>Float64</td></tr><tr><th>8</th><td>Column8</td><td>1.56897</td><td>1.0</td><td>1.0</td><td>3.0</td><td>0</td><td>Float64</td></tr><tr><th>9</th><td>Column9</td><td></td><td>amc ambassador brougham</td><td></td><td>vw rabbit custom</td><td>0</td><td>String</td></tr></tbody></table></div><p>Columns 1 to 7 contain  characteristics of the car, while column 8 encodes the country or origin (&quot;1&quot; -&gt; US, &quot;2&quot; -&gt; EU, &quot;3&quot; -&gt; Japan). That&#39;s the variable we want to be able to predict.</p><p>Columns 9 contains the car name, but we are not going to use this information in this tutorial. Note also that some fields have missing data.</p><p>Our first step is hence to divide the dataset in features (the x) and the labels (the y) we want to predict. The <code>x</code> is then a Julia standard <code>Matrix</code> of 406 rows by 7 columns and the <code>y</code> is a vector of the 406 observations:</p><pre><code class="language-julia hljs">x     = Matrix{Union{Missing,Float64}}(data[:,1:7]);
y     = Vector{Int64}(data[:,8]);
x     = fit!(Scaler(),x)</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">406×7 Matrix{Union{Missing, Float64}}:
 -0.706439   1.47635    1.07088    0.643526   0.620107   -1.25708   -1.58146
 -1.09075    1.47635    1.48121    1.54744    0.843522   -1.43566   -1.58146
 -0.706439   1.47635    1.17584    1.16005    0.539725   -1.61424   -1.58146
 -0.962647   1.47635    1.04225    1.16005    0.536179   -1.25708   -1.58146
 -0.834543   1.47635    1.02316    0.901788   0.555092   -1.79281   -1.58146
 -1.09075    1.47635    2.23507    2.39971    1.60951    -1.97139   -1.58146
 -1.21885    1.47635    2.47364    2.96789    1.62488    -2.32855   -1.58146
 -1.21885    1.47635    2.34004    2.83876    1.57523    -2.50712   -1.58146
 -1.21885    1.47635    2.48318    3.09702    1.70881    -1.97139   -1.58146
 -1.09075    1.47635    1.86291    2.1931     1.02911    -2.50712   -1.58146
  ⋮                                                       ⋮         
 -0.194023   0.306793   0.35518    0.178653  -0.17071    -0.292762   1.62356
  1.08702   -0.862764  -0.484569  -0.234567  -0.371665   -0.578486   1.62356
  1.59943   -0.862764  -0.570453  -0.544482  -0.720381   -0.899925   1.62356
  0.446497  -0.862764  -0.417771  -0.389524  -0.0347697   0.635842   1.62356
  0.446497  -0.862764  -0.52274   -0.492829  -0.223904    0.028678   1.62356
  2.62426   -0.862764  -0.933072  -1.37092   -1.00408     3.24307    1.62356
  1.08702   -0.862764  -0.570453  -0.544482  -0.809037   -1.39994    1.62356
  0.574601  -0.862764  -0.713592  -0.673613  -0.418948    1.10014    1.62356
  0.958913  -0.862764  -0.723135  -0.596135  -0.30665     1.38587    1.62356</code></pre><p>Some algorithms that we will use today don&#39;t accept missing data, so we need to <em>impute</em> them. BetaML provides several imputation models in the <a href="../../Imputation.html#BetaML.Imputation"><code>Imputation</code></a> module. Note that many of these imputation models can be used for Collaborative Filtering / Recomendation Systems. Models as <a href="../../Imputation.html#BetaML.Imputation.GMMImputer"><code>GMMImputer</code></a> have the advantage over traditional algorithms as k-nearest neighbors (KNN) that GMM can &quot;detect&quot; the hidden structure of the observed data, where some observation can be similar to a certain pool of other observvations for a certain characteristic, but similar to an other pool of observations for other characteristics. Here we use <a href="../../Imputation.html#BetaML.Imputation.RFImputer"><code>RFImputer</code></a>. While the model allows for reproducible multiple imputations (with the parameter <code>multiple_imputation=an_integer</code>) and multiple passages trough the various columns (fields) containing missing data (with the option <code>recursive_passages=an_integer</code>), we use here just a single imputation and a single passage. As all <code>BetaML</code> models, <code>RFImputer</code> follows the patters <code>m=ModelConstruction(pars); fit!(m,x,[y]); est = predict(m,x)</code> where <code>est</code> can be an estimation of some labels or be some characteristics of x itself (the imputed version, as in this case, a reprojected version as in <a href="../../Utils.html#BetaML.Utils.PCA"><code>PCA</code></a>), depending if the model is supervised or not. See the <a href="../../Api_v2_user.html#api_usage"><code>API user documentation</code></a><code>for more details. For imputers, the output of</code>predict<code>is the matrix with the imputed values replacing the missing ones, and we write here the model in a single line using a convenience feature that when the default</code>cache<code>parameter is used in the model constructor the</code>fit!` function returns itself the prediciton over the trained data:</p><pre><code class="language-julia hljs">x = fit!(RFImputer(rng=copy(AFIXEDRNG)),x) # Same as `m = RFImputer(rng=copy(AFIXEDRNG)); fit!(m,x); x= predict(m,x)`</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">406×7 Matrix{Float64}:
 -0.706439   1.47635    1.07088    0.643526   0.620107   -1.25708   -1.58146
 -1.09075    1.47635    1.48121    1.54744    0.843522   -1.43566   -1.58146
 -0.706439   1.47635    1.17584    1.16005    0.539725   -1.61424   -1.58146
 -0.962647   1.47635    1.04225    1.16005    0.536179   -1.25708   -1.58146
 -0.834543   1.47635    1.02316    0.901788   0.555092   -1.79281   -1.58146
 -1.09075    1.47635    2.23507    2.39971    1.60951    -1.97139   -1.58146
 -1.21885    1.47635    2.47364    2.96789    1.62488    -2.32855   -1.58146
 -1.21885    1.47635    2.34004    2.83876    1.57523    -2.50712   -1.58146
 -1.21885    1.47635    2.48318    3.09702    1.70881    -1.97139   -1.58146
 -1.09075    1.47635    1.86291    2.1931     1.02911    -2.50712   -1.58146
  ⋮                                                       ⋮         
 -0.194023   0.306793   0.35518    0.178653  -0.17071    -0.292762   1.62356
  1.08702   -0.862764  -0.484569  -0.234567  -0.371665   -0.578486   1.62356
  1.59943   -0.862764  -0.570453  -0.544482  -0.720381   -0.899925   1.62356
  0.446497  -0.862764  -0.417771  -0.389524  -0.0347697   0.635842   1.62356
  0.446497  -0.862764  -0.52274   -0.492829  -0.223904    0.028678   1.62356
  2.62426   -0.862764  -0.933072  -1.37092   -1.00408     3.24307    1.62356
  1.08702   -0.862764  -0.570453  -0.544482  -0.809037   -1.39994    1.62356
  0.574601  -0.862764  -0.713592  -0.673613  -0.418948    1.10014    1.62356
  0.958913  -0.862764  -0.723135  -0.596135  -0.30665     1.38587    1.62356</code></pre><p>Further, some models don&#39;t work with categorical data as well, so we need to represent our <code>y</code> as a matrix with a separate column for each possible categorical value (the so called &quot;one-hot&quot; representation). For example, within a three classes field, the individual value <code>2</code> (or <code>&quot;Europe&quot;</code> for what it matters) would be represented as the vector <code>[0 1 0]</code>, while <code>3</code> (or <code>&quot;Japan&quot;</code>) would become the vector <code>[0 0 1]</code>. To encode as one-hot we use the <a href="../../Utils.html#BetaML.Utils.OneHotEncoder"><code>OneHotEncoder</code></a> in <a href="../../Utils.html#utils_module"><code>BetaML.Utils</code></a>, using the same shortcut as for the imputer we used earlier:</p><pre><code class="language-julia hljs">y_oh  = fit!(OneHotEncoder(),y)</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">406×3 Matrix{Bool}:
 1  0  0
 1  0  0
 1  0  0
 1  0  0
 1  0  0
 1  0  0
 1  0  0
 1  0  0
 1  0  0
 1  0  0
 ⋮     
 1  0  0
 0  0  1
 1  0  0
 1  0  0
 1  0  0
 0  1  0
 1  0  0
 1  0  0
 1  0  0</code></pre><p>In supervised machine learning it is good practice to partition the available data in a <em>training</em>, <em>validation</em>, and <em>test</em> subsets, where the first one is used to train the ML algorithm, the second one to train any eventual &quot;hyper-parameters&quot; of the algorithm and the <em>test</em> subset is finally used to evaluate the quality of the algorithm. Here, for brevity, we use only the <em>train</em> and the <em>test</em> subsets, implicitly assuming we already know the best hyper-parameters. Please refer to the <a href="../Regression - bike sharing/betaml_tutorial_regression_sharingBikes.html#regression_tutorial">regression tutorial</a> for examples of the auto-tune feature of BetaML models to &quot;automatically&quot; train the hyper-parameters (hint: in most cases just add the parameter <code>autotune=true</code> in the model constructor), or the <a href="../Clustering - Iris/betaml_tutorial_cluster_iris.html#clustering_tutorial">clustering tutorial</a> for an example of using the <a href="../../Utils.html#BetaML.Utils.cross_validation"><code>cross_validation</code></a> function to do it manually.</p><p>We use then the <a href="../../Utils.html#BetaML.Utils.partition-Union{Tuple{T}, Tuple{AbstractVector{T}, AbstractVector{Float64}}} where T&lt;:AbstractArray"><code>partition</code></a> function in <a href="../../Utils.html#utils_module">BetaML.Utils</a>, where we can specify the different data to partition (each matrix or vector to partition must have the same number of observations) and the shares of observation that we want in each subset. Here we keep 80% of observations for training (<code>xtrain</code>, and <code>ytrain</code>) and we use 20% of them for testing (<code>xtest</code>, and <code>ytest</code>):</p><pre><code class="language-julia hljs">((xtrain,xtest),(ytrain,ytest),(ytrain_oh,ytest_oh)) = partition([x,y,y_oh],[0.8,1-0.8],rng=copy(AFIXEDRNG));</code></pre><p>We finally set up a dataframe to store the accuracies of the various models we&#39;ll use.</p><pre><code class="language-julia hljs">results = DataFrame(model=String[],train_acc=Float64[],test_acc=Float64[])</code></pre><div class="data-frame"><p>0 rows × 3 columns</p><table class="data-frame"><thead><tr><th></th><th>model</th><th>train_acc</th><th>test_acc</th></tr><tr><th></th><th title="String">String</th><th title="Float64">Float64</th><th title="Float64">Float64</th></tr></thead><tbody></tbody></table></div><h2 id="Random-Forests"><a class="docs-heading-anchor" href="#Random-Forests">Random Forests</a><a id="Random-Forests-1"></a><a class="docs-heading-anchor-permalink" href="#Random-Forests" title="Permalink"></a></h2><p>We are now ready to use our first model, the <a href="../../Trees.html#BetaML.Trees.RandomForestEstimator"><code>RandomForestEstimator</code></a>. Random Forests build a &quot;forest&quot; of decision trees models and then average their predictions in order to make an overall prediction, wheter a regression or a classification.</p><p>While here the missing data has been imputed and the dataset is comprised of only numerical values, one attractive feature of BetaML <code>RandomForestEstimator</code> is that they can work directly with missing and categorical data without any prior processing required.</p><p>However as the labels are encoded using integers, we need also to specify the parameter <code>force_classification=true</code>, otherwise the model would undergo a <em>regression</em> job instead.</p><pre><code class="language-julia hljs">rfm      = RandomForestEstimator(force_classification=true, rng=copy(AFIXEDRNG))</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">RandomForestEstimator - A 30 trees Random Forest model (unfitted)</code></pre><p>Opposite to the <code>RFImputer</code> and <code>OneHotEncoder</code> models used earielr, to train a <code>RandomForestEstimator</code> model we need to provide it with both the training feature matrix and the associated &quot;true&quot; training labels. We use the same shortcut to get the training predictions directly from the <code>fit!</code> function. In this case the predictions correspond to the labels:</p><pre><code class="language-julia hljs">ŷtrain   = fit!(rfm,xtrain,ytrain)</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">325-element Vector{Dict{Int64, Float64}}:
 Dict(2 =&gt; 0.03333333333333333, 3 =&gt; 0.9333333333333332, 1 =&gt; 0.03333333333333333)
 Dict(3 =&gt; 0.03333333333333333, 1 =&gt; 0.9666666666666666)
 Dict(2 =&gt; 0.8999999999999999, 3 =&gt; 0.1)
 Dict(3 =&gt; 0.13333333333333333, 1 =&gt; 0.8666666666666666)
 Dict(2 =&gt; 0.8333333333333333, 3 =&gt; 0.13333333333333333, 1 =&gt; 0.03333333333333333)
 Dict(1 =&gt; 0.9999999999999999)
 Dict(2 =&gt; 0.13333333333333333, 3 =&gt; 0.8166666666666667, 1 =&gt; 0.05)
 Dict(1 =&gt; 0.9999999999999999)
 Dict(1 =&gt; 0.9999999999999999)
 Dict(2 =&gt; 0.3, 3 =&gt; 0.03333333333333333, 1 =&gt; 0.6666666666666666)
 ⋮
 Dict(1 =&gt; 0.9999999999999999)
 Dict(2 =&gt; 0.18333333333333332, 3 =&gt; 0.8166666666666667)
 Dict(3 =&gt; 0.05, 1 =&gt; 0.9499999999999998)
 Dict(1 =&gt; 0.9999999999999999)
 Dict(1 =&gt; 0.9999999999999999)
 Dict(2 =&gt; 0.7999999999999999, 3 =&gt; 0.13333333333333333, 1 =&gt; 0.06666666666666667)
 Dict(1 =&gt; 0.9999999999999999)
 Dict(2 =&gt; 0.03333333333333333, 3 =&gt; 0.9666666666666666)
 Dict(2 =&gt; 0.08333333333333334, 3 =&gt; 0.9166666666666666)</code></pre><p>You can notice that for each record the result is reported in terms of a dictionary with the possible categories and their associated probabilities.</p><div class="admonition is-warning"><header class="admonition-header">Warning</header><div class="admonition-body"><p>Only categories with non-zero probabilities are reported for each record, and being a dictionary, the order of the categories is not undefined</p></div></div><p>For example <code>ŷtrain[1]</code> is a <code>Dict(2 =&gt; 0.0333333, 3 =&gt; 0.933333, 1 =&gt; 0.0333333)</code>, indicating an overhelming probability that that car model originates from Japan. To retrieve the predictions with the highest probabilities use <code>mode(ŷ)</code>:</p><pre><code class="language-julia hljs">ŷtrain_top = mode(ŷtrain,rng=copy(AFIXEDRNG))</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">325-element Vector{Int64}:
 3
 1
 2
 1
 2
 1
 3
 1
 1
 1
 ⋮
 1
 3
 1
 1
 1
 2
 1
 3
 3</code></pre><p>Why <code>mode</code> takes (optionally) a RNG ? I let the answer for you :-)</p><p>To obtain the predicted labels for the test set we simply run the <code>predict</code> function over the features of the test set:</p><pre><code class="language-julia hljs">ŷtest   = predict(rfm,xtest)</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">81-element Vector{Dict{Int64, Float64}}:
 Dict(2 =&gt; 0.6166666666666666, 3 =&gt; 0.1, 1 =&gt; 0.2833333333333333)
 Dict(2 =&gt; 0.5499999999999999, 3 =&gt; 0.03333333333333333, 1 =&gt; 0.41666666666666663)
 Dict(2 =&gt; 0.6333333333333333, 3 =&gt; 0.11666666666666667, 1 =&gt; 0.24999999999999997)
 Dict(1 =&gt; 0.9999999999999999)
 Dict(2 =&gt; 0.19999999999999998, 3 =&gt; 0.24999999999999997, 1 =&gt; 0.5499999999999999)
 Dict(1 =&gt; 0.9999999999999999)
 Dict(2 =&gt; 0.3833333333333333, 3 =&gt; 0.5499999999999999, 1 =&gt; 0.06666666666666667)
 Dict(2 =&gt; 0.5333333333333333, 3 =&gt; 0.2333333333333333, 1 =&gt; 0.2333333333333333)
 Dict(2 =&gt; 0.03333333333333333, 1 =&gt; 0.9666666666666666)
 Dict(1 =&gt; 0.9999999999999999)
 ⋮
 Dict(2 =&gt; 0.13333333333333333, 3 =&gt; 0.15, 1 =&gt; 0.7166666666666666)
 Dict(1 =&gt; 0.9999999999999999)
 Dict(2 =&gt; 0.26666666666666666, 3 =&gt; 0.6166666666666666, 1 =&gt; 0.11666666666666667)
 Dict(1 =&gt; 0.9999999999999999)
 Dict(1 =&gt; 0.9999999999999999)
 Dict(2 =&gt; 0.16666666666666666, 3 =&gt; 0.5166666666666666, 1 =&gt; 0.31666666666666665)
 Dict(2 =&gt; 0.06666666666666667, 3 =&gt; 0.8999999999999999, 1 =&gt; 0.03333333333333333)
 Dict(2 =&gt; 0.35, 3 =&gt; 0.39999999999999997, 1 =&gt; 0.24999999999999997)
 Dict(1 =&gt; 0.9999999999999999)</code></pre><p>Finally we can measure the <em>accuracy</em> of our predictions with the <a href="../../Utils.html#BetaML.Utils.accuracy-Union{Tuple{T}, Tuple{AbstractVector{T}, AbstractVector{T}}} where T"><code>accuracy</code></a> function. We don&#39;t need to explicitly use <code>mode</code>, as <code>accuracy</code> does it itself when it is passed with predictions expressed as a dictionary:</p><pre><code class="language-julia hljs">trainAccuracy,testAccuracy  = accuracy.([ytrain,ytest],[ŷtrain,ŷtest],rng=copy(AFIXEDRNG))</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">2-element Vector{Float64}:
 0.9969230769230769
 0.7654320987654321</code></pre><p>We are now ready to store our first model accuracies in the <code>results</code> dataframe:</p><pre><code class="language-julia hljs">push!(results,[&quot;RF&quot;,trainAccuracy,testAccuracy]);</code></pre><p>The predictions are quite good, for the training set the algoritm predicted almost all cars&#39; origins correctly, while for the testing set (i.e. those records that has <strong>not</strong> been used to train the algorithm), the correct prediction level is still quite high, at around 80% (depends on the random seed)</p><p>While accuracy can sometimes suffice, we may often want to better understand which categories our model has trouble to predict correctly. We can investigate the output of a multi-class classifier more in-deep with a <a href="../../Utils.html#BetaML.Utils.ConfusionMatrix"><code>ConfusionMatrix</code></a> where the true values (<code>y</code>) are given in rows and the predicted ones (<code>ŷ</code>) in columns, together to some per-class metrics like the <em>precision</em> (true class <em>i</em> over predicted in class <em>i</em>), the <em>recall</em> (predicted class <em>i</em> over the true class <em>i</em>) and others.</p><p>We fist build the <a href="../../Utils.html#BetaML.Utils.ConfusionMatrix"><code>ConfusionMatrix</code></a> model, we train it with <code>ŷ</code> and <code>y</code> and then we print it (we do it here for the test subset):</p><pre><code class="language-julia hljs">cfm = ConfusionMatrix(categories_names=Dict(1=&gt;&quot;US&quot;,2=&gt;&quot;EU&quot;,3=&gt;&quot;Japan&quot;),rng=copy(AFIXEDRNG))
fit!(cfm,ytest,ŷtest) # the output is by default the confusion matrix in relative terms
print(cfm)</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">A ConfusionMatrix BetaMLModel (fitted)

-----------------------------------------------------------------

*** CONFUSION MATRIX ***

Scores actual (rows) vs predicted (columns):

4×4 Matrix{Any}:
 &quot;Labels&quot;    &quot;EU&quot;    &quot;US&quot;   &quot;Japan&quot;
 &quot;EU&quot;      10       5      1
 &quot;US&quot;       2      44      3
 &quot;Japan&quot;    2       6      8
Normalised scores actual (rows) vs predicted (columns):

4×4 Matrix{Any}:
 &quot;Labels&quot;   &quot;EU&quot;       &quot;US&quot;      &quot;Japan&quot;
 &quot;EU&quot;      0.625      0.3125    0.0625
 &quot;US&quot;      0.0408163  0.897959  0.0612245
 &quot;Japan&quot;   0.125      0.375     0.5

 *** CONFUSION REPORT ***

- Accuracy:               0.7654320987654321
- Misclassification rate: 0.23456790123456794
- Number of classes:      3

  N Class   precision   recall  specificity  f1score  actual_count  predicted_count
                          TPR       TNR                 support

  1 EU          0.714    0.625        0.938    0.667           16              14
  2 US          0.800    0.898        0.656    0.846           49              55
  3 Japan       0.667    0.500        0.938    0.571           16              12

- Simple   avg.    0.727    0.674        0.844    0.695
- Weigthed avg.    0.757    0.765        0.768    0.756

-----------------------------------------------------------------
Output of `info(cm)`:
- mean_precision:	(0.726984126984127, 0.7567313345091123)
- fitted_records:	81
- specificity:	[0.9384615384615385, 0.65625, 0.9384615384615385]
- precision:	[0.7142857142857143, 0.8, 0.6666666666666666]
- misclassification:	0.23456790123456794
- mean_recall:	(0.6743197278911565, 0.7654320987654321)
- n_categories:	3
- normalised_scores:	[0.625 0.3125 0.0625; 0.04081632653061224 0.8979591836734694 0.061224489795918366; 0.125 0.375 0.5]
- tn:	[61, 21, 61]
- mean_f1score:	(0.6947496947496946, 0.7564328675439785)
- actual_count:	[16, 49, 16]
- accuracy:	0.7654320987654321
- recall:	[0.625, 0.8979591836734694, 0.5]
- f1score:	[0.6666666666666666, 0.8461538461538461, 0.5714285714285714]
- mean_specificity:	(0.8443910256410256, 0.7677409781576449)
- predicted_count:	[14, 55, 12]
- scores:	[10 5 1; 2 44 3; 2 6 8]
- tp:	[10, 44, 8]
- fn:	[6, 5, 8]
- categories:	[&quot;EU&quot;, &quot;US&quot;, &quot;Japan&quot;]
- fp:	[4, 11, 4]</code></pre><p>From the report we can see that Japanese cars have more trouble in being correctly classified, and in particular many Japanease cars are classified as US ones. This is likely a result of the class imbalance of the data set, and could be solved by balancing the dataset with various sampling tecniques before training the model.</p><p>If you prefer a more graphical approach, we can also plot the confusion matrix. In order to do so, we pick up information from the <code>info(cfm)</code> function. Indeed most BetaML models can be queried with <code>info(model)</code> to retrieve additional information, in terms of a dictionary, that is not necessary to the prediciton, but could still be relevant. Other functions that you can use with BetaML models are <code>parameters(m)</code> and <code>hyperparamaeters(m)</code>.</p><pre><code class="language-julia hljs">res = info(cfm)
heatmap(string.(res[&quot;categories&quot;]),string.(res[&quot;categories&quot;]),res[&quot;normalised_scores&quot;],seriescolor=cgrad([:white,:blue]),xlabel=&quot;Predicted&quot;,ylabel=&quot;Actual&quot;, title=&quot;Confusion Matrix (normalised scores)&quot;)</code></pre><?xml version="1.0" encoding="utf-8"?>
<svg xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" width="600" height="400" viewBox="0 0 2400 1600">
<defs>
  <clipPath id="clip470">
    <rect x="0" y="0" width="2400" height="1600"/>
  </clipPath>
</defs>
<path clip-path="url(#clip470)" d="
M0 1600 L2400 1600 L2400 0 L0 0  Z
  " fill="#ffffff" fill-rule="evenodd" fill-opacity="1"/>
<defs>
  <clipPath id="clip471">
    <rect x="480" y="0" width="1681" height="1600"/>
  </clipPath>
</defs>
<path clip-path="url(#clip470)" d="
M280.908 1423.18 L2112.76 1423.18 L2112.76 123.472 L280.908 123.472  Z
  " fill="#ffffff" fill-rule="evenodd" fill-opacity="1"/>
<defs>
  <clipPath id="clip472">
    <rect x="280" y="123" width="1833" height="1301"/>
  </clipPath>
</defs>
<polyline clip-path="url(#clip472)" style="stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none" points="
  586.216,1423.18 586.216,123.472 
  "/>
<polyline clip-path="url(#clip472)" style="stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none" points="
  1196.83,1423.18 1196.83,123.472 
  "/>
<polyline clip-path="url(#clip472)" style="stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none" points="
  1807.45,1423.18 1807.45,123.472 
  "/>
<polyline clip-path="url(#clip470)" style="stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none" points="
  280.908,1423.18 2112.76,1423.18 
  "/>
<polyline clip-path="url(#clip470)" style="stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none" points="
  586.216,1423.18 586.216,1404.28 
  "/>
<polyline clip-path="url(#clip470)" style="stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none" points="
  1196.83,1423.18 1196.83,1404.28 
  "/>
<polyline clip-path="url(#clip470)" style="stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none" points="
  1807.45,1423.18 1807.45,1404.28 
  "/>
<path clip-path="url(#clip470)" d="M558.276 1451.02 L580.128 1451.02 L580.128 1454.96 L562.952 1454.96 L562.952 1465.19 L579.41 1465.19 L579.41 1469.12 L562.952 1469.12 L562.952 1481.64 L580.544 1481.64 L580.544 1485.58 L558.276 1485.58 L558.276 1451.02 Z" fill="#000000" fill-rule="evenodd" fill-opacity="1" /><path clip-path="url(#clip470)" d="M587.697 1451.02 L592.396 1451.02 L592.396 1472.02 Q592.396 1477.57 594.41 1480.02 Q596.424 1482.45 600.938 1482.45 Q605.429 1482.45 607.442 1480.02 Q609.456 1477.57 609.456 1472.02 L609.456 1451.02 L614.155 1451.02 L614.155 1472.59 Q614.155 1479.35 610.799 1482.8 Q607.466 1486.25 600.938 1486.25 Q594.387 1486.25 591.03 1482.8 Q587.697 1479.35 587.697 1472.59 L587.697 1451.02 Z" fill="#000000" fill-rule="evenodd" fill-opacity="1" /><path clip-path="url(#clip470)" d="M1167.82 1451.02 L1172.51 1451.02 L1172.51 1472.02 Q1172.51 1477.57 1174.53 1480.02 Q1176.54 1482.45 1181.06 1482.45 Q1185.55 1482.45 1187.56 1480.02 Q1189.57 1477.57 1189.57 1472.02 L1189.57 1451.02 L1194.27 1451.02 L1194.27 1472.59 Q1194.27 1479.35 1190.92 1482.8 Q1187.58 1486.25 1181.06 1486.25 Q1174.51 1486.25 1171.15 1482.8 Q1167.82 1479.35 1167.82 1472.59 L1167.82 1451.02 Z" fill="#000000" fill-rule="evenodd" fill-opacity="1" /><path clip-path="url(#clip470)" d="M1223.76 1452.15 L1223.76 1456.71 Q1221.1 1455.44 1218.74 1454.82 Q1216.38 1454.19 1214.18 1454.19 Q1210.36 1454.19 1208.28 1455.67 Q1206.22 1457.15 1206.22 1459.89 Q1206.22 1462.18 1207.58 1463.36 Q1208.97 1464.52 1212.82 1465.23 L1215.64 1465.81 Q1220.87 1466.81 1223.35 1469.33 Q1225.85 1471.83 1225.85 1476.04 Q1225.85 1481.07 1222.47 1483.66 Q1219.11 1486.25 1212.61 1486.25 Q1210.15 1486.25 1207.38 1485.7 Q1204.62 1485.14 1201.66 1484.05 L1201.66 1479.24 Q1204.51 1480.83 1207.24 1481.64 Q1209.97 1482.45 1212.61 1482.45 Q1216.61 1482.45 1218.79 1480.88 Q1220.96 1479.31 1220.96 1476.39 Q1220.96 1473.84 1219.39 1472.41 Q1217.84 1470.97 1214.27 1470.26 L1211.43 1469.7 Q1206.2 1468.66 1203.86 1466.44 Q1201.52 1464.21 1201.52 1460.26 Q1201.52 1455.67 1204.74 1453.03 Q1207.98 1450.39 1213.65 1450.39 Q1216.08 1450.39 1218.6 1450.83 Q1221.13 1451.27 1223.76 1452.15 Z" fill="#000000" fill-rule="evenodd" fill-opacity="1" /><path clip-path="url(#clip470)" d="M1749.23 1451.02 L1753.91 1451.02 L1753.91 1483.17 Q1753.91 1489.42 1751.52 1492.25 Q1749.16 1495.07 1743.91 1495.07 L1742.12 1495.07 L1742.12 1491.14 L1743.58 1491.14 Q1746.68 1491.14 1747.96 1489.4 Q1749.23 1487.66 1749.23 1483.17 L1749.23 1451.02 Z" fill="#000000" fill-rule="evenodd" fill-opacity="1" /><path clip-path="url(#clip470)" d="M1774.81 1472.55 Q1769.65 1472.55 1767.66 1473.73 Q1765.67 1474.91 1765.67 1477.76 Q1765.67 1480.02 1767.15 1481.37 Q1768.65 1482.69 1771.22 1482.69 Q1774.76 1482.69 1776.89 1480.19 Q1779.05 1477.66 1779.05 1473.5 L1779.05 1472.55 L1774.81 1472.55 M1783.3 1470.79 L1783.3 1485.58 L1779.05 1485.58 L1779.05 1481.64 Q1777.59 1484.01 1775.41 1485.14 Q1773.24 1486.25 1770.09 1486.25 Q1766.11 1486.25 1763.74 1484.03 Q1761.41 1481.78 1761.41 1478.03 Q1761.41 1473.66 1764.32 1471.44 Q1767.26 1469.21 1773.07 1469.21 L1779.05 1469.21 L1779.05 1468.8 Q1779.05 1465.86 1777.1 1464.26 Q1775.18 1462.64 1771.68 1462.64 Q1769.46 1462.64 1767.36 1463.17 Q1765.25 1463.7 1763.3 1464.77 L1763.3 1460.83 Q1765.64 1459.93 1767.84 1459.49 Q1770.04 1459.03 1772.12 1459.03 Q1777.75 1459.03 1780.53 1461.95 Q1783.3 1464.86 1783.3 1470.79 Z" fill="#000000" fill-rule="evenodd" fill-opacity="1" /><path clip-path="url(#clip470)" d="M1796.2 1481.69 L1796.2 1495.44 L1791.92 1495.44 L1791.92 1459.65 L1796.2 1459.65 L1796.2 1463.59 Q1797.54 1461.27 1799.58 1460.16 Q1801.64 1459.03 1804.48 1459.03 Q1809.21 1459.03 1812.15 1462.78 Q1815.11 1466.53 1815.11 1472.64 Q1815.11 1478.75 1812.15 1482.5 Q1809.21 1486.25 1804.48 1486.25 Q1801.64 1486.25 1799.58 1485.14 Q1797.54 1484.01 1796.2 1481.69 M1810.69 1472.64 Q1810.69 1467.94 1808.74 1465.28 Q1806.82 1462.59 1803.44 1462.59 Q1800.06 1462.59 1798.12 1465.28 Q1796.2 1467.94 1796.2 1472.64 Q1796.2 1477.34 1798.12 1480.02 Q1800.06 1482.69 1803.44 1482.69 Q1806.82 1482.69 1808.74 1480.02 Q1810.69 1477.34 1810.69 1472.64 Z" fill="#000000" fill-rule="evenodd" fill-opacity="1" /><path clip-path="url(#clip470)" d="M1833.95 1472.55 Q1828.79 1472.55 1826.8 1473.73 Q1824.81 1474.91 1824.81 1477.76 Q1824.81 1480.02 1826.29 1481.37 Q1827.79 1482.69 1830.36 1482.69 Q1833.91 1482.69 1836.04 1480.19 Q1838.19 1477.66 1838.19 1473.5 L1838.19 1472.55 L1833.95 1472.55 M1842.45 1470.79 L1842.45 1485.58 L1838.19 1485.58 L1838.19 1481.64 Q1836.73 1484.01 1834.55 1485.14 Q1832.38 1486.25 1829.23 1486.25 Q1825.25 1486.25 1822.89 1484.03 Q1820.55 1481.78 1820.55 1478.03 Q1820.55 1473.66 1823.47 1471.44 Q1826.41 1469.21 1832.22 1469.21 L1838.19 1469.21 L1838.19 1468.8 Q1838.19 1465.86 1836.24 1464.26 Q1834.32 1462.64 1830.83 1462.64 Q1828.61 1462.64 1826.5 1463.17 Q1824.39 1463.7 1822.45 1464.77 L1822.45 1460.83 Q1824.79 1459.93 1826.98 1459.49 Q1829.18 1459.03 1831.27 1459.03 Q1836.89 1459.03 1839.67 1461.95 Q1842.45 1464.86 1842.45 1470.79 Z" fill="#000000" fill-rule="evenodd" fill-opacity="1" /><path clip-path="url(#clip470)" d="M1872.77 1469.93 L1872.77 1485.58 L1868.51 1485.58 L1868.51 1470.07 Q1868.51 1466.39 1867.08 1464.56 Q1865.64 1462.73 1862.77 1462.73 Q1859.32 1462.73 1857.33 1464.93 Q1855.34 1467.13 1855.34 1470.93 L1855.34 1485.58 L1851.06 1485.58 L1851.06 1459.65 L1855.34 1459.65 L1855.34 1463.68 Q1856.87 1461.34 1858.93 1460.19 Q1861.01 1459.03 1863.72 1459.03 Q1868.19 1459.03 1870.48 1461.81 Q1872.77 1464.56 1872.77 1469.93 Z" fill="#000000" fill-rule="evenodd" fill-opacity="1" /><path clip-path="url(#clip470)" d="M1056.42 1525.81 L1056.42 1543.66 L1064.5 1543.66 Q1068.99 1543.66 1071.44 1541.34 Q1073.89 1539.02 1073.89 1534.72 Q1073.89 1530.45 1071.44 1528.13 Q1068.99 1525.81 1064.5 1525.81 L1056.42 1525.81 M1049.99 1520.52 L1064.5 1520.52 Q1072.49 1520.52 1076.57 1524.15 Q1080.67 1527.75 1080.67 1534.72 Q1080.67 1541.75 1076.57 1545.35 Q1072.49 1548.95 1064.5 1548.95 L1056.42 1548.95 L1056.42 1568.04 L1049.99 1568.04 L1049.99 1520.52 Z" fill="#000000" fill-rule="evenodd" fill-opacity="1" /><path clip-path="url(#clip470)" d="M1108.56 1537.87 Q1107.57 1537.3 1106.39 1537.04 Q1105.25 1536.76 1103.84 1536.76 Q1098.88 1536.76 1096.21 1540 Q1093.56 1543.22 1093.56 1549.27 L1093.56 1568.04 L1087.68 1568.04 L1087.68 1532.4 L1093.56 1532.4 L1093.56 1537.93 Q1095.41 1534.69 1098.37 1533.13 Q1101.33 1531.54 1105.56 1531.54 Q1106.17 1531.54 1106.9 1531.63 Q1107.63 1531.7 1108.52 1531.85 L1108.56 1537.87 Z" fill="#000000" fill-rule="evenodd" fill-opacity="1" /><path clip-path="url(#clip470)" d="M1143.76 1548.76 L1143.76 1551.62 L1116.83 1551.62 Q1117.21 1557.67 1120.46 1560.85 Q1123.74 1564 1129.56 1564 Q1132.94 1564 1136.09 1563.17 Q1139.27 1562.35 1142.39 1560.69 L1142.39 1566.23 Q1139.24 1567.57 1135.93 1568.27 Q1132.62 1568.97 1129.21 1568.97 Q1120.68 1568.97 1115.68 1564 Q1110.72 1559.04 1110.72 1550.57 Q1110.72 1541.82 1115.43 1536.69 Q1120.17 1531.54 1128.19 1531.54 Q1135.39 1531.54 1139.56 1536.18 Q1143.76 1540.8 1143.76 1548.76 M1137.9 1547.04 Q1137.84 1542.23 1135.2 1539.37 Q1132.59 1536.5 1128.26 1536.5 Q1123.36 1536.5 1120.4 1539.27 Q1117.47 1542.04 1117.02 1547.07 L1137.9 1547.04 Z" fill="#000000" fill-rule="evenodd" fill-opacity="1" /><path clip-path="url(#clip470)" d="M1176.83 1537.81 L1176.83 1518.52 L1182.68 1518.52 L1182.68 1568.04 L1176.83 1568.04 L1176.83 1562.7 Q1174.98 1565.88 1172.15 1567.44 Q1169.35 1568.97 1165.4 1568.97 Q1158.94 1568.97 1154.87 1563.81 Q1150.82 1558.65 1150.82 1550.25 Q1150.82 1541.85 1154.87 1536.69 Q1158.94 1531.54 1165.4 1531.54 Q1169.35 1531.54 1172.15 1533.1 Q1174.98 1534.62 1176.83 1537.81 M1156.87 1550.25 Q1156.87 1556.71 1159.51 1560.4 Q1162.19 1564.07 1166.83 1564.07 Q1171.48 1564.07 1174.15 1560.4 Q1176.83 1556.71 1176.83 1550.25 Q1176.83 1543.79 1174.15 1540.13 Q1171.48 1536.44 1166.83 1536.44 Q1162.19 1536.44 1159.51 1540.13 Q1156.87 1543.79 1156.87 1550.25 Z" fill="#000000" fill-rule="evenodd" fill-opacity="1" /><path clip-path="url(#clip470)" d="M1194.75 1532.4 L1200.6 1532.4 L1200.6 1568.04 L1194.75 1568.04 L1194.75 1532.4 M1194.75 1518.52 L1200.6 1518.52 L1200.6 1525.93 L1194.75 1525.93 L1194.75 1518.52 Z" fill="#000000" fill-rule="evenodd" fill-opacity="1" /><path clip-path="url(#clip470)" d="M1238.51 1533.76 L1238.51 1539.24 Q1236.03 1537.87 1233.51 1537.2 Q1231.03 1536.5 1228.49 1536.5 Q1222.79 1536.5 1219.64 1540.13 Q1216.49 1543.73 1216.49 1550.25 Q1216.49 1556.78 1219.64 1560.4 Q1222.79 1564 1228.49 1564 Q1231.03 1564 1233.51 1563.33 Q1236.03 1562.63 1238.51 1561.26 L1238.51 1566.68 Q1236.06 1567.82 1233.42 1568.39 Q1230.81 1568.97 1227.85 1568.97 Q1219.8 1568.97 1215.05 1563.91 Q1210.31 1558.85 1210.31 1550.25 Q1210.31 1541.53 1215.09 1536.53 Q1219.89 1531.54 1228.23 1531.54 Q1230.94 1531.54 1233.51 1532.11 Q1236.09 1532.65 1238.51 1533.76 Z" fill="#000000" fill-rule="evenodd" fill-opacity="1" /><path clip-path="url(#clip470)" d="M1254.49 1522.27 L1254.49 1532.4 L1266.55 1532.4 L1266.55 1536.95 L1254.49 1536.95 L1254.49 1556.3 Q1254.49 1560.66 1255.67 1561.9 Q1256.88 1563.14 1260.54 1563.14 L1266.55 1563.14 L1266.55 1568.04 L1260.54 1568.04 Q1253.76 1568.04 1251.18 1565.53 Q1248.6 1562.98 1248.6 1556.3 L1248.6 1536.95 L1244.3 1536.95 L1244.3 1532.4 L1248.6 1532.4 L1248.6 1522.27 L1254.49 1522.27 Z" fill="#000000" fill-rule="evenodd" fill-opacity="1" /><path clip-path="url(#clip470)" d="M1304.75 1548.76 L1304.75 1551.62 L1277.82 1551.62 Q1278.2 1557.67 1281.45 1560.85 Q1284.73 1564 1290.55 1564 Q1293.92 1564 1297.08 1563.17 Q1300.26 1562.35 1303.38 1560.69 L1303.38 1566.23 Q1300.23 1567.57 1296.92 1568.27 Q1293.61 1568.97 1290.2 1568.97 Q1281.67 1568.97 1276.67 1564 Q1271.71 1559.04 1271.71 1550.57 Q1271.71 1541.82 1276.42 1536.69 Q1281.16 1531.54 1289.18 1531.54 Q1296.38 1531.54 1300.55 1536.18 Q1304.75 1540.8 1304.75 1548.76 M1298.89 1547.04 Q1298.83 1542.23 1296.18 1539.37 Q1293.57 1536.5 1289.25 1536.5 Q1284.34 1536.5 1281.38 1539.27 Q1278.46 1542.04 1278.01 1547.07 L1298.89 1547.04 Z" fill="#000000" fill-rule="evenodd" fill-opacity="1" /><path clip-path="url(#clip470)" d="M1337.82 1537.81 L1337.82 1518.52 L1343.67 1518.52 L1343.67 1568.04 L1337.82 1568.04 L1337.82 1562.7 Q1335.97 1565.88 1333.14 1567.44 Q1330.34 1568.97 1326.39 1568.97 Q1319.93 1568.97 1315.85 1563.81 Q1311.81 1558.65 1311.81 1550.25 Q1311.81 1541.85 1315.85 1536.69 Q1319.93 1531.54 1326.39 1531.54 Q1330.34 1531.54 1333.14 1533.1 Q1335.97 1534.62 1337.82 1537.81 M1317.86 1550.25 Q1317.86 1556.71 1320.5 1560.4 Q1323.18 1564.07 1327.82 1564.07 Q1332.47 1564.07 1335.14 1560.4 Q1337.82 1556.71 1337.82 1550.25 Q1337.82 1543.79 1335.14 1540.13 Q1332.47 1536.44 1327.82 1536.44 Q1323.18 1536.44 1320.5 1540.13 Q1317.86 1543.79 1317.86 1550.25 Z" fill="#000000" fill-rule="evenodd" fill-opacity="1" /><polyline clip-path="url(#clip472)" style="stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none" points="
  280.908,1206.56 2112.76,1206.56 
  "/>
<polyline clip-path="url(#clip472)" style="stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none" points="
  280.908,773.326 2112.76,773.326 
  "/>
<polyline clip-path="url(#clip472)" style="stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none" points="
  280.908,340.09 2112.76,340.09 
  "/>
<polyline clip-path="url(#clip470)" style="stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none" points="
  280.908,1423.18 280.908,123.472 
  "/>
<polyline clip-path="url(#clip470)" style="stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none" points="
  280.908,1206.56 297.616,1206.56 
  "/>
<polyline clip-path="url(#clip470)" style="stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none" points="
  280.908,773.326 297.616,773.326 
  "/>
<polyline clip-path="url(#clip470)" style="stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none" points="
  280.908,340.09 297.616,340.09 
  "/>
<path clip-path="url(#clip470)" d="M189.028 1189.28 L210.88 1189.28 L210.88 1193.22 L193.704 1193.22 L193.704 1203.45 L210.162 1203.45 L210.162 1207.38 L193.704 1207.38 L193.704 1219.91 L211.297 1219.91 L211.297 1223.84 L189.028 1223.84 L189.028 1189.28 Z" fill="#000000" fill-rule="evenodd" fill-opacity="1" /><path clip-path="url(#clip470)" d="M218.449 1189.28 L223.148 1189.28 L223.148 1210.28 Q223.148 1215.83 225.162 1218.29 Q227.176 1220.72 231.69 1220.72 Q236.181 1220.72 238.195 1218.29 Q240.209 1215.83 240.209 1210.28 L240.209 1189.28 L244.908 1189.28 L244.908 1210.86 Q244.908 1217.62 241.551 1221.06 Q238.218 1224.51 231.69 1224.51 Q225.139 1224.51 221.783 1221.06 Q218.449 1217.62 218.449 1210.86 L218.449 1189.28 Z" fill="#000000" fill-rule="evenodd" fill-opacity="1" /><path clip-path="url(#clip470)" d="M186.876 756.046 L191.575 756.046 L191.575 777.041 Q191.575 782.597 193.588 785.05 Q195.602 787.481 200.116 787.481 Q204.607 787.481 206.621 785.05 Q208.635 782.597 208.635 777.041 L208.635 756.046 L213.334 756.046 L213.334 777.62 Q213.334 784.379 209.977 787.828 Q206.644 791.277 200.116 791.277 Q193.565 791.277 190.209 787.828 Q186.876 784.379 186.876 777.62 L186.876 756.046 Z" fill="#000000" fill-rule="evenodd" fill-opacity="1" /><path clip-path="url(#clip470)" d="M242.824 757.18 L242.824 761.74 Q240.162 760.467 237.801 759.842 Q235.44 759.217 233.241 759.217 Q229.422 759.217 227.338 760.699 Q225.278 762.18 225.278 764.912 Q225.278 767.203 226.644 768.384 Q228.033 769.541 231.875 770.259 L234.699 770.838 Q239.931 771.833 242.408 774.356 Q244.908 776.856 244.908 781.069 Q244.908 786.092 241.528 788.685 Q238.172 791.277 231.667 791.277 Q229.213 791.277 226.435 790.722 Q223.681 790.166 220.718 789.078 L220.718 784.263 Q223.565 785.861 226.297 786.671 Q229.028 787.481 231.667 787.481 Q235.672 787.481 237.847 785.907 Q240.023 784.333 240.023 781.416 Q240.023 778.87 238.449 777.435 Q236.898 776 233.334 775.282 L230.486 774.726 Q225.255 773.685 222.917 771.463 Q220.579 769.24 220.579 765.282 Q220.579 760.699 223.797 758.06 Q227.037 755.421 232.709 755.421 Q235.139 755.421 237.662 755.861 Q240.185 756.301 242.824 757.18 Z" fill="#000000" fill-rule="evenodd" fill-opacity="1" /><path clip-path="url(#clip470)" d="M121.367 322.81 L126.043 322.81 L126.043 354.963 Q126.043 361.213 123.658 364.037 Q121.297 366.861 116.043 366.861 L114.26 366.861 L114.26 362.926 L115.718 362.926 Q118.82 362.926 120.093 361.19 Q121.367 359.453 121.367 354.963 L121.367 322.81 Z" fill="#000000" fill-rule="evenodd" fill-opacity="1" /><path clip-path="url(#clip470)" d="M146.945 344.338 Q141.783 344.338 139.792 345.518 Q137.802 346.699 137.802 349.546 Q137.802 351.815 139.283 353.157 Q140.788 354.477 143.357 354.477 Q146.899 354.477 149.028 351.977 Q151.181 349.453 151.181 345.287 L151.181 344.338 L146.945 344.338 M155.44 342.578 L155.44 357.37 L151.181 357.37 L151.181 353.435 Q149.723 355.796 147.547 356.93 Q145.371 358.041 142.223 358.041 Q138.242 358.041 135.88 355.819 Q133.542 353.574 133.542 349.824 Q133.542 345.449 136.459 343.227 Q139.399 341.004 145.209 341.004 L151.181 341.004 L151.181 340.588 Q151.181 337.648 149.237 336.051 Q147.316 334.43 143.82 334.43 Q141.598 334.43 139.492 334.963 Q137.385 335.495 135.441 336.56 L135.441 332.625 Q137.779 331.722 139.978 331.282 Q142.177 330.819 144.26 330.819 Q149.885 330.819 152.663 333.736 Q155.44 336.653 155.44 342.578 Z" fill="#000000" fill-rule="evenodd" fill-opacity="1" /><path clip-path="url(#clip470)" d="M168.334 353.481 L168.334 367.231 L164.052 367.231 L164.052 331.444 L168.334 331.444 L168.334 335.379 Q169.677 333.065 171.714 331.954 Q173.774 330.819 176.621 330.819 Q181.343 330.819 184.283 334.569 Q187.246 338.319 187.246 344.43 Q187.246 350.541 184.283 354.291 Q181.343 358.041 176.621 358.041 Q173.774 358.041 171.714 356.93 Q169.677 355.796 168.334 353.481 M182.825 344.43 Q182.825 339.731 180.88 337.069 Q178.959 334.384 175.579 334.384 Q172.2 334.384 170.255 337.069 Q168.334 339.731 168.334 344.43 Q168.334 349.129 170.255 351.815 Q172.2 354.477 175.579 354.477 Q178.959 354.477 180.88 351.815 Q182.825 349.129 182.825 344.43 Z" fill="#000000" fill-rule="evenodd" fill-opacity="1" /><path clip-path="url(#clip470)" d="M206.088 344.338 Q200.926 344.338 198.936 345.518 Q196.945 346.699 196.945 349.546 Q196.945 351.815 198.426 353.157 Q199.931 354.477 202.5 354.477 Q206.042 354.477 208.172 351.977 Q210.324 349.453 210.324 345.287 L210.324 344.338 L206.088 344.338 M214.584 342.578 L214.584 357.37 L210.324 357.37 L210.324 353.435 Q208.866 355.796 206.69 356.93 Q204.514 358.041 201.366 358.041 Q197.385 358.041 195.024 355.819 Q192.686 353.574 192.686 349.824 Q192.686 345.449 195.602 343.227 Q198.542 341.004 204.352 341.004 L210.324 341.004 L210.324 340.588 Q210.324 337.648 208.38 336.051 Q206.459 334.43 202.963 334.43 Q200.741 334.43 198.635 334.963 Q196.528 335.495 194.584 336.56 L194.584 332.625 Q196.922 331.722 199.121 331.282 Q201.32 330.819 203.403 330.819 Q209.028 330.819 211.806 333.736 Q214.584 336.653 214.584 342.578 Z" fill="#000000" fill-rule="evenodd" fill-opacity="1" /><path clip-path="url(#clip470)" d="M244.908 341.722 L244.908 357.37 L240.648 357.37 L240.648 341.861 Q240.648 338.18 239.213 336.352 Q237.778 334.523 234.908 334.523 Q231.459 334.523 229.468 336.722 Q227.477 338.921 227.477 342.717 L227.477 357.37 L223.195 357.37 L223.195 331.444 L227.477 331.444 L227.477 335.472 Q229.005 333.134 231.065 331.977 Q233.148 330.819 235.857 330.819 Q240.324 330.819 242.616 333.597 Q244.908 336.352 244.908 341.722 Z" fill="#000000" fill-rule="evenodd" fill-opacity="1" /><path clip-path="url(#clip470)" d="M22.818 850.351 L46.4666 859.072 L46.4666 841.598 L22.818 850.351 M16.4842 853.979 L16.4842 846.691 L64.0042 828.58 L64.0042 835.264 L51.8138 839.593 L51.8138 861.014 L64.0042 865.342 L64.0042 872.122 L16.4842 853.979 Z" fill="#000000" fill-rule="evenodd" fill-opacity="1" /><path clip-path="url(#clip470)" d="M29.7248 797.388 L35.1993 797.388 Q33.8307 799.871 33.1623 802.385 Q32.4621 804.868 32.4621 807.414 Q32.4621 813.112 36.0905 816.263 Q39.6872 819.414 46.212 819.414 Q52.7369 819.414 56.3653 816.263 Q59.9619 813.112 59.9619 807.414 Q59.9619 804.868 59.2935 802.385 Q58.5933 799.871 57.2247 797.388 L62.6355 797.388 Q63.7814 799.839 64.3543 802.481 Q64.9272 805.091 64.9272 808.051 Q64.9272 816.104 59.8664 820.846 Q54.8057 825.588 46.212 825.588 Q37.491 825.588 32.4939 820.814 Q27.4968 816.008 27.4968 807.669 Q27.4968 804.964 28.0697 802.385 Q28.6108 799.807 29.7248 797.388 Z" fill="#000000" fill-rule="evenodd" fill-opacity="1" /><path clip-path="url(#clip470)" d="M18.2347 781.41 L28.3562 781.41 L28.3562 769.347 L32.9077 769.347 L32.9077 781.41 L52.2594 781.41 Q56.6199 781.41 57.8613 780.233 Q59.1026 779.023 59.1026 775.363 L59.1026 769.347 L64.0042 769.347 L64.0042 775.363 Q64.0042 782.142 61.4897 784.721 Q58.9434 787.299 52.2594 787.299 L32.9077 787.299 L32.9077 791.596 L28.3562 791.596 L28.3562 787.299 L18.2347 787.299 L18.2347 781.41 Z" fill="#000000" fill-rule="evenodd" fill-opacity="1" /><path clip-path="url(#clip470)" d="M49.9359 762.25 L28.3562 762.25 L28.3562 756.393 L49.7131 756.393 Q54.7739 756.393 57.3202 754.42 Q59.8346 752.446 59.8346 748.5 Q59.8346 743.757 56.8109 741.02 Q53.7872 738.251 48.5673 738.251 L28.3562 738.251 L28.3562 732.394 L64.0042 732.394 L64.0042 738.251 L58.5296 738.251 Q61.7762 740.383 63.3676 743.216 Q64.9272 746.017 64.9272 749.741 Q64.9272 755.884 61.1078 759.067 Q57.2883 762.25 49.9359 762.25 M27.4968 747.513 L27.4968 747.513 Z" fill="#000000" fill-rule="evenodd" fill-opacity="1" /><path clip-path="url(#clip470)" d="M46.0847 704.131 Q46.0847 711.229 47.7079 713.966 Q49.3312 716.703 53.2461 716.703 Q56.3653 716.703 58.2114 714.666 Q60.0256 712.597 60.0256 709.064 Q60.0256 704.194 56.5881 701.266 Q53.1188 698.306 47.3897 698.306 L46.0847 698.306 L46.0847 704.131 M43.6657 692.45 L64.0042 692.45 L64.0042 698.306 L58.5933 698.306 Q61.8398 700.311 63.3994 703.303 Q64.9272 706.295 64.9272 710.624 Q64.9272 716.098 61.8716 719.345 Q58.7843 722.559 53.6281 722.559 Q47.6125 722.559 44.5569 718.549 Q41.5014 714.507 41.5014 706.518 L41.5014 698.306 L40.9285 698.306 Q36.8862 698.306 34.6901 700.98 Q32.4621 703.621 32.4621 708.428 Q32.4621 711.483 33.1941 714.38 Q33.9262 717.276 35.3903 719.95 L29.9795 719.95 Q28.7381 716.735 28.1334 713.711 Q27.4968 710.687 27.4968 707.823 Q27.4968 700.089 31.5072 696.269 Q35.5176 692.45 43.6657 692.45 Z" fill="#000000" fill-rule="evenodd" fill-opacity="1" /><path clip-path="url(#clip470)" d="M14.479 680.387 L14.479 674.53 L64.0042 674.53 L64.0042 680.387 L14.479 680.387 Z" fill="#000000" fill-rule="evenodd" fill-opacity="1" /><path clip-path="url(#clip470)" d="M479.234 16.7545 L479.234 25.383 Q475.102 21.5346 470.403 19.6307 Q465.745 17.7268 460.479 17.7268 Q450.108 17.7268 444.599 24.0867 Q439.09 30.4061 439.09 42.3968 Q439.09 54.3469 444.599 60.7069 Q450.108 67.0263 460.479 67.0263 Q465.745 67.0263 470.403 65.1223 Q475.102 63.2184 479.234 59.3701 L479.234 67.9175 Q474.94 70.8341 470.12 72.2924 Q465.34 73.7508 459.993 73.7508 Q446.26 73.7508 438.361 65.3654 Q430.461 56.9395 430.461 42.3968 Q430.461 27.8135 438.361 19.4281 Q446.26 11.0023 459.993 11.0023 Q465.421 11.0023 470.201 12.4606 Q475.021 13.8784 479.234 16.7545 Z" fill="#000000" fill-rule="evenodd" fill-opacity="1" /><path clip-path="url(#clip470)" d="M509.13 32.4315 Q503.135 32.4315 499.651 37.1306 Q496.167 41.7891 496.167 49.9314 Q496.167 58.0738 499.61 62.7728 Q503.094 67.4314 509.13 67.4314 Q515.085 67.4314 518.569 62.7323 Q522.052 58.0333 522.052 49.9314 Q522.052 41.8701 518.569 37.1711 Q515.085 32.4315 509.13 32.4315 M509.13 26.1121 Q518.852 26.1121 524.402 32.4315 Q529.952 38.7509 529.952 49.9314 Q529.952 61.0714 524.402 67.4314 Q518.852 73.7508 509.13 73.7508 Q499.367 73.7508 493.818 67.4314 Q488.308 61.0714 488.308 49.9314 Q488.308 38.7509 493.818 32.4315 Q499.367 26.1121 509.13 26.1121 Z" fill="#000000" fill-rule="evenodd" fill-opacity="1" /><path clip-path="url(#clip470)" d="M580.021 45.1919 L580.021 72.576 L572.567 72.576 L572.567 45.4349 Q572.567 38.994 570.056 35.7938 Q567.544 32.5936 562.521 32.5936 Q556.485 32.5936 553.001 36.4419 Q549.518 40.2903 549.518 46.9338 L549.518 72.576 L542.023 72.576 L542.023 27.2059 L549.518 27.2059 L549.518 34.2544 Q552.191 30.163 555.796 28.1376 Q559.442 26.1121 564.182 26.1121 Q572 26.1121 576.01 30.9732 Q580.021 35.7938 580.021 45.1919 Z" fill="#000000" fill-rule="evenodd" fill-opacity="1" /><path clip-path="url(#clip470)" d="M617.856 9.54393 L617.856 15.7418 L610.727 15.7418 Q606.716 15.7418 605.136 17.3622 Q603.597 18.9825 603.597 23.1955 L603.597 27.2059 L615.871 27.2059 L615.871 32.9987 L603.597 32.9987 L603.597 72.576 L596.103 72.576 L596.103 32.9987 L588.973 32.9987 L588.973 27.2059 L596.103 27.2059 L596.103 24.0462 Q596.103 16.471 599.627 13.0277 Q603.151 9.54393 610.808 9.54393 L617.856 9.54393 Z" fill="#000000" fill-rule="evenodd" fill-opacity="1" /><path clip-path="url(#clip470)" d="M623.325 54.671 L623.325 27.2059 L630.779 27.2059 L630.779 54.3874 Q630.779 60.8284 633.29 64.0691 Q635.802 67.2693 640.825 67.2693 Q646.861 67.2693 650.345 63.421 Q653.869 59.5726 653.869 52.9291 L653.869 27.2059 L661.322 27.2059 L661.322 72.576 L653.869 72.576 L653.869 65.6084 Q651.155 69.7404 647.549 71.7658 Q643.985 73.7508 639.245 73.7508 Q631.427 73.7508 627.376 68.8897 Q623.325 64.0286 623.325 54.671 M642.081 26.1121 L642.081 26.1121 Z" fill="#000000" fill-rule="evenodd" fill-opacity="1" /><path clip-path="url(#clip470)" d="M705.599 28.5427 L705.599 35.5912 Q702.439 33.9709 699.036 33.1607 Q695.634 32.3505 691.988 32.3505 Q686.438 32.3505 683.643 34.0519 Q680.888 35.7533 680.888 39.156 Q680.888 41.7486 682.873 43.2475 Q684.858 44.7058 690.854 46.0426 L693.406 46.6097 Q701.345 48.3111 704.667 51.4303 Q708.029 54.509 708.029 60.0587 Q708.029 66.3781 703.006 70.0644 Q698.024 73.7508 689.274 73.7508 Q685.628 73.7508 681.658 73.0216 Q677.729 72.3329 673.354 70.9151 L673.354 63.2184 Q677.486 65.3654 681.496 66.4591 Q685.506 67.5124 689.436 67.5124 Q694.702 67.5124 697.538 65.73 Q700.373 63.9071 700.373 60.6258 Q700.373 57.5877 698.307 55.9673 Q696.282 54.3469 689.355 52.8481 L686.762 52.2405 Q679.835 50.7821 676.756 47.7845 Q673.678 44.7463 673.678 39.4801 Q673.678 33.0797 678.215 29.5959 Q682.752 26.1121 691.097 26.1121 Q695.229 26.1121 698.874 26.7198 Q702.52 27.3274 705.599 28.5427 Z" fill="#000000" fill-rule="evenodd" fill-opacity="1" /><path clip-path="url(#clip470)" d="M719.899 27.2059 L727.352 27.2059 L727.352 72.576 L719.899 72.576 L719.899 27.2059 M719.899 9.54393 L727.352 9.54393 L727.352 18.9825 L719.899 18.9825 L719.899 9.54393 Z" fill="#000000" fill-rule="evenodd" fill-opacity="1" /><path clip-path="url(#clip470)" d="M760.529 32.4315 Q754.534 32.4315 751.05 37.1306 Q747.566 41.7891 747.566 49.9314 Q747.566 58.0738 751.009 62.7728 Q754.493 67.4314 760.529 67.4314 Q766.484 67.4314 769.968 62.7323 Q773.452 58.0333 773.452 49.9314 Q773.452 41.8701 769.968 37.1711 Q766.484 32.4315 760.529 32.4315 M760.529 26.1121 Q770.251 26.1121 775.801 32.4315 Q781.351 38.7509 781.351 49.9314 Q781.351 61.0714 775.801 67.4314 Q770.251 73.7508 760.529 73.7508 Q750.766 73.7508 745.217 67.4314 Q739.707 61.0714 739.707 49.9314 Q739.707 38.7509 745.217 32.4315 Q750.766 26.1121 760.529 26.1121 Z" fill="#000000" fill-rule="evenodd" fill-opacity="1" /><path clip-path="url(#clip470)" d="M831.42 45.1919 L831.42 72.576 L823.966 72.576 L823.966 45.4349 Q823.966 38.994 821.455 35.7938 Q818.943 32.5936 813.92 32.5936 Q807.884 32.5936 804.4 36.4419 Q800.917 40.2903 800.917 46.9338 L800.917 72.576 L793.422 72.576 L793.422 27.2059 L800.917 27.2059 L800.917 34.2544 Q803.59 30.163 807.196 28.1376 Q810.841 26.1121 815.581 26.1121 Q823.399 26.1121 827.41 30.9732 Q831.42 35.7938 831.42 45.1919 Z" fill="#000000" fill-rule="evenodd" fill-opacity="1" /><path clip-path="url(#clip470)" d="M872.982 12.096 L885.175 12.096 L900.609 53.2532 L916.124 12.096 L928.318 12.096 L928.318 72.576 L920.337 72.576 L920.337 19.4686 L904.741 60.9499 L896.518 60.9499 L880.922 19.4686 L880.922 72.576 L872.982 72.576 L872.982 12.096 Z" fill="#000000" fill-rule="evenodd" fill-opacity="1" /><path clip-path="url(#clip470)" d="M964.857 49.7694 Q955.823 49.7694 952.339 51.8354 Q948.856 53.9013 948.856 58.8839 Q948.856 62.8538 951.448 65.2034 Q954.081 67.5124 958.578 67.5124 Q964.776 67.5124 968.503 63.1374 Q972.27 58.7219 972.27 51.4303 L972.27 49.7694 L964.857 49.7694 M979.724 46.6907 L979.724 72.576 L972.27 72.576 L972.27 65.6895 Q969.718 69.8214 965.91 71.8063 Q962.102 73.7508 956.593 73.7508 Q949.625 73.7508 945.493 69.8619 Q941.402 65.9325 941.402 59.3701 Q941.402 51.7138 946.506 47.825 Q951.651 43.9361 961.819 43.9361 L972.27 43.9361 L972.27 43.2069 Q972.27 38.0623 968.867 35.2672 Q965.505 32.4315 959.388 32.4315 Q955.499 32.4315 951.813 33.3632 Q948.126 34.295 944.724 36.1584 L944.724 29.2718 Q948.815 27.692 952.664 26.9223 Q956.512 26.1121 960.158 26.1121 Q970.001 26.1121 974.862 31.2163 Q979.724 36.3204 979.724 46.6907 Z" fill="#000000" fill-rule="evenodd" fill-opacity="1" /><path clip-path="url(#clip470)" d="M1002.45 14.324 L1002.45 27.2059 L1017.8 27.2059 L1017.8 32.9987 L1002.45 32.9987 L1002.45 57.6282 Q1002.45 63.1779 1003.95 64.7578 Q1005.49 66.3376 1010.15 66.3376 L1017.8 66.3376 L1017.8 72.576 L1010.15 72.576 Q1001.52 72.576 998.236 69.3758 Q994.955 66.1351 994.955 57.6282 L994.955 32.9987 L989.486 32.9987 L989.486 27.2059 L994.955 27.2059 L994.955 14.324 L1002.45 14.324 Z" fill="#000000" fill-rule="evenodd" fill-opacity="1" /><path clip-path="url(#clip470)" d="M1053.9 34.1734 Q1052.64 33.4443 1051.14 33.1202 Q1049.68 32.7556 1047.9 32.7556 Q1041.58 32.7556 1038.18 36.8875 Q1034.82 40.9789 1034.82 48.6757 L1034.82 72.576 L1027.32 72.576 L1027.32 27.2059 L1034.82 27.2059 L1034.82 34.2544 Q1037.17 30.1225 1040.93 28.1376 Q1044.7 26.1121 1050.09 26.1121 Q1050.86 26.1121 1051.79 26.2337 Q1052.72 26.3147 1053.86 26.5172 L1053.9 34.1734 Z" fill="#000000" fill-rule="evenodd" fill-opacity="1" /><path clip-path="url(#clip470)" d="M1061.71 27.2059 L1069.17 27.2059 L1069.17 72.576 L1061.71 72.576 L1061.71 27.2059 M1061.71 9.54393 L1069.17 9.54393 L1069.17 18.9825 L1061.71 18.9825 L1061.71 9.54393 Z" fill="#000000" fill-rule="evenodd" fill-opacity="1" /><path clip-path="url(#clip470)" d="M1122.48 27.2059 L1106.07 49.2833 L1123.33 72.576 L1114.54 72.576 L1101.33 54.752 L1088.13 72.576 L1079.34 72.576 L1096.96 48.8377 L1080.83 27.2059 L1089.62 27.2059 L1101.66 43.369 L1113.69 27.2059 L1122.48 27.2059 Z" fill="#000000" fill-rule="evenodd" fill-opacity="1" /><path clip-path="url(#clip470)" d="M1178.14 9.62495 Q1172.71 18.942 1170.08 28.0566 Q1167.44 37.1711 1167.44 46.5287 Q1167.44 55.8863 1170.08 65.0818 Q1172.75 74.2369 1178.14 83.5134 L1171.66 83.5134 Q1165.58 73.9938 1162.54 64.7983 Q1159.54 55.6027 1159.54 46.5287 Q1159.54 37.4952 1162.54 28.3401 Q1165.54 19.1851 1171.66 9.62495 L1178.14 9.62495 Z" fill="#000000" fill-rule="evenodd" fill-opacity="1" /><path clip-path="url(#clip470)" d="M1230.31 45.1919 L1230.31 72.576 L1222.86 72.576 L1222.86 45.4349 Q1222.86 38.994 1220.35 35.7938 Q1217.84 32.5936 1212.81 32.5936 Q1206.78 32.5936 1203.29 36.4419 Q1199.81 40.2903 1199.81 46.9338 L1199.81 72.576 L1192.31 72.576 L1192.31 27.2059 L1199.81 27.2059 L1199.81 34.2544 Q1202.48 30.163 1206.09 28.1376 Q1209.73 26.1121 1214.47 26.1121 Q1222.29 26.1121 1226.3 30.9732 Q1230.31 35.7938 1230.31 45.1919 Z" fill="#000000" fill-rule="evenodd" fill-opacity="1" /><path clip-path="url(#clip470)" d="M1262.76 32.4315 Q1256.76 32.4315 1253.28 37.1306 Q1249.8 41.7891 1249.8 49.9314 Q1249.8 58.0738 1253.24 62.7728 Q1256.72 67.4314 1262.76 67.4314 Q1268.72 67.4314 1272.2 62.7323 Q1275.68 58.0333 1275.68 49.9314 Q1275.68 41.8701 1272.2 37.1711 Q1268.72 32.4315 1262.76 32.4315 M1262.76 26.1121 Q1272.48 26.1121 1278.03 32.4315 Q1283.58 38.7509 1283.58 49.9314 Q1283.58 61.0714 1278.03 67.4314 Q1272.48 73.7508 1262.76 73.7508 Q1253 73.7508 1247.45 67.4314 Q1241.94 61.0714 1241.94 49.9314 Q1241.94 38.7509 1247.45 32.4315 Q1253 26.1121 1262.76 26.1121 Z" fill="#000000" fill-rule="evenodd" fill-opacity="1" /><path clip-path="url(#clip470)" d="M1322.23 34.1734 Q1320.97 33.4443 1319.47 33.1202 Q1318.01 32.7556 1316.23 32.7556 Q1309.91 32.7556 1306.51 36.8875 Q1303.15 40.9789 1303.15 48.6757 L1303.15 72.576 L1295.65 72.576 L1295.65 27.2059 L1303.15 27.2059 L1303.15 34.2544 Q1305.5 30.1225 1309.26 28.1376 Q1313.03 26.1121 1318.42 26.1121 Q1319.19 26.1121 1320.12 26.2337 Q1321.05 26.3147 1322.19 26.5172 L1322.23 34.1734 Z" fill="#000000" fill-rule="evenodd" fill-opacity="1" /><path clip-path="url(#clip470)" d="M1363.91 35.9153 Q1366.71 30.8922 1370.6 28.5022 Q1374.48 26.1121 1379.75 26.1121 Q1386.84 26.1121 1390.69 31.0947 Q1394.54 36.0368 1394.54 45.1919 L1394.54 72.576 L1387.04 72.576 L1387.04 45.4349 Q1387.04 38.913 1384.73 35.7533 Q1382.42 32.5936 1377.68 32.5936 Q1371.89 32.5936 1368.53 36.4419 Q1365.17 40.2903 1365.17 46.9338 L1365.17 72.576 L1357.67 72.576 L1357.67 45.4349 Q1357.67 38.8725 1355.36 35.7533 Q1353.05 32.5936 1348.23 32.5936 Q1342.52 32.5936 1339.16 36.4824 Q1335.8 40.3308 1335.8 46.9338 L1335.8 72.576 L1328.3 72.576 L1328.3 27.2059 L1335.8 27.2059 L1335.8 34.2544 Q1338.35 30.082 1341.91 28.0971 Q1345.48 26.1121 1350.38 26.1121 Q1355.32 26.1121 1358.77 28.6237 Q1362.25 31.1352 1363.91 35.9153 Z" fill="#000000" fill-rule="evenodd" fill-opacity="1" /><path clip-path="url(#clip470)" d="M1430.02 49.7694 Q1420.99 49.7694 1417.5 51.8354 Q1414.02 53.9013 1414.02 58.8839 Q1414.02 62.8538 1416.61 65.2034 Q1419.25 67.5124 1423.74 67.5124 Q1429.94 67.5124 1433.67 63.1374 Q1437.44 58.7219 1437.44 51.4303 L1437.44 49.7694 L1430.02 49.7694 M1444.89 46.6907 L1444.89 72.576 L1437.44 72.576 L1437.44 65.6895 Q1434.88 69.8214 1431.08 71.8063 Q1427.27 73.7508 1421.76 73.7508 Q1414.79 73.7508 1410.66 69.8619 Q1406.57 65.9325 1406.57 59.3701 Q1406.57 51.7138 1411.67 47.825 Q1416.82 43.9361 1426.98 43.9361 L1437.44 43.9361 L1437.44 43.2069 Q1437.44 38.0623 1434.03 35.2672 Q1430.67 32.4315 1424.55 32.4315 Q1420.66 32.4315 1416.98 33.3632 Q1413.29 34.295 1409.89 36.1584 L1409.89 29.2718 Q1413.98 27.692 1417.83 26.9223 Q1421.68 26.1121 1425.32 26.1121 Q1435.17 26.1121 1440.03 31.2163 Q1444.89 36.3204 1444.89 46.6907 Z" fill="#000000" fill-rule="evenodd" fill-opacity="1" /><path clip-path="url(#clip470)" d="M1460.24 9.54393 L1467.7 9.54393 L1467.7 72.576 L1460.24 72.576 L1460.24 9.54393 Z" fill="#000000" fill-rule="evenodd" fill-opacity="1" /><path clip-path="url(#clip470)" d="M1483.29 27.2059 L1490.75 27.2059 L1490.75 72.576 L1483.29 72.576 L1483.29 27.2059 M1483.29 9.54393 L1490.75 9.54393 L1490.75 18.9825 L1483.29 18.9825 L1483.29 9.54393 Z" fill="#000000" fill-rule="evenodd" fill-opacity="1" /><path clip-path="url(#clip470)" d="M1535.26 28.5427 L1535.26 35.5912 Q1532.1 33.9709 1528.7 33.1607 Q1525.3 32.3505 1521.65 32.3505 Q1516.1 32.3505 1513.31 34.0519 Q1510.55 35.7533 1510.55 39.156 Q1510.55 41.7486 1512.54 43.2475 Q1514.52 44.7058 1520.52 46.0426 L1523.07 46.6097 Q1531.01 48.3111 1534.33 51.4303 Q1537.7 54.509 1537.7 60.0587 Q1537.7 66.3781 1532.67 70.0644 Q1527.69 73.7508 1518.94 73.7508 Q1515.29 73.7508 1511.32 73.0216 Q1507.39 72.3329 1503.02 70.9151 L1503.02 63.2184 Q1507.15 65.3654 1511.16 66.4591 Q1515.17 67.5124 1519.1 67.5124 Q1524.37 67.5124 1527.2 65.73 Q1530.04 63.9071 1530.04 60.6258 Q1530.04 57.5877 1527.97 55.9673 Q1525.95 54.3469 1519.02 52.8481 L1516.43 52.2405 Q1509.5 50.7821 1506.42 47.7845 Q1503.34 44.7463 1503.34 39.4801 Q1503.34 33.0797 1507.88 29.5959 Q1512.42 26.1121 1520.76 26.1121 Q1524.89 26.1121 1528.54 26.7198 Q1532.19 27.3274 1535.26 28.5427 Z" fill="#000000" fill-rule="evenodd" fill-opacity="1" /><path clip-path="url(#clip470)" d="M1588.37 48.0275 L1588.37 51.6733 L1554.1 51.6733 Q1554.59 59.3701 1558.72 63.421 Q1562.89 67.4314 1570.3 67.4314 Q1574.6 67.4314 1578.61 66.3781 Q1582.66 65.3249 1586.63 63.2184 L1586.63 70.267 Q1582.62 71.9684 1578.41 72.8596 Q1574.19 73.7508 1569.86 73.7508 Q1559 73.7508 1552.64 67.4314 Q1546.32 61.1119 1546.32 50.3365 Q1546.32 39.1965 1552.32 32.6746 Q1558.35 26.1121 1568.56 26.1121 Q1577.72 26.1121 1583.02 32.0264 Q1588.37 37.9003 1588.37 48.0275 M1580.92 45.84 Q1580.84 39.7232 1577.47 36.0774 Q1574.15 32.4315 1568.64 32.4315 Q1562.41 32.4315 1558.64 35.9558 Q1554.91 39.4801 1554.34 45.8805 L1580.92 45.84 Z" fill="#000000" fill-rule="evenodd" fill-opacity="1" /><path clip-path="url(#clip470)" d="M1630.46 34.0924 L1630.46 9.54393 L1637.91 9.54393 L1637.91 72.576 L1630.46 72.576 L1630.46 65.7705 Q1628.11 69.8214 1624.51 71.8063 Q1620.94 73.7508 1615.92 73.7508 Q1607.69 73.7508 1602.51 67.1883 Q1597.36 60.6258 1597.36 49.9314 Q1597.36 39.2371 1602.51 32.6746 Q1607.69 26.1121 1615.92 26.1121 Q1620.94 26.1121 1624.51 28.0971 Q1628.11 30.0415 1630.46 34.0924 M1605.06 49.9314 Q1605.06 58.1548 1608.42 62.8538 Q1611.83 67.5124 1617.74 67.5124 Q1623.66 67.5124 1627.06 62.8538 Q1630.46 58.1548 1630.46 49.9314 Q1630.46 41.7081 1627.06 37.0496 Q1623.66 32.3505 1617.74 32.3505 Q1611.83 32.3505 1608.42 37.0496 Q1605.06 41.7081 1605.06 49.9314 Z" fill="#000000" fill-rule="evenodd" fill-opacity="1" /><path clip-path="url(#clip470)" d="M1708.56 28.5427 L1708.56 35.5912 Q1705.4 33.9709 1702 33.1607 Q1698.6 32.3505 1694.95 32.3505 Q1689.4 32.3505 1686.61 34.0519 Q1683.85 35.7533 1683.85 39.156 Q1683.85 41.7486 1685.84 43.2475 Q1687.82 44.7058 1693.82 46.0426 L1696.37 46.6097 Q1704.31 48.3111 1707.63 51.4303 Q1710.99 54.509 1710.99 60.0587 Q1710.99 66.3781 1705.97 70.0644 Q1700.99 73.7508 1692.24 73.7508 Q1688.59 73.7508 1684.62 73.0216 Q1680.69 72.3329 1676.32 70.9151 L1676.32 63.2184 Q1680.45 65.3654 1684.46 66.4591 Q1688.47 67.5124 1692.4 67.5124 Q1697.67 67.5124 1700.5 65.73 Q1703.34 63.9071 1703.34 60.6258 Q1703.34 57.5877 1701.27 55.9673 Q1699.25 54.3469 1692.32 52.8481 L1689.73 52.2405 Q1682.8 50.7821 1679.72 47.7845 Q1676.64 44.7463 1676.64 39.4801 Q1676.64 33.0797 1681.18 29.5959 Q1685.72 26.1121 1694.06 26.1121 Q1698.19 26.1121 1701.84 26.7198 Q1705.48 27.3274 1708.56 28.5427 Z" fill="#000000" fill-rule="evenodd" fill-opacity="1" /><path clip-path="url(#clip470)" d="M1755.51 28.9478 L1755.51 35.9153 Q1752.35 34.1734 1749.15 33.3227 Q1745.99 32.4315 1742.75 32.4315 Q1735.5 32.4315 1731.49 37.0496 Q1727.48 41.6271 1727.48 49.9314 Q1727.48 58.2358 1731.49 62.8538 Q1735.5 67.4314 1742.75 67.4314 Q1745.99 67.4314 1749.15 66.5807 Q1752.35 65.6895 1755.51 63.9476 L1755.51 70.8341 Q1752.39 72.2924 1749.03 73.0216 Q1745.71 73.7508 1741.94 73.7508 Q1731.69 73.7508 1725.66 67.3098 Q1719.62 60.8689 1719.62 49.9314 Q1719.62 38.832 1725.7 32.472 Q1731.81 26.1121 1742.43 26.1121 Q1745.87 26.1121 1749.15 26.8413 Q1752.43 27.5299 1755.51 28.9478 Z" fill="#000000" fill-rule="evenodd" fill-opacity="1" /><path clip-path="url(#clip470)" d="M1786.06 32.4315 Q1780.06 32.4315 1776.58 37.1306 Q1773.09 41.7891 1773.09 49.9314 Q1773.09 58.0738 1776.54 62.7728 Q1780.02 67.4314 1786.06 67.4314 Q1792.01 67.4314 1795.49 62.7323 Q1798.98 58.0333 1798.98 49.9314 Q1798.98 41.8701 1795.49 37.1711 Q1792.01 32.4315 1786.06 32.4315 M1786.06 26.1121 Q1795.78 26.1121 1801.33 32.4315 Q1806.88 38.7509 1806.88 49.9314 Q1806.88 61.0714 1801.33 67.4314 Q1795.78 73.7508 1786.06 73.7508 Q1776.29 73.7508 1770.74 67.4314 Q1765.23 61.0714 1765.23 49.9314 Q1765.23 38.7509 1770.74 32.4315 Q1776.29 26.1121 1786.06 26.1121 Z" fill="#000000" fill-rule="evenodd" fill-opacity="1" /><path clip-path="url(#clip470)" d="M1845.52 34.1734 Q1844.27 33.4443 1842.77 33.1202 Q1841.31 32.7556 1839.53 32.7556 Q1833.21 32.7556 1829.81 36.8875 Q1826.44 40.9789 1826.44 48.6757 L1826.44 72.576 L1818.95 72.576 L1818.95 27.2059 L1826.44 27.2059 L1826.44 34.2544 Q1828.79 30.1225 1832.56 28.1376 Q1836.33 26.1121 1841.72 26.1121 Q1842.49 26.1121 1843.42 26.2337 Q1844.35 26.3147 1845.48 26.5172 L1845.52 34.1734 Z" fill="#000000" fill-rule="evenodd" fill-opacity="1" /><path clip-path="url(#clip470)" d="M1890.33 48.0275 L1890.33 51.6733 L1856.06 51.6733 Q1856.54 59.3701 1860.67 63.421 Q1864.85 67.4314 1872.26 67.4314 Q1876.55 67.4314 1880.56 66.3781 Q1884.61 65.3249 1888.58 63.2184 L1888.58 70.267 Q1884.57 71.9684 1880.36 72.8596 Q1876.15 73.7508 1871.81 73.7508 Q1860.96 73.7508 1854.6 67.4314 Q1848.28 61.1119 1848.28 50.3365 Q1848.28 39.1965 1854.27 32.6746 Q1860.31 26.1121 1870.52 26.1121 Q1879.67 26.1121 1884.98 32.0264 Q1890.33 37.9003 1890.33 48.0275 M1882.87 45.84 Q1882.79 39.7232 1879.43 36.0774 Q1876.11 32.4315 1870.6 32.4315 Q1864.36 32.4315 1860.59 35.9558 Q1856.87 39.4801 1856.3 45.8805 L1882.87 45.84 Z" fill="#000000" fill-rule="evenodd" fill-opacity="1" /><path clip-path="url(#clip470)" d="M1931.48 28.5427 L1931.48 35.5912 Q1928.32 33.9709 1924.92 33.1607 Q1921.52 32.3505 1917.87 32.3505 Q1912.32 32.3505 1909.53 34.0519 Q1906.77 35.7533 1906.77 39.156 Q1906.77 41.7486 1908.76 43.2475 Q1910.74 44.7058 1916.74 46.0426 L1919.29 46.6097 Q1927.23 48.3111 1930.55 51.4303 Q1933.91 54.509 1933.91 60.0587 Q1933.91 66.3781 1928.89 70.0644 Q1923.91 73.7508 1915.16 73.7508 Q1911.51 73.7508 1907.54 73.0216 Q1903.61 72.3329 1899.24 70.9151 L1899.24 63.2184 Q1903.37 65.3654 1907.38 66.4591 Q1911.39 67.5124 1915.32 67.5124 Q1920.59 67.5124 1923.42 65.73 Q1926.26 63.9071 1926.26 60.6258 Q1926.26 57.5877 1924.19 55.9673 Q1922.17 54.3469 1915.24 52.8481 L1912.65 52.2405 Q1905.72 50.7821 1902.64 47.7845 Q1899.56 44.7463 1899.56 39.4801 Q1899.56 33.0797 1904.1 29.5959 Q1908.64 26.1121 1916.98 26.1121 Q1921.11 26.1121 1924.76 26.7198 Q1928.4 27.3274 1931.48 28.5427 Z" fill="#000000" fill-rule="evenodd" fill-opacity="1" /><path clip-path="url(#clip470)" d="M1944.61 9.62495 L1951.09 9.62495 Q1957.17 19.1851 1960.16 28.3401 Q1963.2 37.4952 1963.2 46.5287 Q1963.2 55.6027 1960.16 64.7983 Q1957.17 73.9938 1951.09 83.5134 L1944.61 83.5134 Q1950 74.2369 1952.63 65.0818 Q1955.3 55.8863 1955.3 46.5287 Q1955.3 37.1711 1952.63 28.0566 Q1950 18.942 1944.61 9.62495 Z" fill="#000000" fill-rule="evenodd" fill-opacity="1" /><g clip-path="url(#clip472)">
<image width="1832" height="1300" xlink:href="data:image/png;base64,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" transform="translate(281, 123)"/>
</g>
<defs>
  <clipPath id="clip473">
    <rect x="2160" y="123" width="73" height="1301"/>
  </clipPath>
</defs>
<g clip-path="url(#clip473)">
<image width="72" height="1300" xlink:href="data:image/png;base64,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" transform="translate(2161, 123)"/>
</g>
<path clip-path="url(#clip470)" d="M2280.7 1315.61 Q2277.09 1315.61 2275.26 1319.17 Q2273.45 1322.71 2273.45 1329.84 Q2273.45 1336.95 2275.26 1340.52 Q2277.09 1344.06 2280.7 1344.06 Q2284.33 1344.06 2286.14 1340.52 Q2287.97 1336.95 2287.97 1329.84 Q2287.97 1322.71 2286.14 1319.17 Q2284.33 1315.61 2280.7 1315.61 M2280.7 1311.9 Q2286.51 1311.9 2289.57 1316.51 Q2292.64 1321.09 2292.64 1329.84 Q2292.64 1338.57 2289.57 1343.18 Q2286.51 1347.76 2280.7 1347.76 Q2274.89 1347.76 2271.81 1343.18 Q2268.76 1338.57 2268.76 1329.84 Q2268.76 1321.09 2271.81 1316.51 Q2274.89 1311.9 2280.7 1311.9 Z" fill="#000000" fill-rule="evenodd" fill-opacity="1" /><path clip-path="url(#clip470)" d="M2300.86 1341.21 L2305.75 1341.21 L2305.75 1347.09 L2300.86 1347.09 L2300.86 1341.21 Z" fill="#000000" fill-rule="evenodd" fill-opacity="1" /><path clip-path="url(#clip470)" d="M2316.74 1343.15 L2324.38 1343.15 L2324.38 1316.79 L2316.07 1318.46 L2316.07 1314.2 L2324.33 1312.53 L2329.01 1312.53 L2329.01 1343.15 L2336.65 1343.15 L2336.65 1347.09 L2316.74 1347.09 L2316.74 1343.15 Z" fill="#000000" fill-rule="evenodd" fill-opacity="1" /><path clip-path="url(#clip470)" d="M2280.7 1163.98 Q2277.09 1163.98 2275.26 1167.54 Q2273.45 1171.08 2273.45 1178.21 Q2273.45 1185.32 2275.26 1188.88 Q2277.09 1192.42 2280.7 1192.42 Q2284.33 1192.42 2286.14 1188.88 Q2287.97 1185.32 2287.97 1178.21 Q2287.97 1171.08 2286.14 1167.54 Q2284.33 1163.98 2280.7 1163.98 M2280.7 1160.27 Q2286.51 1160.27 2289.57 1164.88 Q2292.64 1169.46 2292.64 1178.21 Q2292.64 1186.94 2289.57 1191.54 Q2286.51 1196.13 2280.7 1196.13 Q2274.89 1196.13 2271.81 1191.54 Q2268.76 1186.94 2268.76 1178.21 Q2268.76 1169.46 2271.81 1164.88 Q2274.89 1160.27 2280.7 1160.27 Z" fill="#000000" fill-rule="evenodd" fill-opacity="1" /><path clip-path="url(#clip470)" d="M2300.86 1189.58 L2305.75 1189.58 L2305.75 1195.46 L2300.86 1195.46 L2300.86 1189.58 Z" fill="#000000" fill-rule="evenodd" fill-opacity="1" /><path clip-path="url(#clip470)" d="M2319.96 1191.52 L2336.28 1191.52 L2336.28 1195.46 L2314.33 1195.46 L2314.33 1191.52 Q2317 1188.77 2321.58 1184.14 Q2326.19 1179.48 2327.37 1178.14 Q2329.61 1175.62 2330.49 1173.88 Q2331.39 1172.12 2331.39 1170.43 Q2331.39 1167.68 2329.45 1165.94 Q2327.53 1164.21 2324.43 1164.21 Q2322.23 1164.21 2319.77 1164.97 Q2317.34 1165.73 2314.57 1167.29 L2314.57 1162.56 Q2317.39 1161.43 2319.84 1160.85 Q2322.3 1160.27 2324.33 1160.27 Q2329.7 1160.27 2332.9 1162.96 Q2336.09 1165.64 2336.09 1170.13 Q2336.09 1172.26 2335.28 1174.18 Q2334.5 1176.08 2332.39 1178.67 Q2331.81 1179.35 2328.71 1182.56 Q2325.61 1185.76 2319.96 1191.52 Z" fill="#000000" fill-rule="evenodd" fill-opacity="1" /><path clip-path="url(#clip470)" d="M2280.7 1012.34 Q2277.09 1012.34 2275.26 1015.91 Q2273.45 1019.45 2273.45 1026.58 Q2273.45 1033.69 2275.26 1037.25 Q2277.09 1040.79 2280.7 1040.79 Q2284.33 1040.79 2286.14 1037.25 Q2287.97 1033.69 2287.97 1026.58 Q2287.97 1019.45 2286.14 1015.91 Q2284.33 1012.34 2280.7 1012.34 M2280.7 1008.64 Q2286.51 1008.64 2289.57 1013.25 Q2292.64 1017.83 2292.64 1026.58 Q2292.64 1035.31 2289.57 1039.91 Q2286.51 1044.5 2280.7 1044.5 Q2274.89 1044.5 2271.81 1039.91 Q2268.76 1035.31 2268.76 1026.58 Q2268.76 1017.83 2271.81 1013.25 Q2274.89 1008.64 2280.7 1008.64 Z" fill="#000000" fill-rule="evenodd" fill-opacity="1" /><path clip-path="url(#clip470)" d="M2300.86 1037.94 L2305.75 1037.94 L2305.75 1043.82 L2300.86 1043.82 L2300.86 1037.94 Z" fill="#000000" fill-rule="evenodd" fill-opacity="1" /><path clip-path="url(#clip470)" d="M2330.1 1025.19 Q2333.45 1025.91 2335.33 1028.18 Q2337.23 1030.44 2337.23 1033.78 Q2337.23 1038.89 2333.71 1041.69 Q2330.19 1044.5 2323.71 1044.5 Q2321.53 1044.5 2319.22 1044.06 Q2316.93 1043.64 2314.47 1042.78 L2314.47 1038.27 Q2316.42 1039.4 2318.73 1039.98 Q2321.05 1040.56 2323.57 1040.56 Q2327.97 1040.56 2330.26 1038.82 Q2332.58 1037.09 2332.58 1033.78 Q2332.58 1030.72 2330.42 1029.01 Q2328.29 1027.27 2324.47 1027.27 L2320.45 1027.27 L2320.45 1023.43 L2324.66 1023.43 Q2328.11 1023.43 2329.94 1022.07 Q2331.76 1020.68 2331.76 1018.08 Q2331.76 1015.42 2329.87 1014.01 Q2327.99 1012.57 2324.47 1012.57 Q2322.55 1012.57 2320.35 1012.99 Q2318.15 1013.41 2315.51 1014.29 L2315.51 1010.12 Q2318.18 1009.38 2320.49 1009.01 Q2322.83 1008.64 2324.89 1008.64 Q2330.21 1008.64 2333.32 1011.07 Q2336.42 1013.48 2336.42 1017.6 Q2336.42 1020.47 2334.77 1022.46 Q2333.13 1024.43 2330.1 1025.19 Z" fill="#000000" fill-rule="evenodd" fill-opacity="1" /><path clip-path="url(#clip470)" d="M2280.7 860.71 Q2277.09 860.71 2275.26 864.275 Q2273.45 867.817 2273.45 874.946 Q2273.45 882.053 2275.26 885.618 Q2277.09 889.159 2280.7 889.159 Q2284.33 889.159 2286.14 885.618 Q2287.97 882.053 2287.97 874.946 Q2287.97 867.817 2286.14 864.275 Q2284.33 860.71 2280.7 860.71 M2280.7 857.007 Q2286.51 857.007 2289.57 861.613 Q2292.64 866.196 2292.64 874.946 Q2292.64 883.673 2289.57 888.28 Q2286.51 892.863 2280.7 892.863 Q2274.89 892.863 2271.81 888.28 Q2268.76 883.673 2268.76 874.946 Q2268.76 866.196 2271.81 861.613 Q2274.89 857.007 2280.7 857.007 Z" fill="#000000" fill-rule="evenodd" fill-opacity="1" /><path clip-path="url(#clip470)" d="M2300.86 886.312 L2305.75 886.312 L2305.75 892.192 L2300.86 892.192 L2300.86 886.312 Z" fill="#000000" fill-rule="evenodd" fill-opacity="1" /><path clip-path="url(#clip470)" d="M2328.78 861.706 L2316.97 880.155 L2328.78 880.155 L2328.78 861.706 M2327.55 857.632 L2333.43 857.632 L2333.43 880.155 L2338.36 880.155 L2338.36 884.044 L2333.43 884.044 L2333.43 892.192 L2328.78 892.192 L2328.78 884.044 L2313.18 884.044 L2313.18 879.53 L2327.55 857.632 Z" fill="#000000" fill-rule="evenodd" fill-opacity="1" /><path clip-path="url(#clip470)" d="M2280.7 709.078 Q2277.09 709.078 2275.26 712.643 Q2273.45 716.184 2273.45 723.314 Q2273.45 730.42 2275.26 733.985 Q2277.09 737.527 2280.7 737.527 Q2284.33 737.527 2286.14 733.985 Q2287.97 730.42 2287.97 723.314 Q2287.97 716.184 2286.14 712.643 Q2284.33 709.078 2280.7 709.078 M2280.7 705.374 Q2286.51 705.374 2289.57 709.981 Q2292.64 714.564 2292.64 723.314 Q2292.64 732.041 2289.57 736.647 Q2286.51 741.23 2280.7 741.23 Q2274.89 741.23 2271.81 736.647 Q2268.76 732.041 2268.76 723.314 Q2268.76 714.564 2271.81 709.981 Q2274.89 705.374 2280.7 705.374 Z" fill="#000000" fill-rule="evenodd" fill-opacity="1" /><path clip-path="url(#clip470)" d="M2300.86 734.679 L2305.75 734.679 L2305.75 740.559 L2300.86 740.559 L2300.86 734.679 Z" fill="#000000" fill-rule="evenodd" fill-opacity="1" /><path clip-path="url(#clip470)" d="M2315.98 705.999 L2334.33 705.999 L2334.33 709.934 L2320.26 709.934 L2320.26 718.406 Q2321.28 718.059 2322.3 717.897 Q2323.32 717.712 2324.33 717.712 Q2330.12 717.712 2333.5 720.883 Q2336.88 724.055 2336.88 729.471 Q2336.88 735.05 2333.41 738.152 Q2329.94 741.23 2323.62 741.23 Q2321.44 741.23 2319.17 740.86 Q2316.93 740.49 2314.52 739.749 L2314.52 735.05 Q2316.6 736.184 2318.83 736.74 Q2321.05 737.295 2323.52 737.295 Q2327.53 737.295 2329.87 735.189 Q2332.2 733.082 2332.2 729.471 Q2332.2 725.86 2329.87 723.754 Q2327.53 721.647 2323.52 721.647 Q2321.65 721.647 2319.77 722.064 Q2317.92 722.48 2315.98 723.36 L2315.98 705.999 Z" fill="#000000" fill-rule="evenodd" fill-opacity="1" /><path clip-path="url(#clip470)" d="M2280.7 557.445 Q2277.09 557.445 2275.26 561.01 Q2273.45 564.552 2273.45 571.681 Q2273.45 578.788 2275.26 582.352 Q2277.09 585.894 2280.7 585.894 Q2284.33 585.894 2286.14 582.352 Q2287.97 578.788 2287.97 571.681 Q2287.97 564.552 2286.14 561.01 Q2284.33 557.445 2280.7 557.445 M2280.7 553.741 Q2286.51 553.741 2289.57 558.348 Q2292.64 562.931 2292.64 571.681 Q2292.64 580.408 2289.57 585.014 Q2286.51 589.598 2280.7 589.598 Q2274.89 589.598 2271.81 585.014 Q2268.76 580.408 2268.76 571.681 Q2268.76 562.931 2271.81 558.348 Q2274.89 553.741 2280.7 553.741 Z" fill="#000000" fill-rule="evenodd" fill-opacity="1" /><path clip-path="url(#clip470)" d="M2300.86 583.047 L2305.75 583.047 L2305.75 588.926 L2300.86 588.926 L2300.86 583.047 Z" fill="#000000" fill-rule="evenodd" fill-opacity="1" /><path clip-path="url(#clip470)" d="M2326.51 569.783 Q2323.36 569.783 2321.51 571.936 Q2319.68 574.089 2319.68 577.839 Q2319.68 581.565 2321.51 583.741 Q2323.36 585.894 2326.51 585.894 Q2329.66 585.894 2331.49 583.741 Q2333.34 581.565 2333.34 577.839 Q2333.34 574.089 2331.49 571.936 Q2329.66 569.783 2326.51 569.783 M2335.79 555.13 L2335.79 559.39 Q2334.03 558.556 2332.23 558.116 Q2330.45 557.677 2328.69 557.677 Q2324.06 557.677 2321.6 560.802 Q2319.17 563.927 2318.83 570.246 Q2320.19 568.232 2322.25 567.167 Q2324.31 566.079 2326.79 566.079 Q2332 566.079 2335.01 569.251 Q2338.04 572.399 2338.04 577.839 Q2338.04 583.163 2334.89 586.38 Q2331.74 589.598 2326.51 589.598 Q2320.51 589.598 2317.34 585.014 Q2314.17 580.408 2314.17 571.681 Q2314.17 563.487 2318.06 558.626 Q2321.95 553.741 2328.5 553.741 Q2330.26 553.741 2332.04 554.089 Q2333.85 554.436 2335.79 555.13 Z" fill="#000000" fill-rule="evenodd" fill-opacity="1" /><path clip-path="url(#clip470)" d="M2280.7 405.813 Q2277.09 405.813 2275.26 409.377 Q2273.45 412.919 2273.45 420.049 Q2273.45 427.155 2275.26 430.72 Q2277.09 434.262 2280.7 434.262 Q2284.33 434.262 2286.14 430.72 Q2287.97 427.155 2287.97 420.049 Q2287.97 412.919 2286.14 409.377 Q2284.33 405.813 2280.7 405.813 M2280.7 402.109 Q2286.51 402.109 2289.57 406.715 Q2292.64 411.299 2292.64 420.049 Q2292.64 428.775 2289.57 433.382 Q2286.51 437.965 2280.7 437.965 Q2274.89 437.965 2271.81 433.382 Q2268.76 428.775 2268.76 420.049 Q2268.76 411.299 2271.81 406.715 Q2274.89 402.109 2280.7 402.109 Z" fill="#000000" fill-rule="evenodd" fill-opacity="1" /><path clip-path="url(#clip470)" d="M2300.86 431.414 L2305.75 431.414 L2305.75 437.294 L2300.86 437.294 L2300.86 431.414 Z" fill="#000000" fill-rule="evenodd" fill-opacity="1" /><path clip-path="url(#clip470)" d="M2314.75 402.734 L2336.97 402.734 L2336.97 404.725 L2324.43 437.294 L2319.54 437.294 L2331.35 406.669 L2314.75 406.669 L2314.75 402.734 Z" fill="#000000" fill-rule="evenodd" fill-opacity="1" /><path clip-path="url(#clip470)" d="M2280.7 254.18 Q2277.09 254.18 2275.26 257.745 Q2273.45 261.286 2273.45 268.416 Q2273.45 275.523 2275.26 279.087 Q2277.09 282.629 2280.7 282.629 Q2284.33 282.629 2286.14 279.087 Q2287.97 275.523 2287.97 268.416 Q2287.97 261.286 2286.14 257.745 Q2284.33 254.18 2280.7 254.18 M2280.7 250.476 Q2286.51 250.476 2289.57 255.083 Q2292.64 259.666 2292.64 268.416 Q2292.64 277.143 2289.57 281.749 Q2286.51 286.333 2280.7 286.333 Q2274.89 286.333 2271.81 281.749 Q2268.76 277.143 2268.76 268.416 Q2268.76 259.666 2271.81 255.083 Q2274.89 250.476 2280.7 250.476 Z" fill="#000000" fill-rule="evenodd" fill-opacity="1" /><path clip-path="url(#clip470)" d="M2300.86 279.782 L2305.75 279.782 L2305.75 285.661 L2300.86 285.661 L2300.86 279.782 Z" fill="#000000" fill-rule="evenodd" fill-opacity="1" /><path clip-path="url(#clip470)" d="M2325.93 269.249 Q2322.6 269.249 2320.68 271.032 Q2318.78 272.814 2318.78 275.939 Q2318.78 279.064 2320.68 280.847 Q2322.6 282.629 2325.93 282.629 Q2329.26 282.629 2331.19 280.847 Q2333.11 279.041 2333.11 275.939 Q2333.11 272.814 2331.19 271.032 Q2329.29 269.249 2325.93 269.249 M2321.26 267.259 Q2318.25 266.518 2316.56 264.458 Q2314.89 262.398 2314.89 259.435 Q2314.89 255.291 2317.83 252.884 Q2320.79 250.476 2325.93 250.476 Q2331.09 250.476 2334.03 252.884 Q2336.97 255.291 2336.97 259.435 Q2336.97 262.398 2335.28 264.458 Q2333.62 266.518 2330.63 267.259 Q2334.01 268.046 2335.89 270.337 Q2337.78 272.629 2337.78 275.939 Q2337.78 280.962 2334.7 283.647 Q2331.65 286.333 2325.93 286.333 Q2320.21 286.333 2317.14 283.647 Q2314.08 280.962 2314.08 275.939 Q2314.08 272.629 2315.98 270.337 Q2317.88 268.046 2321.26 267.259 M2319.54 259.874 Q2319.54 262.56 2321.21 264.064 Q2322.9 265.569 2325.93 265.569 Q2328.94 265.569 2330.63 264.064 Q2332.34 262.56 2332.34 259.874 Q2332.34 257.189 2330.63 255.685 Q2328.94 254.18 2325.93 254.18 Q2322.9 254.18 2321.21 255.685 Q2319.54 257.189 2319.54 259.874 Z" fill="#000000" fill-rule="evenodd" fill-opacity="1" /><polyline clip-path="url(#clip470)" style="stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none" points="
  2232.76,1423.18 2232.76,1333.44 2256.76,1333.44 2232.76,1333.44 2232.76,1181.81 2256.76,1181.81 2232.76,1181.81 2232.76,1030.17 2256.76,1030.17 2232.76,1030.17 
  2232.76,878.54 2256.76,878.54 2232.76,878.54 2232.76,726.908 2256.76,726.908 2232.76,726.908 2232.76,575.275 2256.76,575.275 2232.76,575.275 2232.76,423.643 
  2256.76,423.643 2232.76,423.643 2232.76,272.01 2256.76,272.01 2232.76,272.01 2232.76,123.472 
  "/>
</svg>
<h3 id="Comparision-with-DecisionTree.jl"><a class="docs-heading-anchor" href="#Comparision-with-DecisionTree.jl">Comparision with DecisionTree.jl</a><a id="Comparision-with-DecisionTree.jl-1"></a><a class="docs-heading-anchor-permalink" href="#Comparision-with-DecisionTree.jl" title="Permalink"></a></h3><p>We now compare BetaML [<code>RandomForestEstimator</code>] with the random forest estimator of the package <a href="https://github.com/JuliaAI/DecisionTree.jl"><code>DecisionTrees.jl</code></a>` random forests are similar in usage: we first &quot;build&quot; (train) the forest and we then make predictions out of the trained model.</p><pre><code class="language-julia hljs"># We train the model...
model = DecisionTree.build_forest(ytrain, xtrain,rng=seed)
# ..and we generate predictions and measure their error
(ŷtrain,ŷtest) = DecisionTree.apply_forest.([model],[xtrain,xtest]);
(trainAccuracy,testAccuracy) = accuracy.([ytrain,ytest],[ŷtrain,ŷtest])
push!(results,[&quot;RF (DecisionTrees.jl)&quot;,trainAccuracy,testAccuracy]);</code></pre><p>While the accuracy on the training set is exactly the same as for <code>BetaML</code> random forets, <code>DecisionTree.jl</code> random forests are slighly less accurate in the testing sample. Where however <code>DecisionTrees.jl</code> excell is in the efficiency: they are extremelly fast and memory thrifty, even if we should consider also the resources needed to impute the missing values, as they don&#39;t work with missing data.</p><p>Also, one of the reasons DecisionTrees are such efficient is that internally the data is sorted to avoid repeated comparision, but in this way they work only with features that are sortable, while BetaML random forests accept virtually any kind of input without the needs to process it.</p><h3 id="Neural-network"><a class="docs-heading-anchor" href="#Neural-network">Neural network</a><a id="Neural-network-1"></a><a class="docs-heading-anchor-permalink" href="#Neural-network" title="Permalink"></a></h3><p>Neural networks (NN) can be very powerfull, but have two &quot;inconvenients&quot; compared with random forests: first, are a bit &quot;picky&quot;. We need to do a bit of work to provide data in specific format. Note that this is <em>not</em> feature engineering. One of the advantages on neural network is that for the most this is not needed for neural networks. However we still need to &quot;clean&quot; the data. One issue is that NN don&#39;t like missing data. So we need to provide them with the feature matrix &quot;clean&quot; of missing data. Secondly, they work only with numerical data. So we need to use the one-hot encoding we saw earlier. Further, they work best if the features are scaled such that each feature has mean zero and standard deviation 1. This is why we scaled the data back at the beginning of this tutorial.</p><p>We firt measure the dimensions of our data in input (i.e. the column of the feature matrix) and the dimensions of our output, i.e. the number of categories or columns in out one-hot encoded y.</p><pre><code class="language-julia hljs">D               = size(xtrain,2)
classes         = unique(y)
nCl             = length(classes)</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">3</code></pre><p>The second &quot;inconvenient&quot; of NN is that, while not requiring feature engineering, they still need a bit of practice on the way the structure of the network is built . It&#39;s not as simple as <code>fit!(Model(),x,y)</code> (altougth BetaML provides a &quot;default&quot; neural network structure that can be used, it isn&#39;t often adapted to the specific task). We need instead to specify how we want our layers, <em>chain</em> the layers together and then decide a <em>loss</em> overall function. Only when we done these steps, we have the model ready for training. Here we define 2 <a href="../../Nn.html#BetaML.Nn.DenseLayer"><code>DenseLayer</code></a> where, for each of them, we specify the number of neurons in input (the first layer being equal to the dimensions of the data), the output layer (for a classification task, the last layer output size beying equal to the number of classes) and an <em>activation function</em> for each layer (default the <code>identity</code> function).</p><pre><code class="language-julia hljs">ls   = 50 # number of neurons in the inned layer
l1   = DenseLayer(D,ls,f=relu,rng=copy(AFIXEDRNG))
l2   = DenseLayer(ls,nCl,f=relu,rng=copy(AFIXEDRNG))</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">DenseLayer([-0.2146463925907584 -0.3077087587320811 … -0.28256208289474877 0.21510681158042189; -0.08916953797649538 -0.041727530915651345 … -0.30444064706465346 -0.22349634154766507; 0.11376391271810127 -0.011244515923068743 … 0.12916068649773038 -0.2518581440082599], [0.2918467648814228, -0.004167534280141383, 0.29060333096888613], BetaML.Utils.relu, nothing)</code></pre><p>For a classification task, the last layer is a <a href="../../Nn.html#BetaML.Nn.VectorFunctionLayer"><code>VectorFunctionLayer</code></a> that has no learnable parameters but whose activation function is applied to the ensemble of the neurons, rather than individually on each neuron. In particular, for classification we pass the <a href="../../Utils.html#BetaML.Utils.softmax-Tuple{Any}"><code>softmax</code></a> function whose output has the same size as the input (i.e. the number of classes to predict), but we can use the <code>VectorFunctionLayer</code> with any function, including the <a href="../../Utils.html#BetaML.Utils.pool1d"><code>pool1d</code></a> function to create a &quot;pooling&quot; layer (using maximum, mean or whatever other sub-function we pass to <code>pool1d</code>)</p><pre><code class="language-julia hljs">l3   = VectorFunctionLayer(nCl,f=softmax) ## Add a (parameterless) layer whose activation function (softMax in this case) is defined to all its nodes at once</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">VectorFunctionLayer{0}(fill(NaN), 3, 3, BetaML.Utils.softmax, nothing, nothing)</code></pre><p>Finally we <em>chain</em> the layers and assign a loss function and the number of epochs we want to train the model to the constructor of <a href="../../Nn.html#BetaML.Nn.NeuralNetworkEstimator"><code>NeuralNetworkEstimator</code></a>:</p><pre><code class="language-julia hljs">nn = NeuralNetworkEstimator(layers=[l1,l2,l3],loss=crossentropy,rng=copy(AFIXEDRNG),epochs=500)</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">NeuralNetworkEstimator - A Feed-forward neural network (unfitted)</code></pre><p>Aside the layer structure and size and the number of epochs, other hyper-parameters you may want to try are the <code>batch_size</code> and the optimisation algoritm to employ (<code>opt_alg</code>).</p><p>Now we can train our network:</p><pre><code class="language-julia hljs">ŷtrain = fit!(nn, xtrain, ytrain_oh)</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">325×3 Matrix{Float64}:
 0.0304015  0.0289379    0.940661
 0.991867   0.00738674   0.000746272
 0.0399541  0.843972     0.116074
 0.513289   0.243355     0.243355
 0.0285072  0.883499     0.0879939
 0.998413   0.00152558   6.13402e-5
 0.231549   0.0770623    0.691389
 0.999994   3.00452e-6   3.00452e-6
 0.995447   0.0043127    0.000240414
 0.593925   0.340815     0.0652599
 ⋮                       
 0.999969   2.0363e-5    1.02212e-5
 0.229337   0.229337     0.541325
 0.999631   0.000184645  0.000184645
 0.987164   0.0046535    0.00818228
 0.999938   3.72306e-5   2.50369e-5
 0.384705   0.565345     0.0499499
 0.999995   4.86224e-6   5.58623e-7
 0.0829834  0.0829834    0.834033
 0.0174231  0.00939345   0.973183</code></pre><p>Predictions are in form of a <em>n</em>records_ by <em>n</em>classes_ matrix of the probabilities of each record being in that class. To retrieve the classes with the highest probabilities we can use again the <code>mode</code> function:</p><pre><code class="language-julia hljs">ŷtrain_top = mode(ŷtrain)</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">325-element Vector{Int64}:
 3
 1
 2
 1
 2
 1
 3
 1
 1
 1
 ⋮
 1
 3
 1
 1
 1
 2
 1
 3
 3</code></pre><p>Once trained, we can predict the test labels. As the trained was based on the scaled feature matrix, so must be for the predictions</p><pre><code class="language-julia hljs">ŷtest  = predict(nn,xtest)</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">81×3 Matrix{Float64}:
 0.0124853    0.985527     0.0019875
 0.000650148  0.999333     1.65958e-5
 0.122348     0.0481034    0.829549
 0.999968     2.0712e-5    1.11656e-5
 0.13173      0.0405937    0.827676
 0.999922     6.36843e-5   1.38534e-5
 0.257813     0.20418      0.538007
 0.333073     0.287826     0.379102
 0.784735     0.0997047    0.11556
 0.999769     0.000227454  3.13415e-6
 ⋮                         
 0.842043     0.0789785    0.0789785
 0.999997     2.42049e-6   7.66595e-7
 0.0875678    0.0885742    0.823858
 0.999921     7.81347e-5   8.9221e-7
 0.999873     6.35047e-5   6.35047e-5
 0.134362     0.0428267    0.822811
 0.0426093    0.0426093    0.914781
 0.000447849  0.999104     0.000447849
 0.999379     0.000601083  1.94822e-5</code></pre><p>And finally we can measure the accuracies and store the accuracies in the <code>result</code> dataframe:</p><pre><code class="language-julia hljs">trainAccuracy, testAccuracy   = accuracy.([ytrain,ytest],[ŷtrain,ŷtest],rng=copy(AFIXEDRNG))
push!(results,[&quot;NN&quot;,trainAccuracy,testAccuracy]);</code></pre><pre><code class="language-julia hljs">cfm = ConfusionMatrix(categories_names=Dict(1=&gt;&quot;US&quot;,2=&gt;&quot;EU&quot;,3=&gt;&quot;Japan&quot;),rng=copy(AFIXEDRNG))
fit!(cfm,ytest,ŷtest)
print(cfm)
res = info(cfm)
heatmap(string.(res[&quot;categories&quot;]),string.(res[&quot;categories&quot;]),res[&quot;normalised_scores&quot;],seriescolor=cgrad([:white,:blue]),xlabel=&quot;Predicted&quot;,ylabel=&quot;Actual&quot;, title=&quot;Confusion Matrix (normalised scores)&quot;)</code></pre><?xml version="1.0" encoding="utf-8"?>
<svg xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" width="600" height="400" viewBox="0 0 2400 1600">
<defs>
  <clipPath id="clip530">
    <rect x="0" y="0" width="2400" height="1600"/>
  </clipPath>
</defs>
<path clip-path="url(#clip530)" d="
M0 1600 L2400 1600 L2400 0 L0 0  Z
  " fill="#ffffff" fill-rule="evenodd" fill-opacity="1"/>
<defs>
  <clipPath id="clip531">
    <rect x="480" y="0" width="1681" height="1600"/>
  </clipPath>
</defs>
<path clip-path="url(#clip530)" d="
M280.908 1423.18 L2112.76 1423.18 L2112.76 123.472 L280.908 123.472  Z
  " fill="#ffffff" fill-rule="evenodd" fill-opacity="1"/>
<defs>
  <clipPath id="clip532">
    <rect x="280" y="123" width="1833" height="1301"/>
  </clipPath>
</defs>
<polyline clip-path="url(#clip532)" style="stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none" points="
  586.216,1423.18 586.216,123.472 
  "/>
<polyline clip-path="url(#clip532)" style="stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none" points="
  1196.83,1423.18 1196.83,123.472 
  "/>
<polyline clip-path="url(#clip532)" style="stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none" points="
  1807.45,1423.18 1807.45,123.472 
  "/>
<polyline clip-path="url(#clip530)" style="stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none" points="
  280.908,1423.18 2112.76,1423.18 
  "/>
<polyline clip-path="url(#clip530)" style="stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none" points="
  586.216,1423.18 586.216,1404.28 
  "/>
<polyline clip-path="url(#clip530)" style="stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none" points="
  1196.83,1423.18 1196.83,1404.28 
  "/>
<polyline clip-path="url(#clip530)" style="stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none" points="
  1807.45,1423.18 1807.45,1404.28 
  "/>
<path clip-path="url(#clip530)" d="M558.276 1451.02 L580.128 1451.02 L580.128 1454.96 L562.952 1454.96 L562.952 1465.19 L579.41 1465.19 L579.41 1469.12 L562.952 1469.12 L562.952 1481.64 L580.544 1481.64 L580.544 1485.58 L558.276 1485.58 L558.276 1451.02 Z" fill="#000000" fill-rule="evenodd" fill-opacity="1" /><path clip-path="url(#clip530)" d="M587.697 1451.02 L592.396 1451.02 L592.396 1472.02 Q592.396 1477.57 594.41 1480.02 Q596.424 1482.45 600.938 1482.45 Q605.429 1482.45 607.442 1480.02 Q609.456 1477.57 609.456 1472.02 L609.456 1451.02 L614.155 1451.02 L614.155 1472.59 Q614.155 1479.35 610.799 1482.8 Q607.466 1486.25 600.938 1486.25 Q594.387 1486.25 591.03 1482.8 Q587.697 1479.35 587.697 1472.59 L587.697 1451.02 Z" fill="#000000" fill-rule="evenodd" fill-opacity="1" /><path clip-path="url(#clip530)" d="M1167.82 1451.02 L1172.51 1451.02 L1172.51 1472.02 Q1172.51 1477.57 1174.53 1480.02 Q1176.54 1482.45 1181.06 1482.45 Q1185.55 1482.45 1187.56 1480.02 Q1189.57 1477.57 1189.57 1472.02 L1189.57 1451.02 L1194.27 1451.02 L1194.27 1472.59 Q1194.27 1479.35 1190.92 1482.8 Q1187.58 1486.25 1181.06 1486.25 Q1174.51 1486.25 1171.15 1482.8 Q1167.82 1479.35 1167.82 1472.59 L1167.82 1451.02 Z" fill="#000000" fill-rule="evenodd" fill-opacity="1" /><path clip-path="url(#clip530)" d="M1223.76 1452.15 L1223.76 1456.71 Q1221.1 1455.44 1218.74 1454.82 Q1216.38 1454.19 1214.18 1454.19 Q1210.36 1454.19 1208.28 1455.67 Q1206.22 1457.15 1206.22 1459.89 Q1206.22 1462.18 1207.58 1463.36 Q1208.97 1464.52 1212.82 1465.23 L1215.64 1465.81 Q1220.87 1466.81 1223.35 1469.33 Q1225.85 1471.83 1225.85 1476.04 Q1225.85 1481.07 1222.47 1483.66 Q1219.11 1486.25 1212.61 1486.25 Q1210.15 1486.25 1207.38 1485.7 Q1204.62 1485.14 1201.66 1484.05 L1201.66 1479.24 Q1204.51 1480.83 1207.24 1481.64 Q1209.97 1482.45 1212.61 1482.45 Q1216.61 1482.45 1218.79 1480.88 Q1220.96 1479.31 1220.96 1476.39 Q1220.96 1473.84 1219.39 1472.41 Q1217.84 1470.97 1214.27 1470.26 L1211.43 1469.7 Q1206.2 1468.66 1203.86 1466.44 Q1201.52 1464.21 1201.52 1460.26 Q1201.52 1455.67 1204.74 1453.03 Q1207.98 1450.39 1213.65 1450.39 Q1216.08 1450.39 1218.6 1450.83 Q1221.13 1451.27 1223.76 1452.15 Z" fill="#000000" fill-rule="evenodd" fill-opacity="1" /><path clip-path="url(#clip530)" d="M1749.23 1451.02 L1753.91 1451.02 L1753.91 1483.17 Q1753.91 1489.42 1751.52 1492.25 Q1749.16 1495.07 1743.91 1495.07 L1742.12 1495.07 L1742.12 1491.14 L1743.58 1491.14 Q1746.68 1491.14 1747.96 1489.4 Q1749.23 1487.66 1749.23 1483.17 L1749.23 1451.02 Z" fill="#000000" fill-rule="evenodd" fill-opacity="1" /><path clip-path="url(#clip530)" d="M1774.81 1472.55 Q1769.65 1472.55 1767.66 1473.73 Q1765.67 1474.91 1765.67 1477.76 Q1765.67 1480.02 1767.15 1481.37 Q1768.65 1482.69 1771.22 1482.69 Q1774.76 1482.69 1776.89 1480.19 Q1779.05 1477.66 1779.05 1473.5 L1779.05 1472.55 L1774.81 1472.55 M1783.3 1470.79 L1783.3 1485.58 L1779.05 1485.58 L1779.05 1481.64 Q1777.59 1484.01 1775.41 1485.14 Q1773.24 1486.25 1770.09 1486.25 Q1766.11 1486.25 1763.74 1484.03 Q1761.41 1481.78 1761.41 1478.03 Q1761.41 1473.66 1764.32 1471.44 Q1767.26 1469.21 1773.07 1469.21 L1779.05 1469.21 L1779.05 1468.8 Q1779.05 1465.86 1777.1 1464.26 Q1775.18 1462.64 1771.68 1462.64 Q1769.46 1462.64 1767.36 1463.17 Q1765.25 1463.7 1763.3 1464.77 L1763.3 1460.83 Q1765.64 1459.93 1767.84 1459.49 Q1770.04 1459.03 1772.12 1459.03 Q1777.75 1459.03 1780.53 1461.95 Q1783.3 1464.86 1783.3 1470.79 Z" fill="#000000" fill-rule="evenodd" fill-opacity="1" /><path clip-path="url(#clip530)" d="M1796.2 1481.69 L1796.2 1495.44 L1791.92 1495.44 L1791.92 1459.65 L1796.2 1459.65 L1796.2 1463.59 Q1797.54 1461.27 1799.58 1460.16 Q1801.64 1459.03 1804.48 1459.03 Q1809.21 1459.03 1812.15 1462.78 Q1815.11 1466.53 1815.11 1472.64 Q1815.11 1478.75 1812.15 1482.5 Q1809.21 1486.25 1804.48 1486.25 Q1801.64 1486.25 1799.58 1485.14 Q1797.54 1484.01 1796.2 1481.69 M1810.69 1472.64 Q1810.69 1467.94 1808.74 1465.28 Q1806.82 1462.59 1803.44 1462.59 Q1800.06 1462.59 1798.12 1465.28 Q1796.2 1467.94 1796.2 1472.64 Q1796.2 1477.34 1798.12 1480.02 Q1800.06 1482.69 1803.44 1482.69 Q1806.82 1482.69 1808.74 1480.02 Q1810.69 1477.34 1810.69 1472.64 Z" fill="#000000" fill-rule="evenodd" fill-opacity="1" /><path clip-path="url(#clip530)" d="M1833.95 1472.55 Q1828.79 1472.55 1826.8 1473.73 Q1824.81 1474.91 1824.81 1477.76 Q1824.81 1480.02 1826.29 1481.37 Q1827.79 1482.69 1830.36 1482.69 Q1833.91 1482.69 1836.04 1480.19 Q1838.19 1477.66 1838.19 1473.5 L1838.19 1472.55 L1833.95 1472.55 M1842.45 1470.79 L1842.45 1485.58 L1838.19 1485.58 L1838.19 1481.64 Q1836.73 1484.01 1834.55 1485.14 Q1832.38 1486.25 1829.23 1486.25 Q1825.25 1486.25 1822.89 1484.03 Q1820.55 1481.78 1820.55 1478.03 Q1820.55 1473.66 1823.47 1471.44 Q1826.41 1469.21 1832.22 1469.21 L1838.19 1469.21 L1838.19 1468.8 Q1838.19 1465.86 1836.24 1464.26 Q1834.32 1462.64 1830.83 1462.64 Q1828.61 1462.64 1826.5 1463.17 Q1824.39 1463.7 1822.45 1464.77 L1822.45 1460.83 Q1824.79 1459.93 1826.98 1459.49 Q1829.18 1459.03 1831.27 1459.03 Q1836.89 1459.03 1839.67 1461.95 Q1842.45 1464.86 1842.45 1470.79 Z" fill="#000000" fill-rule="evenodd" fill-opacity="1" /><path clip-path="url(#clip530)" d="M1872.77 1469.93 L1872.77 1485.58 L1868.51 1485.58 L1868.51 1470.07 Q1868.51 1466.39 1867.08 1464.56 Q1865.64 1462.73 1862.77 1462.73 Q1859.32 1462.73 1857.33 1464.93 Q1855.34 1467.13 1855.34 1470.93 L1855.34 1485.58 L1851.06 1485.58 L1851.06 1459.65 L1855.34 1459.65 L1855.34 1463.68 Q1856.87 1461.34 1858.93 1460.19 Q1861.01 1459.03 1863.72 1459.03 Q1868.19 1459.03 1870.48 1461.81 Q1872.77 1464.56 1872.77 1469.93 Z" fill="#000000" fill-rule="evenodd" fill-opacity="1" /><path clip-path="url(#clip530)" d="M1056.42 1525.81 L1056.42 1543.66 L1064.5 1543.66 Q1068.99 1543.66 1071.44 1541.34 Q1073.89 1539.02 1073.89 1534.72 Q1073.89 1530.45 1071.44 1528.13 Q1068.99 1525.81 1064.5 1525.81 L1056.42 1525.81 M1049.99 1520.52 L1064.5 1520.52 Q1072.49 1520.52 1076.57 1524.15 Q1080.67 1527.75 1080.67 1534.72 Q1080.67 1541.75 1076.57 1545.35 Q1072.49 1548.95 1064.5 1548.95 L1056.42 1548.95 L1056.42 1568.04 L1049.99 1568.04 L1049.99 1520.52 Z" fill="#000000" fill-rule="evenodd" fill-opacity="1" /><path clip-path="url(#clip530)" d="M1108.56 1537.87 Q1107.57 1537.3 1106.39 1537.04 Q1105.25 1536.76 1103.84 1536.76 Q1098.88 1536.76 1096.21 1540 Q1093.56 1543.22 1093.56 1549.27 L1093.56 1568.04 L1087.68 1568.04 L1087.68 1532.4 L1093.56 1532.4 L1093.56 1537.93 Q1095.41 1534.69 1098.37 1533.13 Q1101.33 1531.54 1105.56 1531.54 Q1106.17 1531.54 1106.9 1531.63 Q1107.63 1531.7 1108.52 1531.85 L1108.56 1537.87 Z" fill="#000000" fill-rule="evenodd" fill-opacity="1" /><path clip-path="url(#clip530)" d="M1143.76 1548.76 L1143.76 1551.62 L1116.83 1551.62 Q1117.21 1557.67 1120.46 1560.85 Q1123.74 1564 1129.56 1564 Q1132.94 1564 1136.09 1563.17 Q1139.27 1562.35 1142.39 1560.69 L1142.39 1566.23 Q1139.24 1567.57 1135.93 1568.27 Q1132.62 1568.97 1129.21 1568.97 Q1120.68 1568.97 1115.68 1564 Q1110.72 1559.04 1110.72 1550.57 Q1110.72 1541.82 1115.43 1536.69 Q1120.17 1531.54 1128.19 1531.54 Q1135.39 1531.54 1139.56 1536.18 Q1143.76 1540.8 1143.76 1548.76 M1137.9 1547.04 Q1137.84 1542.23 1135.2 1539.37 Q1132.59 1536.5 1128.26 1536.5 Q1123.36 1536.5 1120.4 1539.27 Q1117.47 1542.04 1117.02 1547.07 L1137.9 1547.04 Z" fill="#000000" fill-rule="evenodd" fill-opacity="1" /><path clip-path="url(#clip530)" d="M1176.83 1537.81 L1176.83 1518.52 L1182.68 1518.52 L1182.68 1568.04 L1176.83 1568.04 L1176.83 1562.7 Q1174.98 1565.88 1172.15 1567.44 Q1169.35 1568.97 1165.4 1568.97 Q1158.94 1568.97 1154.87 1563.81 Q1150.82 1558.65 1150.82 1550.25 Q1150.82 1541.85 1154.87 1536.69 Q1158.94 1531.54 1165.4 1531.54 Q1169.35 1531.54 1172.15 1533.1 Q1174.98 1534.62 1176.83 1537.81 M1156.87 1550.25 Q1156.87 1556.71 1159.51 1560.4 Q1162.19 1564.07 1166.83 1564.07 Q1171.48 1564.07 1174.15 1560.4 Q1176.83 1556.71 1176.83 1550.25 Q1176.83 1543.79 1174.15 1540.13 Q1171.48 1536.44 1166.83 1536.44 Q1162.19 1536.44 1159.51 1540.13 Q1156.87 1543.79 1156.87 1550.25 Z" fill="#000000" fill-rule="evenodd" fill-opacity="1" /><path clip-path="url(#clip530)" d="M1194.75 1532.4 L1200.6 1532.4 L1200.6 1568.04 L1194.75 1568.04 L1194.75 1532.4 M1194.75 1518.52 L1200.6 1518.52 L1200.6 1525.93 L1194.75 1525.93 L1194.75 1518.52 Z" fill="#000000" fill-rule="evenodd" fill-opacity="1" /><path clip-path="url(#clip530)" d="M1238.51 1533.76 L1238.51 1539.24 Q1236.03 1537.87 1233.51 1537.2 Q1231.03 1536.5 1228.49 1536.5 Q1222.79 1536.5 1219.64 1540.13 Q1216.49 1543.73 1216.49 1550.25 Q1216.49 1556.78 1219.64 1560.4 Q1222.79 1564 1228.49 1564 Q1231.03 1564 1233.51 1563.33 Q1236.03 1562.63 1238.51 1561.26 L1238.51 1566.68 Q1236.06 1567.82 1233.42 1568.39 Q1230.81 1568.97 1227.85 1568.97 Q1219.8 1568.97 1215.05 1563.91 Q1210.31 1558.85 1210.31 1550.25 Q1210.31 1541.53 1215.09 1536.53 Q1219.89 1531.54 1228.23 1531.54 Q1230.94 1531.54 1233.51 1532.11 Q1236.09 1532.65 1238.51 1533.76 Z" fill="#000000" fill-rule="evenodd" fill-opacity="1" /><path clip-path="url(#clip530)" d="M1254.49 1522.27 L1254.49 1532.4 L1266.55 1532.4 L1266.55 1536.95 L1254.49 1536.95 L1254.49 1556.3 Q1254.49 1560.66 1255.67 1561.9 Q1256.88 1563.14 1260.54 1563.14 L1266.55 1563.14 L1266.55 1568.04 L1260.54 1568.04 Q1253.76 1568.04 1251.18 1565.53 Q1248.6 1562.98 1248.6 1556.3 L1248.6 1536.95 L1244.3 1536.95 L1244.3 1532.4 L1248.6 1532.4 L1248.6 1522.27 L1254.49 1522.27 Z" fill="#000000" fill-rule="evenodd" fill-opacity="1" /><path clip-path="url(#clip530)" d="M1304.75 1548.76 L1304.75 1551.62 L1277.82 1551.62 Q1278.2 1557.67 1281.45 1560.85 Q1284.73 1564 1290.55 1564 Q1293.92 1564 1297.08 1563.17 Q1300.26 1562.35 1303.38 1560.69 L1303.38 1566.23 Q1300.23 1567.57 1296.92 1568.27 Q1293.61 1568.97 1290.2 1568.97 Q1281.67 1568.97 1276.67 1564 Q1271.71 1559.04 1271.71 1550.57 Q1271.71 1541.82 1276.42 1536.69 Q1281.16 1531.54 1289.18 1531.54 Q1296.38 1531.54 1300.55 1536.18 Q1304.75 1540.8 1304.75 1548.76 M1298.89 1547.04 Q1298.83 1542.23 1296.18 1539.37 Q1293.57 1536.5 1289.25 1536.5 Q1284.34 1536.5 1281.38 1539.27 Q1278.46 1542.04 1278.01 1547.07 L1298.89 1547.04 Z" fill="#000000" fill-rule="evenodd" fill-opacity="1" /><path clip-path="url(#clip530)" d="M1337.82 1537.81 L1337.82 1518.52 L1343.67 1518.52 L1343.67 1568.04 L1337.82 1568.04 L1337.82 1562.7 Q1335.97 1565.88 1333.14 1567.44 Q1330.34 1568.97 1326.39 1568.97 Q1319.93 1568.97 1315.85 1563.81 Q1311.81 1558.65 1311.81 1550.25 Q1311.81 1541.85 1315.85 1536.69 Q1319.93 1531.54 1326.39 1531.54 Q1330.34 1531.54 1333.14 1533.1 Q1335.97 1534.62 1337.82 1537.81 M1317.86 1550.25 Q1317.86 1556.71 1320.5 1560.4 Q1323.18 1564.07 1327.82 1564.07 Q1332.47 1564.07 1335.14 1560.4 Q1337.82 1556.71 1337.82 1550.25 Q1337.82 1543.79 1335.14 1540.13 Q1332.47 1536.44 1327.82 1536.44 Q1323.18 1536.44 1320.5 1540.13 Q1317.86 1543.79 1317.86 1550.25 Z" fill="#000000" fill-rule="evenodd" fill-opacity="1" /><polyline clip-path="url(#clip532)" style="stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none" points="
  280.908,1206.56 2112.76,1206.56 
  "/>
<polyline clip-path="url(#clip532)" style="stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none" points="
  280.908,773.326 2112.76,773.326 
  "/>
<polyline clip-path="url(#clip532)" style="stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none" points="
  280.908,340.09 2112.76,340.09 
  "/>
<polyline clip-path="url(#clip530)" style="stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none" points="
  280.908,1423.18 280.908,123.472 
  "/>
<polyline clip-path="url(#clip530)" style="stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none" points="
  280.908,1206.56 297.616,1206.56 
  "/>
<polyline clip-path="url(#clip530)" style="stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none" points="
  280.908,773.326 297.616,773.326 
  "/>
<polyline clip-path="url(#clip530)" style="stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none" points="
  280.908,340.09 297.616,340.09 
  "/>
<path clip-path="url(#clip530)" d="M189.028 1189.28 L210.88 1189.28 L210.88 1193.22 L193.704 1193.22 L193.704 1203.45 L210.162 1203.45 L210.162 1207.38 L193.704 1207.38 L193.704 1219.91 L211.297 1219.91 L211.297 1223.84 L189.028 1223.84 L189.028 1189.28 Z" fill="#000000" fill-rule="evenodd" fill-opacity="1" /><path clip-path="url(#clip530)" d="M218.449 1189.28 L223.148 1189.28 L223.148 1210.28 Q223.148 1215.83 225.162 1218.29 Q227.176 1220.72 231.69 1220.72 Q236.181 1220.72 238.195 1218.29 Q240.209 1215.83 240.209 1210.28 L240.209 1189.28 L244.908 1189.28 L244.908 1210.86 Q244.908 1217.62 241.551 1221.06 Q238.218 1224.51 231.69 1224.51 Q225.139 1224.51 221.783 1221.06 Q218.449 1217.62 218.449 1210.86 L218.449 1189.28 Z" fill="#000000" fill-rule="evenodd" fill-opacity="1" /><path clip-path="url(#clip530)" d="M186.876 756.046 L191.575 756.046 L191.575 777.041 Q191.575 782.597 193.588 785.05 Q195.602 787.481 200.116 787.481 Q204.607 787.481 206.621 785.05 Q208.635 782.597 208.635 777.041 L208.635 756.046 L213.334 756.046 L213.334 777.62 Q213.334 784.379 209.977 787.828 Q206.644 791.277 200.116 791.277 Q193.565 791.277 190.209 787.828 Q186.876 784.379 186.876 777.62 L186.876 756.046 Z" fill="#000000" fill-rule="evenodd" fill-opacity="1" /><path clip-path="url(#clip530)" d="M242.824 757.18 L242.824 761.74 Q240.162 760.467 237.801 759.842 Q235.44 759.217 233.241 759.217 Q229.422 759.217 227.338 760.699 Q225.278 762.18 225.278 764.912 Q225.278 767.203 226.644 768.384 Q228.033 769.541 231.875 770.259 L234.699 770.838 Q239.931 771.833 242.408 774.356 Q244.908 776.856 244.908 781.069 Q244.908 786.092 241.528 788.685 Q238.172 791.277 231.667 791.277 Q229.213 791.277 226.435 790.722 Q223.681 790.166 220.718 789.078 L220.718 784.263 Q223.565 785.861 226.297 786.671 Q229.028 787.481 231.667 787.481 Q235.672 787.481 237.847 785.907 Q240.023 784.333 240.023 781.416 Q240.023 778.87 238.449 777.435 Q236.898 776 233.334 775.282 L230.486 774.726 Q225.255 773.685 222.917 771.463 Q220.579 769.24 220.579 765.282 Q220.579 760.699 223.797 758.06 Q227.037 755.421 232.709 755.421 Q235.139 755.421 237.662 755.861 Q240.185 756.301 242.824 757.18 Z" fill="#000000" fill-rule="evenodd" fill-opacity="1" /><path clip-path="url(#clip530)" d="M121.367 322.81 L126.043 322.81 L126.043 354.963 Q126.043 361.213 123.658 364.037 Q121.297 366.861 116.043 366.861 L114.26 366.861 L114.26 362.926 L115.718 362.926 Q118.82 362.926 120.093 361.19 Q121.367 359.453 121.367 354.963 L121.367 322.81 Z" fill="#000000" fill-rule="evenodd" fill-opacity="1" /><path clip-path="url(#clip530)" d="M146.945 344.338 Q141.783 344.338 139.792 345.518 Q137.802 346.699 137.802 349.546 Q137.802 351.815 139.283 353.157 Q140.788 354.477 143.357 354.477 Q146.899 354.477 149.028 351.977 Q151.181 349.453 151.181 345.287 L151.181 344.338 L146.945 344.338 M155.44 342.578 L155.44 357.37 L151.181 357.37 L151.181 353.435 Q149.723 355.796 147.547 356.93 Q145.371 358.041 142.223 358.041 Q138.242 358.041 135.88 355.819 Q133.542 353.574 133.542 349.824 Q133.542 345.449 136.459 343.227 Q139.399 341.004 145.209 341.004 L151.181 341.004 L151.181 340.588 Q151.181 337.648 149.237 336.051 Q147.316 334.43 143.82 334.43 Q141.598 334.43 139.492 334.963 Q137.385 335.495 135.441 336.56 L135.441 332.625 Q137.779 331.722 139.978 331.282 Q142.177 330.819 144.26 330.819 Q149.885 330.819 152.663 333.736 Q155.44 336.653 155.44 342.578 Z" fill="#000000" fill-rule="evenodd" fill-opacity="1" /><path clip-path="url(#clip530)" d="M168.334 353.481 L168.334 367.231 L164.052 367.231 L164.052 331.444 L168.334 331.444 L168.334 335.379 Q169.677 333.065 171.714 331.954 Q173.774 330.819 176.621 330.819 Q181.343 330.819 184.283 334.569 Q187.246 338.319 187.246 344.43 Q187.246 350.541 184.283 354.291 Q181.343 358.041 176.621 358.041 Q173.774 358.041 171.714 356.93 Q169.677 355.796 168.334 353.481 M182.825 344.43 Q182.825 339.731 180.88 337.069 Q178.959 334.384 175.579 334.384 Q172.2 334.384 170.255 337.069 Q168.334 339.731 168.334 344.43 Q168.334 349.129 170.255 351.815 Q172.2 354.477 175.579 354.477 Q178.959 354.477 180.88 351.815 Q182.825 349.129 182.825 344.43 Z" fill="#000000" fill-rule="evenodd" fill-opacity="1" /><path clip-path="url(#clip530)" d="M206.088 344.338 Q200.926 344.338 198.936 345.518 Q196.945 346.699 196.945 349.546 Q196.945 351.815 198.426 353.157 Q199.931 354.477 202.5 354.477 Q206.042 354.477 208.172 351.977 Q210.324 349.453 210.324 345.287 L210.324 344.338 L206.088 344.338 M214.584 342.578 L214.584 357.37 L210.324 357.37 L210.324 353.435 Q208.866 355.796 206.69 356.93 Q204.514 358.041 201.366 358.041 Q197.385 358.041 195.024 355.819 Q192.686 353.574 192.686 349.824 Q192.686 345.449 195.602 343.227 Q198.542 341.004 204.352 341.004 L210.324 341.004 L210.324 340.588 Q210.324 337.648 208.38 336.051 Q206.459 334.43 202.963 334.43 Q200.741 334.43 198.635 334.963 Q196.528 335.495 194.584 336.56 L194.584 332.625 Q196.922 331.722 199.121 331.282 Q201.32 330.819 203.403 330.819 Q209.028 330.819 211.806 333.736 Q214.584 336.653 214.584 342.578 Z" fill="#000000" fill-rule="evenodd" fill-opacity="1" /><path clip-path="url(#clip530)" d="M244.908 341.722 L244.908 357.37 L240.648 357.37 L240.648 341.861 Q240.648 338.18 239.213 336.352 Q237.778 334.523 234.908 334.523 Q231.459 334.523 229.468 336.722 Q227.477 338.921 227.477 342.717 L227.477 357.37 L223.195 357.37 L223.195 331.444 L227.477 331.444 L227.477 335.472 Q229.005 333.134 231.065 331.977 Q233.148 330.819 235.857 330.819 Q240.324 330.819 242.616 333.597 Q244.908 336.352 244.908 341.722 Z" fill="#000000" fill-rule="evenodd" fill-opacity="1" /><path clip-path="url(#clip530)" d="M22.818 850.351 L46.4666 859.072 L46.4666 841.598 L22.818 850.351 M16.4842 853.979 L16.4842 846.691 L64.0042 828.58 L64.0042 835.264 L51.8138 839.593 L51.8138 861.014 L64.0042 865.342 L64.0042 872.122 L16.4842 853.979 Z" fill="#000000" fill-rule="evenodd" fill-opacity="1" /><path clip-path="url(#clip530)" d="M29.7248 797.388 L35.1993 797.388 Q33.8307 799.871 33.1623 802.385 Q32.4621 804.868 32.4621 807.414 Q32.4621 813.112 36.0905 816.263 Q39.6872 819.414 46.212 819.414 Q52.7369 819.414 56.3653 816.263 Q59.9619 813.112 59.9619 807.414 Q59.9619 804.868 59.2935 802.385 Q58.5933 799.871 57.2247 797.388 L62.6355 797.388 Q63.7814 799.839 64.3543 802.481 Q64.9272 805.091 64.9272 808.051 Q64.9272 816.104 59.8664 820.846 Q54.8057 825.588 46.212 825.588 Q37.491 825.588 32.4939 820.814 Q27.4968 816.008 27.4968 807.669 Q27.4968 804.964 28.0697 802.385 Q28.6108 799.807 29.7248 797.388 Z" fill="#000000" fill-rule="evenodd" fill-opacity="1" /><path clip-path="url(#clip530)" d="M18.2347 781.41 L28.3562 781.41 L28.3562 769.347 L32.9077 769.347 L32.9077 781.41 L52.2594 781.41 Q56.6199 781.41 57.8613 780.233 Q59.1026 779.023 59.1026 775.363 L59.1026 769.347 L64.0042 769.347 L64.0042 775.363 Q64.0042 782.142 61.4897 784.721 Q58.9434 787.299 52.2594 787.299 L32.9077 787.299 L32.9077 791.596 L28.3562 791.596 L28.3562 787.299 L18.2347 787.299 L18.2347 781.41 Z" fill="#000000" fill-rule="evenodd" fill-opacity="1" /><path clip-path="url(#clip530)" d="M49.9359 762.25 L28.3562 762.25 L28.3562 756.393 L49.7131 756.393 Q54.7739 756.393 57.3202 754.42 Q59.8346 752.446 59.8346 748.5 Q59.8346 743.757 56.8109 741.02 Q53.7872 738.251 48.5673 738.251 L28.3562 738.251 L28.3562 732.394 L64.0042 732.394 L64.0042 738.251 L58.5296 738.251 Q61.7762 740.383 63.3676 743.216 Q64.9272 746.017 64.9272 749.741 Q64.9272 755.884 61.1078 759.067 Q57.2883 762.25 49.9359 762.25 M27.4968 747.513 L27.4968 747.513 Z" fill="#000000" fill-rule="evenodd" fill-opacity="1" /><path clip-path="url(#clip530)" d="M46.0847 704.131 Q46.0847 711.229 47.7079 713.966 Q49.3312 716.703 53.2461 716.703 Q56.3653 716.703 58.2114 714.666 Q60.0256 712.597 60.0256 709.064 Q60.0256 704.194 56.5881 701.266 Q53.1188 698.306 47.3897 698.306 L46.0847 698.306 L46.0847 704.131 M43.6657 692.45 L64.0042 692.45 L64.0042 698.306 L58.5933 698.306 Q61.8398 700.311 63.3994 703.303 Q64.9272 706.295 64.9272 710.624 Q64.9272 716.098 61.8716 719.345 Q58.7843 722.559 53.6281 722.559 Q47.6125 722.559 44.5569 718.549 Q41.5014 714.507 41.5014 706.518 L41.5014 698.306 L40.9285 698.306 Q36.8862 698.306 34.6901 700.98 Q32.4621 703.621 32.4621 708.428 Q32.4621 711.483 33.1941 714.38 Q33.9262 717.276 35.3903 719.95 L29.9795 719.95 Q28.7381 716.735 28.1334 713.711 Q27.4968 710.687 27.4968 707.823 Q27.4968 700.089 31.5072 696.269 Q35.5176 692.45 43.6657 692.45 Z" fill="#000000" fill-rule="evenodd" fill-opacity="1" /><path clip-path="url(#clip530)" d="M14.479 680.387 L14.479 674.53 L64.0042 674.53 L64.0042 680.387 L14.479 680.387 Z" fill="#000000" fill-rule="evenodd" fill-opacity="1" /><path clip-path="url(#clip530)" d="M479.234 16.7545 L479.234 25.383 Q475.102 21.5346 470.403 19.6307 Q465.745 17.7268 460.479 17.7268 Q450.108 17.7268 444.599 24.0867 Q439.09 30.4061 439.09 42.3968 Q439.09 54.3469 444.599 60.7069 Q450.108 67.0263 460.479 67.0263 Q465.745 67.0263 470.403 65.1223 Q475.102 63.2184 479.234 59.3701 L479.234 67.9175 Q474.94 70.8341 470.12 72.2924 Q465.34 73.7508 459.993 73.7508 Q446.26 73.7508 438.361 65.3654 Q430.461 56.9395 430.461 42.3968 Q430.461 27.8135 438.361 19.4281 Q446.26 11.0023 459.993 11.0023 Q465.421 11.0023 470.201 12.4606 Q475.021 13.8784 479.234 16.7545 Z" fill="#000000" fill-rule="evenodd" fill-opacity="1" /><path clip-path="url(#clip530)" d="M509.13 32.4315 Q503.135 32.4315 499.651 37.1306 Q496.167 41.7891 496.167 49.9314 Q496.167 58.0738 499.61 62.7728 Q503.094 67.4314 509.13 67.4314 Q515.085 67.4314 518.569 62.7323 Q522.052 58.0333 522.052 49.9314 Q522.052 41.8701 518.569 37.1711 Q515.085 32.4315 509.13 32.4315 M509.13 26.1121 Q518.852 26.1121 524.402 32.4315 Q529.952 38.7509 529.952 49.9314 Q529.952 61.0714 524.402 67.4314 Q518.852 73.7508 509.13 73.7508 Q499.367 73.7508 493.818 67.4314 Q488.308 61.0714 488.308 49.9314 Q488.308 38.7509 493.818 32.4315 Q499.367 26.1121 509.13 26.1121 Z" fill="#000000" fill-rule="evenodd" fill-opacity="1" /><path clip-path="url(#clip530)" d="M580.021 45.1919 L580.021 72.576 L572.567 72.576 L572.567 45.4349 Q572.567 38.994 570.056 35.7938 Q567.544 32.5936 562.521 32.5936 Q556.485 32.5936 553.001 36.4419 Q549.518 40.2903 549.518 46.9338 L549.518 72.576 L542.023 72.576 L542.023 27.2059 L549.518 27.2059 L549.518 34.2544 Q552.191 30.163 555.796 28.1376 Q559.442 26.1121 564.182 26.1121 Q572 26.1121 576.01 30.9732 Q580.021 35.7938 580.021 45.1919 Z" fill="#000000" fill-rule="evenodd" fill-opacity="1" /><path clip-path="url(#clip530)" d="M617.856 9.54393 L617.856 15.7418 L610.727 15.7418 Q606.716 15.7418 605.136 17.3622 Q603.597 18.9825 603.597 23.1955 L603.597 27.2059 L615.871 27.2059 L615.871 32.9987 L603.597 32.9987 L603.597 72.576 L596.103 72.576 L596.103 32.9987 L588.973 32.9987 L588.973 27.2059 L596.103 27.2059 L596.103 24.0462 Q596.103 16.471 599.627 13.0277 Q603.151 9.54393 610.808 9.54393 L617.856 9.54393 Z" fill="#000000" fill-rule="evenodd" fill-opacity="1" /><path clip-path="url(#clip530)" d="M623.325 54.671 L623.325 27.2059 L630.779 27.2059 L630.779 54.3874 Q630.779 60.8284 633.29 64.0691 Q635.802 67.2693 640.825 67.2693 Q646.861 67.2693 650.345 63.421 Q653.869 59.5726 653.869 52.9291 L653.869 27.2059 L661.322 27.2059 L661.322 72.576 L653.869 72.576 L653.869 65.6084 Q651.155 69.7404 647.549 71.7658 Q643.985 73.7508 639.245 73.7508 Q631.427 73.7508 627.376 68.8897 Q623.325 64.0286 623.325 54.671 M642.081 26.1121 L642.081 26.1121 Z" fill="#000000" fill-rule="evenodd" fill-opacity="1" /><path clip-path="url(#clip530)" d="M705.599 28.5427 L705.599 35.5912 Q702.439 33.9709 699.036 33.1607 Q695.634 32.3505 691.988 32.3505 Q686.438 32.3505 683.643 34.0519 Q680.888 35.7533 680.888 39.156 Q680.888 41.7486 682.873 43.2475 Q684.858 44.7058 690.854 46.0426 L693.406 46.6097 Q701.345 48.3111 704.667 51.4303 Q708.029 54.509 708.029 60.0587 Q708.029 66.3781 703.006 70.0644 Q698.024 73.7508 689.274 73.7508 Q685.628 73.7508 681.658 73.0216 Q677.729 72.3329 673.354 70.9151 L673.354 63.2184 Q677.486 65.3654 681.496 66.4591 Q685.506 67.5124 689.436 67.5124 Q694.702 67.5124 697.538 65.73 Q700.373 63.9071 700.373 60.6258 Q700.373 57.5877 698.307 55.9673 Q696.282 54.3469 689.355 52.8481 L686.762 52.2405 Q679.835 50.7821 676.756 47.7845 Q673.678 44.7463 673.678 39.4801 Q673.678 33.0797 678.215 29.5959 Q682.752 26.1121 691.097 26.1121 Q695.229 26.1121 698.874 26.7198 Q702.52 27.3274 705.599 28.5427 Z" fill="#000000" fill-rule="evenodd" fill-opacity="1" /><path clip-path="url(#clip530)" d="M719.899 27.2059 L727.352 27.2059 L727.352 72.576 L719.899 72.576 L719.899 27.2059 M719.899 9.54393 L727.352 9.54393 L727.352 18.9825 L719.899 18.9825 L719.899 9.54393 Z" fill="#000000" fill-rule="evenodd" fill-opacity="1" /><path clip-path="url(#clip530)" d="M760.529 32.4315 Q754.534 32.4315 751.05 37.1306 Q747.566 41.7891 747.566 49.9314 Q747.566 58.0738 751.009 62.7728 Q754.493 67.4314 760.529 67.4314 Q766.484 67.4314 769.968 62.7323 Q773.452 58.0333 773.452 49.9314 Q773.452 41.8701 769.968 37.1711 Q766.484 32.4315 760.529 32.4315 M760.529 26.1121 Q770.251 26.1121 775.801 32.4315 Q781.351 38.7509 781.351 49.9314 Q781.351 61.0714 775.801 67.4314 Q770.251 73.7508 760.529 73.7508 Q750.766 73.7508 745.217 67.4314 Q739.707 61.0714 739.707 49.9314 Q739.707 38.7509 745.217 32.4315 Q750.766 26.1121 760.529 26.1121 Z" fill="#000000" fill-rule="evenodd" fill-opacity="1" /><path clip-path="url(#clip530)" d="M831.42 45.1919 L831.42 72.576 L823.966 72.576 L823.966 45.4349 Q823.966 38.994 821.455 35.7938 Q818.943 32.5936 813.92 32.5936 Q807.884 32.5936 804.4 36.4419 Q800.917 40.2903 800.917 46.9338 L800.917 72.576 L793.422 72.576 L793.422 27.2059 L800.917 27.2059 L800.917 34.2544 Q803.59 30.163 807.196 28.1376 Q810.841 26.1121 815.581 26.1121 Q823.399 26.1121 827.41 30.9732 Q831.42 35.7938 831.42 45.1919 Z" fill="#000000" fill-rule="evenodd" fill-opacity="1" /><path clip-path="url(#clip530)" d="M872.982 12.096 L885.175 12.096 L900.609 53.2532 L916.124 12.096 L928.318 12.096 L928.318 72.576 L920.337 72.576 L920.337 19.4686 L904.741 60.9499 L896.518 60.9499 L880.922 19.4686 L880.922 72.576 L872.982 72.576 L872.982 12.096 Z" fill="#000000" fill-rule="evenodd" fill-opacity="1" /><path clip-path="url(#clip530)" d="M964.857 49.7694 Q955.823 49.7694 952.339 51.8354 Q948.856 53.9013 948.856 58.8839 Q948.856 62.8538 951.448 65.2034 Q954.081 67.5124 958.578 67.5124 Q964.776 67.5124 968.503 63.1374 Q972.27 58.7219 972.27 51.4303 L972.27 49.7694 L964.857 49.7694 M979.724 46.6907 L979.724 72.576 L972.27 72.576 L972.27 65.6895 Q969.718 69.8214 965.91 71.8063 Q962.102 73.7508 956.593 73.7508 Q949.625 73.7508 945.493 69.8619 Q941.402 65.9325 941.402 59.3701 Q941.402 51.7138 946.506 47.825 Q951.651 43.9361 961.819 43.9361 L972.27 43.9361 L972.27 43.2069 Q972.27 38.0623 968.867 35.2672 Q965.505 32.4315 959.388 32.4315 Q955.499 32.4315 951.813 33.3632 Q948.126 34.295 944.724 36.1584 L944.724 29.2718 Q948.815 27.692 952.664 26.9223 Q956.512 26.1121 960.158 26.1121 Q970.001 26.1121 974.862 31.2163 Q979.724 36.3204 979.724 46.6907 Z" fill="#000000" fill-rule="evenodd" fill-opacity="1" /><path clip-path="url(#clip530)" d="M1002.45 14.324 L1002.45 27.2059 L1017.8 27.2059 L1017.8 32.9987 L1002.45 32.9987 L1002.45 57.6282 Q1002.45 63.1779 1003.95 64.7578 Q1005.49 66.3376 1010.15 66.3376 L1017.8 66.3376 L1017.8 72.576 L1010.15 72.576 Q1001.52 72.576 998.236 69.3758 Q994.955 66.1351 994.955 57.6282 L994.955 32.9987 L989.486 32.9987 L989.486 27.2059 L994.955 27.2059 L994.955 14.324 L1002.45 14.324 Z" fill="#000000" fill-rule="evenodd" fill-opacity="1" /><path clip-path="url(#clip530)" d="M1053.9 34.1734 Q1052.64 33.4443 1051.14 33.1202 Q1049.68 32.7556 1047.9 32.7556 Q1041.58 32.7556 1038.18 36.8875 Q1034.82 40.9789 1034.82 48.6757 L1034.82 72.576 L1027.32 72.576 L1027.32 27.2059 L1034.82 27.2059 L1034.82 34.2544 Q1037.17 30.1225 1040.93 28.1376 Q1044.7 26.1121 1050.09 26.1121 Q1050.86 26.1121 1051.79 26.2337 Q1052.72 26.3147 1053.86 26.5172 L1053.9 34.1734 Z" fill="#000000" fill-rule="evenodd" fill-opacity="1" /><path clip-path="url(#clip530)" d="M1061.71 27.2059 L1069.17 27.2059 L1069.17 72.576 L1061.71 72.576 L1061.71 27.2059 M1061.71 9.54393 L1069.17 9.54393 L1069.17 18.9825 L1061.71 18.9825 L1061.71 9.54393 Z" fill="#000000" fill-rule="evenodd" fill-opacity="1" /><path clip-path="url(#clip530)" d="M1122.48 27.2059 L1106.07 49.2833 L1123.33 72.576 L1114.54 72.576 L1101.33 54.752 L1088.13 72.576 L1079.34 72.576 L1096.96 48.8377 L1080.83 27.2059 L1089.62 27.2059 L1101.66 43.369 L1113.69 27.2059 L1122.48 27.2059 Z" fill="#000000" fill-rule="evenodd" fill-opacity="1" /><path clip-path="url(#clip530)" d="M1178.14 9.62495 Q1172.71 18.942 1170.08 28.0566 Q1167.44 37.1711 1167.44 46.5287 Q1167.44 55.8863 1170.08 65.0818 Q1172.75 74.2369 1178.14 83.5134 L1171.66 83.5134 Q1165.58 73.9938 1162.54 64.7983 Q1159.54 55.6027 1159.54 46.5287 Q1159.54 37.4952 1162.54 28.3401 Q1165.54 19.1851 1171.66 9.62495 L1178.14 9.62495 Z" fill="#000000" fill-rule="evenodd" fill-opacity="1" /><path clip-path="url(#clip530)" d="M1230.31 45.1919 L1230.31 72.576 L1222.86 72.576 L1222.86 45.4349 Q1222.86 38.994 1220.35 35.7938 Q1217.84 32.5936 1212.81 32.5936 Q1206.78 32.5936 1203.29 36.4419 Q1199.81 40.2903 1199.81 46.9338 L1199.81 72.576 L1192.31 72.576 L1192.31 27.2059 L1199.81 27.2059 L1199.81 34.2544 Q1202.48 30.163 1206.09 28.1376 Q1209.73 26.1121 1214.47 26.1121 Q1222.29 26.1121 1226.3 30.9732 Q1230.31 35.7938 1230.31 45.1919 Z" fill="#000000" fill-rule="evenodd" fill-opacity="1" /><path clip-path="url(#clip530)" d="M1262.76 32.4315 Q1256.76 32.4315 1253.28 37.1306 Q1249.8 41.7891 1249.8 49.9314 Q1249.8 58.0738 1253.24 62.7728 Q1256.72 67.4314 1262.76 67.4314 Q1268.72 67.4314 1272.2 62.7323 Q1275.68 58.0333 1275.68 49.9314 Q1275.68 41.8701 1272.2 37.1711 Q1268.72 32.4315 1262.76 32.4315 M1262.76 26.1121 Q1272.48 26.1121 1278.03 32.4315 Q1283.58 38.7509 1283.58 49.9314 Q1283.58 61.0714 1278.03 67.4314 Q1272.48 73.7508 1262.76 73.7508 Q1253 73.7508 1247.45 67.4314 Q1241.94 61.0714 1241.94 49.9314 Q1241.94 38.7509 1247.45 32.4315 Q1253 26.1121 1262.76 26.1121 Z" fill="#000000" fill-rule="evenodd" fill-opacity="1" /><path clip-path="url(#clip530)" d="M1322.23 34.1734 Q1320.97 33.4443 1319.47 33.1202 Q1318.01 32.7556 1316.23 32.7556 Q1309.91 32.7556 1306.51 36.8875 Q1303.15 40.9789 1303.15 48.6757 L1303.15 72.576 L1295.65 72.576 L1295.65 27.2059 L1303.15 27.2059 L1303.15 34.2544 Q1305.5 30.1225 1309.26 28.1376 Q1313.03 26.1121 1318.42 26.1121 Q1319.19 26.1121 1320.12 26.2337 Q1321.05 26.3147 1322.19 26.5172 L1322.23 34.1734 Z" fill="#000000" fill-rule="evenodd" fill-opacity="1" /><path clip-path="url(#clip530)" d="M1363.91 35.9153 Q1366.71 30.8922 1370.6 28.5022 Q1374.48 26.1121 1379.75 26.1121 Q1386.84 26.1121 1390.69 31.0947 Q1394.54 36.0368 1394.54 45.1919 L1394.54 72.576 L1387.04 72.576 L1387.04 45.4349 Q1387.04 38.913 1384.73 35.7533 Q1382.42 32.5936 1377.68 32.5936 Q1371.89 32.5936 1368.53 36.4419 Q1365.17 40.2903 1365.17 46.9338 L1365.17 72.576 L1357.67 72.576 L1357.67 45.4349 Q1357.67 38.8725 1355.36 35.7533 Q1353.05 32.5936 1348.23 32.5936 Q1342.52 32.5936 1339.16 36.4824 Q1335.8 40.3308 1335.8 46.9338 L1335.8 72.576 L1328.3 72.576 L1328.3 27.2059 L1335.8 27.2059 L1335.8 34.2544 Q1338.35 30.082 1341.91 28.0971 Q1345.48 26.1121 1350.38 26.1121 Q1355.32 26.1121 1358.77 28.6237 Q1362.25 31.1352 1363.91 35.9153 Z" fill="#000000" fill-rule="evenodd" fill-opacity="1" /><path clip-path="url(#clip530)" d="M1430.02 49.7694 Q1420.99 49.7694 1417.5 51.8354 Q1414.02 53.9013 1414.02 58.8839 Q1414.02 62.8538 1416.61 65.2034 Q1419.25 67.5124 1423.74 67.5124 Q1429.94 67.5124 1433.67 63.1374 Q1437.44 58.7219 1437.44 51.4303 L1437.44 49.7694 L1430.02 49.7694 M1444.89 46.6907 L1444.89 72.576 L1437.44 72.576 L1437.44 65.6895 Q1434.88 69.8214 1431.08 71.8063 Q1427.27 73.7508 1421.76 73.7508 Q1414.79 73.7508 1410.66 69.8619 Q1406.57 65.9325 1406.57 59.3701 Q1406.57 51.7138 1411.67 47.825 Q1416.82 43.9361 1426.98 43.9361 L1437.44 43.9361 L1437.44 43.2069 Q1437.44 38.0623 1434.03 35.2672 Q1430.67 32.4315 1424.55 32.4315 Q1420.66 32.4315 1416.98 33.3632 Q1413.29 34.295 1409.89 36.1584 L1409.89 29.2718 Q1413.98 27.692 1417.83 26.9223 Q1421.68 26.1121 1425.32 26.1121 Q1435.17 26.1121 1440.03 31.2163 Q1444.89 36.3204 1444.89 46.6907 Z" fill="#000000" fill-rule="evenodd" fill-opacity="1" /><path clip-path="url(#clip530)" d="M1460.24 9.54393 L1467.7 9.54393 L1467.7 72.576 L1460.24 72.576 L1460.24 9.54393 Z" fill="#000000" fill-rule="evenodd" fill-opacity="1" /><path clip-path="url(#clip530)" d="M1483.29 27.2059 L1490.75 27.2059 L1490.75 72.576 L1483.29 72.576 L1483.29 27.2059 M1483.29 9.54393 L1490.75 9.54393 L1490.75 18.9825 L1483.29 18.9825 L1483.29 9.54393 Z" fill="#000000" fill-rule="evenodd" fill-opacity="1" /><path clip-path="url(#clip530)" d="M1535.26 28.5427 L1535.26 35.5912 Q1532.1 33.9709 1528.7 33.1607 Q1525.3 32.3505 1521.65 32.3505 Q1516.1 32.3505 1513.31 34.0519 Q1510.55 35.7533 1510.55 39.156 Q1510.55 41.7486 1512.54 43.2475 Q1514.52 44.7058 1520.52 46.0426 L1523.07 46.6097 Q1531.01 48.3111 1534.33 51.4303 Q1537.7 54.509 1537.7 60.0587 Q1537.7 66.3781 1532.67 70.0644 Q1527.69 73.7508 1518.94 73.7508 Q1515.29 73.7508 1511.32 73.0216 Q1507.39 72.3329 1503.02 70.9151 L1503.02 63.2184 Q1507.15 65.3654 1511.16 66.4591 Q1515.17 67.5124 1519.1 67.5124 Q1524.37 67.5124 1527.2 65.73 Q1530.04 63.9071 1530.04 60.6258 Q1530.04 57.5877 1527.97 55.9673 Q1525.95 54.3469 1519.02 52.8481 L1516.43 52.2405 Q1509.5 50.7821 1506.42 47.7845 Q1503.34 44.7463 1503.34 39.4801 Q1503.34 33.0797 1507.88 29.5959 Q1512.42 26.1121 1520.76 26.1121 Q1524.89 26.1121 1528.54 26.7198 Q1532.19 27.3274 1535.26 28.5427 Z" fill="#000000" fill-rule="evenodd" fill-opacity="1" /><path clip-path="url(#clip530)" d="M1588.37 48.0275 L1588.37 51.6733 L1554.1 51.6733 Q1554.59 59.3701 1558.72 63.421 Q1562.89 67.4314 1570.3 67.4314 Q1574.6 67.4314 1578.61 66.3781 Q1582.66 65.3249 1586.63 63.2184 L1586.63 70.267 Q1582.62 71.9684 1578.41 72.8596 Q1574.19 73.7508 1569.86 73.7508 Q1559 73.7508 1552.64 67.4314 Q1546.32 61.1119 1546.32 50.3365 Q1546.32 39.1965 1552.32 32.6746 Q1558.35 26.1121 1568.56 26.1121 Q1577.72 26.1121 1583.02 32.0264 Q1588.37 37.9003 1588.37 48.0275 M1580.92 45.84 Q1580.84 39.7232 1577.47 36.0774 Q1574.15 32.4315 1568.64 32.4315 Q1562.41 32.4315 1558.64 35.9558 Q1554.91 39.4801 1554.34 45.8805 L1580.92 45.84 Z" fill="#000000" fill-rule="evenodd" fill-opacity="1" /><path clip-path="url(#clip530)" d="M1630.46 34.0924 L1630.46 9.54393 L1637.91 9.54393 L1637.91 72.576 L1630.46 72.576 L1630.46 65.7705 Q1628.11 69.8214 1624.51 71.8063 Q1620.94 73.7508 1615.92 73.7508 Q1607.69 73.7508 1602.51 67.1883 Q1597.36 60.6258 1597.36 49.9314 Q1597.36 39.2371 1602.51 32.6746 Q1607.69 26.1121 1615.92 26.1121 Q1620.94 26.1121 1624.51 28.0971 Q1628.11 30.0415 1630.46 34.0924 M1605.06 49.9314 Q1605.06 58.1548 1608.42 62.8538 Q1611.83 67.5124 1617.74 67.5124 Q1623.66 67.5124 1627.06 62.8538 Q1630.46 58.1548 1630.46 49.9314 Q1630.46 41.7081 1627.06 37.0496 Q1623.66 32.3505 1617.74 32.3505 Q1611.83 32.3505 1608.42 37.0496 Q1605.06 41.7081 1605.06 49.9314 Z" fill="#000000" fill-rule="evenodd" fill-opacity="1" /><path clip-path="url(#clip530)" d="M1708.56 28.5427 L1708.56 35.5912 Q1705.4 33.9709 1702 33.1607 Q1698.6 32.3505 1694.95 32.3505 Q1689.4 32.3505 1686.61 34.0519 Q1683.85 35.7533 1683.85 39.156 Q1683.85 41.7486 1685.84 43.2475 Q1687.82 44.7058 1693.82 46.0426 L1696.37 46.6097 Q1704.31 48.3111 1707.63 51.4303 Q1710.99 54.509 1710.99 60.0587 Q1710.99 66.3781 1705.97 70.0644 Q1700.99 73.7508 1692.24 73.7508 Q1688.59 73.7508 1684.62 73.0216 Q1680.69 72.3329 1676.32 70.9151 L1676.32 63.2184 Q1680.45 65.3654 1684.46 66.4591 Q1688.47 67.5124 1692.4 67.5124 Q1697.67 67.5124 1700.5 65.73 Q1703.34 63.9071 1703.34 60.6258 Q1703.34 57.5877 1701.27 55.9673 Q1699.25 54.3469 1692.32 52.8481 L1689.73 52.2405 Q1682.8 50.7821 1679.72 47.7845 Q1676.64 44.7463 1676.64 39.4801 Q1676.64 33.0797 1681.18 29.5959 Q1685.72 26.1121 1694.06 26.1121 Q1698.19 26.1121 1701.84 26.7198 Q1705.48 27.3274 1708.56 28.5427 Z" fill="#000000" fill-rule="evenodd" fill-opacity="1" /><path clip-path="url(#clip530)" d="M1755.51 28.9478 L1755.51 35.9153 Q1752.35 34.1734 1749.15 33.3227 Q1745.99 32.4315 1742.75 32.4315 Q1735.5 32.4315 1731.49 37.0496 Q1727.48 41.6271 1727.48 49.9314 Q1727.48 58.2358 1731.49 62.8538 Q1735.5 67.4314 1742.75 67.4314 Q1745.99 67.4314 1749.15 66.5807 Q1752.35 65.6895 1755.51 63.9476 L1755.51 70.8341 Q1752.39 72.2924 1749.03 73.0216 Q1745.71 73.7508 1741.94 73.7508 Q1731.69 73.7508 1725.66 67.3098 Q1719.62 60.8689 1719.62 49.9314 Q1719.62 38.832 1725.7 32.472 Q1731.81 26.1121 1742.43 26.1121 Q1745.87 26.1121 1749.15 26.8413 Q1752.43 27.5299 1755.51 28.9478 Z" fill="#000000" fill-rule="evenodd" fill-opacity="1" /><path clip-path="url(#clip530)" d="M1786.06 32.4315 Q1780.06 32.4315 1776.58 37.1306 Q1773.09 41.7891 1773.09 49.9314 Q1773.09 58.0738 1776.54 62.7728 Q1780.02 67.4314 1786.06 67.4314 Q1792.01 67.4314 1795.49 62.7323 Q1798.98 58.0333 1798.98 49.9314 Q1798.98 41.8701 1795.49 37.1711 Q1792.01 32.4315 1786.06 32.4315 M1786.06 26.1121 Q1795.78 26.1121 1801.33 32.4315 Q1806.88 38.7509 1806.88 49.9314 Q1806.88 61.0714 1801.33 67.4314 Q1795.78 73.7508 1786.06 73.7508 Q1776.29 73.7508 1770.74 67.4314 Q1765.23 61.0714 1765.23 49.9314 Q1765.23 38.7509 1770.74 32.4315 Q1776.29 26.1121 1786.06 26.1121 Z" fill="#000000" fill-rule="evenodd" fill-opacity="1" /><path clip-path="url(#clip530)" d="M1845.52 34.1734 Q1844.27 33.4443 1842.77 33.1202 Q1841.31 32.7556 1839.53 32.7556 Q1833.21 32.7556 1829.81 36.8875 Q1826.44 40.9789 1826.44 48.6757 L1826.44 72.576 L1818.95 72.576 L1818.95 27.2059 L1826.44 27.2059 L1826.44 34.2544 Q1828.79 30.1225 1832.56 28.1376 Q1836.33 26.1121 1841.72 26.1121 Q1842.49 26.1121 1843.42 26.2337 Q1844.35 26.3147 1845.48 26.5172 L1845.52 34.1734 Z" fill="#000000" fill-rule="evenodd" fill-opacity="1" /><path clip-path="url(#clip530)" d="M1890.33 48.0275 L1890.33 51.6733 L1856.06 51.6733 Q1856.54 59.3701 1860.67 63.421 Q1864.85 67.4314 1872.26 67.4314 Q1876.55 67.4314 1880.56 66.3781 Q1884.61 65.3249 1888.58 63.2184 L1888.58 70.267 Q1884.57 71.9684 1880.36 72.8596 Q1876.15 73.7508 1871.81 73.7508 Q1860.96 73.7508 1854.6 67.4314 Q1848.28 61.1119 1848.28 50.3365 Q1848.28 39.1965 1854.27 32.6746 Q1860.31 26.1121 1870.52 26.1121 Q1879.67 26.1121 1884.98 32.0264 Q1890.33 37.9003 1890.33 48.0275 M1882.87 45.84 Q1882.79 39.7232 1879.43 36.0774 Q1876.11 32.4315 1870.6 32.4315 Q1864.36 32.4315 1860.59 35.9558 Q1856.87 39.4801 1856.3 45.8805 L1882.87 45.84 Z" fill="#000000" fill-rule="evenodd" fill-opacity="1" /><path clip-path="url(#clip530)" d="M1931.48 28.5427 L1931.48 35.5912 Q1928.32 33.9709 1924.92 33.1607 Q1921.52 32.3505 1917.87 32.3505 Q1912.32 32.3505 1909.53 34.0519 Q1906.77 35.7533 1906.77 39.156 Q1906.77 41.7486 1908.76 43.2475 Q1910.74 44.7058 1916.74 46.0426 L1919.29 46.6097 Q1927.23 48.3111 1930.55 51.4303 Q1933.91 54.509 1933.91 60.0587 Q1933.91 66.3781 1928.89 70.0644 Q1923.91 73.7508 1915.16 73.7508 Q1911.51 73.7508 1907.54 73.0216 Q1903.61 72.3329 1899.24 70.9151 L1899.24 63.2184 Q1903.37 65.3654 1907.38 66.4591 Q1911.39 67.5124 1915.32 67.5124 Q1920.59 67.5124 1923.42 65.73 Q1926.26 63.9071 1926.26 60.6258 Q1926.26 57.5877 1924.19 55.9673 Q1922.17 54.3469 1915.24 52.8481 L1912.65 52.2405 Q1905.72 50.7821 1902.64 47.7845 Q1899.56 44.7463 1899.56 39.4801 Q1899.56 33.0797 1904.1 29.5959 Q1908.64 26.1121 1916.98 26.1121 Q1921.11 26.1121 1924.76 26.7198 Q1928.4 27.3274 1931.48 28.5427 Z" fill="#000000" fill-rule="evenodd" fill-opacity="1" /><path clip-path="url(#clip530)" d="M1944.61 9.62495 L1951.09 9.62495 Q1957.17 19.1851 1960.16 28.3401 Q1963.2 37.4952 1963.2 46.5287 Q1963.2 55.6027 1960.16 64.7983 Q1957.17 73.9938 1951.09 83.5134 L1944.61 83.5134 Q1950 74.2369 1952.63 65.0818 Q1955.3 55.8863 1955.3 46.5287 Q1955.3 37.1711 1952.63 28.0566 Q1950 18.942 1944.61 9.62495 Z" fill="#000000" fill-rule="evenodd" fill-opacity="1" /><g clip-path="url(#clip532)">
<image width="1832" height="1300" xlink:href="data:image/png;base64,
iVBORw0KGgoAAAANSUhEUgAABygAAAUUCAYAAACERUWfAAAgAElEQVR4nOzbMRHEMBAEwdfzR2M2
BuDMHGQWc4G6EWw+tet59v4BADDiuqYXAACc676nFwAAnOk/PQAAAAAAAAA4h0AJAAAAAAAAZARK
AAAAAAAAICNQAgAAAAAAABmBEgAAAAAAAMgIlAAAAAAAAEBGoAQAAAAAAAAyAiUAAAAAAACQESgB
AAAAAACAjEAJAAAAAAAAZARKAAAAAAAAICNQAgAAAAAAABmBEgAAAAAAAMgIlAAAAAAAAEBGoAQA
AAAAAAAyAiUAAAAAAACQESgBAAAAAACAjEAJAAAAAAAAZARKAAAAAAAAICNQAgAAAAAAABmBEgAA
AAAAAMgIlAAAAAAAAEBGoAQAAAAAAAAyAiUAAAAAAACQESgBAAAAAACAjEAJAAAAAAAAZARKAAAA
AAAAICNQAgAAAAAAABmBEgAAAAAAAMgIlAAAAAAAAEBGoAQAAAAAAAAyAiUAAAAAAACQESgBAAAA
AACAjEAJAAAAAAAAZARKAAAAAAAAICNQAgAAAAAAABmBEgAAAAAAAMgIlAAAAAAAAEBGoAQAAAAA
AAAyAiUAAAAAAACQESgBAAAAAACAjEAJAAAAAAAAZARKAAAAAAAAICNQAgAAAAAAABmBEgAAAAAA
AMgIlAAAAAAAAEBGoAQAAAAAAAAyAiUAAAAAAACQESgBAAAAAACAjEAJAAAAAAAAZARKAAAAAAAA
ICNQAgAAAAAAABmBEgAAAAAAAMgIlAAAAAAAAEBGoAQAAAAAAAAyAiUAAAAAAACQESgBAAAAAACA
jEAJAAAAAAAAZARKAAAAAAAAICNQAgAAAAAAABmBEgAAAAAAAMgIlAAAAAAAAEBGoAQAAAAAAAAy
AiUAAAAAAACQESgBAAAAAACAjEAJAAAAAAAAZARKAAAAAAAAICNQAgAAAAAAABmBEgAAAAAAAMgI
lAAAAAAAAEBGoAQAAAAAAAAyAiUAAAAAAACQESgBAAAAAACAjEAJAAAAAAAAZARKAAAAAAAAICNQ
AgAAAAAAABmBEgAAAAAAAMgIlAAAAAAAAEBGoAQAAAAAAAAyAiUAAAAAAACQESgBAAAAAACAjEAJ
AAAAAAAAZARKAAAAAAAAICNQAgAAAAAAABmBEgAAAAAAAMgIlAAAAAAAAEBGoAQAAAAAAAAyAiUA
AAAAAACQESgBAAAAAACAjEAJAAAAAAAAZARKAAAAAAAAICNQAgAAAAAAABmBEgAAAAAAAMgIlAAA
AAAAAEBGoAQAAAAAAAAyAiUAAAAAAACQESgBAAAAAACAjEAJAAAAAAAAZARKAAAAAAAAICNQAgAA
AAAAABmBEgAAAAAAAMgIlAAAAAAAAEBGoAQAAAAAAAAyAiUAAAAAAACQESgBAAAAAACAjEAJAAAA
AAAAZARKAAAAAAAAICNQAgAAAAAAABmBEgAAAAAAAMgIlAAAAAAAAEBGoAQAAAAAAAAyAiUAAAAA
AACQESgBAAAAAACAjEAJAAAAAAAAZARKAAAAAAAAICNQAgAAAAAAABmBEgAAAAAAAMgIlAAAAAAA
AEBGoAQAAAAAAAAyAiUAAAAAAACQESgBAAAAAACAjEAJAAAAAAAAZARKAAAAAAAAICNQAgAAAAAA
ABmBEgAAAAAAAMgIlAAAAAAAAEBGoAQAAAAAAAAyAiUAAAAAAACQESgBAAAAAACAjEAJAAAAAAAA
ZARKAAAAAAAAICNQAgAAAAAAABmBEgAAAAAAAMgIlAAAAAAAAEBGoAQAAAAAAAAyAiUAAAAAAACQ
ESgBAAAAAACAjEAJAAAAAAAAZARKAAAAAAAAICNQAgAAAAAAABmBEgAAAAAAAMgIlAAAAAAAAEBG
oAQAAAAAAAAyAiUAAAAAAACQESgBAAAAAACAjEAJAAAAAAAAZARKAAAAAAAAICNQAgAAAAAAABmB
EgAAAAAAAMgIlAAAAAAAAEBGoAQAAAAAAAAyAiUAAAAAAACQESgBAAAAAACAjEAJAAAAAAAAZARK
AAAAAAAAICNQAgAAAAAAABmBEgAAAAAAAMgIlAAAAAAAAEBGoAQAAAAAAAAyAiUAAAAAAACQESgB
AAAAAACAjEAJAAAAAAAAZARKAAAAAAAAICNQAgAAAAAAABmBEgAAAAAAAMgIlAAAAAAAAEBGoAQA
AAAAAAAyAiUAAAAAAACQESgBAAAAAACAjEAJAAAAAAAAZARKAAAAAAAAICNQAgAAAAAAABmBEgAA
AAAAAMgIlAAAAAAAAEBGoAQAAAAAAAAyAiUAAAAAAACQESgBAAAAAACAjEAJAAAAAAAAZARKAAAA
AAAAICNQAgAAAAAAABmBEgAAAAAAAMgIlAAAAAAAAEBGoAQAAAAAAAAyAiUAAAAAAACQESgBAAAA
AACAjEAJAAAAAAAAZARKAAAAAAAAICNQAgAAAAAAABmBEgAAAAAAAMgIlAAAAAAAAEBGoAQAAAAA
AAAyAiUAAAAAAACQESgBAAAAAACAjEAJAAAAAAAAZARKAAAAAAAAICNQAgAAAAAAABmBEgAAAAAA
AMgIlAAAAAAAAEBGoAQAAAAAAAAyAiUAAAAAAACQESgBAAAAAACAjEAJAAAAAAAAZARKAAAAAAAA
ICNQAgAAAAAAABmBEgAAAAAAAMgIlAAAAAAAAEBGoAQAAAAAAAAyAiUAAAAAAACQESgBAAAAAACA
jEAJAAAAAAAAZARKAAAAAAAAICNQAgAAAAAAABmBEgAAAAAAAMgIlAAAAAAAAEBGoAQAAAAAAAAy
AiUAAAAAAACQESgBAAAAAACAjEAJAAAAAAAAZARKAAAAAAAAICNQAgAAAAAAABmBEgAAAAAAAMgI
lAAAAAAAAEBGoAQAAAAAAAAyAiUAAAAAAACQESgBAAAAAACAjEAJAAAAAAAAZARKAAAAAAAAICNQ
AgAAAAAAABmBEgAAAAAAAMgIlAAAAAAAAEBGoAQAAAAAAAAyAiUAAAAAAACQESgBAAAAAACAjEAJ
AAAAAAAAZARKAAAAAAAAICNQAgAAAAAAABmBEgAAAAAAAMgIlAAAAAAAAEBGoAQAAAAAAAAyAiUA
AAAAAACQESgBAAAAAACAjEAJAAAAAAAAZARKAAAAAAAAICNQAgAAAAAAABmBEgAAAAAAAMgIlAAA
AAAAAEBGoAQAAAAAAAAyAiUAAAAAAACQESgBAAAAAACAjEAJAAAAAAAAZARKAAAAAAAAICNQAgAA
AAAAABmBEgAAAAAAAMgIlAAAAAAAAEBGoAQAAAAAAAAyAiUAAAAAAACQESgBAAAAAACAjEAJAAAA
AAAAZARKAAAAAAAAICNQAgAAAAAAABmBEgAAAAAAAMgIlAAAAAAAAEBGoAQAAAAAAAAyAiUAAAAA
AACQESgBAAAAAACAjEAJAAAAAAAAZARKAAAAAAAAICNQAgAAAAAAABmBEgAAAAAAAMgIlAAAAAAA
AEBGoAQAAAAAAAAyAiUAAAAAAACQESgBAAAAAACAjEAJAAAAAAAAZARKAAAAAAAAICNQAgAAAAAA
ABmBEgAAAAAAAMgIlAAAAAAAAEBGoAQAAAAAAAAyAiUAAAAAAACQESgBAAAAAACAjEAJAAAAAAAA
ZARKAAAAAAAAICNQAgAAAAAAABmBEgAAAAAAAMgIlAAAAAAAAEBGoAQAAAAAAAAyAiUAAAAAAACQ
ESgBAAAAAACAjEAJAAAAAAAAZARKAAAAAAAAICNQAgAAAAAAABmBEgAAAAAAAMgIlAAAAAAAAEBG
oAQAAAAAAAAyAiUAAAAAAACQESgBAAAAAACAjEAJAAAAAAAAZARKAAAAAAAAICNQAgAAAAAAABmB
EgAAAAAAAMgIlAAAAAAAAEBGoAQAAAAAAAAyAiUAAAAAAACQESgBAAAAAACAjEAJAAAAAAAAZARK
AAAAAAAAICNQAgAAAAAAABmBEgAAAAAAAMgIlAAAAAAAAEBGoAQAAAAAAAAyAiUAAAAAAACQESgB
AAAAAACAjEAJAAAAAAAAZARKAAAAAAAAICNQAgAAAAAAABmBEgAAAAAAAMgIlAAAAAAAAEBGoAQA
AAAAAAAyAiUAAAAAAACQESgBAAAAAACAjEAJAAAAAAAAZARKAAAAAAAAICNQAgAAAAAAABmBEgAA
AAAAAMgIlAAAAAAAAEBGoAQAAAAAAAAyAiUAAAAAAACQESgBAAAAAACAjEAJAAAAAAAAZARKAAAA
AAAAICNQAgAAAAAAABmBEgAAAAAAAMgIlAAAAAAAAEBGoAQAAAAAAAAyAiUAAAAAAACQESgBAAAA
AACAjEAJAAAAAAAAZARKAAAAAAAAICNQAgAAAAAAABmBEgAAAAAAAMgIlAAAAAAAAEBGoAQAAAAA
AAAyAiUAAAAAAACQESgBAAAAAACAjEAJAAAAAAAAZARKAAAAAAAAICNQAgAAAAAAABmBEgAAAAAA
AMgIlAAAAAAAAEBGoAQAAAAAAAAyAiUAAAAAAACQESgBAAAAAACAjEAJAAAAAAAAZARKAAAAAAAA
ICNQAgAAAAAAABmBEgAAAAAAAMgIlAAAAAAAAEBGoAQAAAAAAAAyAiUAAAAAAACQESgBAAAAAACA
jEAJAAAAAAAAZARKAAAAAAAAICNQAgAAAAAAABmBEgAAAAAAAMgIlAAAAAAAAEBGoAQAAAAAAAAy
AiUAAAAAAACQESgBAAAAAACAjEAJAAAAAAAAZARKAAAAAAAAICNQAgAAAAAAABmBEgAAAAAAAMgI
lAAAAAAAAEBGoAQAAAAAAAAyAiUAAAAAAACQESgBAAAAAACAjEAJAAAAAAAAZARKAAAAAAAAICNQ
AgAAAAAAABmBEgAAAAAAAMgIlAAAAAAAAEBGoAQAAAAAAAAyAiUAAAAAAACQESgBAAAAAACAjEAJ
AAAAAAAAZARKAAAAAAAAICNQAgAAAAAAABmBEgAAAAAAAMgIlAAAAAAAAEBGoAQAAAAAAAAyAiUA
AAAAAACQESgBAAAAAACAjEAJAAAAAAAAZARKAAAAAAAAICNQAgAAAAAAABmBEgAAAAAAAMgIlAAA
AAAAAEBGoAQAAAAAAAAyAiUAAAAAAACQESgBAAAAAACAjEAJAAAAAAAAZARKAAAAAAAAICNQAgAA
AAAAABmBEgAAAAAAAMgIlAAAAAAAAEBGoAQAAAAAAAAyAiUAAAAAAACQESgBAAAAAACAjEAJAAAA
AAAAZARKAAAAAAAAICNQAgAAAAAAABmBEgAAAAAAAMgIlAAAAAAAAEBGoAQAAAAAAAAyAiUAAAAA
AACQESgBAAAAAACAjEAJAAAAAAAAZARKAAAAAAAAICNQAgAAAAAAABmBEgAAAAAAAMgIlAAAAAAA
AEBGoAQAAAAAAAAyAiUAAAAAAACQESgBAAAAAACAjEAJAAAAAAAAZARKAAAAAAAAICNQAgAAAAAA
ABmBEgAAAAAAAMgIlAAAAAAAAEBGoAQAAAAAAAAyAiUAAAAAAACQESgBAAAAAACAzNp77+kRAACn
Wmt6AQDAud53egEAwJk8KAEAAAAAAICMQAkAAAAAAABkBEoAAAAAAAAgI1ACAAAAAAAAGYESAAAA
AAAAyAiUAAAAAAAAQEagBAAAAAAAADICJQAAAAAAAJARKAEAAAAAAICMQAkAAAAAAABkBEoAAAAA
AAAgI1ACAAAAAAAAGYESAAAAAAAAyAiUAAAAAAAAQEagBAAAAAAAADICJQAAAAAAAJARKAEAAAAA
AICMQAkAAAAAAABkBEoAAAAAAAAgI1ACAAAAAAAAGYESAAAAAAAAyAiUAAAAAAAAQEagBAAAAAAA
ADICJQAAAAAAAJARKAEAAAAAAICMQAkAAAAAAABkBEoAAAAAAAAgI1ACAAAAAAAAGYESAAAAAAAA
yAiUAAAAAAAAQEagBAAAAAAAADICJQAAAAAAAJARKAEAAAAAAICMQAkAAAAAAABkBEoAAAAAAAAg
I1ACAAAAAAAAGYESAAAAAAAAyAiUAAAAAAAAQEagBAAAAAAAADICJQAAAAAAAJARKAEAAAAAAICM
QAkAAAAAAABkBEoAAAAAAAAgI1ACAAAAAAAAGYESAAAAAAAAyAiUAAAAAAAAQEagBAAAAAAAADIC
JQAAAAAAAJARKAEAAAAAAICMQAkAAAAAAABkBEoAAAAAAAAgI1ACAAAAAAAAGYESAAAAAAAAyAiU
AAAAAAAAQEagBAAAAAAAADICJQAAAAAAAJARKAEAAAAAAICMQAkAAAAAAABkBEoAAAAAAAAgI1AC
AAAAAAAAGYESAAAAAAAAyAiUAAAAAAAAQEagBAAAAAAAADICJQAAAAAAAJARKAEAAAAAAICMQAkA
AAAAAABkBEoAAAAAAAAgI1ACAAAAAAAAGYESAAAAAAAAyAiUAAAAAAAAQEagBAAAAAAAADICJQAA
AAAAAJARKAEAAAAAAICMQAkAAAAAAABkBEoAAAAAAAAgI1ACAAAAAAAAGYESAAAAAAAAyAiUAAAA
AAAAQEagBAAAAAAAADICJQAAAAAAAJARKAEAAAAAAICMQAkAAAAAAABkBEoAAAAAAAAgI1ACAAAA
AAAAGYESAAAAAAAAyAiUAAAAAAAAQEagBAAAAAAAADICJQAAAAAAAJARKAEAAAAAAICMQAkAAAAA
AABkBEoAAAAAAAAgI1ACAAAAAAAAGYESAAAAAAAAyAiUAAAAAAAAQEagBAAAAAAAADICJQAAAAAA
AJARKAEAAAAAAICMQAkAAAAAAABkBEoAAAAAAAAgI1ACAAAAAAAAGYESAAAAAAAAyAiUAAAAAAAA
QEagBAAAAAAAADICJQAAAAAAAJARKAEAAAAAAICMQAkAAAAAAABkBEoAAAAAAAAgI1ACAAAAAAAA
GYESAAAAAAAAyAiUAAAAAAAAQEagBAAAAAAAADICJQAAAAAAAJARKAEAAAAAAICMQAkAAAAAAABk
BEoAAAAAAAAgI1ACAAAAAAAAGYESAAAAAAAAyAiUAAAAAAAAQEagBAAAAAAAADICJQAAAAAAAJAR
KAEAAAAAAICMQAkAAAAAAABkBEoAAAAAAAAgI1ACAAAAAAAAGYESAAAAAAAAyAiUAAAAAAAAQEag
BAAAAAAAADICJQAAAAAAAJARKAEAAAAAAICMQAnwsW/HRADDQBDEmqAzfHMwj7DYL15CcP3OAQAA
AAAAGYESAAAAAAAAyAiUAAAAAAAAQEagBAAAAAAAADICJQAAAAAAAJARKAEAAAAAAICMQAkAAAAA
AABkBEoAAAAAAAAgI1ACAAAAAAAAGYESAAAAAAAAyAiUAAAAAAAAQEagBAAAAAAAADICJQAAAAAA
AJARKAEAAAAAAICMQAkAAAAAAABkBEoAAAAAAAAgI1ACAAAAAAAAGYESAAAAAAAAyAiUAAAAAAAA
QEagBAAAAAAAADICJQAAAAAAAJARKAEAAAAAAICMQAkAAAAAAABkBEoAAAAAAAAgI1ACAAAAAAAA
GYESAAAAAAAAyAiUAAAAAAAAQEagBAAAAAAAADICJQAAAAAAAJARKAEAAAAAAICMQAkAAAAAAABk
BEoAAAAAAAAgI1ACAAAAAAAAGYESAAAAAAAAyAiUAAAAAAAAQEagBAAAAAAAADICJQAAAAAAAJAR
KAEAAAAAAICMQAkAAAAAAABkBEoAAAAAAAAgI1ACAAAAAAAAGYESAAAAAAAAyAiUAAAAAAAAQEag
BAAAAAAAADICJQAAAAAAAJARKAEAAAAAAICMQAkAAAAAAABkBEoAAAAAAAAgI1ACAAAAAAAAGYES
AAAAAAAAyAiUAAAAAAAAQEagBAAAAAAAADICJQAAAAAAAJARKAEAAAAAAICMQAkAAAAAAABkBEoA
AAAAAAAgI1ACAAAAAAAAGYESAAAAAAAAyAiUAAAAAAAAQEagBAAAAAAAADICJQAAAAAAAJARKAEA
AAAAAICMQAkAAAAAAABkBEoAAAAAAAAgI1ACAAAAAAAAGYESAAAAAAAAyAiUAAAAAAAAQEagBAAA
AAAAADICJQAAAAAAAJARKAEAAAAAAICMQAkAAAAAAABkBEoAAAAAAAAgI1ACAAAAAAAAGYESAAAA
AAAAyAiUAAAAAAAAQEagBAAAAAAAADICJQAAAAAAAJARKAEAAAAAAICMQAkAAAAAAABkBEoAAAAA
AAAgI1ACAAAAAAAAGYESAAAAAAAAyAiUAAAAAAAAQEagBAAAAAAAADICJQAAAAAAAJARKAEAAAAA
AICMQAkAAAAAAABkBEoAAAAAAAAgI1ACAAAAAAAAGYESAAAAAAAAyAiUAAAAAAAAQEagBAAAAAAA
ADICJQAAAAAAAJARKAEAAAAAAICMQAkAAAAAAABkBEoAAAAAAAAgI1ACAAAAAAAAGYESAAAAAAAA
yAiUAAAAAAAAQEagBAAAAAAAADICJQAAAAAAAJARKAEAAAAAAICMQAkAAAAAAABkBEoAAAAAAAAg
I1ACAAAAAAAAGYESAAAAAAAAyAiUAAAAAAAAQEagBAAAAAAAADICJQAAAAAAAJARKAEAAAAAAICM
QAkAAAAAAABkBEoAAAAAAAAgI1ACAAAAAAAAGYESAAAAAAAAyAiUAAAAAAAAQEagBAAAAAAAADIC
JQAAAAAAAJARKAEAAAAAAICMQAkAAAAAAABkBEoAAAAAAAAgI1ACAAAAAAAAGYESAAAAAAAAyAiU
AAAAAAAAQEagBAAAAAAAADICJQAAAAAAAJARKAEAAAAAAICMQAkAAAAAAABkBEoAAAAAAAAgI1AC
AAAAAAAAGYESAAAAAAAAyAiUAAAAAAAAQEagBAAAAAAAADICJQAAAAAAAJARKAEAAAAAAICMQAkA
AAAAAABkBEoAAAAAAAAgI1ACAAAAAAAAGYESAAAAAAAAyAiUAAAAAAAAQEagBAAAAAAAADICJQAA
AAAAAJARKAEAAAAAAICMQAkAAAAAAABkBEoAAAAAAAAgI1ACAAAAAAAAGYESAAAAAAAAyAiUAAAA
AAAAQEagBAAAAAAAADICJQAAAAAAAJARKAEAAAAAAICMQAkAAAAAAABkBEoAAAAAAAAgI1ACAAAA
AAAAGYESAAAAAAAAyAiUAAAAAAAAQEagBAAAAAAAADICJQAAAAAAAJARKAEAAAAAAICMQAkAAAAA
AABkBEoAAAAAAAAgI1ACAAAAAAAAGYESAAAAAAAAyAiUAAAAAAAAQEagBAAAAAAAADICJQAAAAAA
AJARKAEAAAAAAICMQAkAAAAAAABkBEoAAAAAAAAgI1ACAAAAAAAAGYESAAAAAAAAyAiUAAAAAAAA
QEagBAAAAAAAADICJQAAAAAAAJARKAEAAAAAAICMQAkAAAAAAABkBEoAAAAAAAAgI1ACAAAAAAAA
GYESAAAAAAAAyAiUAAAAAAAAQEagBAAAAAAAADICJQAAAAAAAJARKAEAAAAAAICMQAkAAAAAAABk
BEoAAAAAAAAgI1ACAAAAAAAAGYESAAAAAAAAyAiUAAAAAAAAQEagBAAAAAAAADICJQAAAAAAAJAR
KAEAAAAAAICMQAkAAAAAAABkBEoAAAAAAAAgI1ACAAAAAAAAGYESAAAAAAAAyAiUAAAAAAAAQEag
BAAAAAAAADICJQAAAAAAAJARKAEAAAAAAICMQAkAAAAAAABkBEoAAAAAAAAgI1ACAAAAAAAAGYES
AAAAAAAAyAiUAAAAAAAAQEagBAAAAAAAADICJQAAAAAAAJARKAEAAAAAAICMQAkAAAAAAABkBEoA
AAAAAAAgI1ACAAAAAAAAGYESAAAAAAAAyAiUAAAAAAAAQEagBAAAAAAAADICJQAAAAAAAJARKAEA
AAAAAICMQAkAAAAAAABkBEoAAAAAAAAgI1ACAAAAAAAAGYESAAAAAAAAyAiUAAAAAAAAQEagBAAA
AAAAADICJQAAAAAAAJARKAEAAAAAAICMQAkAAAAAAABkBEoAAAAAAAAgI1ACAAAAAAAAGYESAAAA
AAAAyAiUAAAAAAAAQEagBAAAAAAAADICJQAAAAAAAJARKAEAAAAAAICMQAkAAAAAAABkBEoAAAAA
AAAgI1ACAAAAAAAAGYESAAAAAAAAyAiUAAAAAAAAQEagBAAAAAAAADICJQAAAAAAAJARKAEAAAAA
AICMQAkAAAAAAABkBEoAAAAAAAAgI1ACAAAAAAAAGYESAAAAAAAAyAiUAAAAAAAAQEagBAAAAAAA
ADICJQAAAAAAAJARKAEAAAAAAICMQAkAAAAAAABkBEoAAAAAAAAgI1ACAAAAAAAAGYESAAAAAAAA
yAiUAAAAAAAAQEagBAAAAAAAADICJQAAAAAAAJARKAEAAAAAAICMQAkAAAAAAABkBEoAAAAAAAAg
I1ACAAAAAAAAGYESAAAAAAAAyAiUAAAAAAAAQEagBAAAAAAAADICJQAAAAAAAJARKAEAAAAAAICM
QAkAAAAAAABkBEoAAAAAAAAgI1ACAAAAAAAAGYESAAAAAAAAyAiUAAAAAAAAQEagBAAAAAAAADIC
JQAAAAAAAJARKAEAAAAAAICMQAkAAAAAAABkBEoAAAAAAAAgI1ACAAAAAAAAGYESAAAAAAAAyAiU
AAAAAAAAQEagBAAAAAAAADICJQAAAAAAAJARKAEAAAAAAICMQAkAAAAAAABkBEoAAAAAAAAgI1AC
AAAAAAAAGYESAAAAAAAAyAiUAAAAAAAAQEagBAAAAAAAADICJQAAAAAAAJARKAEAAAAAAICMQAkA
AAAAAABkBEoAAAAAAAAgI1ACAAAAAAAAGYESAAAAAAAAyAiUAAAAAAAAQEagBAAAAAAAADICJQAA
AAAAAJARKAEAAAAAAICMQAkAAAAAAABkBEoAAAAAAAAgI1ACAAAAAAAAGYESAAAAAAAAyAiUAAAA
AAAAQEagBAAAAAAAADICJQAAAAAAAJARKIZ+E9QAAA/3SURBVAEAAAAAAICMQAkAAAAAAABkvnOm
JwAA7PXe9AIAgL3unV4AALCTByUAAAAAAACQESgBAAAAAACAjEAJAAAAAAAAZARKAAAAAAAAICNQ
AgAAAAAAABmBEgAAAAAAAMgIlAAAAAAAAEBGoAQAAAAAAAAyAiUAAAAAAACQESgBAAAAAACAjEAJ
AAAAAAAAZARKAAAAAAAAICNQAgAAAAAAABmBEgAAAAAAAMgIlAAAAAAAAEBGoAQAAAAAAAAyAiUA
AAAAAACQESgBAAAAAACAjEAJAAAAAAAAZARKAAAAAAAAICNQAgAAAAAAABmBEgAAAAAAAMgIlAAA
AAAAAEBGoAQAAAAAAAAyAiUAAAAAAACQESgBAAAAAACAjEAJAAAAAAAAZARKAAAAAAAAICNQAgAA
AAAAABmBEgAAAAAAAMgIlAAAAAAAAEBGoAQAAAAAAAAyAiUAAAAAAACQESgBAAAAAACAjEAJAAAA
AAAAZARKAAAAAAAAICNQAgAAAAAAABmBEgAAAAAAAMgIlAAAAAAAAEBGoAQAAAAAAAAyAiUAAAAA
AACQESgBAAAAAACAjEAJAAAAAAAAZARKAAAAAAAAICNQAgAAAAAAABmBEgAAAAAAAMgIlAAAAAAA
AEBGoAQAAAAAAAAyAiUAAAAAAACQESgBAAAAAACAjEAJAAAAAAAAZARKAAAAAAAAICNQAgAAAAAA
ABmBEgAAAAAAAMgIlAAAAAAAAEBGoAQAAAAAAAAyAiUAAAAAAACQESgBAAAAAACAjEAJAAAAAAAA
ZARKAAAAAAAAICNQAgAAAAAAABmBEgAAAAAAAMgIlAAAAAAAAEBGoAQAAAAAAAAyAiUAAAAAAACQ
ESgBAAAAAACAjEAJAAAAAAAAZARKAAAAAAAAICNQAgAAAAAAABmBEgAAAAAAAMgIlAAAAAAAAEBG
oAQAAAAAAAAyAiUAAAAAAACQESgBAAAAAACAjEAJAAAAAAAAZARKAAAAAAAAICNQAgAAAAAAABmB
EgAAAAAAAMgIlAAAAAAAAEBGoAQAAAAAAAAyAiUAAAAAAACQESgBAAAAAACAjEAJAAAAAAAAZARK
AAAAAAAAICNQAgAAAAAAABmBEgAAAAAAAMgIlAAAAAAAAEBGoAQAAAAAAAAyAiUAAAAAAACQESgB
AAAAAACAjEAJAAAAAAAAZARKAAAAAAAAICNQAgAAAAAAABmBEgAAAAAAAMgIlAAAAAAAAEBGoAQA
AAAAAAAyAiUAAAAAAACQESgBAAAAAACAjEAJAAAAAAAAZARKAAAAAAAAICNQAgAAAAAAABmBEgAA
AAAAAMgIlAAAAAAAAEBGoAQAAAAAAAAyAiUAAAAAAACQESgBAAAAAACAjEAJAAAAAAAAZARKAAAA
AAAAICNQAgAAAAAAABmBEgAAAAAAAMgIlAAAAAAAAEBGoAQAAAAAAAAyAiUAAAAAAACQESgBAAAA
AACAjEAJAAAAAAAAZARKAAAAAAAAICNQAgAAAAAAABmBEgAAAAAAAMgIlAAAAAAAAEBGoAQAAAAA
AAAyAiUAAAAAAACQESgBAAAAAACAjEAJAAAAAAAAZARKAAAAAAAAICNQAgAAAAAAABmBEgAAAAAA
AMgIlAAAAAAAAEBGoAQAAAAAAAAyAiUAAAAAAACQESgBAAAAAACAjEAJAAAAAAAAZARKAAAAAAAA
ICNQAgAAAAAAABmBEgAAAAAAAMgIlAAAAAAAAEBGoAQAAAAAAAAyAiUAAAAAAACQESgBAAAAAACA
jEAJAAAAAAAAZARKAAAAAAAAICNQAgAAAAAAABmBEgAAAAAAAMgIlAAAAAAAAEBGoAQAAAAAAAAy
AiUAAAAAAACQESgBAAAAAACAjEAJAAAAAAAAZARKAAAAAAAAICNQAgAAAAAAABmBEgAAAAAAAMgI
lAAAAAAAAEBGoAQAAAAAAAAyAiUAAAAAAACQESgBAAAAAACAjEAJAAAAAAAAZARKAAAAAAAAICNQ
AgAAAAAAABmBEgAAAAAAAMgIlAAAAAAAAEBGoAQAAAAAAAAyAiUAAAAAAACQESgBAAAAAACAjEAJ
AAAAAAAAZARKAAAAAAAAICNQAgAAAAAAABmBEgAAAAAAAMgIlAAAAAAAAEBGoAQAAAAAAAAyAiUA
AAAAAACQESgBAAAAAACAjEAJAAAAAAAAZARKAAAAAAAAICNQAgAAAAAAABmBEgAAAAAAAMgIlAAA
AAAAAEBGoAQAAAAAAAAyAiUAAAAAAACQESgBAAAAAACAjEAJAAAAAAAAZARKAAAAAAAAICNQAgAA
AAAAABmBEgAAAAAAAMgIlAAAAAAAAEBGoAQAAAAAAAAyAiUAAAAAAACQESgBAAAAAACAjEAJAAAA
AAAAZARKAAAAAAAAICNQAgAAAAAAABmBEgAAAAAAAMgIlAAAAAAAAEBGoAQAAAAAAAAyAiUAAAAA
AACQESgBAAAAAACAjEAJAAAAAAAAZARKAAAAAAAAICNQAgAAAAAAABmBEgAAAAAAAMgIlAAAAAAA
AEBGoAQAAAAAAAAyAiUAAAAAAACQESgBAAAAAACAjEAJAAAAAAAAZARKAAAAAAAAICNQAgAAAAAA
ABmBEgAAAAAAAMgIlAAAAAAAAEBGoAQAAAAAAAAyAiUAAAAAAACQESgBAAAAAACAjEAJAAAAAAAA
ZARKAAAAAAAAICNQAgAAAAAAABmBEgAAAAAAAMgIlAAAAAAAAEBGoAQAAAAAAAAyAiUAAAAAAACQ
ESgBAAAAAACAjEAJAAAAAAAAZARKAAAAAAAAICNQAgAAAAAAABmBEgAAAAAAAMgIlAAAAAAAAEBG
oAQAAAAAAAAyAiUAAAAAAACQESgBAAAAAACAjEAJAAAAAAAAZARKAAAAAAAAICNQAgAAAAAAABmB
EgAAAAAAAMgIlAAAAAAAAEBGoAQAAAAAAAAyAiUAAAAAAACQESgBAAAAAACAjEAJAAAAAAAAZARK
AAAAAAAAICNQAgAAAAAAABmBEgAAAAAAAMgIlAAAAAAAAEBGoAQAAAAAAAAyAiUAAAAAAACQESgB
AAAAAACAjEAJAAAAAAAAZARKAAAAAAAAICNQAgAAAAAAABmBEgAAAAAAAMgIlAAAAAAAAEBGoAQA
AAAAAAAyAiUAAAAAAACQESgBAAAAAACAjEAJAAAAAAAAZARKAAAAAAAAICNQAgAAAAAAABmBEgAA
AAAAAMgIlAAAAAAAAEBGoAQAAAAAAAAyAiUAAAAAAACQESgBAAAAAACAjEAJAAAAAAAAZARKAAAA
AAAAICNQAgAAAAAAf3t2LAAAAAAwyN96FrtKI4CNoAQAAAAAAAA2ghIAAAAAAADYCEoAAAAAAABg
IygBAAAAAACAjaAEAAAAAAAANoISAAAAAAAA2AhKAAAAAAAAYCMoAQAAAAAAgI2gBAAAAAAAADaC
EgAAAAAAANgISgAAAAAAAGAjKAEAAAAAAICNoAQAAAAAAAA2ghIAAAAAAADYCEoAAAAAAABgIygB
AAAAAACAjaAEAAAAAAAANoISAAAAAAAA2AhKAAAAAAAAYCMoAQAAAAAAgI2gBAAAAAAAADaCEgAA
AAAAANgISgAAAAAAAGAjKAEAAAAAAICNoAQAAAAAAAA2ghIAAAAAAADYCEoAAAAAAABgIygBAAAA
AACAjaAEAAAAAAAANoISAAAAAAAA2AhKAAAAAAAAYCMoAQAAAAAAgI2gBAAAAAAAADaCEgAAAAAA
ANgISgAAAAAAAGAjKAEAAAAAAICNoAQAAAAAAAA2ghIAAAAAAADYCEoAAAAAAABgIygBAAAAAACA
jaAEAAAAAAAANoISAAAAAAAA2AhKAAAAAAAAYCMoAQAAAAAAgI2gBAAAAAAAADaCEgAAAAAAANgI
SgAAAAAAAGAjKAEAAAAAAICNoAQAAAAAAAA2ghIAAAAAAADYCEoAAAAAAABgIygBAAAAAACAjaAE
AAAAAAAANoISAAAAAAAA2AhKAAAAAAAAYCMoAQAAAAAAgI2gBAAAAAAAADaCEgAAAAAAANgISgAA
AAAAAGAjKAEAAAAAAICNoAQAAAAAAAA2ghIAAAAAAADYCEoAAAAAAABgIygBAAAAAACAjaAEAAAA
AAAANoISAAAAAAAA2AhKAAAAAAAAYCMoAQAAAAAAgI2gBAAAAAAAADaCEgAAAAAAANgISgAAAAAA
AGAjKAEAAAAAAICNoAQAAAAAAAA2ghIAAAAAAADYCEoAAAAAAABgIygBAAAAAACAjaAEAAAAAAAA
NoISAAAAAAAA2AhKAAAAAAAAYCMoAQAAAAAAgI2gBAAAAAAAADaCEgAAAAAAANgISgAAAAAAAGAj
KAEAAAAAAICNoAQAAAAAAAA2ghIAAAAAAADYCEoAAAAAAABgIygBAAAAAACAjaAEAAAAAAAANoIS
AAAAAAAA2AhKAAAAAAAAYCMoAQAAAAAAgI2gBAAAAAAAADaCEgAAAAAAANgISgAAAAAAAGAjKAEA
AAAAAICNoAQAAAAAAAA2ghIAAAAAAADYCEoAAAAAAABgIygBAAAAAACAjaAEAAAAAAAANoISAAAA
AAAA2AhKAAAAAAAAYCMoAQAAAAAAgI2gBAAAAAAAADaCEgAAAAAAANgISgAAAAAAAGAjKAEAAAAA
AICNoAQAAAAAAAA2ghIAAAAAAADYCEoAAAAAAABgIygBAAAAAACAjaAEAAAAAAAANoISAAAAAAAA
2AhKAAAAAAAAYCMoAQAAAAAAgI2gBAAAAAAAADaCEgAAAAAAANgISgAAAAAAAGAjKAEAAAAAAICN
oAQAAAAAAAA2ghIAAAAAAADYCEoAAAAAAABgIygBAAAAAACAjaAEAAAAAAAANoISAAAAAAAA2AhK
AAAAAAAAYCMoAQAAAAAAgI2gBAAAAAAAADaCEgAAAAAAANgISgAAAAAAAGAjKAEAAAAAAICNoAQA
AAAAAAA2ghIAAAAAAADYCEoAAAAAAABgIygBAAAAAACAjaAEAAAAAAAANoISAAAAAAAA2AhKAAAA
AAAAYCMoAQAAAAAAgI2gBAAAAAAAADaCEgAAAAAAANgISgAAAAAAAGAjKAEAAAAAAICNoAQAAAAA
AAA2ghIAAAAAAADYCEoAAAAAAABgIygBAAAAAACAjaAEAAAAAAAANoISAAAAAAAA2AhKAAAAAAAA
YCMoAQAAAAAAgI2gBAAAAAAAADaCEgAAAAAAANgISgAAAAAAAGAjKAEAAAAAAICNoAQAAAAAAAA2
ghIAAAAAAADYCEoAAAAAAABgIygBAAAAAACAjaAEAAAAAAAANoISAAAAAAAA2AhKAAAAAAAAYCMo
AQAAAAAAgI2gBAAAAAAAADaCEgAAAAAAANgISgAAAAAAAGAjKAEAAAAAAICNoAQAAAAAAAA2ghIA
AAAAAADYCEoAAAAAAABgEyutGt8yiKc/AAAAAElFTkSuQmCC
" transform="translate(281, 123)"/>
</g>
<defs>
  <clipPath id="clip533">
    <rect x="2160" y="123" width="73" height="1301"/>
  </clipPath>
</defs>
<g clip-path="url(#clip533)">
<image width="72" height="1300" xlink:href="data:image/png;base64,
iVBORw0KGgoAAAANSUhEUgAAAEgAAAUUCAYAAAB8mVRsAAAKgUlEQVR4nO3dwY3kMBAEQZJo/13W
WDDMp/SIsOCQKLS4wGJvr/U8i7/O2/+ArxMoCBQECgIFgYJAQaAgUBAozN5v/xO+zYKCQEGgIFBw
pIMFBYGCQEGg4EgHCwoCBYGCQMGRDhYUBAoCBYGCIx0sKAgUBAoCBUc6WFAQKAgUBAqOdLCgIFAQ
KAgUHOlgQUGgIFAQKDjSwYKCQEGgIFBwpIMFBYGCQEGgMEeiK3mCQEGgIFAQKPhRI1hQECgIFAQK
jnSwoCBQECgIFBzpYEFBoCBQECg40sGCgkBBoCBQECgIFAQKAgWBgpd0sKAgUBAoCBQc6WBBQaAg
UBAo+EXyIE8QKAgUBApe0sGCgkBBoCBQcKSDBQWBgkBBoOBIBwsKAgWBgkDBkQ4WFAQKAgWBgiMd
LCgIFAQKAgWBgq9YsKAgUBAoCBQc6WBBQaAgUBAoONLBgoJAQaAgUHCkgwUFgYJAQaDgSAcLCgIF
gYJAwZEOFhQECgIFgYIjHSwoCBQECgIFgYJAQaAgUBAoeEkHCwoCBYGCQMGRDhYUBAoCBYGCIx0s
KAgUBAoCBUc6WFAQKAgUBAqOdLCgIFAQKAgUBAq+YsGCgkBBoCBQcKSDBQWBgkBBoOBIBwsKAgWB
gkDBkQ4WFAQKAgWBgiMdLCgIFAQKAoU5El3JEwQKAgWBgpd0sKAgUBAoCBQc6WBBQaAgUBAoONLB
goJAQaAgUHCkgwUFgYJAQaAgUBAoCBQECgIFL+lgQUGgIFAQKDjSwYKCQEGgIFAQKPhF8iBPECgI
FAQKftQIFhQECgIFgYIjHSwoCBQECgIFRzpYUBAoCBQECo50sKAgUBAoCBQc6WBBQaAgUBAoONLB
goJAQaAgUHCkgwUFgYJAQaDgSAcLCgIFgYJAwZEOFhQECgIFgYIjHSwoCBQECgIFRzpYUBAoCBQE
Co50sKAgUBAoCBQECr5iwYKCQEGgIFAQKAgUBAoCBYGCl3SwoCBQECgIFBzpYEFBoCBQECg40sGC
gkBBoCBQcKSDBQWBgkBBoOBIBwsKAgWBgkDBkQ4WFAQKAgWBgiMdLCgIFAQKAgVHOlhQECgIFAQK
jnSwoCBQECgIFOZIdCVPECgIFAQKAgU/agQLCgIFgYJAwZEOFhQECgIFgYIjHSwoCBQECgIFRzpY
UBAoCBQECo50sKAgUBAoCBQECgIFgYJAQaDgJR0sKAgUBAoCBb9IHuQJAgWBgkDBSzpYUBAoCBQE
Co50sKAgUBAoCBQc6WBBQaAgUBAoONLBgoJAQaAgUHCkgwUFgYJAQaAgUPAVCxYUBAoCBYGCIx0s
KAgUBAoCBUc6WFAQKAgUBAqOdLCgIFAQKAgUHOlgQUGgIFAQKDjSwYKCQEGgIFBwpIMFBYGCQEGg
4EgHCwoCBYGCQEGgIFAQKAgUBApe0sGCgkBBoCBQcKSDBQWBgkBBoOBIBwsKAgWBgkDBkQ4WFAQK
AgWBgkDBVyxYUBAoCBQECo50sKAgUBAoCBQc6WBBQaAgUBAoONLBgoJAQaAgUHCkgwUFgYJAQaAw
R6IreYJAQaAgUPCSDhYUBAoCBYGCIx0sKAgUBAoCBUc6WFAQKAgUBAqOdLCgIFAQKAgUHOlgQUGg
IFAQKAgUBAoCBYGCQMFLOlhQECgIFAQKAgW/SB7kCQIFgYJAwY8awYKCQEGgIFBwpIMFBYGCQEGg
4EgHCwoCBYGCQMGRDhYUBAoCBYGCIx0sKAgUBAoCBUc6WFAQKAgUBAqOdLCgIFAQKAgUHOlgQUGg
IFAQKDjSwYKCQEGgIFBwpIMFBYGCQEGg4EgHCwoCBYGCQMGRDhYUBAoCBYGCQMFXLFhQECgIFAQK
jnSwoCBQECgIFBzpYEFBoCBQECgIFAQKAgWBgkDBSzpYUBAoCBQECo50sKAgUBAoCBQc6WBBQaAg
UBAoONLBgoJAQaAgUHCkgwUFgYJAQaDgv1MP8gSBgkBBoOAlHSwoCBQECgIFRzpYUBAoCBQECgIF
X7FgQUGgIFAQKDjSwYKCQEGgIFBwpIMFBYGCQEGg4EgHCwoCBYGCQMGRDhYUBAoCBYGCIx0sKAgU
BAoCBUc6WFAQKAgUBAoCBYGCQEGgIFDwkg4WFAQKAgWBgj9uEuQJAgWBgkDBSzpYUBAoCBQECo50
sKAgUBAoCBQc6WBBQaAgUBAoCBR8xYIFBYGCQEGg4EgHCwoCBYGCQMGRDhYUBAoCBYGCIx0sKAgU
BAoCBUc6WFAQKAgUBAqOdLCgIFAQKAgUHOlgQUGgIFAQKDjSwYKCQEGgIFBwpIMFBYGCQEGg4EgH
CwoCBYGCQEGgIFAQKAgUBApe0sGCgkBBoCBQcKSDBQWBgkBBoCBQ8BULFhQECgIFgYIjHSwoCBQE
CgIFRzpYUBAoCBQECv6XzCBPECgIFAQKXtLBgoJAQaAgUHCkgwUFgYJAQaDgSAcLCgIFgYJAwZEO
FhQECgIFgYIjHSwoCBQECgIFRzpYUBAoCBQECo50sKAgUBAoCBQc6WBBQaAgUBAoONLBgoJAQaAg
UBAoCBQECgIFgYJAwY8awYKCQEGgIFDwx02CPEGgIFAQKHhJBwsKAgWBgkDBkQ4WFAQKAgWBgiMd
LCgIFAQKAgVHOlhQECgIFAQKjnSwoCBQECgIFBzpYEFBoCBQECg40sGCgkBBoCBQcKSDBQWBgkBB
oOBIBwsKAgWBgkDBkQ4WFAQKAgWBgkDBVyxYUBAoCBQECo50sKAgUBAoCBQc6WBBQaAgUBAoCBQE
CgIFgYJAwUs6WFAQKAgUBAqOdLCgIFAQKAgUHOlgQUGgIFAQKDjSwYKCQEGgIFDw/4sFeYJAQaAg
UPCSDhYUBAoCBYGCIx0sKAgUBAoCBUc6WFAQKAgUBAoCBV+xYEFBoCBQECg40sGCgkBBoCBQcKSD
BQWBgkBBoOBIBwsKAgWBgkDBkQ4WFAQKAgWBgiMdLCgIFAQKAgVHOlhQECgIFAQKAgWBgkBBoCBQ
8JIOFhQECgIFgYIjHSwoCBQECgIFfyYwyBMECgIFgYKXdLCgIFAQKAgUHOlgQUGgIFAQKAgUfMWC
BQWBgkBBoOBIBwsKAgWBgkDBkQ4WFAQKAgWBgiMdLCgIFAQKAgVHOlhQECgIFAQKjnSwoCBQECgI
FBzpYEFBoCBQECg40sGCgkBBoCBQcKSDBQWBgkBBoOBIBwsKAgWBgkBBoCBQECgIFAQKXtLBgoJA
QaAgUHCkgwUFgYJAQaAgUPAVCxYUBAoCBYGC/zYiyBMECgIFgYKXdLCgIFAQKAgUHOlgQUGgIFAQ
KDjSwYKCQEGgIFBwpIMFBYGCQEGg4EgHCwoCBYGCQMGRDhYUBAoCBYGCIx0sKAgUBAoCBUc6WFAQ
KAgUBAqOdLCgIFAQKAgUHOlgQUGgIFAQKDjSwYKCQEGgIFAQKAgUBAoCBYGCQMGPGsGCgkBBoCBQ
cKSDBQWBgkBBoOBvuQZ5gkBBoCBQ8JIOFhQECgIFgYIjHSwoCBQECgIFRzpYUBAoCBQECo50sKAg
UBAoCBQc6WBBQaAgUBAoONLBgoJAQaAgUHCkgwUFgYJAQaDgSAcLCgIFgYJA4QdMMRACBEi2bgAA
AABJRU5ErkJggg==
" transform="translate(2161, 123)"/>
</g>
<path clip-path="url(#clip530)" d="M2280.7 1311.12 Q2277.09 1311.12 2275.26 1314.69 Q2273.45 1318.23 2273.45 1325.36 Q2273.45 1332.46 2275.26 1336.03 Q2277.09 1339.57 2280.7 1339.57 Q2284.33 1339.57 2286.14 1336.03 Q2287.97 1332.46 2287.97 1325.36 Q2287.97 1318.23 2286.14 1314.69 Q2284.33 1311.12 2280.7 1311.12 M2280.7 1307.42 Q2286.51 1307.42 2289.57 1312.02 Q2292.64 1316.61 2292.64 1325.36 Q2292.64 1334.08 2289.57 1338.69 Q2286.51 1343.27 2280.7 1343.27 Q2274.89 1343.27 2271.81 1338.69 Q2268.76 1334.08 2268.76 1325.36 Q2268.76 1316.61 2271.81 1312.02 Q2274.89 1307.42 2280.7 1307.42 Z" fill="#000000" fill-rule="evenodd" fill-opacity="1" /><path clip-path="url(#clip530)" d="M2300.86 1336.72 L2305.75 1336.72 L2305.75 1342.6 L2300.86 1342.6 L2300.86 1336.72 Z" fill="#000000" fill-rule="evenodd" fill-opacity="1" /><path clip-path="url(#clip530)" d="M2316.74 1338.67 L2324.38 1338.67 L2324.38 1312.3 L2316.07 1313.97 L2316.07 1309.71 L2324.33 1308.04 L2329.01 1308.04 L2329.01 1338.67 L2336.65 1338.67 L2336.65 1342.6 L2316.74 1342.6 L2316.74 1338.67 Z" fill="#000000" fill-rule="evenodd" fill-opacity="1" /><path clip-path="url(#clip530)" d="M2280.7 1151.91 Q2277.09 1151.91 2275.26 1155.47 Q2273.45 1159.01 2273.45 1166.14 Q2273.45 1173.25 2275.26 1176.81 Q2277.09 1180.36 2280.7 1180.36 Q2284.33 1180.36 2286.14 1176.81 Q2287.97 1173.25 2287.97 1166.14 Q2287.97 1159.01 2286.14 1155.47 Q2284.33 1151.91 2280.7 1151.91 M2280.7 1148.2 Q2286.51 1148.2 2289.57 1152.81 Q2292.64 1157.39 2292.64 1166.14 Q2292.64 1174.87 2289.57 1179.48 Q2286.51 1184.06 2280.7 1184.06 Q2274.89 1184.06 2271.81 1179.48 Q2268.76 1174.87 2268.76 1166.14 Q2268.76 1157.39 2271.81 1152.81 Q2274.89 1148.2 2280.7 1148.2 Z" fill="#000000" fill-rule="evenodd" fill-opacity="1" /><path clip-path="url(#clip530)" d="M2300.86 1177.51 L2305.75 1177.51 L2305.75 1183.39 L2300.86 1183.39 L2300.86 1177.51 Z" fill="#000000" fill-rule="evenodd" fill-opacity="1" /><path clip-path="url(#clip530)" d="M2319.96 1179.45 L2336.28 1179.45 L2336.28 1183.39 L2314.33 1183.39 L2314.33 1179.45 Q2317 1176.7 2321.58 1172.07 Q2326.19 1167.42 2327.37 1166.07 Q2329.61 1163.55 2330.49 1161.81 Q2331.39 1160.05 2331.39 1158.37 Q2331.39 1155.61 2329.45 1153.87 Q2327.53 1152.14 2324.43 1152.14 Q2322.23 1152.14 2319.77 1152.9 Q2317.34 1153.67 2314.57 1155.22 L2314.57 1150.49 Q2317.39 1149.36 2319.84 1148.78 Q2322.3 1148.2 2324.33 1148.2 Q2329.7 1148.2 2332.9 1150.89 Q2336.09 1153.57 2336.09 1158.06 Q2336.09 1160.19 2335.28 1162.12 Q2334.5 1164.01 2332.39 1166.61 Q2331.81 1167.28 2328.71 1170.49 Q2325.61 1173.69 2319.96 1179.45 Z" fill="#000000" fill-rule="evenodd" fill-opacity="1" /><path clip-path="url(#clip530)" d="M2280.7 992.693 Q2277.09 992.693 2275.26 996.257 Q2273.45 999.799 2273.45 1006.93 Q2273.45 1014.04 2275.26 1017.6 Q2277.09 1021.14 2280.7 1021.14 Q2284.33 1021.14 2286.14 1017.6 Q2287.97 1014.04 2287.97 1006.93 Q2287.97 999.799 2286.14 996.257 Q2284.33 992.693 2280.7 992.693 M2280.7 988.989 Q2286.51 988.989 2289.57 993.595 Q2292.64 998.179 2292.64 1006.93 Q2292.64 1015.66 2289.57 1020.26 Q2286.51 1024.85 2280.7 1024.85 Q2274.89 1024.85 2271.81 1020.26 Q2268.76 1015.66 2268.76 1006.93 Q2268.76 998.179 2271.81 993.595 Q2274.89 988.989 2280.7 988.989 Z" fill="#000000" fill-rule="evenodd" fill-opacity="1" /><path clip-path="url(#clip530)" d="M2300.86 1018.29 L2305.75 1018.29 L2305.75 1024.17 L2300.86 1024.17 L2300.86 1018.29 Z" fill="#000000" fill-rule="evenodd" fill-opacity="1" /><path clip-path="url(#clip530)" d="M2330.1 1005.54 Q2333.45 1006.26 2335.33 1008.53 Q2337.23 1010.79 2337.23 1014.13 Q2337.23 1019.24 2333.71 1022.04 Q2330.19 1024.85 2323.71 1024.85 Q2321.53 1024.85 2319.22 1024.41 Q2316.93 1023.99 2314.47 1023.13 L2314.47 1018.62 Q2316.42 1019.75 2318.73 1020.33 Q2321.05 1020.91 2323.57 1020.91 Q2327.97 1020.91 2330.26 1019.17 Q2332.58 1017.44 2332.58 1014.13 Q2332.58 1011.07 2330.42 1009.36 Q2328.29 1007.62 2324.47 1007.62 L2320.45 1007.62 L2320.45 1003.78 L2324.66 1003.78 Q2328.11 1003.78 2329.94 1002.41 Q2331.76 1001.03 2331.76 998.433 Q2331.76 995.771 2329.87 994.359 Q2327.99 992.924 2324.47 992.924 Q2322.55 992.924 2320.35 993.341 Q2318.15 993.757 2315.51 994.637 L2315.51 990.47 Q2318.18 989.73 2320.49 989.359 Q2322.83 988.989 2324.89 988.989 Q2330.21 988.989 2333.32 991.419 Q2336.42 993.827 2336.42 997.947 Q2336.42 1000.82 2334.77 1002.81 Q2333.13 1004.78 2330.1 1005.54 Z" fill="#000000" fill-rule="evenodd" fill-opacity="1" /><path clip-path="url(#clip530)" d="M2280.7 833.478 Q2277.09 833.478 2275.26 837.043 Q2273.45 840.585 2273.45 847.714 Q2273.45 854.821 2275.26 858.386 Q2277.09 861.927 2280.7 861.927 Q2284.33 861.927 2286.14 858.386 Q2287.97 854.821 2287.97 847.714 Q2287.97 840.585 2286.14 837.043 Q2284.33 833.478 2280.7 833.478 M2280.7 829.775 Q2286.51 829.775 2289.57 834.381 Q2292.64 838.964 2292.64 847.714 Q2292.64 856.441 2289.57 861.048 Q2286.51 865.631 2280.7 865.631 Q2274.89 865.631 2271.81 861.048 Q2268.76 856.441 2268.76 847.714 Q2268.76 838.964 2271.81 834.381 Q2274.89 829.775 2280.7 829.775 Z" fill="#000000" fill-rule="evenodd" fill-opacity="1" /><path clip-path="url(#clip530)" d="M2300.86 859.08 L2305.75 859.08 L2305.75 864.96 L2300.86 864.96 L2300.86 859.08 Z" fill="#000000" fill-rule="evenodd" fill-opacity="1" /><path clip-path="url(#clip530)" d="M2328.78 834.474 L2316.97 852.923 L2328.78 852.923 L2328.78 834.474 M2327.55 830.4 L2333.43 830.4 L2333.43 852.923 L2338.36 852.923 L2338.36 856.812 L2333.43 856.812 L2333.43 864.96 L2328.78 864.96 L2328.78 856.812 L2313.18 856.812 L2313.18 852.298 L2327.55 830.4 Z" fill="#000000" fill-rule="evenodd" fill-opacity="1" /><path clip-path="url(#clip530)" d="M2280.7 674.264 Q2277.09 674.264 2275.26 677.829 Q2273.45 681.371 2273.45 688.5 Q2273.45 695.607 2275.26 699.171 Q2277.09 702.713 2280.7 702.713 Q2284.33 702.713 2286.14 699.171 Q2287.97 695.607 2287.97 688.5 Q2287.97 681.371 2286.14 677.829 Q2284.33 674.264 2280.7 674.264 M2280.7 670.56 Q2286.51 670.56 2289.57 675.167 Q2292.64 679.75 2292.64 688.5 Q2292.64 697.227 2289.57 701.833 Q2286.51 706.417 2280.7 706.417 Q2274.89 706.417 2271.81 701.833 Q2268.76 697.227 2268.76 688.5 Q2268.76 679.75 2271.81 675.167 Q2274.89 670.56 2280.7 670.56 Z" fill="#000000" fill-rule="evenodd" fill-opacity="1" /><path clip-path="url(#clip530)" d="M2300.86 699.866 L2305.75 699.866 L2305.75 705.745 L2300.86 705.745 L2300.86 699.866 Z" fill="#000000" fill-rule="evenodd" fill-opacity="1" /><path clip-path="url(#clip530)" d="M2315.98 671.185 L2334.33 671.185 L2334.33 675.121 L2320.26 675.121 L2320.26 683.593 Q2321.28 683.246 2322.3 683.084 Q2323.32 682.898 2324.33 682.898 Q2330.12 682.898 2333.5 686.07 Q2336.88 689.241 2336.88 694.658 Q2336.88 700.236 2333.41 703.338 Q2329.94 706.417 2323.62 706.417 Q2321.44 706.417 2319.17 706.046 Q2316.93 705.676 2314.52 704.935 L2314.52 700.236 Q2316.6 701.37 2318.83 701.926 Q2321.05 702.482 2323.52 702.482 Q2327.53 702.482 2329.87 700.375 Q2332.2 698.269 2332.2 694.658 Q2332.2 691.046 2329.87 688.94 Q2327.53 686.834 2323.52 686.834 Q2321.65 686.834 2319.77 687.25 Q2317.92 687.667 2315.98 688.546 L2315.98 671.185 Z" fill="#000000" fill-rule="evenodd" fill-opacity="1" /><path clip-path="url(#clip530)" d="M2280.7 515.05 Q2277.09 515.05 2275.26 518.615 Q2273.45 522.156 2273.45 529.286 Q2273.45 536.392 2275.26 539.957 Q2277.09 543.499 2280.7 543.499 Q2284.33 543.499 2286.14 539.957 Q2287.97 536.392 2287.97 529.286 Q2287.97 522.156 2286.14 518.615 Q2284.33 515.05 2280.7 515.05 M2280.7 511.346 Q2286.51 511.346 2289.57 515.953 Q2292.64 520.536 2292.64 529.286 Q2292.64 538.013 2289.57 542.619 Q2286.51 547.203 2280.7 547.203 Q2274.89 547.203 2271.81 542.619 Q2268.76 538.013 2268.76 529.286 Q2268.76 520.536 2271.81 515.953 Q2274.89 511.346 2280.7 511.346 Z" fill="#000000" fill-rule="evenodd" fill-opacity="1" /><path clip-path="url(#clip530)" d="M2300.86 540.652 L2305.75 540.652 L2305.75 546.531 L2300.86 546.531 L2300.86 540.652 Z" fill="#000000" fill-rule="evenodd" fill-opacity="1" /><path clip-path="url(#clip530)" d="M2326.51 527.388 Q2323.36 527.388 2321.51 529.541 Q2319.68 531.693 2319.68 535.443 Q2319.68 539.17 2321.51 541.346 Q2323.36 543.499 2326.51 543.499 Q2329.66 543.499 2331.49 541.346 Q2333.34 539.17 2333.34 535.443 Q2333.34 531.693 2331.49 529.541 Q2329.66 527.388 2326.51 527.388 M2335.79 512.735 L2335.79 516.994 Q2334.03 516.161 2332.23 515.721 Q2330.45 515.281 2328.69 515.281 Q2324.06 515.281 2321.6 518.406 Q2319.17 521.531 2318.83 527.851 Q2320.19 525.837 2322.25 524.772 Q2324.31 523.684 2326.79 523.684 Q2332 523.684 2335.01 526.855 Q2338.04 530.004 2338.04 535.443 Q2338.04 540.767 2334.89 543.985 Q2331.74 547.203 2326.51 547.203 Q2320.51 547.203 2317.34 542.619 Q2314.17 538.013 2314.17 529.286 Q2314.17 521.092 2318.06 516.23 Q2321.95 511.346 2328.5 511.346 Q2330.26 511.346 2332.04 511.693 Q2333.85 512.041 2335.79 512.735 Z" fill="#000000" fill-rule="evenodd" fill-opacity="1" /><path clip-path="url(#clip530)" d="M2280.7 355.836 Q2277.09 355.836 2275.26 359.401 Q2273.45 362.942 2273.45 370.072 Q2273.45 377.178 2275.26 380.743 Q2277.09 384.285 2280.7 384.285 Q2284.33 384.285 2286.14 380.743 Q2287.97 377.178 2287.97 370.072 Q2287.97 362.942 2286.14 359.401 Q2284.33 355.836 2280.7 355.836 M2280.7 352.132 Q2286.51 352.132 2289.57 356.739 Q2292.64 361.322 2292.64 370.072 Q2292.64 378.799 2289.57 383.405 Q2286.51 387.988 2280.7 387.988 Q2274.89 387.988 2271.81 383.405 Q2268.76 378.799 2268.76 370.072 Q2268.76 361.322 2271.81 356.739 Q2274.89 352.132 2280.7 352.132 Z" fill="#000000" fill-rule="evenodd" fill-opacity="1" /><path clip-path="url(#clip530)" d="M2300.86 381.437 L2305.75 381.437 L2305.75 387.317 L2300.86 387.317 L2300.86 381.437 Z" fill="#000000" fill-rule="evenodd" fill-opacity="1" /><path clip-path="url(#clip530)" d="M2314.75 352.757 L2336.97 352.757 L2336.97 354.748 L2324.43 387.317 L2319.54 387.317 L2331.35 356.692 L2314.75 356.692 L2314.75 352.757 Z" fill="#000000" fill-rule="evenodd" fill-opacity="1" /><path clip-path="url(#clip530)" d="M2280.7 196.622 Q2277.09 196.622 2275.26 200.186 Q2273.45 203.728 2273.45 210.858 Q2273.45 217.964 2275.26 221.529 Q2277.09 225.07 2280.7 225.07 Q2284.33 225.07 2286.14 221.529 Q2287.97 217.964 2287.97 210.858 Q2287.97 203.728 2286.14 200.186 Q2284.33 196.622 2280.7 196.622 M2280.7 192.918 Q2286.51 192.918 2289.57 197.524 Q2292.64 202.108 2292.64 210.858 Q2292.64 219.584 2289.57 224.191 Q2286.51 228.774 2280.7 228.774 Q2274.89 228.774 2271.81 224.191 Q2268.76 219.584 2268.76 210.858 Q2268.76 202.108 2271.81 197.524 Q2274.89 192.918 2280.7 192.918 Z" fill="#000000" fill-rule="evenodd" fill-opacity="1" /><path clip-path="url(#clip530)" d="M2300.86 222.223 L2305.75 222.223 L2305.75 228.103 L2300.86 228.103 L2300.86 222.223 Z" fill="#000000" fill-rule="evenodd" fill-opacity="1" /><path clip-path="url(#clip530)" d="M2325.93 211.691 Q2322.6 211.691 2320.68 213.473 Q2318.78 215.256 2318.78 218.381 Q2318.78 221.506 2320.68 223.288 Q2322.6 225.07 2325.93 225.07 Q2329.26 225.07 2331.19 223.288 Q2333.11 221.483 2333.11 218.381 Q2333.11 215.256 2331.19 213.473 Q2329.29 211.691 2325.93 211.691 M2321.26 209.7 Q2318.25 208.959 2316.56 206.899 Q2314.89 204.839 2314.89 201.876 Q2314.89 197.733 2317.83 195.325 Q2320.79 192.918 2325.93 192.918 Q2331.09 192.918 2334.03 195.325 Q2336.97 197.733 2336.97 201.876 Q2336.97 204.839 2335.28 206.899 Q2333.62 208.959 2330.63 209.7 Q2334.01 210.487 2335.89 212.779 Q2337.78 215.071 2337.78 218.381 Q2337.78 223.404 2334.7 226.089 Q2331.65 228.774 2325.93 228.774 Q2320.21 228.774 2317.14 226.089 Q2314.08 223.404 2314.08 218.381 Q2314.08 215.071 2315.98 212.779 Q2317.88 210.487 2321.26 209.7 M2319.54 202.316 Q2319.54 205.001 2321.21 206.506 Q2322.9 208.01 2325.93 208.01 Q2328.94 208.01 2330.63 206.506 Q2332.34 205.001 2332.34 202.316 Q2332.34 199.631 2330.63 198.126 Q2328.94 196.622 2325.93 196.622 Q2322.9 196.622 2321.21 198.126 Q2319.54 199.631 2319.54 202.316 Z" fill="#000000" fill-rule="evenodd" fill-opacity="1" /><polyline clip-path="url(#clip530)" style="stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none" points="
  2232.76,1423.18 2232.76,1328.95 2256.76,1328.95 2232.76,1328.95 2232.76,1169.74 2256.76,1169.74 2232.76,1169.74 2232.76,1010.52 2256.76,1010.52 2232.76,1010.52 
  2232.76,851.308 2256.76,851.308 2232.76,851.308 2232.76,692.094 2256.76,692.094 2232.76,692.094 2232.76,532.88 2256.76,532.88 2232.76,532.88 2232.76,373.666 
  2256.76,373.666 2232.76,373.666 2232.76,214.452 2256.76,214.452 2232.76,214.452 2232.76,123.472 
  "/>
</svg>
<p>While accuracies are a bit lower, the distribution of misclassification is similar, with many Jamanease cars misclassified as US ones (here we have also some EU cars misclassified as Japanease ones).</p><h3 id="Comparisons-with-Flux"><a class="docs-heading-anchor" href="#Comparisons-with-Flux">Comparisons with Flux</a><a id="Comparisons-with-Flux-1"></a><a class="docs-heading-anchor-permalink" href="#Comparisons-with-Flux" title="Permalink"></a></h3><p>As we did for Random Forests, we compare BetaML neural networks with the leading package for deep learning in Julia, <a href="https://github.com/FluxML/Flux.jl"><code>Flux.jl</code></a>.</p><p>In Flux the input must be in the form (fields, observations), so we transpose our original matrices</p><pre><code class="language-julia hljs">xtrainT, ytrain_ohT = transpose.([xtrain, ytrain_oh])
xtestT, ytest_ohT   = transpose.([xtest, ytest_oh])</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">2-element Vector{LinearAlgebra.Transpose{Float64, Matrix{Float64}}}:
 [-0.9370258544446618 0.8308089913068687 … 1.6506744270177232 -0.5783347263211628; 0.30679255888470214 -0.8627640505724731 … -0.27798574584388547 0.30679255888470214; … ; 0.10010898192256414 2.2430393858184523 … 1.564444757918087 -0.007037538272230526; 0.5552223059557987 1.0893935292213297 … 1.0893935292213297 -1.31437697547356]
 [0.0 0.0 … 0.0 1.0; 1.0 1.0 … 1.0 0.0; 0.0 0.0 … 0.0 0.0]</code></pre><p>We define the Flux neural network model in a similar way than BetaML and load it with data, we train it, predict and measure the accuracies on the training and the test sets:</p><p>We fix the random seed for Flux, altough you may still get different results depending on the number of threads used.. this is a problem we solve in BetaML with <a href="../../Utils.html#BetaML.Utils.generate_parallel_rngs-Tuple{Random.AbstractRNG, Integer}"><code>generate_parallel_rngs</code></a>.</p><pre><code class="language-julia hljs">Random.seed!(seed)

l1         = Flux.Dense(D,ls,Flux.relu)
l2         = Flux.Dense(ls,nCl,Flux.relu)
Flux_nn    = Flux.Chain(l1,l2)
fluxloss(x, y) = Flux.logitcrossentropy(Flux_nn(x), y)
ps         = Flux.params(Flux_nn)
nndata     = Flux.Data.DataLoader((xtrainT, ytrain_ohT),shuffle=true)
begin for i in 1:500  Flux.train!(fluxloss, ps, nndata, Flux.ADAM()) end end
ŷtrain     = Flux.onecold(Flux_nn(xtrainT),1:3)
ŷtest      = Flux.onecold(Flux_nn(xtestT),1:3)
trainAccuracy, testAccuracy   = accuracy.([ytrain,ytest],[ŷtrain,ŷtest])</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">2-element Vector{Float64}:
 0.9692307692307692
 0.7530864197530864</code></pre><pre><code class="language-julia hljs">push!(results,[&quot;NN (Flux.jl)&quot;,trainAccuracy,testAccuracy]);</code></pre><p>While the train accuracy is little bit higher that BetaML, the test accuracy remains comparable</p><h2 id="Perceptron-like-classifiers."><a class="docs-heading-anchor" href="#Perceptron-like-classifiers.">Perceptron-like classifiers.</a><a id="Perceptron-like-classifiers.-1"></a><a class="docs-heading-anchor-permalink" href="#Perceptron-like-classifiers." title="Permalink"></a></h2><p>We finaly test 3 &quot;perceptron-like&quot; classifiers, the &quot;classical&quot; Perceptron (<a href="../../Perceptron.html#BetaML.Perceptron.PerceptronClassifier"><code>PerceptronClassifier</code></a>), one of the first ML algorithms (a linear classifier), a &quot;kernellised&quot; version of it (<a href="../../Perceptron.html#BetaML.Perceptron.KernelPerceptronClassifier"><code>KernelPerceptronClassifier</code></a>, default to using the radial kernel) and &quot;Pegasos&quot; (<a href="../../Perceptron.html#BetaML.Perceptron.PegasosClassifier"><code>PegasosClassifier</code></a>) another linear algorithm that starts considering a gradient-based optimisation, altought without the regularisation term as in the Support Vector Machines (SVM).</p><p>As for the previous classifiers we construct the model object, we train and predict and we compute the train and test accuracies:</p><pre><code class="language-julia hljs">pm = PerceptronClassifier(rng=copy(AFIXEDRNG))
ŷtrain = fit!(pm, xtrain, ytrain)
ŷtest  = predict(pm, xtest)
(trainAccuracy,testAccuracy) = accuracy.([ytrain,ytest],[ŷtrain,ŷtest])
push!(results,[&quot;Perceptron&quot;,trainAccuracy,testAccuracy]);

kpm = KernelPerceptronClassifier(rng=copy(AFIXEDRNG))
ŷtrain = fit!(kpm, xtrain, ytrain)
ŷtest  = predict(kpm, xtest)
(trainAccuracy,testAccuracy) = accuracy.([ytrain,ytest],[ŷtrain,ŷtest])
push!(results,[&quot;KernelPerceptron&quot;,trainAccuracy,testAccuracy]);


pegm = PegasosClassifier(rng=copy(AFIXEDRNG))
ŷtrain = fit!(pegm, xtrain, ytrain)
ŷtest  = predict(pm, xtest)
(trainAccuracy,testAccuracy) = accuracy.([ytrain,ytest],[ŷtrain,ŷtest])
push!(results,[&quot;Pegasaus&quot;,trainAccuracy,testAccuracy]);</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">Running function BetaML.Perceptron.#perceptronBinary#8 at /home/runner/work/BetaML.jl/BetaML.jl/src/Perceptron/Perceptron_classic.jl:124
Type `]dev BetaML` to modify the source code (this would change its location on disk)
***
*** Training perceptron for maximum 1000 iterations. Random shuffle: false
Avg. error after iteration 1 : 0.2676923076923077
Avg. error after iteration 100 : 0.2
Avg. error after iteration 200 : 0.17846153846153845
Avg. error after iteration 300 : 0.20307692307692307
Avg. error after iteration 400 : 0.21846153846153846
Avg. error after iteration 500 : 0.20307692307692307
Avg. error after iteration 600 : 0.2
Avg. error after iteration 700 : 0.20307692307692307
Avg. error after iteration 800 : 0.22153846153846155
Avg. error after iteration 900 : 0.20615384615384616
Avg. error after iteration 1000 : 0.2
Running function BetaML.Perceptron.#perceptronBinary#8 at /home/runner/work/BetaML.jl/BetaML.jl/src/Perceptron/Perceptron_classic.jl:124
Type `]dev BetaML` to modify the source code (this would change its location on disk)
***
*** Training perceptron for maximum 1000 iterations. Random shuffle: false
Avg. error after iteration 1 : 0.2553846153846154
Avg. error after iteration 100 : 0.14153846153846153
Avg. error after iteration 200 : 0.13538461538461538
Avg. error after iteration 300 : 0.1476923076923077
Avg. error after iteration 400 : 0.14153846153846153
Avg. error after iteration 500 : 0.15076923076923077
Avg. error after iteration 600 : 0.1476923076923077
Avg. error after iteration 700 : 0.16615384615384615
Avg. error after iteration 800 : 0.16
Avg. error after iteration 900 : 0.1476923076923077
Avg. error after iteration 1000 : 0.13538461538461538
Running function BetaML.Perceptron.#perceptronBinary#8 at /home/runner/work/BetaML.jl/BetaML.jl/src/Perceptron/Perceptron_classic.jl:124
Type `]dev BetaML` to modify the source code (this would change its location on disk)
***
*** Training perceptron for maximum 1000 iterations. Random shuffle: false
Avg. error after iteration 1 : 0.24615384615384617
Avg. error after iteration 100 : 0.19076923076923077
Avg. error after iteration 200 : 0.18461538461538463
Avg. error after iteration 300 : 0.1753846153846154
Avg. error after iteration 400 : 0.16615384615384615
Avg. error after iteration 500 : 0.19692307692307692
Avg. error after iteration 600 : 0.19692307692307692
Avg. error after iteration 700 : 0.17846153846153845
Avg. error after iteration 800 : 0.17846153846153845
Avg. error after iteration 900 : 0.19384615384615383
Avg. error after iteration 1000 : 0.18769230769230769
Running function BetaML.Perceptron.#kernelPerceptronBinary#17 at /home/runner/work/BetaML.jl/BetaML.jl/src/Perceptron/Perceptron_kernel.jl:108
Type `]dev BetaML` to modify the source code (this would change its location on disk)
***
*** Training kernel perceptron for maximum 100 iterations. Random shuffle: false
Avg. error after iteration 1 : 0.1791044776119403
Training Kernel Perceptron...   4%|▉                     |  ETA: 0:00:14Training Kernel Perceptron...   8%|█▊                    |  ETA: 0:00:13Avg. error after iteration 10 : 0.08955223880597014
Training Kernel Perceptron...  12%|██▋                   |  ETA: 0:00:12Training Kernel Perceptron...  16%|███▌                  |  ETA: 0:00:12Avg. error after iteration 20 : 0.055970149253731345
Training Kernel Perceptron...  20%|████▍                 |  ETA: 0:00:11Training Kernel Perceptron...  24%|█████▎                |  ETA: 0:00:11Training Kernel Perceptron...  28%|██████▏               |  ETA: 0:00:10Avg. error after iteration 30 : 0.048507462686567165
Training Kernel Perceptron...  32%|███████               |  ETA: 0:00:10Training Kernel Perceptron...  36%|███████▉              |  ETA: 0:00:09Avg. error after iteration 40 : 0.03731343283582089
Training Kernel Perceptron...  40%|████████▊             |  ETA: 0:00:08Training Kernel Perceptron...  44%|█████████▋            |  ETA: 0:00:08Training Kernel Perceptron...  48%|██████████▌           |  ETA: 0:00:07Avg. error after iteration 50 : 0.029850746268656716
Training Kernel Perceptron...  52%|███████████▌          |  ETA: 0:00:07Training Kernel Perceptron...  56%|████████████▍         |  ETA: 0:00:06Avg. error after iteration 60 : 0.03731343283582089
Training Kernel Perceptron...  60%|█████████████▎        |  ETA: 0:00:06Training Kernel Perceptron...  64%|██████████████▏       |  ETA: 0:00:05Training Kernel Perceptron...  68%|███████████████       |  ETA: 0:00:04Avg. error after iteration 70 : 0.029850746268656716
Training Kernel Perceptron...  72%|███████████████▉      |  ETA: 0:00:04Training Kernel Perceptron...  76%|████████████████▊     |  ETA: 0:00:03Avg. error after iteration 80 : 0.03731343283582089
Training Kernel Perceptron...  80%|█████████████████▋    |  ETA: 0:00:03Training Kernel Perceptron...  84%|██████████████████▌   |  ETA: 0:00:02Training Kernel Perceptron...  88%|███████████████████▍  |  ETA: 0:00:02Avg. error after iteration 90 : 0.022388059701492536
Training Kernel Perceptron...  92%|████████████████████▎ |  ETA: 0:00:01Training Kernel Perceptron...  96%|█████████████████████▏|  ETA: 0:00:01Avg. error after iteration 100 : 0.029850746268656716
Training Kernel Perceptron... 100%|██████████████████████| Time: 0:00:13
Running function BetaML.Perceptron.#kernelPerceptronBinary#17 at /home/runner/work/BetaML.jl/BetaML.jl/src/Perceptron/Perceptron_kernel.jl:108
Type `]dev BetaML` to modify the source code (this would change its location on disk)
***
*** Training kernel perceptron for maximum 100 iterations. Random shuffle: false
Avg. error after iteration 1 : 0.35833333333333334
Avg. error after iteration 10 : 0.11666666666666667
Training Kernel Perceptron...  18%|████                  |  ETA: 0:00:02Avg. error after iteration 20 : 0.15
Avg. error after iteration 30 : 0.025
Training Kernel Perceptron...  36%|███████▉              |  ETA: 0:00:02Avg. error after iteration 40 : 0.1
Avg. error after iteration 50 : 0.075
Training Kernel Perceptron...  54%|███████████▉          |  ETA: 0:00:01Avg. error after iteration 60 : 0.10833333333333334
Avg. error after iteration 70 : 0.025
Training Kernel Perceptron...  72%|███████████████▉      |  ETA: 0:00:01Avg. error after iteration 80 : 0.03333333333333333
*** Avg. error after epoch 88 : 0.0 (all elements of the set has been correctly classified
Training Kernel Perceptron... 100%|██████████████████████| Time: 0:00:02
Running function BetaML.Perceptron.#kernelPerceptronBinary#17 at /home/runner/work/BetaML.jl/BetaML.jl/src/Perceptron/Perceptron_kernel.jl:108
Type `]dev BetaML` to modify the source code (this would change its location on disk)
***
*** Training kernel perceptron for maximum 100 iterations. Random shuffle: false
Avg. error after iteration 1 : 0.2099236641221374
Training Kernel Perceptron...   4%|▉                     |  ETA: 0:00:13Training Kernel Perceptron...   8%|█▊                    |  ETA: 0:00:12Avg. error after iteration 10 : 0.03816793893129771
Training Kernel Perceptron...  12%|██▋                   |  ETA: 0:00:12Training Kernel Perceptron...  16%|███▌                  |  ETA: 0:00:11Avg. error after iteration 20 : 0.026717557251908396
Training Kernel Perceptron...  20%|████▍                 |  ETA: 0:00:11Training Kernel Perceptron...  24%|█████▎                |  ETA: 0:00:10*** Avg. error after epoch 28 : 0.0 (all elements of the set has been correctly classified
Training Kernel Perceptron... 100%|██████████████████████| Time: 0:00:03
Running function BetaML.Perceptron.#pegasosBinary#42 at /home/runner/work/BetaML.jl/BetaML.jl/src/Perceptron/Perceptron_pegasos.jl:125
Type `]dev BetaML` to modify the source code (this would change its location on disk)
***
*** Training pegasos for maximum 1000 iterations. Random shuffle: false
Avg. error after iteration 1 : 0.28923076923076924
Avg. error after iteration 100 : 0.2523076923076923
Avg. error after iteration 200 : 0.2523076923076923
Avg. error after iteration 300 : 0.26461538461538464
Avg. error after iteration 400 : 0.2676923076923077
Avg. error after iteration 500 : 0.26461538461538464
Avg. error after iteration 600 : 0.26461538461538464
Avg. error after iteration 700 : 0.2553846153846154
Avg. error after iteration 800 : 0.24615384615384617
Avg. error after iteration 900 : 0.26461538461538464
Avg. error after iteration 1000 : 0.26461538461538464
Running function BetaML.Perceptron.#pegasosBinary#42 at /home/runner/work/BetaML.jl/BetaML.jl/src/Perceptron/Perceptron_pegasos.jl:125
Type `]dev BetaML` to modify the source code (this would change its location on disk)
***
*** Training pegasos for maximum 1000 iterations. Random shuffle: false
Avg. error after iteration 1 : 0.38461538461538464
Avg. error after iteration 100 : 0.3076923076923077
Avg. error after iteration 200 : 0.27692307692307694
Avg. error after iteration 300 : 0.27692307692307694
Avg. error after iteration 400 : 0.24
Avg. error after iteration 500 : 0.28307692307692306
Avg. error after iteration 600 : 0.27692307692307694
Avg. error after iteration 700 : 0.28307692307692306
Avg. error after iteration 800 : 0.27692307692307694
Avg. error after iteration 900 : 0.28307692307692306
Avg. error after iteration 1000 : 0.27692307692307694
Running function BetaML.Perceptron.#pegasosBinary#42 at /home/runner/work/BetaML.jl/BetaML.jl/src/Perceptron/Perceptron_pegasos.jl:125
Type `]dev BetaML` to modify the source code (this would change its location on disk)
***
*** Training pegasos for maximum 1000 iterations. Random shuffle: false
Avg. error after iteration 1 : 0.28307692307692306
Avg. error after iteration 100 : 0.2276923076923077
Avg. error after iteration 200 : 0.23692307692307693
Avg. error after iteration 300 : 0.23076923076923078
Avg. error after iteration 400 : 0.22153846153846155
Avg. error after iteration 500 : 0.2523076923076923
Avg. error after iteration 600 : 0.2276923076923077
Avg. error after iteration 700 : 0.2523076923076923
Avg. error after iteration 800 : 0.24
Avg. error after iteration 900 : 0.24923076923076923
Avg. error after iteration 1000 : 0.24307692307692308</code></pre><h2 id="Summary"><a class="docs-heading-anchor" href="#Summary">Summary</a><a id="Summary-1"></a><a class="docs-heading-anchor-permalink" href="#Summary" title="Permalink"></a></h2><p>This is the summary of the results we had trying to predict the country of origin of the cars, based on their technical characteristics:</p><pre><code class="language-julia hljs">println(results)</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">7×3 DataFrame
 Row │ model                  train_acc  test_acc
     │ String                 Float64    Float64
─────┼────────────────────────────────────────────
   1 │ RF                      0.996923  0.765432
   2 │ RF (DecisionTrees.jl)   0.981538  0.728395
   3 │ NN                      0.886154  0.728395
   4 │ NN (Flux.jl)            0.969231  0.753086
   5 │ Perceptron              0.778462  0.703704
   6 │ KernelPerceptron        0.987692  0.703704
   7 │ Pegasaus                0.732308  0.703704</code></pre><p>If you clone BetaML repository</p><p>Model accuracies on my machine with seedd 123, 1000 and 10000 respectivelly</p><table><tr><th style="text-align: right">model</th><th style="text-align: right">train 1</th><th style="text-align: right">test 1</th><th style="text-align: right">train 2</th><th style="text-align: right">test 2</th><th style="text-align: right">train 3</th><th style="text-align: right">test 3</th></tr><tr><td style="text-align: right">RF</td><td style="text-align: right">0.996923</td><td style="text-align: right">0.765432</td><td style="text-align: right">1.000000</td><td style="text-align: right">0.802469</td><td style="text-align: right">1.000000</td><td style="text-align: right">0.888889</td></tr><tr><td style="text-align: right">RF (DecisionTrees.jl)</td><td style="text-align: right">0.975385</td><td style="text-align: right">0.765432</td><td style="text-align: right">0.984615</td><td style="text-align: right">0.777778</td><td style="text-align: right">0.975385</td><td style="text-align: right">0.864198</td></tr><tr><td style="text-align: right">NN</td><td style="text-align: right">0.886154</td><td style="text-align: right">0.728395</td><td style="text-align: right">0.916923</td><td style="text-align: right">0.827160</td><td style="text-align: right">0.895385</td><td style="text-align: right">0.876543</td></tr><tr><td style="text-align: right">│ NN (Flux.jl)</td><td style="text-align: right">0.793846</td><td style="text-align: right">0.654321</td><td style="text-align: right">0.938462</td><td style="text-align: right">0.790123</td><td style="text-align: right">0.935385</td><td style="text-align: right">0.851852</td></tr><tr><td style="text-align: right">│ Perceptron</td><td style="text-align: right">0.778462</td><td style="text-align: right">0.703704</td><td style="text-align: right">0.720000</td><td style="text-align: right">0.753086</td><td style="text-align: right">0.670769</td><td style="text-align: right">0.654321</td></tr><tr><td style="text-align: right">│ KernelPerceptron</td><td style="text-align: right">0.987692</td><td style="text-align: right">0.703704</td><td style="text-align: right">0.978462</td><td style="text-align: right">0.777778</td><td style="text-align: right">0.944615</td><td style="text-align: right">0.827160</td></tr><tr><td style="text-align: right">│ Pegasaus</td><td style="text-align: right">0.732308</td><td style="text-align: right">0.703704</td><td style="text-align: right">0.633846</td><td style="text-align: right">0.753086</td><td style="text-align: right">0.575385</td><td style="text-align: right">0.654321</td></tr></table><p>We warn that this table just provides a rought idea of the various algorithms performances. Indeed there is a large amount of stochasticity both in the sampling of the data used for training/testing and in the initial settings of the parameters of the algorithm. For a statistically significant comparision we would have to repeat the analysis with multiple sampling (e.g. by cross-validation, see the <a href="../Clustering - Iris/betaml_tutorial_cluster_iris.html#clustering_tutorial">clustering tutorial</a> for an example) and initial random parameters.</p><p>Neverthless the table above shows that, when we compare BetaML with the algorithm-specific leading packages, we found similar results in terms of accuracy, but often the leading packages are better optimised and run more efficiently (but sometimes at the cost of being less verstatile). Also, for this dataset, Random Forests seems to remain marginally more accurate than Neural Network, altought of course this depends on the hyper-parameters and, with a single run of the models, we don&#39;t know if this difference is significant.</p><p><a href="https://github.com/sylvaticus/BetaML.jl/blob/master/docs/src/tutorials/Classification - cars/betaml_tutorial_classification_cars.jl">View this file on Github</a>.</p><hr/><p><em>This page was generated using <a href="https://github.com/fredrikekre/Literate.jl">Literate.jl</a>.</em></p></article><nav class="docs-footer"><a class="docs-footer-prevpage" href="../Betaml_tutorial_getting_started.html">« Getting started</a><a class="docs-footer-nextpage" href="../Regression - bike sharing/betaml_tutorial_regression_sharingBikes.html">A regression task: the prediction of  bike  sharing demand »</a><div class="flexbox-break"></div><p class="footer-message">Powered by <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> and the <a href="https://julialang.org/">Julia Programming Language</a>.</p></nav></div><div class="modal" id="documenter-settings"><div class="modal-background"></div><div class="modal-card"><header class="modal-card-head"><p class="modal-card-title">Settings</p><button class="delete"></button></header><section class="modal-card-body"><p><label class="label">Theme</label><div class="select"><select id="documenter-themepicker"><option value="documenter-light">documenter-light</option><option value="documenter-dark">documenter-dark</option></select></div></p><hr/><p>This document was generated with <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> version 0.27.23 on <span class="colophon-date" title="Tuesday 4 October 2022 15:49">Tuesday 4 October 2022</span>. Using Julia version 1.6.7.</p></section><footer class="modal-card-foot"></footer></div></div></div></body></html>
