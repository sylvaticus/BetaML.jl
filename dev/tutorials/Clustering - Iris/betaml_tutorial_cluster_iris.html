<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>A clustering task: the prediction of  plant species from floreal measures (the iris dataset) · BetaML.jl Documentation</title><script async src="https://www.googletagmanager.com/gtag/js?id=G-JYKX8QY5JW"></script><script>  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());
  gtag('config', 'G-JYKX8QY5JW', {'page_path': location.pathname + location.search + location.hash});
</script><script data-outdated-warner src="../../assets/warner.js"></script><link href="https://cdnjs.cloudflare.com/ajax/libs/lato-font/3.0.0/css/lato-font.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/juliamono/0.045/juliamono.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.4/css/fontawesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.4/css/solid.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.4/css/brands.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.24/katex.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL="../.."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" data-main="../../assets/documenter.js"></script><script src="../../siteinfo.js"></script><script src="../../../versions.js"></script><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/documenter-dark.css" data-theme-name="documenter-dark" data-theme-primary-dark/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/documenter-light.css" data-theme-name="documenter-light" data-theme-primary/><script src="../../assets/themeswap.js"></script></head><body><div id="documenter"><nav class="docs-sidebar"><div class="docs-package-name"><span class="docs-autofit"><a href="../../index.html">BetaML.jl Documentation</a></span></div><form class="docs-search" action="../../search.html"><input class="docs-search-query" id="documenter-search-query" name="q" type="text" placeholder="Search docs"/></form><ul class="docs-menu"><li><a class="tocitem" href="../../index.html">Index</a></li><li><span class="tocitem">Tutorial</span><ul><li><a class="tocitem" href="../Betaml_tutorial_getting_started.html">Getting started</a></li><li><input class="collapse-toggle" id="menuitem-2-2" type="checkbox"/><label class="tocitem" for="menuitem-2-2"><span class="docs-label">Classification - cars</span><i class="docs-chevron"></i></label><ul class="collapsed"><li><a class="tocitem" href="../Classification - cars/betaml_tutorial_classification_cars.html">A classification task when labels are known - determining the country of origin of cars given the cars characteristics</a></li></ul></li><li><input class="collapse-toggle" id="menuitem-2-3" type="checkbox"/><label class="tocitem" for="menuitem-2-3"><span class="docs-label">Regression - bike sharing</span><i class="docs-chevron"></i></label><ul class="collapsed"><li><a class="tocitem" href="../Regression - bike sharing/betaml_tutorial_regression_sharingBikes.html">A regression task: the prediction of  bike  sharing demand</a></li></ul></li><li><input class="collapse-toggle" id="menuitem-2-4" type="checkbox" checked/><label class="tocitem" for="menuitem-2-4"><span class="docs-label">Clustering - Iris</span><i class="docs-chevron"></i></label><ul class="collapsed"><li class="is-active"><a class="tocitem" href="betaml_tutorial_cluster_iris.html">A clustering task: the prediction of  plant species from floreal measures (the iris dataset)</a><ul class="internal"><li><a class="tocitem" href="#Library-and-data-loading"><span>Library and data loading</span></a></li><li><a class="tocitem" href="#Data-preparation"><span>Data preparation</span></a></li><li><a class="tocitem" href="#Main-analysis"><span>Main analysis</span></a></li><li><a class="tocitem" href="#Working-without-the-labels"><span>Working without the labels</span></a></li><li><a class="tocitem" href="#Conclusions"><span>Conclusions</span></a></li></ul></li></ul></li><li><input class="collapse-toggle" id="menuitem-2-5" type="checkbox"/><label class="tocitem" for="menuitem-2-5"><span class="docs-label">Multi-branch neural network</span><i class="docs-chevron"></i></label><ul class="collapsed"><li><a class="tocitem" href="../Multi-branch neural network/betaml_tutorial_multibranch_nn.html">A deep neural network with multi-branch architecture</a></li></ul></li></ul></li><li><span class="tocitem">API (Reference manual)</span><ul><li><input class="collapse-toggle" id="menuitem-3-1" type="checkbox"/><label class="tocitem" for="menuitem-3-1"><span class="docs-label">API V2 (current)</span><i class="docs-chevron"></i></label><ul class="collapsed"><li><a class="tocitem" href="../../Api_v2_user.html">Introduction for user</a></li><li><input class="collapse-toggle" id="menuitem-3-1-2" type="checkbox"/><label class="tocitem" for="menuitem-3-1-2"><span class="docs-label">For developers</span><i class="docs-chevron"></i></label><ul class="collapsed"><li><a class="tocitem" href="../../Api_v2_developer.html">API implementation</a></li><li><a class="tocitem" href="../../StyleGuide_templates.html">Style guide</a></li></ul></li><li><a class="tocitem" href="../../Api.html">The Api module</a></li></ul></li><li><a class="tocitem" href="../../Perceptron.html">Perceptron</a></li><li><a class="tocitem" href="../../Trees.html">Trees</a></li><li><a class="tocitem" href="../../Nn.html">Nn</a></li><li><a class="tocitem" href="../../Clustering.html">Clustering</a></li><li><a class="tocitem" href="../../GMM.html">GMM</a></li><li><a class="tocitem" href="../../Imputation.html">Imputation</a></li><li><a class="tocitem" href="../../Utils.html">Utils</a></li></ul></li></ul><div class="docs-version-selector field has-addons"><div class="control"><span class="docs-label button is-static is-size-7">Version</span></div><div class="docs-selector control is-expanded"><div class="select is-fullwidth is-size-7"><select id="documenter-version-selector"></select></div></div></div></nav><div class="docs-main"><header class="docs-navbar"><nav class="breadcrumb"><ul class="is-hidden-mobile"><li><a class="is-disabled">Tutorial</a></li><li><a class="is-disabled">Clustering - Iris</a></li><li class="is-active"><a href="betaml_tutorial_cluster_iris.html">A clustering task: the prediction of  plant species from floreal measures (the iris dataset)</a></li></ul><ul class="is-hidden-tablet"><li class="is-active"><a href="betaml_tutorial_cluster_iris.html">A clustering task: the prediction of  plant species from floreal measures (the iris dataset)</a></li></ul></nav><div class="docs-right"><a class="docs-edit-link" href="https://github.com/sylvaticus/BetaML.jl/blob/master/docs/src/tutorials/Clustering - Iris/betaml_tutorial_cluster_iris.jl" title="Edit on GitHub"><span class="docs-icon fab"></span><span class="docs-label is-hidden-touch">Edit on GitHub</span></a><a class="docs-settings-button fas fa-cog" id="documenter-settings-button" href="#" title="Settings"></a><a class="docs-sidebar-button fa fa-bars is-hidden-desktop" id="documenter-sidebar-button" href="#"></a></div></header><article class="content" id="documenter-page"><h1 id="clustering_tutorial"><a class="docs-heading-anchor" href="#clustering_tutorial">A clustering task: the prediction of  plant species from floreal measures (the iris dataset)</a><a id="clustering_tutorial-1"></a><a class="docs-heading-anchor-permalink" href="#clustering_tutorial" title="Permalink"></a></h1><p>The task is to estimate the species of a plant given some floreal measurements. It use the classical &quot;Iris&quot; dataset. Note that in this example we are using clustering approaches, so we try to understand the &quot;structure&quot; of our data, without relying to actually knowing the true labels (&quot;classes&quot; or &quot;factors&quot;). However we have chosen a dataset for which the true labels are actually known, so we can compare the accuracy of the algorithms we use, but these labels will not be used during the algorithms training.</p><p>Data origin:</p><ul><li>dataset description: <a href="https://en.wikipedia.org/wiki/Iris_flower_data_set">https://en.wikipedia.org/wiki/Iris<em>flower</em>data_set</a></li><li>data source we use here: <a href="https://github.com/JuliaStats/RDatasets.jl">https://github.com/JuliaStats/RDatasets.jl</a></li></ul><h2 id="Library-and-data-loading"><a class="docs-heading-anchor" href="#Library-and-data-loading">Library and data loading</a><a id="Library-and-data-loading-1"></a><a class="docs-heading-anchor-permalink" href="#Library-and-data-loading" title="Permalink"></a></h2><p>Activating the local environment specific to BetaML documentation</p><pre><code class="language-julia hljs">using Pkg
Pkg.activate(joinpath(@__DIR__,&quot;..&quot;,&quot;..&quot;,&quot;..&quot;))</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">  Activating environment at `~/work/BetaML.jl/BetaML.jl/docs/Project.toml`</code></pre><p>We load the Beta Machine Learning Toolkit as well as some other packages that we use in this tutorial</p><pre><code class="language-julia hljs">using BetaML
using Random, Statistics, Logging, BenchmarkTools, StableRNGs, RDatasets, Plots, DataFrames</code></pre><p>We are also going to compare our results with two other leading packages in Julia for clustering analysis, <a href="https://github.com/JuliaStats/Clustering.jl"><code>Clustering.jl</code></a> that provides (inter alia) kmeans and kmedoids algorithms and <a href="https://github.com/davidavdav/GaussianMixtures.jl"><code>GaussianMixtures.jl</code></a> that provides, as the name says, Gaussian Mixture Models. So we import them (we &quot;import&quot; them, rather than &quot;use&quot;, not to bound their full names into namespace as some would collide with BetaML).</p><pre><code class="language-julia hljs">import Clustering, GaussianMixtures</code></pre><p>Here we are explicit and we use our own fixed RNG:</p><pre><code class="language-julia hljs">seed = 123 # The table at the end of this tutorial has been obtained with seeds 123, 1000 and 10000
AFIXEDRNG = StableRNG(seed)</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">StableRNGs.LehmerRNG(state=0x000000000000000000000000000000f7)</code></pre><p>We do a few tweeks for the Clustering and GaussianMixtures packages. Note that in BetaML we can also control both the random seed and the verbosity in the algorithm call, not only globally</p><pre><code class="language-julia hljs">Random.seed!(seed)
#logger  = Logging.SimpleLogger(stdout, Logging.Error); global_logger(logger); ## For suppressing GaussianMixtures output</code></pre><p>Differently from the <a href="../Regression - bike sharing/betaml_tutorial_regression_sharingBikes.html#regression_tutorial">regression tutorial</a>, we load the data here from [<code>RDatasets</code>](https://github.com/JuliaStats/RDatasets.jl](https://github.com/JuliaStats/RDatasets.jl), a package providing standard datasets.</p><pre><code class="language-julia hljs">iris = dataset(&quot;datasets&quot;, &quot;iris&quot;)
describe(iris)</code></pre><div><div style = "float: left;"><span>5×7 DataFrame</span></div><div style = "clear: both;"></div></div><div class = "data-frame" style = "overflow-x: scroll;"><table class = "data-frame" style = "margin-bottom: 6px;"><thead><tr class = "header"><th class = "rowNumber" style = "font-weight: bold; text-align: right;">Row</th><th style = "text-align: left;">variable</th><th style = "text-align: left;">mean</th><th style = "text-align: left;">min</th><th style = "text-align: left;">median</th><th style = "text-align: left;">max</th><th style = "text-align: left;">nmissing</th><th style = "text-align: left;">eltype</th></tr><tr class = "subheader headerLastRow"><th class = "rowNumber" style = "font-weight: bold; text-align: right;"></th><th title = "Symbol" style = "text-align: left;">Symbol</th><th title = "Union{Nothing, Float64}" style = "text-align: left;">Union…</th><th title = "Any" style = "text-align: left;">Any</th><th title = "Union{Nothing, Float64}" style = "text-align: left;">Union…</th><th title = "Any" style = "text-align: left;">Any</th><th title = "Int64" style = "text-align: left;">Int64</th><th title = "DataType" style = "text-align: left;">DataType</th></tr></thead><tbody><tr><td class = "rowNumber" style = "font-weight: bold; text-align: right;">1</td><td style = "text-align: left;">SepalLength</td><td style = "text-align: left;">5.84333</td><td style = "text-align: left;">4.3</td><td style = "text-align: left;">5.8</td><td style = "text-align: left;">7.9</td><td style = "text-align: right;">0</td><td style = "text-align: left;">Float64</td></tr><tr><td class = "rowNumber" style = "font-weight: bold; text-align: right;">2</td><td style = "text-align: left;">SepalWidth</td><td style = "text-align: left;">3.05733</td><td style = "text-align: left;">2.0</td><td style = "text-align: left;">3.0</td><td style = "text-align: left;">4.4</td><td style = "text-align: right;">0</td><td style = "text-align: left;">Float64</td></tr><tr><td class = "rowNumber" style = "font-weight: bold; text-align: right;">3</td><td style = "text-align: left;">PetalLength</td><td style = "text-align: left;">3.758</td><td style = "text-align: left;">1.0</td><td style = "text-align: left;">4.35</td><td style = "text-align: left;">6.9</td><td style = "text-align: right;">0</td><td style = "text-align: left;">Float64</td></tr><tr><td class = "rowNumber" style = "font-weight: bold; text-align: right;">4</td><td style = "text-align: left;">PetalWidth</td><td style = "text-align: left;">1.19933</td><td style = "text-align: left;">0.1</td><td style = "text-align: left;">1.3</td><td style = "text-align: left;">2.5</td><td style = "text-align: right;">0</td><td style = "text-align: left;">Float64</td></tr><tr><td class = "rowNumber" style = "font-weight: bold; text-align: right;">5</td><td style = "text-align: left;">Species</td><td style = "font-style: italic; text-align: left;"></td><td style = "text-align: left;">setosa</td><td style = "font-style: italic; text-align: left;"></td><td style = "text-align: left;">virginica</td><td style = "text-align: right;">0</td><td style = "text-align: left;">CategoricalValue{String, UInt8}</td></tr></tbody></table></div><p>The iris dataset  provides floreal measures in columns 1 to 4 and the assigned species name in column 5. There are no missing values</p><h2 id="Data-preparation"><a class="docs-heading-anchor" href="#Data-preparation">Data preparation</a><a id="Data-preparation-1"></a><a class="docs-heading-anchor-permalink" href="#Data-preparation" title="Permalink"></a></h2><p>The first step is to prepare the data for the analysis. We collect the first 4 columns as our <em>feature</em> <code>x</code> matrix and the last one as our <code>y</code> label vector. As we are using clustering algorithms, we are not actually using the labels to train the algorithms, we&#39;ll behave like we do not know them, we&#39;ll just let the algorithm &quot;learn&quot; from the structure of the data itself. We&#39;ll however use it to judge the accuracy that the various algorithms reach.</p><pre><code class="language-julia hljs">x       = Matrix{Float64}(iris[:,1:4]);
yLabels = unique(iris[:,5])</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">3-element Vector{String}:
 &quot;setosa&quot;
 &quot;versicolor&quot;
 &quot;virginica&quot;</code></pre><p>As the labels are expressed as strings, the first thing we do is encode them as integers for our analysis using the <a href="../../Utils.html#BetaML.Utils.OrdinalEncoder"><code>OrdinalEncoder</code></a> model (data isn&#39;t really needed to be actually ordered):</p><pre><code class="language-julia hljs">y  = fit!(OrdinalEncoder(categories=yLabels),iris[:,5])</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">150-element Vector{Int64}:
 1
 1
 1
 1
 1
 1
 1
 1
 1
 1
 ⋮
 3
 3
 3
 3
 3
 3
 3
 3
 3</code></pre><p>The dataset from RDatasets is ordered by species, so we need to shuffle it to avoid biases. Shuffling happens by default in cross_validation, but we are keeping here a copy of the shuffled version for later. Note that the version of <a href="tutorials/Clustering - Iris/@ref"><code>shuffle</code></a> that is included in BetaML accepts several n-dimensional arrays and shuffle them (by default on rows, by we can specify the dimension) keeping the association  between the various arrays in the shuffled output.</p><pre><code class="language-julia hljs">(xs,ys) = consistent_shuffle([x,y], rng=copy(AFIXEDRNG));</code></pre><h2 id="Main-analysis"><a class="docs-heading-anchor" href="#Main-analysis">Main analysis</a><a id="Main-analysis-1"></a><a class="docs-heading-anchor-permalink" href="#Main-analysis" title="Permalink"></a></h2><p>We will try 3 BetaML models (<a href="../../Clustering.html#BetaML.Clustering.KMeansClusterer"><code>KMeansClusterer</code></a>, <a href="../../Clustering.html#BetaML.Clustering.KMedoidsClusterer"><code>KMedoidsClusterer</code></a> and <a href="../../GMM.html#BetaML.GMM.GMMClusterer"><code>GMMClusterer</code></a>) and we compare them with <code>kmeans</code> from Clusterings.jl and <code>GMM</code> from GaussianMixtures.jl</p><p><code>KMeansClusterer</code> and <code>KMedoidsClusterer</code> works by first initialising the centers of the k-clusters (step a ). These centers, also known as the &quot;representatives&quot;, must be selected within the data for kmedoids, while for kmeans they are the geometrical centers.</p><p>Then ( step b ) the algorithms iterates toward each point to assign the point to the cluster of the closest representative (according with a user defined distance metric, default to Euclidean), and ( step c ) moves each representative at the center of its newly acquired cluster (where &quot;center&quot; depends again from the metric).</p><p>Steps <em>b</em> and <em>c</em> are reiterated until the algorithm converge, i.e. the tentative k representative points (and their relative clusters) don&#39;t move any more. The result (output of the algorithm) is that each point is assigned to one of the clusters (classes).</p><p>The algorithm in <code>GMMClusterer</code> is similar in that it employs an iterative approach (the Expectation<em>Minimisation algorithm, &quot;em&quot;) but here we make the hipothesis that the data points are the observed outcomes of some _mixture</em> probabilistic models where we have first a k-categorical variables whose outcomes are the (unobservble) parameters of a probabilistic distribution from which the data is finally drawn. Because the parameters of each of the k-possible distributions is unobservable this is also called a model with latent variables.</p><p>Most <code>gmm</code> models use the Gaussain distribution as the family of the mixture components, so we can tought the <code>gmm</code> acronym to indicate <em>Gaussian Mixture Model</em>. In BetaML we have currently implemented only Gaussain components, but any distribution could be used by just subclassing <code>AbstractMixture</code> and implementing a couple of methids (you are invited to contribute or just ask for a distribution family you are interested), so I prefer to think &quot;gmm&quot; as an acronym for <em>Generative Mixture Model</em>.</p><p>The algorithm tries to find the mixture that maximises the likelihood that the data has been generated indeed from such mixture, where the &quot;E&quot; step refers to computing the probability that each point belongs to each of the k-composants (somehow similar to the step <em>b</em> in the kmeans/kmedoids algorithms), and the &quot;M&quot; step estimates, giving the association probabilities in step &quot;E&quot;, the parameters of the mixture and of the individual components (similar to step <em>c</em>).</p><p>The result here is that each point has a categorical distribution (PMF) representing the probabilities that it belongs to any of the k-components (our classes or clusters). This is interesting, as <code>gmm</code> can be used for many other things that clustering. It forms the backbone of the <a href="../../Imputation.html#BetaML.Imputation.GMMImputer"><code>GMMImputer</code></a> model to impute missing values (on some or all dimensions) based to how close the record seems to its pears. For the same reasons, <code>GMMImputer</code> can also be used to predict user&#39;s behaviours (or users&#39; appreciation) according to the behaviour/ranking made by pears (&quot;collaborative filtering&quot;).</p><p>While the result of <code>GMMClusterer</code> is a vector of PMFs (one for each record), error measures and reports with the true values (if known) can be directly applied, as in BetaML they internally call <code>mode()</code> to retrieve the class with the highest probability for each record.</p><p>As we are here, we also try different versions of the BetaML models, even if the default &quot;versions&quot; should be fine. For <code>KMeansClusterer</code> and <code>KMedoidsClusterer</code> we will try different initialisation strategies (&quot;gird&quot;, the default one, &quot;random&quot; and &quot;shuffle&quot;), while for the <code>GMMClusterer</code> model we&#39;ll choose different distributions of the Gaussain family (<code>SphericalGaussian</code> - where the variance is a scalar, <code>DiagonalGaussian</code> - with a vector variance, and <code>FullGaussian</code>, where the covariance is a matrix).</p><p>As the result would depend on stochasticity both in the data selected and in the random initialisation, we use a cross-validation approach to run our models several times (with different data) and then we average their results. Cross-Validation in BetaML is very flexible and it is done using the <a href="../../Utils.html#BetaML.Utils.cross_validation"><code>cross_validation</code></a> function. It is used by default for hyperparameters autotuning of the BetaML supervised models. <code>cross_validation</code> works by calling the function <code>f</code>, defined by the user, passing to it the tuple <code>trainData</code>, <code>valData</code> and <code>rng</code> and collecting the result of the function f. The specific method for which <code>trainData</code>, and <code>valData</code> are selected at each iteration depends on the specific <code>sampler</code>.</p><p>We start by selectign a k-fold sampler that split our data in 5 different parts, it uses 4 for training and 1 part (not used here) for validation. We run the simulations twice and, to be sure to have replicable results, we fix the random seed (at the whole crossValidaiton level, not on each iteration).</p><pre><code class="language-julia hljs">sampler = KFold(nsplits=5,nrepeats=3,shuffle=true, rng=copy(AFIXEDRNG))</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">KFold(5, 3, true, StableRNGs.LehmerRNG(state=0x000000000000000000000000000000f7))</code></pre><p>We can now run the cross-validation with our models. Note that instead of defining the function <code>f</code> and then calling <code>cross_validation[f(trainData,testData,rng),[x,y],...)</code> we use the Julia <code>do</code> block syntax and we write directly the content of the <code>f</code> function in the <code>do</code> block. Also, by default cross<em>validation already returns the mean and the standard deviation of the output of the user-provided <code>f</code> function (or the <code>do</code> block). However this requires that the <code>f</code> function returns a single scalar. Here we are returning a vector of the accuracies of the different models (so we can run the cross-validation only once), and hence we indicate with `return</em>statistics=false` to cross_validation not to attempt to generate statistics but rather report the whole output. We&#39;ll compute the statistics ex-post.</p><p>Inside the <code>do</code> block we do 4 things:</p><ul><li>we recover from <code>trainData</code> (a tuple, as we passed a tuple to <code>cross_validation</code> too) the <code>xtrain</code> features and <code>ytrain</code> labels;</li><li>we run the various clustering algorithms</li><li>we use the real labels to compute the model accuracy. Note that the clustering algorithm know nothing about the specific label name or even their order. This is why <a href="../../Utils.html#BetaML.Utils.accuracy-Union{Tuple{T}, Tuple{AbstractVector{Int64}, AbstractMatrix{T}}} where T&lt;:Number"><code>accuracy</code></a> has the parameter <code>ignorelabels</code> to compute the accuracy oven any possible permutation of the classes found.</li><li>we return the various models&#39; accuracies</li></ul><pre><code class="language-julia hljs">cOut = cross_validation([x,y],sampler,return_statistics=false) do trainData,testData,rng
          # For unsupervised learning we use only the train data.
          # Also, we use the associated labels only to measure the performances
         (xtrain,ytrain)  = trainData;
         # We run the clustering algorithm and then and we compute the accuracy using the real labels:
         estcl = fit!(KMeansClusterer(n_classes=3,initialisation_strategy=&quot;grid&quot;,rng=rng),xtrain)
         kMeansGAccuracy    = accuracy(ytrain,estcl,ignorelabels=true)
         estcl = fit!(KMeansClusterer(n_classes=3,initialisation_strategy=&quot;random&quot;,rng=rng),xtrain)
         kMeansRAccuracy   = accuracy(ytrain,estcl,ignorelabels=true)
         estcl = fit!(KMeansClusterer(n_classes=3,initialisation_strategy=&quot;shuffle&quot;,rng=rng),xtrain)
         kMeansSAccuracy   = accuracy(ytrain,estcl,ignorelabels=true)
         estcl = fit!(KMedoidsClusterer(n_classes=3,initialisation_strategy=&quot;grid&quot;,rng=rng),xtrain)
         kMedoidsGAccuracy  = accuracy(ytrain,estcl,ignorelabels=true)
         estcl = fit!(KMedoidsClusterer(n_classes=3,initialisation_strategy=&quot;random&quot;,rng=rng),xtrain)
         kMedoidsRAccuracy = accuracy(ytrain,estcl,ignorelabels=true)
         estcl = fit!(KMedoidsClusterer(n_classes=3,initialisation_strategy=&quot;shuffle&quot;,rng=rng),xtrain)
         kMedoidsSAccuracy = accuracy(ytrain,estcl,ignorelabels=true)
         estcl = fit!(GMMClusterer(n_classes=3,mixtures=SphericalGaussian,rng=rng,verbosity=NONE),xtrain)
         gmmSpherAccuracy  = accuracy(ytrain,estcl,ignorelabels=true, rng=rng)
         estcl = fit!(GMMClusterer(n_classes=3,mixtures=DiagonalGaussian,rng=rng,verbosity=NONE),xtrain)
         gmmDiagAccuracy   = accuracy(ytrain,estcl,ignorelabels=true, rng=rng)
         estcl = fit!(GMMClusterer(n_classes=3,mixtures=FullGaussian,rng=rng,verbosity=NONE),xtrain)
         gmmFullAccuracy   = accuracy(ytrain,estcl,ignorelabels=true, rng=rng)
         # For comparision with Clustering.jl
         clusteringOut     = Clustering.kmeans(xtrain&#39;, 3)
         kMeans2Accuracy   = accuracy(ytrain,clusteringOut.assignments,ignorelabels=true)
         # For comparision with GaussianMistures.jl - sometimes GaussianMistures.jl em! fails with a PosDefException
         dGMM              = GaussianMixtures.GMM(3, xtrain; method=:kmeans, kind=:diag)
         GaussianMixtures.em!(dGMM, xtrain)
         gmmDiag2Accuracy  = accuracy(ytrain,GaussianMixtures.gmmposterior(dGMM, xtrain)[1],ignorelabels=true)
         fGMM              = GaussianMixtures.GMM(3, xtrain; method=:kmeans, kind=:full)
         GaussianMixtures.em!(fGMM, xtrain)
         gmmFull2Accuracy  = accuracy(ytrain,GaussianMixtures.gmmposterior(fGMM, xtrain)[1],ignorelabels=true)
         # Returning the accuracies
         return kMeansGAccuracy,kMeansRAccuracy,kMeansSAccuracy,kMedoidsGAccuracy,kMedoidsRAccuracy,kMedoidsSAccuracy,gmmSpherAccuracy,gmmDiagAccuracy,gmmFullAccuracy,kMeans2Accuracy,gmmDiag2Accuracy,gmmFull2Accuracy
 end

# We transform the output in matrix for easier analysis
accuracies = fill(0.0,(length(cOut),length(cOut[1])))
[accuracies[r,c] = cOut[r][c] for r in 1:length(cOut),c in 1:length(cOut[1])]
μs = mean(accuracies,dims=1)
σs = std(accuracies,dims=1)


modelLabels=[&quot;kMeansG&quot;,&quot;kMeansR&quot;,&quot;kMeansS&quot;,&quot;kMedoidsG&quot;,&quot;kMedoidsR&quot;,&quot;kMedoidsS&quot;,&quot;gmmSpher&quot;,&quot;gmmDiag&quot;,&quot;gmmFull&quot;,&quot;kMeans (Clustering.jl)&quot;,&quot;gmmDiag (GaussianMixtures.jl)&quot;,&quot;gmmFull (GaussianMixtures.jl)&quot;]

report = DataFrame(mName = modelLabels, avgAccuracy = dropdims(round.(μs&#39;,digits=3),dims=2), stdAccuracy = dropdims(round.(σs&#39;,digits=3),dims=2))</code></pre><div><div style = "float: left;"><span>12×3 DataFrame</span></div><div style = "clear: both;"></div></div><div class = "data-frame" style = "overflow-x: scroll;"><table class = "data-frame" style = "margin-bottom: 6px;"><thead><tr class = "header"><th class = "rowNumber" style = "font-weight: bold; text-align: right;">Row</th><th style = "text-align: left;">mName</th><th style = "text-align: left;">avgAccuracy</th><th style = "text-align: left;">stdAccuracy</th></tr><tr class = "subheader headerLastRow"><th class = "rowNumber" style = "font-weight: bold; text-align: right;"></th><th title = "String" style = "text-align: left;">String</th><th title = "Float64" style = "text-align: left;">Float64</th><th title = "Float64" style = "text-align: left;">Float64</th></tr></thead><tbody><tr><td class = "rowNumber" style = "font-weight: bold; text-align: right;">1</td><td style = "text-align: left;">kMeansG</td><td style = "text-align: right;">0.892</td><td style = "text-align: right;">0.015</td></tr><tr><td class = "rowNumber" style = "font-weight: bold; text-align: right;">2</td><td style = "text-align: left;">kMeansR</td><td style = "text-align: right;">0.835</td><td style = "text-align: right;">0.098</td></tr><tr><td class = "rowNumber" style = "font-weight: bold; text-align: right;">3</td><td style = "text-align: left;">kMeansS</td><td style = "text-align: right;">0.825</td><td style = "text-align: right;">0.134</td></tr><tr><td class = "rowNumber" style = "font-weight: bold; text-align: right;">4</td><td style = "text-align: left;">kMedoidsG</td><td style = "text-align: right;">0.897</td><td style = "text-align: right;">0.014</td></tr><tr><td class = "rowNumber" style = "font-weight: bold; text-align: right;">5</td><td style = "text-align: left;">kMedoidsR</td><td style = "text-align: right;">0.817</td><td style = "text-align: right;">0.136</td></tr><tr><td class = "rowNumber" style = "font-weight: bold; text-align: right;">6</td><td style = "text-align: left;">kMedoidsS</td><td style = "text-align: right;">0.851</td><td style = "text-align: right;">0.124</td></tr><tr><td class = "rowNumber" style = "font-weight: bold; text-align: right;">7</td><td style = "text-align: left;">gmmSpher</td><td style = "text-align: right;">0.894</td><td style = "text-align: right;">0.015</td></tr><tr><td class = "rowNumber" style = "font-weight: bold; text-align: right;">8</td><td style = "text-align: left;">gmmDiag</td><td style = "text-align: right;">0.918</td><td style = "text-align: right;">0.019</td></tr><tr><td class = "rowNumber" style = "font-weight: bold; text-align: right;">9</td><td style = "text-align: left;">gmmFull</td><td style = "text-align: right;">0.974</td><td style = "text-align: right;">0.027</td></tr><tr><td class = "rowNumber" style = "font-weight: bold; text-align: right;">10</td><td style = "text-align: left;">kMeans (Clustering.jl)</td><td style = "text-align: right;">0.871</td><td style = "text-align: right;">0.09</td></tr><tr><td class = "rowNumber" style = "font-weight: bold; text-align: right;">11</td><td style = "text-align: left;">gmmDiag (GaussianMixtures.jl)</td><td style = "text-align: right;">0.881</td><td style = "text-align: right;">0.1</td></tr><tr><td class = "rowNumber" style = "font-weight: bold; text-align: right;">12</td><td style = "text-align: left;">gmmFull (GaussianMixtures.jl)</td><td style = "text-align: right;">0.909</td><td style = "text-align: right;">0.138</td></tr></tbody></table></div><p>Accuracies (mean and its standard dev.) running this scripts with different random seeds (<code>123</code>, <code>1000</code> and <code>10000</code>):</p><table><tr><th style="text-align: right">model</th><th style="text-align: right">μ 1</th><th style="text-align: right">σ² 1</th><th style="text-align: right">μ 2</th><th style="text-align: right">σ² 2</th><th style="text-align: right">μ 3</th><th style="text-align: right">σ² 3</th></tr><tr><td style="text-align: right">│ kMeansG</td><td style="text-align: right">0.891</td><td style="text-align: right">0.017</td><td style="text-align: right">0.892</td><td style="text-align: right">0.012</td><td style="text-align: right">0.893</td><td style="text-align: right">0.017</td></tr><tr><td style="text-align: right">│ kMeansR</td><td style="text-align: right">0.866</td><td style="text-align: right">0.083</td><td style="text-align: right">0.831</td><td style="text-align: right">0.127</td><td style="text-align: right">0.836</td><td style="text-align: right">0.114</td></tr><tr><td style="text-align: right">│ kMeansS</td><td style="text-align: right">0.764</td><td style="text-align: right">0.174</td><td style="text-align: right">0.822</td><td style="text-align: right">0.145</td><td style="text-align: right">0.779</td><td style="text-align: right">0.170</td></tr><tr><td style="text-align: right">│ kMedoidsG</td><td style="text-align: right">0.894</td><td style="text-align: right">0.015</td><td style="text-align: right">0.896</td><td style="text-align: right">0.012</td><td style="text-align: right">0.894</td><td style="text-align: right">0.017</td></tr><tr><td style="text-align: right">│ kMedoidsR</td><td style="text-align: right">0.804</td><td style="text-align: right">0.144</td><td style="text-align: right">0.841</td><td style="text-align: right">0.123</td><td style="text-align: right">0.825</td><td style="text-align: right">0.134</td></tr><tr><td style="text-align: right">│ kMedoidsS</td><td style="text-align: right">0.893</td><td style="text-align: right">0.018</td><td style="text-align: right">0.834</td><td style="text-align: right">0.130</td><td style="text-align: right">0.877</td><td style="text-align: right">0.085</td></tr><tr><td style="text-align: right">│ gmmSpher</td><td style="text-align: right">0.893</td><td style="text-align: right">0.016</td><td style="text-align: right">0.891</td><td style="text-align: right">0.016</td><td style="text-align: right">0.895</td><td style="text-align: right">0.017</td></tr><tr><td style="text-align: right">│ gmmDiag</td><td style="text-align: right">0.917</td><td style="text-align: right">0.022</td><td style="text-align: right">0.912</td><td style="text-align: right">0.016</td><td style="text-align: right">0.916</td><td style="text-align: right">0.014</td></tr><tr><td style="text-align: right">│ gmmFull</td><td style="text-align: right">0.970</td><td style="text-align: right">0.035</td><td style="text-align: right">0.982</td><td style="text-align: right">0.013</td><td style="text-align: right">0.981</td><td style="text-align: right">0.009</td></tr><tr><td style="text-align: right">│ kMeans (Clustering.jl)</td><td style="text-align: right">0.856</td><td style="text-align: right">0.112</td><td style="text-align: right">0.873</td><td style="text-align: right">0.083</td><td style="text-align: right">0.873</td><td style="text-align: right">0.089</td></tr><tr><td style="text-align: right">│ gmmDiag (GaussianMixtures.jl)</td><td style="text-align: right">0.865</td><td style="text-align: right">0.127</td><td style="text-align: right">0.872</td><td style="text-align: right">0.090</td><td style="text-align: right">0.833</td><td style="text-align: right">0.152</td></tr><tr><td style="text-align: right">│ gmmFull (GaussianMixtures.jl)</td><td style="text-align: right">0.907</td><td style="text-align: right">0.133</td><td style="text-align: right">0.914</td><td style="text-align: right">0.160</td><td style="text-align: right">0.917</td><td style="text-align: right">0.141</td></tr></table><p>We can see that running the script multiple times with different random seed confirm the estimated standard deviations collected with the cross_validation, with the BetaML GMM-based models and grid based ones being the most stable ones.</p><h3 id="BetaML-model-accuracies"><a class="docs-heading-anchor" href="#BetaML-model-accuracies">BetaML model accuracies</a><a id="BetaML-model-accuracies-1"></a><a class="docs-heading-anchor-permalink" href="#BetaML-model-accuracies" title="Permalink"></a></h3><p>From the output We see that the gmm models perform for this dataset generally better than kmeans or kmedoids algorithms, and they further have very low variances. In detail, it is the (default) <code>grid</code> initialisation that leads to the better results for <code>kmeans</code> and <code>kmedoids</code>, while for the <code>gmm</code> models it is the <code>FullGaussian</code> to perform better.</p><h3 id="Comparisions-with-Clustering.jl-and-GaussianMixtures.jl"><a class="docs-heading-anchor" href="#Comparisions-with-Clustering.jl-and-GaussianMixtures.jl">Comparisions with <code>Clustering.jl</code> and <code>GaussianMixtures.jl</code></a><a id="Comparisions-with-Clustering.jl-and-GaussianMixtures.jl-1"></a><a class="docs-heading-anchor-permalink" href="#Comparisions-with-Clustering.jl-and-GaussianMixtures.jl" title="Permalink"></a></h3><p>For this specific case, both <code>Clustering.jl</code> and <code>GaussianMixtures.jl</code> report substantially worst accuracies, and with very high variances. But we maintain the ranking that Full Gaussian gmm &gt; Diagonal Gaussian &gt; Kmeans accuracy. I suspect the reason that BetaML gmm works so well is in relation to the usage of kmeans algorithm for initialisation of the mixtures, itself initialized with a &quot;grid&quot; arpproach. The grid initialisation &quot;guarantee&quot; indeed that the initial means of the mixture components are well spread across the multidimensional space defined by the data, and it helps avoiding the EM algoritm to converge to a bad local optimus.</p><h2 id="Working-without-the-labels"><a class="docs-heading-anchor" href="#Working-without-the-labels">Working without the labels</a><a id="Working-without-the-labels-1"></a><a class="docs-heading-anchor-permalink" href="#Working-without-the-labels" title="Permalink"></a></h2><p>Up to now we used the real labels to compare the model accuracies. But in real clustering examples we don&#39;t have the true classes, or we wouln&#39;t need to do clustering in the first instance, so we don&#39;t know the number of classes to use. There are several methods to judge clusters algorithms goodness. For likelyhood based algorithms as <code>GMMClusterer</code> we can use a information criteria that trade the goodness of the lickelyhood with the number of parameters used to do the fit. BetaML provides by default in the gmm clustering outputs both the <em>Bayesian information criterion</em>  (<a href="../../Utils.html#BetaML.Utils.bic-Tuple{Any, Any, Any}"><code>BIC</code></a>) and the <em>Akaike information criterion</em>  (<a href="../../Utils.html#BetaML.Utils.aic-Tuple{Any, Any}"><code>AIC</code></a>), where for both a lower value is better.</p><p>We can then run the model with different number of classes and see which one leads to the lower BIC or AIC. We run hence <code>cross_validation</code> again with the <code>FullGaussian</code> gmm model. Note that we use the BIC/AIC criteria here for establishing the &quot;best&quot; number of classes but we could have used it also to select the kind of Gaussain distribution to use. This is one example of hyper-parameter tuning that we developed more in detail using autotuning in the <a href="../Regression - bike sharing/betaml_tutorial_regression_sharingBikes.html#regression_tutorial">regression tutorial</a>.</p><p>Let&#39;s try up to 4 possible classes:</p><pre><code class="language-julia hljs">K = 4
sampler = KFold(nsplits=5,nrepeats=2,shuffle=true, rng=copy(AFIXEDRNG))
cOut = cross_validation([x,y],sampler,return_statistics=false) do trainData,testData,rng
    (xtrain,ytrain)  = trainData;
    BICS = []
    AICS = []
    for k in 1:K
        m = GMMClusterer(n_classes=k,mixtures=FullGaussian,rng=rng,verbosity=NONE)
        fit!(m,xtrain)
        push!(BICS,info(m)[&quot;BIC&quot;])
        push!(AICS,info(m)[&quot;AIC&quot;])
    end
    return (BICS,AICS)
end

# Transforming the output in matrices for easier analysis
Nit = length(cOut)

BICS = fill(0.0,(Nit,K))
AICS = fill(0.0,(Nit,K))
[BICS[r,c] = cOut[r][1][c] for r in 1:Nit,c in 1:K]
[AICS[r,c] = cOut[r][2][c] for r in 1:Nit,c in 1:K]

μsBICS = mean(BICS,dims=1)</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">1×4 Matrix{Float64}:
 762.112  516.031  539.392  593.272</code></pre><pre><code class="language-julia hljs">σsBICS = std(BICS,dims=1)</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">1×4 Matrix{Float64}:
 12.2912  15.8085  17.7181  24.6026</code></pre><pre><code class="language-julia hljs">μsAICS = mean(AICS,dims=1)</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">1×4 Matrix{Float64}:
 723.087  435.194  416.743  428.81</code></pre><pre><code class="language-julia hljs">σsAICS = std(AICS,dims=1)</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">1×4 Matrix{Float64}:
 12.2912  15.8085  17.7181  24.6026</code></pre><pre><code class="language-julia hljs">plot(1:K,[μsBICS&#39; μsAICS&#39;], labels=[&quot;BIC&quot; &quot;AIC&quot;], title=&quot;Information criteria by number of classes&quot;, xlabel=&quot;number of classes&quot;, ylabel=&quot;lower is better&quot;)</code></pre><?xml version="1.0" encoding="utf-8"?>
<svg xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" width="600" height="400" viewBox="0 0 2400 1600">
<defs>
  <clipPath id="clip390">
    <rect x="0" y="0" width="2400" height="1600"/>
  </clipPath>
</defs>
<path clip-path="url(#clip390)" d="M0 1600 L2400 1600 L2400 0 L0 0  Z" fill="#ffffff" fill-rule="evenodd" fill-opacity="1"/>
<defs>
  <clipPath id="clip391">
    <rect x="480" y="0" width="1681" height="1600"/>
  </clipPath>
</defs>
<path clip-path="url(#clip390)" d="M235.283 1423.18 L2352.76 1423.18 L2352.76 123.472 L235.283 123.472  Z" fill="#ffffff" fill-rule="evenodd" fill-opacity="1"/>
<defs>
  <clipPath id="clip392">
    <rect x="235" y="123" width="2118" height="1301"/>
  </clipPath>
</defs>
<polyline clip-path="url(#clip392)" style="stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none" points="295.211,1423.18 295.211,123.472 "/>
<polyline clip-path="url(#clip392)" style="stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none" points="961.083,1423.18 961.083,123.472 "/>
<polyline clip-path="url(#clip392)" style="stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none" points="1626.96,1423.18 1626.96,123.472 "/>
<polyline clip-path="url(#clip392)" style="stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none" points="2292.83,1423.18 2292.83,123.472 "/>
<polyline clip-path="url(#clip390)" style="stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none" points="235.283,1423.18 2352.76,1423.18 "/>
<polyline clip-path="url(#clip390)" style="stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none" points="295.211,1423.18 295.211,1404.28 "/>
<polyline clip-path="url(#clip390)" style="stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none" points="961.083,1423.18 961.083,1404.28 "/>
<polyline clip-path="url(#clip390)" style="stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none" points="1626.96,1423.18 1626.96,1404.28 "/>
<polyline clip-path="url(#clip390)" style="stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none" points="2292.83,1423.18 2292.83,1404.28 "/>
<path clip-path="url(#clip390)" d="M285.593 1481.64 L293.232 1481.64 L293.232 1455.28 L284.922 1456.95 L284.922 1452.69 L293.186 1451.02 L297.862 1451.02 L297.862 1481.64 L305.501 1481.64 L305.501 1485.58 L285.593 1485.58 L285.593 1481.64 Z" fill="#000000" fill-rule="nonzero" fill-opacity="1" /><path clip-path="url(#clip390)" d="M955.736 1481.64 L972.056 1481.64 L972.056 1485.58 L950.111 1485.58 L950.111 1481.64 Q952.773 1478.89 957.357 1474.26 Q961.963 1469.61 963.144 1468.27 Q965.389 1465.74 966.269 1464.01 Q967.171 1462.25 967.171 1460.56 Q967.171 1457.8 965.227 1456.07 Q963.306 1454.33 960.204 1454.33 Q958.005 1454.33 955.551 1455.09 Q953.12 1455.86 950.343 1457.41 L950.343 1452.69 Q953.167 1451.55 955.62 1450.97 Q958.074 1450.39 960.111 1450.39 Q965.481 1450.39 968.676 1453.08 Q971.87 1455.77 971.87 1460.26 Q971.87 1462.39 971.06 1464.31 Q970.273 1466.2 968.167 1468.8 Q967.588 1469.47 964.486 1472.69 Q961.384 1475.88 955.736 1481.64 Z" fill="#000000" fill-rule="nonzero" fill-opacity="1" /><path clip-path="url(#clip390)" d="M1631.2 1466.95 Q1634.56 1467.66 1636.43 1469.93 Q1638.33 1472.2 1638.33 1475.53 Q1638.33 1480.65 1634.81 1483.45 Q1631.3 1486.25 1624.81 1486.25 Q1622.64 1486.25 1620.32 1485.81 Q1618.03 1485.39 1615.58 1484.54 L1615.58 1480.02 Q1617.52 1481.16 1619.84 1481.74 Q1622.15 1482.32 1624.68 1482.32 Q1629.07 1482.32 1631.37 1480.58 Q1633.68 1478.84 1633.68 1475.53 Q1633.68 1472.48 1631.53 1470.77 Q1629.4 1469.03 1625.58 1469.03 L1621.55 1469.03 L1621.55 1465.19 L1625.76 1465.19 Q1629.21 1465.19 1631.04 1463.82 Q1632.87 1462.43 1632.87 1459.84 Q1632.87 1457.18 1630.97 1455.77 Q1629.1 1454.33 1625.58 1454.33 Q1623.66 1454.33 1621.46 1454.75 Q1619.26 1455.16 1616.62 1456.04 L1616.62 1451.88 Q1619.28 1451.14 1621.6 1450.77 Q1623.93 1450.39 1625.99 1450.39 Q1631.32 1450.39 1634.42 1452.83 Q1637.52 1455.23 1637.52 1459.35 Q1637.52 1462.22 1635.88 1464.21 Q1634.24 1466.18 1631.2 1466.95 Z" fill="#000000" fill-rule="nonzero" fill-opacity="1" /><path clip-path="url(#clip390)" d="M2295.84 1455.09 L2284.03 1473.54 L2295.84 1473.54 L2295.84 1455.09 M2294.61 1451.02 L2300.49 1451.02 L2300.49 1473.54 L2305.42 1473.54 L2305.42 1477.43 L2300.49 1477.43 L2300.49 1485.58 L2295.84 1485.58 L2295.84 1477.43 L2280.23 1477.43 L2280.23 1472.92 L2294.61 1451.02 Z" fill="#000000" fill-rule="nonzero" fill-opacity="1" /><path clip-path="url(#clip390)" d="M1031.15 1546.53 L1031.15 1568.04 L1025.29 1568.04 L1025.29 1546.72 Q1025.29 1541.66 1023.32 1539.14 Q1021.34 1536.63 1017.4 1536.63 Q1012.66 1536.63 1009.92 1539.65 Q1007.18 1542.68 1007.18 1547.9 L1007.18 1568.04 L1001.29 1568.04 L1001.29 1532.4 L1007.18 1532.4 L1007.18 1537.93 Q1009.28 1534.72 1012.11 1533.13 Q1014.98 1531.54 1018.7 1531.54 Q1024.85 1531.54 1028 1535.36 Q1031.15 1539.14 1031.15 1546.53 Z" fill="#000000" fill-rule="nonzero" fill-opacity="1" /><path clip-path="url(#clip390)" d="M1042.22 1553.98 L1042.22 1532.4 L1048.08 1532.4 L1048.08 1553.75 Q1048.08 1558.81 1050.05 1561.36 Q1052.03 1563.87 1055.97 1563.87 Q1060.72 1563.87 1063.45 1560.85 Q1066.22 1557.83 1066.22 1552.61 L1066.22 1532.4 L1072.08 1532.4 L1072.08 1568.04 L1066.22 1568.04 L1066.22 1562.57 Q1064.09 1565.82 1061.26 1567.41 Q1058.46 1568.97 1054.73 1568.97 Q1048.59 1568.97 1045.41 1565.15 Q1042.22 1561.33 1042.22 1553.98 M1056.96 1531.54 L1056.96 1531.54 Z" fill="#000000" fill-rule="nonzero" fill-opacity="1" /><path clip-path="url(#clip390)" d="M1111.9 1539.24 Q1114.09 1535.29 1117.15 1533.41 Q1120.2 1531.54 1124.34 1531.54 Q1129.91 1531.54 1132.94 1535.45 Q1135.96 1539.33 1135.96 1546.53 L1135.96 1568.04 L1130.07 1568.04 L1130.07 1546.72 Q1130.07 1541.59 1128.26 1539.11 Q1126.44 1536.63 1122.72 1536.63 Q1118.17 1536.63 1115.52 1539.65 Q1112.88 1542.68 1112.88 1547.9 L1112.88 1568.04 L1106.99 1568.04 L1106.99 1546.72 Q1106.99 1541.56 1105.18 1539.11 Q1103.37 1536.63 1099.58 1536.63 Q1095.09 1536.63 1092.45 1539.68 Q1089.81 1542.71 1089.81 1547.9 L1089.81 1568.04 L1083.92 1568.04 L1083.92 1532.4 L1089.81 1532.4 L1089.81 1537.93 Q1091.81 1534.66 1094.61 1533.1 Q1097.41 1531.54 1101.27 1531.54 Q1105.15 1531.54 1107.85 1533.51 Q1110.59 1535.48 1111.9 1539.24 Z" fill="#000000" fill-rule="nonzero" fill-opacity="1" /><path clip-path="url(#clip390)" d="M1173.23 1550.25 Q1173.23 1543.79 1170.56 1540.13 Q1167.91 1536.44 1163.27 1536.44 Q1158.62 1536.44 1155.95 1540.13 Q1153.31 1543.79 1153.31 1550.25 Q1153.31 1556.71 1155.95 1560.4 Q1158.62 1564.07 1163.27 1564.07 Q1167.91 1564.07 1170.56 1560.4 Q1173.23 1556.71 1173.23 1550.25 M1153.31 1537.81 Q1155.15 1534.62 1157.95 1533.1 Q1160.79 1531.54 1164.7 1531.54 Q1171.19 1531.54 1175.24 1536.69 Q1179.31 1541.85 1179.31 1550.25 Q1179.31 1558.65 1175.24 1563.81 Q1171.19 1568.97 1164.7 1568.97 Q1160.79 1568.97 1157.95 1567.44 Q1155.15 1565.88 1153.31 1562.7 L1153.31 1568.04 L1147.42 1568.04 L1147.42 1518.52 L1153.31 1518.52 L1153.31 1537.81 Z" fill="#000000" fill-rule="nonzero" fill-opacity="1" /><path clip-path="url(#clip390)" d="M1219.51 1548.76 L1219.51 1551.62 L1192.58 1551.62 Q1192.96 1557.67 1196.21 1560.85 Q1199.49 1564 1205.31 1564 Q1208.69 1564 1211.84 1563.17 Q1215.02 1562.35 1218.14 1560.69 L1218.14 1566.23 Q1214.99 1567.57 1211.68 1568.27 Q1208.37 1568.97 1204.96 1568.97 Q1196.43 1568.97 1191.44 1564 Q1186.47 1559.04 1186.47 1550.57 Q1186.47 1541.82 1191.18 1536.69 Q1195.92 1531.54 1203.94 1531.54 Q1211.14 1531.54 1215.31 1536.18 Q1219.51 1540.8 1219.51 1548.76 M1213.65 1547.04 Q1213.59 1542.23 1210.95 1539.37 Q1208.34 1536.5 1204.01 1536.5 Q1199.11 1536.5 1196.15 1539.27 Q1193.22 1542.04 1192.77 1547.07 L1213.65 1547.04 Z" fill="#000000" fill-rule="nonzero" fill-opacity="1" /><path clip-path="url(#clip390)" d="M1249.78 1537.87 Q1248.79 1537.3 1247.61 1537.04 Q1246.47 1536.76 1245.07 1536.76 Q1240.1 1536.76 1237.43 1540 Q1234.79 1543.22 1234.79 1549.27 L1234.79 1568.04 L1228.9 1568.04 L1228.9 1532.4 L1234.79 1532.4 L1234.79 1537.93 Q1236.63 1534.69 1239.59 1533.13 Q1242.55 1531.54 1246.79 1531.54 Q1247.39 1531.54 1248.12 1531.63 Q1248.85 1531.7 1249.75 1531.85 L1249.78 1537.87 Z" fill="#000000" fill-rule="nonzero" fill-opacity="1" /><path clip-path="url(#clip390)" d="M1290.45 1536.5 Q1285.74 1536.5 1283.01 1540.19 Q1280.27 1543.85 1280.27 1550.25 Q1280.27 1556.65 1282.97 1560.34 Q1285.71 1564 1290.45 1564 Q1295.13 1564 1297.87 1560.31 Q1300.61 1556.62 1300.61 1550.25 Q1300.61 1543.92 1297.87 1540.23 Q1295.13 1536.5 1290.45 1536.5 M1290.45 1531.54 Q1298.09 1531.54 1302.45 1536.5 Q1306.81 1541.47 1306.81 1550.25 Q1306.81 1559 1302.45 1564 Q1298.09 1568.97 1290.45 1568.97 Q1282.78 1568.97 1278.42 1564 Q1274.09 1559 1274.09 1550.25 Q1274.09 1541.47 1278.42 1536.5 Q1282.78 1531.54 1290.45 1531.54 Z" fill="#000000" fill-rule="nonzero" fill-opacity="1" /><path clip-path="url(#clip390)" d="M1334.57 1518.52 L1334.57 1523.39 L1328.97 1523.39 Q1325.82 1523.39 1324.57 1524.66 Q1323.37 1525.93 1323.37 1529.24 L1323.37 1532.4 L1333.01 1532.4 L1333.01 1536.95 L1323.37 1536.95 L1323.37 1568.04 L1317.48 1568.04 L1317.48 1536.95 L1311.88 1536.95 L1311.88 1532.4 L1317.48 1532.4 L1317.48 1529.91 Q1317.48 1523.96 1320.25 1521.26 Q1323.02 1518.52 1329.03 1518.52 L1334.57 1518.52 Z" fill="#000000" fill-rule="nonzero" fill-opacity="1" /><path clip-path="url(#clip390)" d="M1385.84 1533.76 L1385.84 1539.24 Q1383.36 1537.87 1380.85 1537.2 Q1378.36 1536.5 1375.82 1536.5 Q1370.12 1536.5 1366.97 1540.13 Q1363.82 1543.73 1363.82 1550.25 Q1363.82 1556.78 1366.97 1560.4 Q1370.12 1564 1375.82 1564 Q1378.36 1564 1380.85 1563.33 Q1383.36 1562.63 1385.84 1561.26 L1385.84 1566.68 Q1383.39 1567.82 1380.75 1568.39 Q1378.14 1568.97 1375.18 1568.97 Q1367.13 1568.97 1362.39 1563.91 Q1357.64 1558.85 1357.64 1550.25 Q1357.64 1541.53 1362.42 1536.53 Q1367.23 1531.54 1375.56 1531.54 Q1378.27 1531.54 1380.85 1532.11 Q1383.43 1532.65 1385.84 1533.76 Z" fill="#000000" fill-rule="nonzero" fill-opacity="1" /><path clip-path="url(#clip390)" d="M1396.03 1518.52 L1401.89 1518.52 L1401.89 1568.04 L1396.03 1568.04 L1396.03 1518.52 Z" fill="#000000" fill-rule="nonzero" fill-opacity="1" /><path clip-path="url(#clip390)" d="M1430.34 1550.12 Q1423.24 1550.12 1420.51 1551.75 Q1417.77 1553.37 1417.77 1557.29 Q1417.77 1560.4 1419.81 1562.25 Q1421.87 1564.07 1425.41 1564.07 Q1430.28 1564.07 1433.21 1560.63 Q1436.17 1557.16 1436.17 1551.43 L1436.17 1550.12 L1430.34 1550.12 M1442.02 1547.71 L1442.02 1568.04 L1436.17 1568.04 L1436.17 1562.63 Q1434.16 1565.88 1431.17 1567.44 Q1428.18 1568.97 1423.85 1568.97 Q1418.37 1568.97 1415.13 1565.91 Q1411.91 1562.82 1411.91 1557.67 Q1411.91 1551.65 1415.92 1548.6 Q1419.96 1545.54 1427.95 1545.54 L1436.17 1545.54 L1436.17 1544.97 Q1436.17 1540.93 1433.49 1538.73 Q1430.85 1536.5 1426.04 1536.5 Q1422.99 1536.5 1420.09 1537.23 Q1417.2 1537.97 1414.52 1539.43 L1414.52 1534.02 Q1417.74 1532.78 1420.76 1532.17 Q1423.78 1531.54 1426.65 1531.54 Q1434.38 1531.54 1438.2 1535.55 Q1442.02 1539.56 1442.02 1547.71 Z" fill="#000000" fill-rule="nonzero" fill-opacity="1" /><path clip-path="url(#clip390)" d="M1476.81 1533.45 L1476.81 1538.98 Q1474.33 1537.71 1471.65 1537.07 Q1468.98 1536.44 1466.12 1536.44 Q1461.76 1536.44 1459.56 1537.77 Q1457.4 1539.11 1457.4 1541.79 Q1457.4 1543.82 1458.95 1545 Q1460.51 1546.15 1465.23 1547.2 L1467.23 1547.64 Q1473.47 1548.98 1476.08 1551.43 Q1478.72 1553.85 1478.72 1558.21 Q1478.72 1563.17 1474.77 1566.07 Q1470.86 1568.97 1463.98 1568.97 Q1461.12 1568.97 1458 1568.39 Q1454.91 1567.85 1451.48 1566.74 L1451.48 1560.69 Q1454.72 1562.38 1457.87 1563.24 Q1461.02 1564.07 1464.11 1564.07 Q1468.25 1564.07 1470.48 1562.66 Q1472.7 1561.23 1472.7 1558.65 Q1472.7 1556.27 1471.08 1554.99 Q1469.49 1553.72 1464.05 1552.54 L1462.01 1552.07 Q1456.57 1550.92 1454.15 1548.56 Q1451.73 1546.18 1451.73 1542.04 Q1451.73 1537.01 1455.29 1534.27 Q1458.86 1531.54 1465.42 1531.54 Q1468.66 1531.54 1471.53 1532.01 Q1474.39 1532.49 1476.81 1533.45 Z" fill="#000000" fill-rule="nonzero" fill-opacity="1" /><path clip-path="url(#clip390)" d="M1510.77 1533.45 L1510.77 1538.98 Q1508.29 1537.71 1505.62 1537.07 Q1502.94 1536.44 1500.08 1536.44 Q1495.72 1536.44 1493.52 1537.77 Q1491.36 1539.11 1491.36 1541.79 Q1491.36 1543.82 1492.92 1545 Q1494.48 1546.15 1499.19 1547.2 L1501.19 1547.64 Q1507.43 1548.98 1510.04 1551.43 Q1512.68 1553.85 1512.68 1558.21 Q1512.68 1563.17 1508.73 1566.07 Q1504.82 1568.97 1497.94 1568.97 Q1495.08 1568.97 1491.96 1568.39 Q1488.87 1567.85 1485.44 1566.74 L1485.44 1560.69 Q1488.68 1562.38 1491.83 1563.24 Q1494.98 1564.07 1498.07 1564.07 Q1502.21 1564.07 1504.44 1562.66 Q1506.67 1561.23 1506.67 1558.65 Q1506.67 1556.27 1505.04 1554.99 Q1503.45 1553.72 1498.01 1552.54 L1495.97 1552.07 Q1490.53 1550.92 1488.11 1548.56 Q1485.69 1546.18 1485.69 1542.04 Q1485.69 1537.01 1489.26 1534.27 Q1492.82 1531.54 1499.38 1531.54 Q1502.62 1531.54 1505.49 1532.01 Q1508.35 1532.49 1510.77 1533.45 Z" fill="#000000" fill-rule="nonzero" fill-opacity="1" /><path clip-path="url(#clip390)" d="M1552.5 1548.76 L1552.5 1551.62 L1525.57 1551.62 Q1525.95 1557.67 1529.2 1560.85 Q1532.48 1564 1538.3 1564 Q1541.68 1564 1544.83 1563.17 Q1548.01 1562.35 1551.13 1560.69 L1551.13 1566.23 Q1547.98 1567.57 1544.67 1568.27 Q1541.36 1568.97 1537.95 1568.97 Q1529.42 1568.97 1524.43 1564 Q1519.46 1559.04 1519.46 1550.57 Q1519.46 1541.82 1524.17 1536.69 Q1528.91 1531.54 1536.93 1531.54 Q1544.13 1531.54 1548.3 1536.18 Q1552.5 1540.8 1552.5 1548.76 M1546.64 1547.04 Q1546.58 1542.23 1543.94 1539.37 Q1541.33 1536.5 1537 1536.5 Q1532.1 1536.5 1529.14 1539.27 Q1526.21 1542.04 1525.76 1547.07 L1546.64 1547.04 Z" fill="#000000" fill-rule="nonzero" fill-opacity="1" /><path clip-path="url(#clip390)" d="M1584.84 1533.45 L1584.84 1538.98 Q1582.35 1537.71 1579.68 1537.07 Q1577.01 1536.44 1574.14 1536.44 Q1569.78 1536.44 1567.59 1537.77 Q1565.42 1539.11 1565.42 1541.79 Q1565.42 1543.82 1566.98 1545 Q1568.54 1546.15 1573.25 1547.2 L1575.26 1547.64 Q1581.49 1548.98 1584.1 1551.43 Q1586.75 1553.85 1586.75 1558.21 Q1586.75 1563.17 1582.8 1566.07 Q1578.88 1568.97 1572.01 1568.97 Q1569.15 1568.97 1566.03 1568.39 Q1562.94 1567.85 1559.5 1566.74 L1559.5 1560.69 Q1562.75 1562.38 1565.9 1563.24 Q1569.05 1564.07 1572.14 1564.07 Q1576.27 1564.07 1578.5 1562.66 Q1580.73 1561.23 1580.73 1558.65 Q1580.73 1556.27 1579.11 1554.99 Q1577.52 1553.72 1572.07 1552.54 L1570.04 1552.07 Q1564.59 1550.92 1562.17 1548.56 Q1559.76 1546.18 1559.76 1542.04 Q1559.76 1537.01 1563.32 1534.27 Q1566.89 1531.54 1573.44 1531.54 Q1576.69 1531.54 1579.55 1532.01 Q1582.42 1532.49 1584.84 1533.45 Z" fill="#000000" fill-rule="nonzero" fill-opacity="1" /><polyline clip-path="url(#clip392)" style="stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none" points="235.283,1268.32 2352.76,1268.32 "/>
<polyline clip-path="url(#clip392)" style="stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none" points="235.283,1090.81 2352.76,1090.81 "/>
<polyline clip-path="url(#clip392)" style="stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none" points="235.283,913.302 2352.76,913.302 "/>
<polyline clip-path="url(#clip392)" style="stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none" points="235.283,735.791 2352.76,735.791 "/>
<polyline clip-path="url(#clip392)" style="stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none" points="235.283,558.28 2352.76,558.28 "/>
<polyline clip-path="url(#clip392)" style="stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none" points="235.283,380.768 2352.76,380.768 "/>
<polyline clip-path="url(#clip392)" style="stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none" points="235.283,203.257 2352.76,203.257 "/>
<polyline clip-path="url(#clip390)" style="stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none" points="235.283,1423.18 235.283,123.472 "/>
<polyline clip-path="url(#clip390)" style="stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none" points="235.283,1268.32 254.18,1268.32 "/>
<polyline clip-path="url(#clip390)" style="stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none" points="235.283,1090.81 254.18,1090.81 "/>
<polyline clip-path="url(#clip390)" style="stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none" points="235.283,913.302 254.18,913.302 "/>
<polyline clip-path="url(#clip390)" style="stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none" points="235.283,735.791 254.18,735.791 "/>
<polyline clip-path="url(#clip390)" style="stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none" points="235.283,558.28 254.18,558.28 "/>
<polyline clip-path="url(#clip390)" style="stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none" points="235.283,380.768 254.18,380.768 "/>
<polyline clip-path="url(#clip390)" style="stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none" points="235.283,203.257 254.18,203.257 "/>
<path clip-path="url(#clip390)" d="M129.862 1255.12 L118.056 1273.57 L129.862 1273.57 L129.862 1255.12 M128.635 1251.04 L134.515 1251.04 L134.515 1273.57 L139.445 1273.57 L139.445 1277.46 L134.515 1277.46 L134.515 1285.6 L129.862 1285.6 L129.862 1277.46 L114.26 1277.46 L114.26 1272.94 L128.635 1251.04 Z" fill="#000000" fill-rule="nonzero" fill-opacity="1" /><path clip-path="url(#clip390)" d="M147.223 1251.04 L165.579 1251.04 L165.579 1254.98 L151.505 1254.98 L151.505 1263.45 Q152.524 1263.1 153.542 1262.94 Q154.561 1262.76 155.579 1262.76 Q161.366 1262.76 164.746 1265.93 Q168.126 1269.1 168.126 1274.52 Q168.126 1280.1 164.653 1283.2 Q161.181 1286.28 154.862 1286.28 Q152.686 1286.28 150.417 1285.91 Q148.172 1285.54 145.765 1284.79 L145.765 1280.1 Q147.848 1281.23 150.07 1281.79 Q152.292 1282.34 154.769 1282.34 Q158.774 1282.34 161.112 1280.23 Q163.45 1278.13 163.45 1274.52 Q163.45 1270.91 161.112 1268.8 Q158.774 1266.69 154.769 1266.69 Q152.894 1266.69 151.019 1267.11 Q149.167 1267.53 147.223 1268.41 L147.223 1251.04 Z" fill="#000000" fill-rule="nonzero" fill-opacity="1" /><path clip-path="url(#clip390)" d="M187.338 1254.12 Q183.727 1254.12 181.899 1257.69 Q180.093 1261.23 180.093 1268.36 Q180.093 1275.47 181.899 1279.03 Q183.727 1282.57 187.338 1282.57 Q190.973 1282.57 192.778 1279.03 Q194.607 1275.47 194.607 1268.36 Q194.607 1261.23 192.778 1257.69 Q190.973 1254.12 187.338 1254.12 M187.338 1250.42 Q193.149 1250.42 196.204 1255.03 Q199.283 1259.61 199.283 1268.36 Q199.283 1277.09 196.204 1281.69 Q193.149 1286.28 187.338 1286.28 Q181.528 1286.28 178.45 1281.69 Q175.394 1277.09 175.394 1268.36 Q175.394 1259.61 178.45 1255.03 Q181.528 1250.42 187.338 1250.42 Z" fill="#000000" fill-rule="nonzero" fill-opacity="1" /><path clip-path="url(#clip390)" d="M117.061 1073.53 L135.417 1073.53 L135.417 1077.47 L121.343 1077.47 L121.343 1085.94 Q122.362 1085.59 123.38 1085.43 Q124.399 1085.25 125.418 1085.25 Q131.205 1085.25 134.584 1088.42 Q137.964 1091.59 137.964 1097.01 Q137.964 1102.58 134.492 1105.69 Q131.019 1108.76 124.7 1108.76 Q122.524 1108.76 120.255 1108.39 Q118.01 1108.02 115.603 1107.28 L115.603 1102.58 Q117.686 1103.72 119.908 1104.27 Q122.13 1104.83 124.607 1104.83 Q128.612 1104.83 130.95 1102.72 Q133.288 1100.62 133.288 1097.01 Q133.288 1093.39 130.95 1091.29 Q128.612 1089.18 124.607 1089.18 Q122.732 1089.18 120.857 1089.6 Q119.006 1090.01 117.061 1090.89 L117.061 1073.53 Z" fill="#000000" fill-rule="nonzero" fill-opacity="1" /><path clip-path="url(#clip390)" d="M157.177 1076.61 Q153.566 1076.61 151.737 1080.18 Q149.931 1083.72 149.931 1090.85 Q149.931 1097.95 151.737 1101.52 Q153.566 1105.06 157.177 1105.06 Q160.811 1105.06 162.616 1101.52 Q164.445 1097.95 164.445 1090.85 Q164.445 1083.72 162.616 1080.18 Q160.811 1076.61 157.177 1076.61 M157.177 1072.91 Q162.987 1072.91 166.042 1077.52 Q169.121 1082.1 169.121 1090.85 Q169.121 1099.58 166.042 1104.18 Q162.987 1108.76 157.177 1108.76 Q151.366 1108.76 148.288 1104.18 Q145.232 1099.58 145.232 1090.85 Q145.232 1082.1 148.288 1077.52 Q151.366 1072.91 157.177 1072.91 Z" fill="#000000" fill-rule="nonzero" fill-opacity="1" /><path clip-path="url(#clip390)" d="M187.338 1076.61 Q183.727 1076.61 181.899 1080.18 Q180.093 1083.72 180.093 1090.85 Q180.093 1097.95 181.899 1101.52 Q183.727 1105.06 187.338 1105.06 Q190.973 1105.06 192.778 1101.52 Q194.607 1097.95 194.607 1090.85 Q194.607 1083.72 192.778 1080.18 Q190.973 1076.61 187.338 1076.61 M187.338 1072.91 Q193.149 1072.91 196.204 1077.52 Q199.283 1082.1 199.283 1090.85 Q199.283 1099.58 196.204 1104.18 Q193.149 1108.76 187.338 1108.76 Q181.528 1108.76 178.45 1104.18 Q175.394 1099.58 175.394 1090.85 Q175.394 1082.1 178.45 1077.52 Q181.528 1072.91 187.338 1072.91 Z" fill="#000000" fill-rule="nonzero" fill-opacity="1" /><path clip-path="url(#clip390)" d="M117.061 896.022 L135.417 896.022 L135.417 899.957 L121.343 899.957 L121.343 908.43 Q122.362 908.082 123.38 907.92 Q124.399 907.735 125.418 907.735 Q131.205 907.735 134.584 910.906 Q137.964 914.078 137.964 919.494 Q137.964 925.073 134.492 928.175 Q131.019 931.254 124.7 931.254 Q122.524 931.254 120.255 930.883 Q118.01 930.513 115.603 929.772 L115.603 925.073 Q117.686 926.207 119.908 926.763 Q122.13 927.318 124.607 927.318 Q128.612 927.318 130.95 925.212 Q133.288 923.105 133.288 919.494 Q133.288 915.883 130.95 913.777 Q128.612 911.67 124.607 911.67 Q122.732 911.67 120.857 912.087 Q119.006 912.504 117.061 913.383 L117.061 896.022 Z" fill="#000000" fill-rule="nonzero" fill-opacity="1" /><path clip-path="url(#clip390)" d="M147.223 896.022 L165.579 896.022 L165.579 899.957 L151.505 899.957 L151.505 908.43 Q152.524 908.082 153.542 907.92 Q154.561 907.735 155.579 907.735 Q161.366 907.735 164.746 910.906 Q168.126 914.078 168.126 919.494 Q168.126 925.073 164.653 928.175 Q161.181 931.254 154.862 931.254 Q152.686 931.254 150.417 930.883 Q148.172 930.513 145.765 929.772 L145.765 925.073 Q147.848 926.207 150.07 926.763 Q152.292 927.318 154.769 927.318 Q158.774 927.318 161.112 925.212 Q163.45 923.105 163.45 919.494 Q163.45 915.883 161.112 913.777 Q158.774 911.67 154.769 911.67 Q152.894 911.67 151.019 912.087 Q149.167 912.504 147.223 913.383 L147.223 896.022 Z" fill="#000000" fill-rule="nonzero" fill-opacity="1" /><path clip-path="url(#clip390)" d="M187.338 899.101 Q183.727 899.101 181.899 902.666 Q180.093 906.207 180.093 913.337 Q180.093 920.443 181.899 924.008 Q183.727 927.55 187.338 927.55 Q190.973 927.55 192.778 924.008 Q194.607 920.443 194.607 913.337 Q194.607 906.207 192.778 902.666 Q190.973 899.101 187.338 899.101 M187.338 895.397 Q193.149 895.397 196.204 900.004 Q199.283 904.587 199.283 913.337 Q199.283 922.064 196.204 926.67 Q193.149 931.254 187.338 931.254 Q181.528 931.254 178.45 926.67 Q175.394 922.064 175.394 913.337 Q175.394 904.587 178.45 900.004 Q181.528 895.397 187.338 895.397 Z" fill="#000000" fill-rule="nonzero" fill-opacity="1" /><path clip-path="url(#clip390)" d="M127.593 733.928 Q124.445 733.928 122.593 736.08 Q120.765 738.233 120.765 741.983 Q120.765 745.71 122.593 747.886 Q124.445 750.039 127.593 750.039 Q130.742 750.039 132.57 747.886 Q134.422 745.71 134.422 741.983 Q134.422 738.233 132.57 736.08 Q130.742 733.928 127.593 733.928 M136.876 719.275 L136.876 723.534 Q135.117 722.701 133.311 722.261 Q131.529 721.821 129.769 721.821 Q125.14 721.821 122.686 724.946 Q120.255 728.071 119.908 734.391 Q121.274 732.377 123.334 731.312 Q125.394 730.224 127.871 730.224 Q133.08 730.224 136.089 733.395 Q139.121 736.543 139.121 741.983 Q139.121 747.307 135.973 750.525 Q132.825 753.742 127.593 753.742 Q121.598 753.742 118.427 749.159 Q115.256 744.553 115.256 735.826 Q115.256 727.631 119.144 722.77 Q123.033 717.886 129.584 717.886 Q131.343 717.886 133.126 718.233 Q134.931 718.58 136.876 719.275 Z" fill="#000000" fill-rule="nonzero" fill-opacity="1" /><path clip-path="url(#clip390)" d="M157.177 721.59 Q153.566 721.59 151.737 725.154 Q149.931 728.696 149.931 735.826 Q149.931 742.932 151.737 746.497 Q153.566 750.039 157.177 750.039 Q160.811 750.039 162.616 746.497 Q164.445 742.932 164.445 735.826 Q164.445 728.696 162.616 725.154 Q160.811 721.59 157.177 721.59 M157.177 717.886 Q162.987 717.886 166.042 722.492 Q169.121 727.076 169.121 735.826 Q169.121 744.553 166.042 749.159 Q162.987 753.742 157.177 753.742 Q151.366 753.742 148.288 749.159 Q145.232 744.553 145.232 735.826 Q145.232 727.076 148.288 722.492 Q151.366 717.886 157.177 717.886 Z" fill="#000000" fill-rule="nonzero" fill-opacity="1" /><path clip-path="url(#clip390)" d="M187.338 721.59 Q183.727 721.59 181.899 725.154 Q180.093 728.696 180.093 735.826 Q180.093 742.932 181.899 746.497 Q183.727 750.039 187.338 750.039 Q190.973 750.039 192.778 746.497 Q194.607 742.932 194.607 735.826 Q194.607 728.696 192.778 725.154 Q190.973 721.59 187.338 721.59 M187.338 717.886 Q193.149 717.886 196.204 722.492 Q199.283 727.076 199.283 735.826 Q199.283 744.553 196.204 749.159 Q193.149 753.742 187.338 753.742 Q181.528 753.742 178.45 749.159 Q175.394 744.553 175.394 735.826 Q175.394 727.076 178.45 722.492 Q181.528 717.886 187.338 717.886 Z" fill="#000000" fill-rule="nonzero" fill-opacity="1" /><path clip-path="url(#clip390)" d="M127.593 556.416 Q124.445 556.416 122.593 558.569 Q120.765 560.722 120.765 564.472 Q120.765 568.199 122.593 570.375 Q124.445 572.527 127.593 572.527 Q130.742 572.527 132.57 570.375 Q134.422 568.199 134.422 564.472 Q134.422 560.722 132.57 558.569 Q130.742 556.416 127.593 556.416 M136.876 541.764 L136.876 546.023 Q135.117 545.19 133.311 544.75 Q131.529 544.31 129.769 544.31 Q125.14 544.31 122.686 547.435 Q120.255 550.56 119.908 556.879 Q121.274 554.865 123.334 553.801 Q125.394 552.713 127.871 552.713 Q133.08 552.713 136.089 555.884 Q139.121 559.032 139.121 564.472 Q139.121 569.796 135.973 573.013 Q132.825 576.231 127.593 576.231 Q121.598 576.231 118.427 571.648 Q115.256 567.041 115.256 558.314 Q115.256 550.12 119.144 545.259 Q123.033 540.375 129.584 540.375 Q131.343 540.375 133.126 540.722 Q134.931 541.069 136.876 541.764 Z" fill="#000000" fill-rule="nonzero" fill-opacity="1" /><path clip-path="url(#clip390)" d="M147.223 541 L165.579 541 L165.579 544.935 L151.505 544.935 L151.505 553.407 Q152.524 553.06 153.542 552.898 Q154.561 552.713 155.579 552.713 Q161.366 552.713 164.746 555.884 Q168.126 559.055 168.126 564.472 Q168.126 570.051 164.653 573.152 Q161.181 576.231 154.862 576.231 Q152.686 576.231 150.417 575.861 Q148.172 575.49 145.765 574.75 L145.765 570.051 Q147.848 571.185 150.07 571.74 Q152.292 572.296 154.769 572.296 Q158.774 572.296 161.112 570.189 Q163.45 568.083 163.45 564.472 Q163.45 560.861 161.112 558.754 Q158.774 556.648 154.769 556.648 Q152.894 556.648 151.019 557.064 Q149.167 557.481 147.223 558.361 L147.223 541 Z" fill="#000000" fill-rule="nonzero" fill-opacity="1" /><path clip-path="url(#clip390)" d="M187.338 544.078 Q183.727 544.078 181.899 547.643 Q180.093 551.185 180.093 558.314 Q180.093 565.421 181.899 568.986 Q183.727 572.527 187.338 572.527 Q190.973 572.527 192.778 568.986 Q194.607 565.421 194.607 558.314 Q194.607 551.185 192.778 547.643 Q190.973 544.078 187.338 544.078 M187.338 540.375 Q193.149 540.375 196.204 544.981 Q199.283 549.565 199.283 558.314 Q199.283 567.041 196.204 571.648 Q193.149 576.231 187.338 576.231 Q181.528 576.231 178.45 571.648 Q175.394 567.041 175.394 558.314 Q175.394 549.565 178.45 544.981 Q181.528 540.375 187.338 540.375 Z" fill="#000000" fill-rule="nonzero" fill-opacity="1" /><path clip-path="url(#clip390)" d="M115.834 363.488 L138.056 363.488 L138.056 365.479 L125.51 398.048 L120.626 398.048 L132.431 367.424 L115.834 367.424 L115.834 363.488 Z" fill="#000000" fill-rule="nonzero" fill-opacity="1" /><path clip-path="url(#clip390)" d="M157.177 366.567 Q153.566 366.567 151.737 370.132 Q149.931 373.674 149.931 380.803 Q149.931 387.91 151.737 391.474 Q153.566 395.016 157.177 395.016 Q160.811 395.016 162.616 391.474 Q164.445 387.91 164.445 380.803 Q164.445 373.674 162.616 370.132 Q160.811 366.567 157.177 366.567 M157.177 362.863 Q162.987 362.863 166.042 367.47 Q169.121 372.053 169.121 380.803 Q169.121 389.53 166.042 394.136 Q162.987 398.72 157.177 398.72 Q151.366 398.72 148.288 394.136 Q145.232 389.53 145.232 380.803 Q145.232 372.053 148.288 367.47 Q151.366 362.863 157.177 362.863 Z" fill="#000000" fill-rule="nonzero" fill-opacity="1" /><path clip-path="url(#clip390)" d="M187.338 366.567 Q183.727 366.567 181.899 370.132 Q180.093 373.674 180.093 380.803 Q180.093 387.91 181.899 391.474 Q183.727 395.016 187.338 395.016 Q190.973 395.016 192.778 391.474 Q194.607 387.91 194.607 380.803 Q194.607 373.674 192.778 370.132 Q190.973 366.567 187.338 366.567 M187.338 362.863 Q193.149 362.863 196.204 367.47 Q199.283 372.053 199.283 380.803 Q199.283 389.53 196.204 394.136 Q193.149 398.72 187.338 398.72 Q181.528 398.72 178.45 394.136 Q175.394 389.53 175.394 380.803 Q175.394 372.053 178.45 367.47 Q181.528 362.863 187.338 362.863 Z" fill="#000000" fill-rule="nonzero" fill-opacity="1" /><path clip-path="url(#clip390)" d="M115.834 185.977 L138.056 185.977 L138.056 187.968 L125.51 220.537 L120.626 220.537 L132.431 189.912 L115.834 189.912 L115.834 185.977 Z" fill="#000000" fill-rule="nonzero" fill-opacity="1" /><path clip-path="url(#clip390)" d="M147.223 185.977 L165.579 185.977 L165.579 189.912 L151.505 189.912 L151.505 198.385 Q152.524 198.037 153.542 197.875 Q154.561 197.69 155.579 197.69 Q161.366 197.69 164.746 200.861 Q168.126 204.033 168.126 209.449 Q168.126 215.028 164.653 218.13 Q161.181 221.208 154.862 221.208 Q152.686 221.208 150.417 220.838 Q148.172 220.468 145.765 219.727 L145.765 215.028 Q147.848 216.162 150.07 216.718 Q152.292 217.273 154.769 217.273 Q158.774 217.273 161.112 215.167 Q163.45 213.06 163.45 209.449 Q163.45 205.838 161.112 203.732 Q158.774 201.625 154.769 201.625 Q152.894 201.625 151.019 202.042 Q149.167 202.459 147.223 203.338 L147.223 185.977 Z" fill="#000000" fill-rule="nonzero" fill-opacity="1" /><path clip-path="url(#clip390)" d="M187.338 189.056 Q183.727 189.056 181.899 192.621 Q180.093 196.162 180.093 203.292 Q180.093 210.398 181.899 213.963 Q183.727 217.505 187.338 217.505 Q190.973 217.505 192.778 213.963 Q194.607 210.398 194.607 203.292 Q194.607 196.162 192.778 192.621 Q190.973 189.056 187.338 189.056 M187.338 185.352 Q193.149 185.352 196.204 189.959 Q199.283 194.542 199.283 203.292 Q199.283 212.019 196.204 216.625 Q193.149 221.208 187.338 221.208 Q181.528 221.208 178.45 216.625 Q175.394 212.019 175.394 203.292 Q175.394 194.542 178.45 189.959 Q181.528 185.352 187.338 185.352 Z" fill="#000000" fill-rule="nonzero" fill-opacity="1" /><path clip-path="url(#clip390)" d="M14.479 1005.87 L14.479 1000.01 L64.0042 1000.01 L64.0042 1005.87 L14.479 1005.87 Z" fill="#000000" fill-rule="nonzero" fill-opacity="1" /><path clip-path="url(#clip390)" d="M32.4621 973.941 Q32.4621 978.652 36.1542 981.389 Q39.8145 984.126 46.212 984.126 Q52.6095 984.126 56.3017 981.421 Q59.9619 978.684 59.9619 973.941 Q59.9619 969.262 56.2698 966.525 Q52.5777 963.788 46.212 963.788 Q39.8781 963.788 36.186 966.525 Q32.4621 969.262 32.4621 973.941 M27.4968 973.941 Q27.4968 966.302 32.4621 961.942 Q37.4273 957.581 46.212 957.581 Q54.9649 957.581 59.9619 961.942 Q64.9272 966.302 64.9272 973.941 Q64.9272 981.612 59.9619 985.972 Q54.9649 990.301 46.212 990.301 Q37.4273 990.301 32.4621 985.972 Q27.4968 981.612 27.4968 973.941 Z" fill="#000000" fill-rule="nonzero" fill-opacity="1" /><path clip-path="url(#clip390)" d="M28.3562 951.279 L28.3562 945.423 L56.1743 938.102 L28.3562 930.814 L28.3562 923.907 L56.1743 916.586 L28.3562 909.297 L28.3562 903.441 L64.0042 912.767 L64.0042 919.674 L34.7856 927.344 L64.0042 935.047 L64.0042 941.954 L28.3562 951.279 Z" fill="#000000" fill-rule="nonzero" fill-opacity="1" /><path clip-path="url(#clip390)" d="M44.7161 864.069 L47.5806 864.069 L47.5806 890.996 Q53.6281 890.614 56.8109 887.368 Q59.9619 884.089 59.9619 878.265 Q59.9619 874.891 59.1344 871.74 Q58.3069 868.557 56.6518 865.438 L62.1899 865.438 Q63.5267 868.589 64.227 871.899 Q64.9272 875.209 64.9272 878.615 Q64.9272 887.145 59.9619 892.142 Q54.9967 897.107 46.5303 897.107 Q37.7774 897.107 32.6531 892.397 Q27.4968 887.654 27.4968 879.633 Q27.4968 872.44 32.1438 868.27 Q36.7589 864.069 44.7161 864.069 M42.9973 869.926 Q38.1912 869.989 35.3266 872.631 Q32.4621 875.241 32.4621 879.57 Q32.4621 884.471 35.2312 887.431 Q38.0002 890.36 43.0292 890.805 L42.9973 869.926 Z" fill="#000000" fill-rule="nonzero" fill-opacity="1" /><path clip-path="url(#clip390)" d="M33.8307 833.8 Q33.2578 834.787 33.0032 835.965 Q32.7167 837.11 32.7167 838.511 Q32.7167 843.476 35.9632 846.15 Q39.1779 848.791 45.2253 848.791 L64.0042 848.791 L64.0042 854.68 L28.3562 854.68 L28.3562 848.791 L33.8944 848.791 Q30.6479 846.945 29.0883 843.985 Q27.4968 841.025 27.4968 836.792 Q27.4968 836.187 27.5923 835.455 Q27.656 834.723 27.8151 833.832 L33.8307 833.8 Z" fill="#000000" fill-rule="nonzero" fill-opacity="1" /><path clip-path="url(#clip390)" d="M28.3562 806.937 L28.3562 801.08 L64.0042 801.08 L64.0042 806.937 L28.3562 806.937 M14.479 806.937 L14.479 801.08 L21.895 801.08 L21.895 806.937 L14.479 806.937 Z" fill="#000000" fill-rule="nonzero" fill-opacity="1" /><path clip-path="url(#clip390)" d="M29.4065 766.101 L34.9447 766.101 Q33.6716 768.584 33.035 771.257 Q32.3984 773.931 32.3984 776.795 Q32.3984 781.156 33.7352 783.352 Q35.072 785.516 37.7456 785.516 Q39.7826 785.516 40.9603 783.957 Q42.1061 782.397 43.1565 777.686 L43.6021 775.681 Q44.9389 769.443 47.3897 766.833 Q49.8086 764.191 54.1691 764.191 Q59.1344 764.191 62.0308 768.138 Q64.9272 772.053 64.9272 778.928 Q64.9272 781.792 64.3543 784.912 Q63.8132 787.999 62.6992 791.436 L56.6518 791.436 Q58.3387 788.19 59.198 785.039 Q60.0256 781.888 60.0256 778.8 Q60.0256 774.663 58.6251 772.435 Q57.1929 770.207 54.6147 770.207 Q52.2276 770.207 50.9545 771.83 Q49.6813 773.421 48.5037 778.864 L48.0262 780.901 Q46.8804 786.344 44.5251 788.763 Q42.138 791.182 38.0002 791.182 Q32.9713 791.182 30.2341 787.617 Q27.4968 784.052 27.4968 777.496 Q27.4968 774.249 27.9743 771.384 Q28.4517 768.52 29.4065 766.101 Z" fill="#000000" fill-rule="nonzero" fill-opacity="1" /><path clip-path="url(#clip390)" d="M46.212 708.555 Q39.7508 708.555 36.0905 711.229 Q32.3984 713.87 32.3984 718.517 Q32.3984 723.164 36.0905 725.838 Q39.7508 728.48 46.212 728.48 Q52.6732 728.48 56.3653 725.838 Q60.0256 723.164 60.0256 718.517 Q60.0256 713.87 56.3653 711.229 Q52.6732 708.555 46.212 708.555 M33.7671 728.48 Q30.5842 726.634 29.0564 723.833 Q27.4968 721 27.4968 717.085 Q27.4968 710.592 32.6531 706.55 Q37.8093 702.476 46.212 702.476 Q54.6147 702.476 59.771 706.55 Q64.9272 710.592 64.9272 717.085 Q64.9272 721 63.3994 723.833 Q61.8398 726.634 58.657 728.48 L64.0042 728.48 L64.0042 734.368 L14.479 734.368 L14.479 728.48 L33.7671 728.48 Z" fill="#000000" fill-rule="nonzero" fill-opacity="1" /><path clip-path="url(#clip390)" d="M44.7161 662.276 L47.5806 662.276 L47.5806 689.203 Q53.6281 688.821 56.8109 685.575 Q59.9619 682.296 59.9619 676.472 Q59.9619 673.098 59.1344 669.947 Q58.3069 666.764 56.6518 663.645 L62.1899 663.645 Q63.5267 666.796 64.227 670.106 Q64.9272 673.416 64.9272 676.822 Q64.9272 685.352 59.9619 690.349 Q54.9967 695.314 46.5303 695.314 Q37.7774 695.314 32.6531 690.604 Q27.4968 685.861 27.4968 677.84 Q27.4968 670.647 32.1438 666.478 Q36.7589 662.276 44.7161 662.276 M42.9973 668.133 Q38.1912 668.196 35.3266 670.838 Q32.4621 673.448 32.4621 677.777 Q32.4621 682.678 35.2312 685.638 Q38.0002 688.567 43.0292 689.012 L42.9973 668.133 Z" fill="#000000" fill-rule="nonzero" fill-opacity="1" /><path clip-path="url(#clip390)" d="M18.2347 646.871 L28.3562 646.871 L28.3562 634.808 L32.9077 634.808 L32.9077 646.871 L52.2594 646.871 Q56.6199 646.871 57.8613 645.694 Q59.1026 644.484 59.1026 640.824 L59.1026 634.808 L64.0042 634.808 L64.0042 640.824 Q64.0042 647.603 61.4897 650.181 Q58.9434 652.76 52.2594 652.76 L32.9077 652.76 L32.9077 657.056 L28.3562 657.056 L28.3562 652.76 L18.2347 652.76 L18.2347 646.871 Z" fill="#000000" fill-rule="nonzero" fill-opacity="1" /><path clip-path="url(#clip390)" d="M18.2347 621.313 L28.3562 621.313 L28.3562 609.25 L32.9077 609.25 L32.9077 621.313 L52.2594 621.313 Q56.6199 621.313 57.8613 620.135 Q59.1026 618.926 59.1026 615.265 L59.1026 609.25 L64.0042 609.25 L64.0042 615.265 Q64.0042 622.045 61.4897 624.623 Q58.9434 627.201 52.2594 627.201 L32.9077 627.201 L32.9077 631.498 L28.3562 631.498 L28.3562 627.201 L18.2347 627.201 L18.2347 621.313 Z" fill="#000000" fill-rule="nonzero" fill-opacity="1" /><path clip-path="url(#clip390)" d="M44.7161 571.056 L47.5806 571.056 L47.5806 597.983 Q53.6281 597.601 56.8109 594.354 Q59.9619 591.076 59.9619 585.251 Q59.9619 581.877 59.1344 578.726 Q58.3069 575.543 56.6518 572.424 L62.1899 572.424 Q63.5267 575.575 64.227 578.885 Q64.9272 582.196 64.9272 585.601 Q64.9272 594.131 59.9619 599.128 Q54.9967 604.094 46.5303 604.094 Q37.7774 604.094 32.6531 599.383 Q27.4968 594.641 27.4968 586.62 Q27.4968 579.427 32.1438 575.257 Q36.7589 571.056 44.7161 571.056 M42.9973 576.912 Q38.1912 576.976 35.3266 579.618 Q32.4621 582.227 32.4621 586.556 Q32.4621 591.458 35.2312 594.418 Q38.0002 597.346 43.0292 597.792 L42.9973 576.912 Z" fill="#000000" fill-rule="nonzero" fill-opacity="1" /><path clip-path="url(#clip390)" d="M33.8307 540.787 Q33.2578 541.773 33.0032 542.951 Q32.7167 544.097 32.7167 545.497 Q32.7167 550.463 35.9632 553.136 Q39.1779 555.778 45.2253 555.778 L64.0042 555.778 L64.0042 561.666 L28.3562 561.666 L28.3562 555.778 L33.8944 555.778 Q30.6479 553.932 29.0883 550.972 Q27.4968 548.012 27.4968 543.779 Q27.4968 543.174 27.5923 542.442 Q27.656 541.71 27.8151 540.819 L33.8307 540.787 Z" fill="#000000" fill-rule="nonzero" fill-opacity="1" /><path clip-path="url(#clip390)" d="M444.038 12.096 L452.221 12.096 L452.221 72.576 L444.038 72.576 L444.038 12.096 Z" fill="#000000" fill-rule="nonzero" fill-opacity="1" /><path clip-path="url(#clip390)" d="M505.896 45.1919 L505.896 72.576 L498.442 72.576 L498.442 45.4349 Q498.442 38.994 495.93 35.7938 Q493.419 32.5936 488.396 32.5936 Q482.36 32.5936 478.876 36.4419 Q475.392 40.2903 475.392 46.9338 L475.392 72.576 L467.898 72.576 L467.898 27.2059 L475.392 27.2059 L475.392 34.2544 Q478.066 30.163 481.671 28.1376 Q485.317 26.1121 490.057 26.1121 Q497.875 26.1121 501.885 30.9732 Q505.896 35.7938 505.896 45.1919 Z" fill="#000000" fill-rule="nonzero" fill-opacity="1" /><path clip-path="url(#clip390)" d="M543.731 9.54393 L543.731 15.7418 L536.602 15.7418 Q532.591 15.7418 531.011 17.3622 Q529.472 18.9825 529.472 23.1955 L529.472 27.2059 L541.746 27.2059 L541.746 32.9987 L529.472 32.9987 L529.472 72.576 L521.978 72.576 L521.978 32.9987 L514.848 32.9987 L514.848 27.2059 L521.978 27.2059 L521.978 24.0462 Q521.978 16.471 525.502 13.0277 Q529.026 9.54393 536.683 9.54393 L543.731 9.54393 Z" fill="#000000" fill-rule="nonzero" fill-opacity="1" /><path clip-path="url(#clip390)" d="M567.55 32.4315 Q561.555 32.4315 558.071 37.1306 Q554.588 41.7891 554.588 49.9314 Q554.588 58.0738 558.031 62.7728 Q561.515 67.4314 567.55 67.4314 Q573.505 67.4314 576.989 62.7323 Q580.473 58.0333 580.473 49.9314 Q580.473 41.8701 576.989 37.1711 Q573.505 32.4315 567.55 32.4315 M567.55 26.1121 Q577.273 26.1121 582.822 32.4315 Q588.372 38.7509 588.372 49.9314 Q588.372 61.0714 582.822 67.4314 Q577.273 73.7508 567.55 73.7508 Q557.788 73.7508 552.238 67.4314 Q546.729 61.0714 546.729 49.9314 Q546.729 38.7509 552.238 32.4315 Q557.788 26.1121 567.55 26.1121 Z" fill="#000000" fill-rule="nonzero" fill-opacity="1" /><path clip-path="url(#clip390)" d="M627.018 34.1734 Q625.762 33.4443 624.263 33.1202 Q622.805 32.7556 621.022 32.7556 Q614.703 32.7556 611.3 36.8875 Q607.938 40.9789 607.938 48.6757 L607.938 72.576 L600.444 72.576 L600.444 27.2059 L607.938 27.2059 L607.938 34.2544 Q610.288 30.1225 614.055 28.1376 Q617.822 26.1121 623.21 26.1121 Q623.98 26.1121 624.911 26.2337 Q625.843 26.3147 626.977 26.5172 L627.018 34.1734 Z" fill="#000000" fill-rule="nonzero" fill-opacity="1" /><path clip-path="url(#clip390)" d="M668.702 35.9153 Q671.497 30.8922 675.386 28.5022 Q679.274 26.1121 684.541 26.1121 Q691.63 26.1121 695.478 31.0947 Q699.326 36.0368 699.326 45.1919 L699.326 72.576 L691.832 72.576 L691.832 45.4349 Q691.832 38.913 689.523 35.7533 Q687.214 32.5936 682.475 32.5936 Q676.682 32.5936 673.32 36.4419 Q669.957 40.2903 669.957 46.9338 L669.957 72.576 L662.463 72.576 L662.463 45.4349 Q662.463 38.8725 660.154 35.7533 Q657.845 32.5936 653.025 32.5936 Q647.313 32.5936 643.951 36.4824 Q640.588 40.3308 640.588 46.9338 L640.588 72.576 L633.094 72.576 L633.094 27.2059 L640.588 27.2059 L640.588 34.2544 Q643.14 30.082 646.705 28.0971 Q650.27 26.1121 655.172 26.1121 Q660.114 26.1121 663.557 28.6237 Q667.041 31.1352 668.702 35.9153 Z" fill="#000000" fill-rule="nonzero" fill-opacity="1" /><path clip-path="url(#clip390)" d="M734.812 49.7694 Q725.779 49.7694 722.295 51.8354 Q718.811 53.9013 718.811 58.8839 Q718.811 62.8538 721.404 65.2034 Q724.037 67.5124 728.533 67.5124 Q734.731 67.5124 738.458 63.1374 Q742.225 58.7219 742.225 51.4303 L742.225 49.7694 L734.812 49.7694 M749.679 46.6907 L749.679 72.576 L742.225 72.576 L742.225 65.6895 Q739.673 69.8214 735.866 71.8063 Q732.058 73.7508 726.548 73.7508 Q719.581 73.7508 715.449 69.8619 Q711.358 65.9325 711.358 59.3701 Q711.358 51.7138 716.462 47.825 Q721.606 43.9361 731.774 43.9361 L742.225 43.9361 L742.225 43.2069 Q742.225 38.0623 738.823 35.2672 Q735.46 32.4315 729.344 32.4315 Q725.455 32.4315 721.768 33.3632 Q718.082 34.295 714.679 36.1584 L714.679 29.2718 Q718.771 27.692 722.619 26.9223 Q726.467 26.1121 730.113 26.1121 Q739.957 26.1121 744.818 31.2163 Q749.679 36.3204 749.679 46.6907 Z" fill="#000000" fill-rule="nonzero" fill-opacity="1" /><path clip-path="url(#clip390)" d="M772.405 14.324 L772.405 27.2059 L787.758 27.2059 L787.758 32.9987 L772.405 32.9987 L772.405 57.6282 Q772.405 63.1779 773.904 64.7578 Q775.443 66.3376 780.101 66.3376 L787.758 66.3376 L787.758 72.576 L780.101 72.576 Q771.473 72.576 768.192 69.3758 Q764.911 66.1351 764.911 57.6282 L764.911 32.9987 L759.442 32.9987 L759.442 27.2059 L764.911 27.2059 L764.911 14.324 L772.405 14.324 Z" fill="#000000" fill-rule="nonzero" fill-opacity="1" /><path clip-path="url(#clip390)" d="M797.561 27.2059 L805.014 27.2059 L805.014 72.576 L797.561 72.576 L797.561 27.2059 M797.561 9.54393 L805.014 9.54393 L805.014 18.9825 L797.561 18.9825 L797.561 9.54393 Z" fill="#000000" fill-rule="nonzero" fill-opacity="1" /><path clip-path="url(#clip390)" d="M838.191 32.4315 Q832.196 32.4315 828.712 37.1306 Q825.228 41.7891 825.228 49.9314 Q825.228 58.0738 828.672 62.7728 Q832.156 67.4314 838.191 67.4314 Q844.146 67.4314 847.63 62.7323 Q851.114 58.0333 851.114 49.9314 Q851.114 41.8701 847.63 37.1711 Q844.146 32.4315 838.191 32.4315 M838.191 26.1121 Q847.914 26.1121 853.463 32.4315 Q859.013 38.7509 859.013 49.9314 Q859.013 61.0714 853.463 67.4314 Q847.914 73.7508 838.191 73.7508 Q828.429 73.7508 822.879 67.4314 Q817.37 61.0714 817.37 49.9314 Q817.37 38.7509 822.879 32.4315 Q828.429 26.1121 838.191 26.1121 Z" fill="#000000" fill-rule="nonzero" fill-opacity="1" /><path clip-path="url(#clip390)" d="M909.082 45.1919 L909.082 72.576 L901.629 72.576 L901.629 45.4349 Q901.629 38.994 899.117 35.7938 Q896.605 32.5936 891.582 32.5936 Q885.546 32.5936 882.063 36.4419 Q878.579 40.2903 878.579 46.9338 L878.579 72.576 L871.085 72.576 L871.085 27.2059 L878.579 27.2059 L878.579 34.2544 Q881.252 30.163 884.858 28.1376 Q888.504 26.1121 893.243 26.1121 Q901.061 26.1121 905.072 30.9732 Q909.082 35.7938 909.082 45.1919 Z" fill="#000000" fill-rule="nonzero" fill-opacity="1" /><path clip-path="url(#clip390)" d="M982.971 28.9478 L982.971 35.9153 Q979.811 34.1734 976.611 33.3227 Q973.451 32.4315 970.21 32.4315 Q962.959 32.4315 958.949 37.0496 Q954.938 41.6271 954.938 49.9314 Q954.938 58.2358 958.949 62.8538 Q962.959 67.4314 970.21 67.4314 Q973.451 67.4314 976.611 66.5807 Q979.811 65.6895 982.971 63.9476 L982.971 70.8341 Q979.852 72.2924 976.489 73.0216 Q973.168 73.7508 969.4 73.7508 Q959.151 73.7508 953.116 67.3098 Q947.08 60.8689 947.08 49.9314 Q947.08 38.832 953.156 32.472 Q959.273 26.1121 969.886 26.1121 Q973.33 26.1121 976.611 26.8413 Q979.892 27.5299 982.971 28.9478 Z" fill="#000000" fill-rule="nonzero" fill-opacity="1" /><path clip-path="url(#clip390)" d="M1022.22 34.1734 Q1020.97 33.4443 1019.47 33.1202 Q1018.01 32.7556 1016.23 32.7556 Q1009.91 32.7556 1006.51 36.8875 Q1003.14 40.9789 1003.14 48.6757 L1003.14 72.576 L995.65 72.576 L995.65 27.2059 L1003.14 27.2059 L1003.14 34.2544 Q1005.49 30.1225 1009.26 28.1376 Q1013.03 26.1121 1018.42 26.1121 Q1019.19 26.1121 1020.12 26.2337 Q1021.05 26.3147 1022.18 26.5172 L1022.22 34.1734 Z" fill="#000000" fill-rule="nonzero" fill-opacity="1" /><path clip-path="url(#clip390)" d="M1030.04 27.2059 L1037.5 27.2059 L1037.5 72.576 L1030.04 72.576 L1030.04 27.2059 M1030.04 9.54393 L1037.5 9.54393 L1037.5 18.9825 L1030.04 18.9825 L1030.04 9.54393 Z" fill="#000000" fill-rule="nonzero" fill-opacity="1" /><path clip-path="url(#clip390)" d="M1060.46 14.324 L1060.46 27.2059 L1075.82 27.2059 L1075.82 32.9987 L1060.46 32.9987 L1060.46 57.6282 Q1060.46 63.1779 1061.96 64.7578 Q1063.5 66.3376 1068.16 66.3376 L1075.82 66.3376 L1075.82 72.576 L1068.16 72.576 Q1059.53 72.576 1056.25 69.3758 Q1052.97 66.1351 1052.97 57.6282 L1052.97 32.9987 L1047.5 32.9987 L1047.5 27.2059 L1052.97 27.2059 L1052.97 14.324 L1060.46 14.324 Z" fill="#000000" fill-rule="nonzero" fill-opacity="1" /><path clip-path="url(#clip390)" d="M1124.43 48.0275 L1124.43 51.6733 L1090.16 51.6733 Q1090.64 59.3701 1094.78 63.421 Q1098.95 67.4314 1106.36 67.4314 Q1110.66 67.4314 1114.67 66.3781 Q1118.72 65.3249 1122.69 63.2184 L1122.69 70.267 Q1118.68 71.9684 1114.46 72.8596 Q1110.25 73.7508 1105.92 73.7508 Q1095.06 73.7508 1088.7 67.4314 Q1082.38 61.1119 1082.38 50.3365 Q1082.38 39.1965 1088.38 32.6746 Q1094.41 26.1121 1104.62 26.1121 Q1113.77 26.1121 1119.08 32.0264 Q1124.43 37.9003 1124.43 48.0275 M1116.97 45.84 Q1116.89 39.7232 1113.53 36.0774 Q1110.21 32.4315 1104.7 32.4315 Q1098.46 32.4315 1094.69 35.9558 Q1090.97 39.4801 1090.4 45.8805 L1116.97 45.84 Z" fill="#000000" fill-rule="nonzero" fill-opacity="1" /><path clip-path="url(#clip390)" d="M1162.95 34.1734 Q1161.7 33.4443 1160.2 33.1202 Q1158.74 32.7556 1156.96 32.7556 Q1150.64 32.7556 1147.23 36.8875 Q1143.87 40.9789 1143.87 48.6757 L1143.87 72.576 L1136.38 72.576 L1136.38 27.2059 L1143.87 27.2059 L1143.87 34.2544 Q1146.22 30.1225 1149.99 28.1376 Q1153.76 26.1121 1159.14 26.1121 Q1159.91 26.1121 1160.85 26.2337 Q1161.78 26.3147 1162.91 26.5172 L1162.95 34.1734 Z" fill="#000000" fill-rule="nonzero" fill-opacity="1" /><path clip-path="url(#clip390)" d="M1170.77 27.2059 L1178.22 27.2059 L1178.22 72.576 L1170.77 72.576 L1170.77 27.2059 M1170.77 9.54393 L1178.22 9.54393 L1178.22 18.9825 L1170.77 18.9825 L1170.77 9.54393 Z" fill="#000000" fill-rule="nonzero" fill-opacity="1" /><path clip-path="url(#clip390)" d="M1214.44 49.7694 Q1205.41 49.7694 1201.92 51.8354 Q1198.44 53.9013 1198.44 58.8839 Q1198.44 62.8538 1201.03 65.2034 Q1203.66 67.5124 1208.16 67.5124 Q1214.36 67.5124 1218.09 63.1374 Q1221.85 58.7219 1221.85 51.4303 L1221.85 49.7694 L1214.44 49.7694 M1229.31 46.6907 L1229.31 72.576 L1221.85 72.576 L1221.85 65.6895 Q1219.3 69.8214 1215.49 71.8063 Q1211.68 73.7508 1206.18 73.7508 Q1199.21 73.7508 1195.08 69.8619 Q1190.98 65.9325 1190.98 59.3701 Q1190.98 51.7138 1196.09 47.825 Q1201.23 43.9361 1211.4 43.9361 L1221.85 43.9361 L1221.85 43.2069 Q1221.85 38.0623 1218.45 35.2672 Q1215.09 32.4315 1208.97 32.4315 Q1205.08 32.4315 1201.4 33.3632 Q1197.71 34.295 1194.31 36.1584 L1194.31 29.2718 Q1198.4 27.692 1202.25 26.9223 Q1206.09 26.1121 1209.74 26.1121 Q1219.58 26.1121 1224.45 31.2163 Q1229.31 36.3204 1229.31 46.6907 Z" fill="#000000" fill-rule="nonzero" fill-opacity="1" /><path clip-path="url(#clip390)" d="M1303.6 49.9314 Q1303.6 41.7081 1300.2 37.0496 Q1296.83 32.3505 1290.92 32.3505 Q1285.01 32.3505 1281.6 37.0496 Q1278.24 41.7081 1278.24 49.9314 Q1278.24 58.1548 1281.6 62.8538 Q1285.01 67.5124 1290.92 67.5124 Q1296.83 67.5124 1300.2 62.8538 Q1303.6 58.1548 1303.6 49.9314 M1278.24 34.0924 Q1280.59 30.0415 1284.16 28.0971 Q1287.76 26.1121 1292.74 26.1121 Q1301.01 26.1121 1306.15 32.6746 Q1311.34 39.2371 1311.34 49.9314 Q1311.34 60.6258 1306.15 67.1883 Q1301.01 73.7508 1292.74 73.7508 Q1287.76 73.7508 1284.16 71.8063 Q1280.59 69.8214 1278.24 65.7705 L1278.24 72.576 L1270.75 72.576 L1270.75 9.54393 L1278.24 9.54393 L1278.24 34.0924 Z" fill="#000000" fill-rule="nonzero" fill-opacity="1" /><path clip-path="url(#clip390)" d="M1342.57 76.7889 Q1339.41 84.8907 1336.41 87.3618 Q1333.41 89.8329 1328.39 89.8329 L1322.44 89.8329 L1322.44 83.5945 L1326.81 83.5945 Q1329.89 83.5945 1331.59 82.1361 Q1333.29 80.6778 1335.36 75.2496 L1336.7 71.8468 L1318.35 27.2059 L1326.24 27.2059 L1340.42 62.6918 L1354.6 27.2059 L1362.5 27.2059 L1342.57 76.7889 Z" fill="#000000" fill-rule="nonzero" fill-opacity="1" /><path clip-path="url(#clip390)" d="M1436.87 45.1919 L1436.87 72.576 L1429.42 72.576 L1429.42 45.4349 Q1429.42 38.994 1426.91 35.7938 Q1424.4 32.5936 1419.37 32.5936 Q1413.34 32.5936 1409.85 36.4419 Q1406.37 40.2903 1406.37 46.9338 L1406.37 72.576 L1398.88 72.576 L1398.88 27.2059 L1406.37 27.2059 L1406.37 34.2544 Q1409.04 30.163 1412.65 28.1376 Q1416.3 26.1121 1421.04 26.1121 Q1428.85 26.1121 1432.86 30.9732 Q1436.87 35.7938 1436.87 45.1919 Z" fill="#000000" fill-rule="nonzero" fill-opacity="1" /><path clip-path="url(#clip390)" d="M1450.97 54.671 L1450.97 27.2059 L1458.43 27.2059 L1458.43 54.3874 Q1458.43 60.8284 1460.94 64.0691 Q1463.45 67.2693 1468.47 67.2693 Q1474.51 67.2693 1477.99 63.421 Q1481.52 59.5726 1481.52 52.9291 L1481.52 27.2059 L1488.97 27.2059 L1488.97 72.576 L1481.52 72.576 L1481.52 65.6084 Q1478.8 69.7404 1475.2 71.7658 Q1471.63 73.7508 1466.89 73.7508 Q1459.07 73.7508 1455.02 68.8897 Q1450.97 64.0286 1450.97 54.671 M1469.73 26.1121 L1469.73 26.1121 Z" fill="#000000" fill-rule="nonzero" fill-opacity="1" /><path clip-path="url(#clip390)" d="M1539.65 35.9153 Q1542.44 30.8922 1546.33 28.5022 Q1550.22 26.1121 1555.48 26.1121 Q1562.57 26.1121 1566.42 31.0947 Q1570.27 36.0368 1570.27 45.1919 L1570.27 72.576 L1562.78 72.576 L1562.78 45.4349 Q1562.78 38.913 1560.47 35.7533 Q1558.16 32.5936 1553.42 32.5936 Q1547.63 32.5936 1544.26 36.4419 Q1540.9 40.2903 1540.9 46.9338 L1540.9 72.576 L1533.41 72.576 L1533.41 45.4349 Q1533.41 38.8725 1531.1 35.7533 Q1528.79 32.5936 1523.97 32.5936 Q1518.26 32.5936 1514.89 36.4824 Q1511.53 40.3308 1511.53 46.9338 L1511.53 72.576 L1504.04 72.576 L1504.04 27.2059 L1511.53 27.2059 L1511.53 34.2544 Q1514.08 30.082 1517.65 28.0971 Q1521.21 26.1121 1526.12 26.1121 Q1531.06 26.1121 1534.5 28.6237 Q1537.99 31.1352 1539.65 35.9153 Z" fill="#000000" fill-rule="nonzero" fill-opacity="1" /><path clip-path="url(#clip390)" d="M1617.71 49.9314 Q1617.71 41.7081 1614.3 37.0496 Q1610.94 32.3505 1605.03 32.3505 Q1599.11 32.3505 1595.71 37.0496 Q1592.35 41.7081 1592.35 49.9314 Q1592.35 58.1548 1595.71 62.8538 Q1599.11 67.5124 1605.03 67.5124 Q1610.94 67.5124 1614.3 62.8538 Q1617.71 58.1548 1617.71 49.9314 M1592.35 34.0924 Q1594.7 30.0415 1598.26 28.0971 Q1601.87 26.1121 1606.85 26.1121 Q1615.11 26.1121 1620.26 32.6746 Q1625.44 39.2371 1625.44 49.9314 Q1625.44 60.6258 1620.26 67.1883 Q1615.11 73.7508 1606.85 73.7508 Q1601.87 73.7508 1598.26 71.8063 Q1594.7 69.8214 1592.35 65.7705 L1592.35 72.576 L1584.85 72.576 L1584.85 9.54393 L1592.35 9.54393 L1592.35 34.0924 Z" fill="#000000" fill-rule="nonzero" fill-opacity="1" /><path clip-path="url(#clip390)" d="M1676.61 48.0275 L1676.61 51.6733 L1642.34 51.6733 Q1642.82 59.3701 1646.95 63.421 Q1651.13 67.4314 1658.54 67.4314 Q1662.83 67.4314 1666.84 66.3781 Q1670.9 65.3249 1674.87 63.2184 L1674.87 70.267 Q1670.85 71.9684 1666.64 72.8596 Q1662.43 73.7508 1658.09 73.7508 Q1647.24 73.7508 1640.88 67.4314 Q1634.56 61.1119 1634.56 50.3365 Q1634.56 39.1965 1640.55 32.6746 Q1646.59 26.1121 1656.8 26.1121 Q1665.95 26.1121 1671.26 32.0264 Q1676.61 37.9003 1676.61 48.0275 M1669.15 45.84 Q1669.07 39.7232 1665.71 36.0774 Q1662.39 32.4315 1656.88 32.4315 Q1650.64 32.4315 1646.87 35.9558 Q1643.15 39.4801 1642.58 45.8805 L1669.15 45.84 Z" fill="#000000" fill-rule="nonzero" fill-opacity="1" /><path clip-path="url(#clip390)" d="M1715.13 34.1734 Q1713.88 33.4443 1712.38 33.1202 Q1710.92 32.7556 1709.14 32.7556 Q1702.82 32.7556 1699.41 36.8875 Q1696.05 40.9789 1696.05 48.6757 L1696.05 72.576 L1688.56 72.576 L1688.56 27.2059 L1696.05 27.2059 L1696.05 34.2544 Q1698.4 30.1225 1702.17 28.1376 Q1705.94 26.1121 1711.32 26.1121 Q1712.09 26.1121 1713.02 26.2337 Q1713.96 26.3147 1715.09 26.5172 L1715.13 34.1734 Z" fill="#000000" fill-rule="nonzero" fill-opacity="1" /><path clip-path="url(#clip390)" d="M1766.9 32.4315 Q1760.91 32.4315 1757.42 37.1306 Q1753.94 41.7891 1753.94 49.9314 Q1753.94 58.0738 1757.38 62.7728 Q1760.87 67.4314 1766.9 67.4314 Q1772.86 67.4314 1776.34 62.7323 Q1779.82 58.0333 1779.82 49.9314 Q1779.82 41.8701 1776.34 37.1711 Q1772.86 32.4315 1766.9 32.4315 M1766.9 26.1121 Q1776.62 26.1121 1782.17 32.4315 Q1787.72 38.7509 1787.72 49.9314 Q1787.72 61.0714 1782.17 67.4314 Q1776.62 73.7508 1766.9 73.7508 Q1757.14 73.7508 1751.59 67.4314 Q1746.08 61.0714 1746.08 49.9314 Q1746.08 38.7509 1751.59 32.4315 Q1757.14 26.1121 1766.9 26.1121 Z" fill="#000000" fill-rule="nonzero" fill-opacity="1" /><path clip-path="url(#clip390)" d="M1823.05 9.54393 L1823.05 15.7418 L1815.92 15.7418 Q1811.91 15.7418 1810.33 17.3622 Q1808.79 18.9825 1808.79 23.1955 L1808.79 27.2059 L1821.06 27.2059 L1821.06 32.9987 L1808.79 32.9987 L1808.79 72.576 L1801.29 72.576 L1801.29 32.9987 L1794.16 32.9987 L1794.16 27.2059 L1801.29 27.2059 L1801.29 24.0462 Q1801.29 16.471 1804.82 13.0277 Q1808.34 9.54393 1816 9.54393 L1823.05 9.54393 Z" fill="#000000" fill-rule="nonzero" fill-opacity="1" /><path clip-path="url(#clip390)" d="M1888.31 28.9478 L1888.31 35.9153 Q1885.15 34.1734 1881.95 33.3227 Q1878.79 32.4315 1875.55 32.4315 Q1868.3 32.4315 1864.29 37.0496 Q1860.28 41.6271 1860.28 49.9314 Q1860.28 58.2358 1864.29 62.8538 Q1868.3 67.4314 1875.55 67.4314 Q1878.79 67.4314 1881.95 66.5807 Q1885.15 65.6895 1888.31 63.9476 L1888.31 70.8341 Q1885.19 72.2924 1881.83 73.0216 Q1878.5 73.7508 1874.74 73.7508 Q1864.49 73.7508 1858.45 67.3098 Q1852.42 60.8689 1852.42 49.9314 Q1852.42 38.832 1858.49 32.472 Q1864.61 26.1121 1875.22 26.1121 Q1878.67 26.1121 1881.95 26.8413 Q1885.23 27.5299 1888.31 28.9478 Z" fill="#000000" fill-rule="nonzero" fill-opacity="1" /><path clip-path="url(#clip390)" d="M1901.27 9.54393 L1908.72 9.54393 L1908.72 72.576 L1901.27 72.576 L1901.27 9.54393 Z" fill="#000000" fill-rule="nonzero" fill-opacity="1" /><path clip-path="url(#clip390)" d="M1944.94 49.7694 Q1935.91 49.7694 1932.42 51.8354 Q1928.94 53.9013 1928.94 58.8839 Q1928.94 62.8538 1931.53 65.2034 Q1934.16 67.5124 1938.66 67.5124 Q1944.86 67.5124 1948.58 63.1374 Q1952.35 58.7219 1952.35 51.4303 L1952.35 49.7694 L1944.94 49.7694 M1959.81 46.6907 L1959.81 72.576 L1952.35 72.576 L1952.35 65.6895 Q1949.8 69.8214 1945.99 71.8063 Q1942.18 73.7508 1936.68 73.7508 Q1929.71 73.7508 1925.58 69.8619 Q1921.48 65.9325 1921.48 59.3701 Q1921.48 51.7138 1926.59 47.825 Q1931.73 43.9361 1941.9 43.9361 L1952.35 43.9361 L1952.35 43.2069 Q1952.35 38.0623 1948.95 35.2672 Q1945.59 32.4315 1939.47 32.4315 Q1935.58 32.4315 1931.9 33.3632 Q1928.21 34.295 1924.81 36.1584 L1924.81 29.2718 Q1928.9 27.692 1932.75 26.9223 Q1936.59 26.1121 1940.24 26.1121 Q1950.08 26.1121 1954.94 31.2163 Q1959.81 36.3204 1959.81 46.6907 Z" fill="#000000" fill-rule="nonzero" fill-opacity="1" /><path clip-path="url(#clip390)" d="M2004.08 28.5427 L2004.08 35.5912 Q2000.92 33.9709 1997.52 33.1607 Q1994.12 32.3505 1990.47 32.3505 Q1984.92 32.3505 1982.13 34.0519 Q1979.37 35.7533 1979.37 39.156 Q1979.37 41.7486 1981.36 43.2475 Q1983.34 44.7058 1989.34 46.0426 L1991.89 46.6097 Q1999.83 48.3111 2003.15 51.4303 Q2006.51 54.509 2006.51 60.0587 Q2006.51 66.3781 2001.49 70.0644 Q1996.51 73.7508 1987.76 73.7508 Q1984.11 73.7508 1980.14 73.0216 Q1976.21 72.3329 1971.84 70.9151 L1971.84 63.2184 Q1975.97 65.3654 1979.98 66.4591 Q1983.99 67.5124 1987.92 67.5124 Q1993.19 67.5124 1996.02 65.73 Q1998.86 63.9071 1998.86 60.6258 Q1998.86 57.5877 1996.79 55.9673 Q1994.77 54.3469 1987.84 52.8481 L1985.25 52.2405 Q1978.32 50.7821 1975.24 47.7845 Q1972.16 44.7463 1972.16 39.4801 Q1972.16 33.0797 1976.7 29.5959 Q1981.24 26.1121 1989.58 26.1121 Q1993.71 26.1121 1997.36 26.7198 Q2001 27.3274 2004.08 28.5427 Z" fill="#000000" fill-rule="nonzero" fill-opacity="1" /><path clip-path="url(#clip390)" d="M2047.31 28.5427 L2047.31 35.5912 Q2044.15 33.9709 2040.74 33.1607 Q2037.34 32.3505 2033.69 32.3505 Q2028.14 32.3505 2025.35 34.0519 Q2022.59 35.7533 2022.59 39.156 Q2022.59 41.7486 2024.58 43.2475 Q2026.56 44.7058 2032.56 46.0426 L2035.11 46.6097 Q2043.05 48.3111 2046.37 51.4303 Q2049.74 54.509 2049.74 60.0587 Q2049.74 66.3781 2044.71 70.0644 Q2039.73 73.7508 2030.98 73.7508 Q2027.33 73.7508 2023.36 73.0216 Q2019.44 72.3329 2015.06 70.9151 L2015.06 63.2184 Q2019.19 65.3654 2023.2 66.4591 Q2027.21 67.5124 2031.14 67.5124 Q2036.41 67.5124 2039.24 65.73 Q2042.08 63.9071 2042.08 60.6258 Q2042.08 57.5877 2040.01 55.9673 Q2037.99 54.3469 2031.06 52.8481 L2028.47 52.2405 Q2021.54 50.7821 2018.46 47.7845 Q2015.38 44.7463 2015.38 39.4801 Q2015.38 33.0797 2019.92 29.5959 Q2024.46 26.1121 2032.8 26.1121 Q2036.93 26.1121 2040.58 26.7198 Q2044.23 27.3274 2047.31 28.5427 Z" fill="#000000" fill-rule="nonzero" fill-opacity="1" /><path clip-path="url(#clip390)" d="M2100.41 48.0275 L2100.41 51.6733 L2066.14 51.6733 Q2066.63 59.3701 2070.76 63.421 Q2074.93 67.4314 2082.35 67.4314 Q2086.64 67.4314 2090.65 66.3781 Q2094.7 65.3249 2098.67 63.2184 L2098.67 70.267 Q2094.66 71.9684 2090.45 72.8596 Q2086.23 73.7508 2081.9 73.7508 Q2071.04 73.7508 2064.68 67.4314 Q2058.36 61.1119 2058.36 50.3365 Q2058.36 39.1965 2064.36 32.6746 Q2070.4 26.1121 2080.6 26.1121 Q2089.76 26.1121 2095.07 32.0264 Q2100.41 37.9003 2100.41 48.0275 M2092.96 45.84 Q2092.88 39.7232 2089.52 36.0774 Q2086.19 32.4315 2080.68 32.4315 Q2074.45 32.4315 2070.68 35.9558 Q2066.95 39.4801 2066.39 45.8805 L2092.96 45.84 Z" fill="#000000" fill-rule="nonzero" fill-opacity="1" /><path clip-path="url(#clip390)" d="M2141.57 28.5427 L2141.57 35.5912 Q2138.41 33.9709 2135.01 33.1607 Q2131.6 32.3505 2127.96 32.3505 Q2122.41 32.3505 2119.61 34.0519 Q2116.86 35.7533 2116.86 39.156 Q2116.86 41.7486 2118.84 43.2475 Q2120.83 44.7058 2126.82 46.0426 L2129.38 46.6097 Q2137.32 48.3111 2140.64 51.4303 Q2144 54.509 2144 60.0587 Q2144 66.3781 2138.98 70.0644 Q2133.99 73.7508 2125.24 73.7508 Q2121.6 73.7508 2117.63 73.0216 Q2113.7 72.3329 2109.32 70.9151 L2109.32 63.2184 Q2113.46 65.3654 2117.47 66.4591 Q2121.48 67.5124 2125.41 67.5124 Q2130.67 67.5124 2133.51 65.73 Q2136.34 63.9071 2136.34 60.6258 Q2136.34 57.5877 2134.28 55.9673 Q2132.25 54.3469 2125.33 52.8481 L2122.73 52.2405 Q2115.81 50.7821 2112.73 47.7845 Q2109.65 44.7463 2109.65 39.4801 Q2109.65 33.0797 2114.19 29.5959 Q2118.72 26.1121 2127.07 26.1121 Q2131.2 26.1121 2134.85 26.7198 Q2138.49 27.3274 2141.57 28.5427 Z" fill="#000000" fill-rule="nonzero" fill-opacity="1" /><polyline clip-path="url(#clip392)" style="stroke:#009af9; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none" points="295.211,160.256 961.083,1033.9 1626.96,950.962 2292.83,759.677 "/>
<polyline clip-path="url(#clip392)" style="stroke:#e26f46; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none" points="295.211,298.803 961.083,1320.89 1626.96,1386.4 2292.83,1343.55 "/>
<path clip-path="url(#clip390)" d="M1993.85 322.316 L2282.17 322.316 L2282.17 166.796 L1993.85 166.796  Z" fill="#ffffff" fill-rule="evenodd" fill-opacity="1"/>
<polyline clip-path="url(#clip390)" style="stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none" points="1993.85,322.316 2282.17,322.316 2282.17,166.796 1993.85,166.796 1993.85,322.316 "/>
<polyline clip-path="url(#clip390)" style="stroke:#009af9; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none" points="2017.38,218.636 2158.54,218.636 "/>
<path clip-path="url(#clip390)" d="M2186.75 219.411 L2186.75 232.073 L2194.25 232.073 Q2198.02 232.073 2199.83 230.522 Q2201.66 228.948 2201.66 225.731 Q2201.66 222.49 2199.83 220.962 Q2198.02 219.411 2194.25 219.411 L2186.75 219.411 M2186.75 205.198 L2186.75 215.615 L2193.67 215.615 Q2197.1 215.615 2198.76 214.342 Q2200.45 213.045 2200.45 210.407 Q2200.45 207.791 2198.76 206.495 Q2197.1 205.198 2193.67 205.198 L2186.75 205.198 M2182.07 201.356 L2194.02 201.356 Q2199.36 201.356 2202.26 203.578 Q2205.15 205.8 2205.15 209.897 Q2205.15 213.069 2203.67 214.944 Q2202.19 216.819 2199.32 217.282 Q2202.77 218.022 2204.66 220.383 Q2206.59 222.721 2206.59 226.24 Q2206.59 230.869 2203.44 233.393 Q2200.29 235.916 2194.48 235.916 L2182.07 235.916 L2182.07 201.356 Z" fill="#000000" fill-rule="nonzero" fill-opacity="1" /><path clip-path="url(#clip390)" d="M2214.6 201.356 L2219.27 201.356 L2219.27 235.916 L2214.6 235.916 L2214.6 201.356 Z" fill="#000000" fill-rule="nonzero" fill-opacity="1" /><path clip-path="url(#clip390)" d="M2254.46 204.018 L2254.46 208.948 Q2252.1 206.749 2249.41 205.661 Q2246.75 204.573 2243.74 204.573 Q2237.81 204.573 2234.66 208.208 Q2231.52 211.819 2231.52 218.67 Q2231.52 225.499 2234.66 229.133 Q2237.81 232.744 2243.74 232.744 Q2246.75 232.744 2249.41 231.656 Q2252.1 230.569 2254.46 228.369 L2254.46 233.254 Q2252 234.92 2249.25 235.754 Q2246.52 236.587 2243.46 236.587 Q2235.61 236.587 2231.1 231.795 Q2226.59 226.981 2226.59 218.67 Q2226.59 210.337 2231.1 205.546 Q2235.61 200.731 2243.46 200.731 Q2246.56 200.731 2249.29 201.564 Q2252.05 202.374 2254.46 204.018 Z" fill="#000000" fill-rule="nonzero" fill-opacity="1" /><polyline clip-path="url(#clip390)" style="stroke:#e26f46; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none" points="2017.38,270.476 2158.54,270.476 "/>
<path clip-path="url(#clip390)" d="M2197.91 257.802 L2191.56 275.001 L2204.27 275.001 L2197.91 257.802 M2195.27 253.196 L2200.57 253.196 L2213.74 287.756 L2208.88 287.756 L2205.73 278.89 L2190.15 278.89 L2187 287.756 L2182.07 287.756 L2195.27 253.196 Z" fill="#000000" fill-rule="nonzero" fill-opacity="1" /><path clip-path="url(#clip390)" d="M2218.79 253.196 L2223.46 253.196 L2223.46 287.756 L2218.79 287.756 L2218.79 253.196 Z" fill="#000000" fill-rule="nonzero" fill-opacity="1" /><path clip-path="url(#clip390)" d="M2258.65 255.858 L2258.65 260.788 Q2256.28 258.589 2253.6 257.501 Q2250.94 256.413 2247.93 256.413 Q2242 256.413 2238.85 260.048 Q2235.71 263.659 2235.71 270.51 Q2235.71 277.339 2238.85 280.973 Q2242 284.584 2247.93 284.584 Q2250.94 284.584 2253.6 283.496 Q2256.28 282.409 2258.65 280.209 L2258.65 285.094 Q2256.19 286.76 2253.44 287.594 Q2250.71 288.427 2247.65 288.427 Q2239.8 288.427 2235.29 283.635 Q2230.78 278.821 2230.78 270.51 Q2230.78 262.177 2235.29 257.386 Q2239.8 252.571 2247.65 252.571 Q2250.75 252.571 2253.48 253.404 Q2256.24 254.214 2258.65 255.858 Z" fill="#000000" fill-rule="nonzero" fill-opacity="1" /></svg>
<p>We see that following the &quot;lowest AIC&quot; rule we would indeed choose three classes, while following the &quot;lowest BIC&quot; criteria we would have choosen only two classes. This means that there is two classes that, concerning the floreal measures used in the database, are very similar, and our models are unsure about them. Perhaps the biologists will end up one day with the conclusion that it is indeed only one specie :-).</p><p>We could study this issue more in detail by analysing the <a href="../../Utils.html#BetaML.Utils.ConfusionMatrix"><code>ConfusionMatrix</code></a>, but the one used in BetaML does not account for the ignorelabels option (yet).</p><h3 id="Analysing-the-silhouette-of-the-cluster"><a class="docs-heading-anchor" href="#Analysing-the-silhouette-of-the-cluster">Analysing the silhouette of the cluster</a><a id="Analysing-the-silhouette-of-the-cluster-1"></a><a class="docs-heading-anchor-permalink" href="#Analysing-the-silhouette-of-the-cluster" title="Permalink"></a></h3><p>A further metric to analyse cluster output is the so-called <a href="https://en.wikipedia.org/wiki/Silhouette_(clustering)">Sinhouette method</a></p><p>Silhouette is a distance-based metric and require as first argument a matrix of pairwise distances. This can be computed with the <a href="../../Utils.html#BetaML.Utils.pairwise-Tuple{AbstractArray}"><code>pairwise</code></a> function, that default to using <code>l2_distance</code> (i.e. Euclidean). Many other distance functions are available in the <a href="../../Clustering.html#BetaML.Clustering"><code>Clustering</code></a> sub-module or one can use the efficiently implemented distances from the <a href="https://github.com/JuliaStats/Distances.jl"><code>Distances</code></a> package, as in this example.</p><p>We&#39;ll use here the <a href="../../Utils.html#BetaML.Utils.silhouette-Tuple{Any, Any}"><code>silhouette</code></a> function over a simple loop:</p><pre><code class="language-julia hljs">x,y = consistent_shuffle([x,y],dims=1)
import Distances
pd = pairwise(x,distance=Distances.euclidean) # we compute the pairwise distances
nclasses = 2:6
models = [KMeansClusterer, KMedoidsClusterer, GMMClusterer]
println(&quot;Silhouette score by model type and class number:&quot;)
for ncl in nclasses, mtype in models
    m = mtype(n_classes=ncl, verbosity=NONE)
    ŷ = fit!(m,x)
    if mtype == GMMClusterer
        ŷ = mode(ŷ)
    end
    s = mean(silhouette(pd,ŷ))
    println(&quot;$mtype \t ($ncl classes): $s&quot;)
end</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">Silhouette score by model type and class number:
KMeansClusterer 	 (2 classes): 0.6810461692117465
KMedoidsClusterer 	 (2 classes): 0.6857881712617193
GMMClusterer 	 (2 classes): 0.6867350732769778
KMeansClusterer 	 (3 classes): 0.5528190123564098
KMedoidsClusterer 	 (3 classes): 0.5528190123564098
GMMClusterer 	 (3 classes): 0.5522806746748189
KMeansClusterer 	 (4 classes): 0.4962511348125098
KMedoidsClusterer 	 (4 classes): 0.4882419477378052
GMMClusterer 	 (4 classes): 0.46095184656234467
KMeansClusterer 	 (5 classes): 0.48874888709310615
KMedoidsClusterer 	 (5 classes): 0.4584191432646477
GMMClusterer 	 (5 classes): 0.4863408030950679
KMeansClusterer 	 (6 classes): 0.3674845748098317
KMedoidsClusterer 	 (6 classes): 0.34916011367198635
GMMClusterer 	 (6 classes): 0.3543173617053886</code></pre><p>Highest levels are better. We see again that 2 classes have better scores !</p><h2 id="Conclusions"><a class="docs-heading-anchor" href="#Conclusions">Conclusions</a><a id="Conclusions-1"></a><a class="docs-heading-anchor-permalink" href="#Conclusions" title="Permalink"></a></h2><p>We have shown in this tutorial how we can easily run clustering algorithms in BetaML with just one line of code <code>fit!(ChoosenClusterer(),x)</code>, but also how can we use cross-validation in order to help the model or parameter selection, with or whithout knowing the real classes. We retrieve here what we observed with supervised models. Globally the accuracy of BetaML models are comparable to those of leading specialised packages (in this case they are even better), but there is a significant gap in computational efficiency that restricts the pratical usage of BetaML to datasets that fits in the pc memory. However we trade this relative inefficiency with very flexible model definition and utility functions (for example <code>GMMClusterer</code> works with missing data, allowing it to be used as the backbone of the <a href="../../Imputation.html#BetaML.Imputation.GMMImputer"><code>GMMImputer</code></a> missing imputation function, or for collaborative reccomendation systems).</p><p><a href="https://github.com/sylvaticus/BetaML.jl/blob/master/docs/src/tutorials/Clustering - Iris/betaml_tutorial_cluster_iris.jl">View this file on Github</a>.</p><hr/><p><em>This page was generated using <a href="https://github.com/fredrikekre/Literate.jl">Literate.jl</a>.</em></p></article><nav class="docs-footer"><a class="docs-footer-prevpage" href="../Regression - bike sharing/betaml_tutorial_regression_sharingBikes.html">« A regression task: the prediction of  bike  sharing demand</a><a class="docs-footer-nextpage" href="../Multi-branch neural network/betaml_tutorial_multibranch_nn.html">A deep neural network with multi-branch architecture »</a><div class="flexbox-break"></div><p class="footer-message">Powered by <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> and the <a href="https://julialang.org/">Julia Programming Language</a>.</p></nav></div><div class="modal" id="documenter-settings"><div class="modal-background"></div><div class="modal-card"><header class="modal-card-head"><p class="modal-card-title">Settings</p><button class="delete"></button></header><section class="modal-card-body"><p><label class="label">Theme</label><div class="select"><select id="documenter-themepicker"><option value="documenter-light">documenter-light</option><option value="documenter-dark">documenter-dark</option></select></div></p><hr/><p>This document was generated with <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> version 0.27.23 on <span class="colophon-date" title="Friday 7 July 2023 20:26">Friday 7 July 2023</span>. Using Julia version 1.6.7.</p></section><footer class="modal-card-foot"></footer></div></div></div></body></html>
