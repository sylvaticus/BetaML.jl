<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>A clustering task: the prediction of  plant species from floreal measures (the iris dataset) · BetaML.jl Documentation</title><meta name="title" content="A clustering task: the prediction of  plant species from floreal measures (the iris dataset) · BetaML.jl Documentation"/><meta property="og:title" content="A clustering task: the prediction of  plant species from floreal measures (the iris dataset) · BetaML.jl Documentation"/><meta property="twitter:title" content="A clustering task: the prediction of  plant species from floreal measures (the iris dataset) · BetaML.jl Documentation"/><meta name="description" content="Documentation for BetaML.jl Documentation."/><meta property="og:description" content="Documentation for BetaML.jl Documentation."/><meta property="twitter:description" content="Documentation for BetaML.jl Documentation."/><script async src="https://www.googletagmanager.com/gtag/js?id=G-JYKX8QY5JW"></script><script>  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());
  gtag('config', 'G-JYKX8QY5JW', {'page_path': location.pathname + location.search + location.hash});
</script><script data-outdated-warner src="../../assets/warner.js"></script><link href="https://cdnjs.cloudflare.com/ajax/libs/lato-font/3.0.0/css/lato-font.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/juliamono/0.050/juliamono.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/fontawesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/solid.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/brands.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.16.8/katex.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL="../.."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" data-main="../../assets/documenter.js"></script><script src="../../search_index.js"></script><script src="../../siteinfo.js"></script><script src="../../../versions.js"></script><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/documenter-dark.css" data-theme-name="documenter-dark" data-theme-primary-dark/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/documenter-light.css" data-theme-name="documenter-light" data-theme-primary/><script src="../../assets/themeswap.js"></script></head><body><div id="documenter"><nav class="docs-sidebar"><div class="docs-package-name"><span class="docs-autofit"><a href="../../index.html">BetaML.jl Documentation</a></span></div><button class="docs-search-query input is-rounded is-small is-clickable my-2 mx-auto py-1 px-2" id="documenter-search-query">Search docs (Ctrl + /)</button><ul class="docs-menu"><li><a class="tocitem" href="../../index.html">Index</a></li><li><span class="tocitem">Tutorial</span><ul><li><a class="tocitem" href="../Betaml_tutorial_getting_started.html">Getting started</a></li><li><input class="collapse-toggle" id="menuitem-2-2" type="checkbox"/><label class="tocitem" for="menuitem-2-2"><span class="docs-label">Classification - cars</span><i class="docs-chevron"></i></label><ul class="collapsed"><li><a class="tocitem" href="../Classification - cars/betaml_tutorial_classification_cars.html">A classification task when labels are known - determining the country of origin of cars given the cars characteristics</a></li></ul></li><li><input class="collapse-toggle" id="menuitem-2-3" type="checkbox"/><label class="tocitem" for="menuitem-2-3"><span class="docs-label">Regression - bike sharing</span><i class="docs-chevron"></i></label><ul class="collapsed"><li><a class="tocitem" href="../Regression - bike sharing/betaml_tutorial_regression_sharingBikes.html">A regression task: the prediction of  bike  sharing demand</a></li></ul></li><li><input class="collapse-toggle" id="menuitem-2-4" type="checkbox" checked/><label class="tocitem" for="menuitem-2-4"><span class="docs-label">Clustering - Iris</span><i class="docs-chevron"></i></label><ul class="collapsed"><li class="is-active"><a class="tocitem" href="betaml_tutorial_cluster_iris.html">A clustering task: the prediction of  plant species from floreal measures (the iris dataset)</a><ul class="internal"><li><a class="tocitem" href="#Library-and-data-loading"><span>Library and data loading</span></a></li><li><a class="tocitem" href="#Data-preparation"><span>Data preparation</span></a></li><li><a class="tocitem" href="#Main-analysis"><span>Main analysis</span></a></li><li><a class="tocitem" href="#Working-without-the-labels"><span>Working without the labels</span></a></li><li><a class="tocitem" href="#Conclusions"><span>Conclusions</span></a></li></ul></li></ul></li><li><input class="collapse-toggle" id="menuitem-2-5" type="checkbox"/><label class="tocitem" for="menuitem-2-5"><span class="docs-label">Multi-branch neural network</span><i class="docs-chevron"></i></label><ul class="collapsed"><li><a class="tocitem" href="../Multi-branch neural network/betaml_tutorial_multibranch_nn.html">A deep neural network with multi-branch architecture</a></li></ul></li><li><input class="collapse-toggle" id="menuitem-2-6" type="checkbox"/><label class="tocitem" for="menuitem-2-6"><span class="docs-label">Feature importance</span><i class="docs-chevron"></i></label><ul class="collapsed"><li><a class="tocitem" href="../Feature importance/Feature_importance.html">Understanding variable importance in black-box machine learning models</a></li></ul></li></ul></li><li><span class="tocitem">API (Reference manual)</span><ul><li><input class="collapse-toggle" id="menuitem-3-1" type="checkbox"/><label class="tocitem" for="menuitem-3-1"><span class="docs-label">API V2 (current)</span><i class="docs-chevron"></i></label><ul class="collapsed"><li><a class="tocitem" href="../../Api_v2_user.html">Introduction for user</a></li><li><input class="collapse-toggle" id="menuitem-3-1-2" type="checkbox"/><label class="tocitem" for="menuitem-3-1-2"><span class="docs-label">For developers</span><i class="docs-chevron"></i></label><ul class="collapsed"><li><a class="tocitem" href="../../Api_v2_developer.html">API implementation</a></li><li><a class="tocitem" href="../../StyleGuide_templates.html">Style guide</a></li></ul></li><li><a class="tocitem" href="../../Api.html">The Api module</a></li></ul></li><li><a class="tocitem" href="../../Perceptron.html">Perceptron</a></li><li><a class="tocitem" href="../../Trees.html">Trees</a></li><li><a class="tocitem" href="../../Nn.html">Nn</a></li><li><a class="tocitem" href="../../Clustering.html">Clustering</a></li><li><a class="tocitem" href="../../GMM.html">GMM</a></li><li><a class="tocitem" href="../../Imputation.html">Imputation</a></li><li><a class="tocitem" href="../../Utils.html">Utils</a></li></ul></li><li><a class="tocitem" href="../../MLJ_interface.html">MLJ interface</a></li></ul><div class="docs-version-selector field has-addons"><div class="control"><span class="docs-label button is-static is-size-7">Version</span></div><div class="docs-selector control is-expanded"><div class="select is-fullwidth is-size-7"><select id="documenter-version-selector"></select></div></div></div></nav><div class="docs-main"><header class="docs-navbar"><a class="docs-sidebar-button docs-navbar-link fa-solid fa-bars is-hidden-desktop" id="documenter-sidebar-button" href="#"></a><nav class="breadcrumb"><ul class="is-hidden-mobile"><li><a class="is-disabled">Tutorial</a></li><li><a class="is-disabled">Clustering - Iris</a></li><li class="is-active"><a href="betaml_tutorial_cluster_iris.html">A clustering task: the prediction of  plant species from floreal measures (the iris dataset)</a></li></ul><ul class="is-hidden-tablet"><li class="is-active"><a href="betaml_tutorial_cluster_iris.html">A clustering task: the prediction of  plant species from floreal measures (the iris dataset)</a></li></ul></nav><div class="docs-right"><a class="docs-navbar-link" href="https://github.com/sylvaticus/BetaML.jl" title="View the repository on GitHub"><span class="docs-icon fa-brands"></span><span class="docs-label is-hidden-touch">GitHub</span></a><a class="docs-navbar-link" href="https://github.com/sylvaticus/BetaML.jl/blob/master/docs/src/tutorials/Clustering - Iris/betaml_tutorial_cluster_iris.jl" title="Edit source on GitHub"><span class="docs-icon fa-solid"></span></a><a class="docs-settings-button docs-navbar-link fa-solid fa-gear" id="documenter-settings-button" href="#" title="Settings"></a><a class="docs-article-toggle-button fa-solid fa-chevron-up" id="documenter-article-toggle-button" href="javascript:;" title="Collapse all docstrings"></a></div></header><article class="content" id="documenter-page"><h1 id="clustering_tutorial"><a class="docs-heading-anchor" href="#clustering_tutorial">A clustering task: the prediction of  plant species from floreal measures (the iris dataset)</a><a id="clustering_tutorial-1"></a><a class="docs-heading-anchor-permalink" href="#clustering_tutorial" title="Permalink"></a></h1><p>The task is to estimate the species of a plant given some floreal measurements. It use the classical &quot;Iris&quot; dataset. Note that in this example we are using clustering approaches, so we try to understand the &quot;structure&quot; of our data, without relying to actually knowing the true labels (&quot;classes&quot; or &quot;factors&quot;). However we have chosen a dataset for which the true labels are actually known, so we can compare the accuracy of the algorithms we use, but these labels will not be used during the algorithms training.</p><p>Data origin:</p><ul><li>dataset description: <a href="https://en.wikipedia.org/wiki/Iris_flower_data_set">https://en.wikipedia.org/wiki/Iris<em>flower</em>data_set</a></li><li>data source we use here: <a href="https://github.com/JuliaStats/RDatasets.jl">https://github.com/JuliaStats/RDatasets.jl</a></li></ul><h2 id="Library-and-data-loading"><a class="docs-heading-anchor" href="#Library-and-data-loading">Library and data loading</a><a id="Library-and-data-loading-1"></a><a class="docs-heading-anchor-permalink" href="#Library-and-data-loading" title="Permalink"></a></h2><p>Activating the local environment specific to BetaML documentation</p><pre><code class="language-julia hljs">using Pkg
Pkg.activate(joinpath(@__DIR__,&quot;..&quot;,&quot;..&quot;,&quot;..&quot;))</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">  Activating environment at `~/work/BetaML.jl/BetaML.jl/docs/Project.toml`</code></pre><p>We load the Beta Machine Learning Toolkit as well as some other packages that we use in this tutorial</p><pre><code class="language-julia hljs">using BetaML
using Random, Statistics, Logging, BenchmarkTools, StableRNGs, RDatasets, Plots, DataFrames</code></pre><p>We are also going to compare our results with two other leading packages in Julia for clustering analysis, <a href="https://github.com/JuliaStats/Clustering.jl"><code>Clustering.jl</code></a> that provides (inter alia) kmeans and kmedoids algorithms and <a href="https://github.com/davidavdav/GaussianMixtures.jl"><code>GaussianMixtures.jl</code></a> that provides, as the name says, Gaussian Mixture Models. So we import them (we &quot;import&quot; them, rather than &quot;use&quot;, not to bound their full names into namespace as some would collide with BetaML).</p><pre><code class="language-julia hljs">import Clustering, GaussianMixtures</code></pre><p>Here we are explicit and we use our own fixed RNG:</p><pre><code class="language-julia hljs">seed = 123 # The table at the end of this tutorial has been obtained with seeds 123, 1000 and 10000
AFIXEDRNG = StableRNG(seed)</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">StableRNGs.LehmerRNG(state=0x000000000000000000000000000000f7)</code></pre><p>We do a few tweeks for the Clustering and GaussianMixtures packages. Note that in BetaML we can also control both the random seed and the verbosity in the algorithm call, not only globally</p><pre><code class="language-julia hljs">Random.seed!(seed)
#logger  = Logging.SimpleLogger(stdout, Logging.Error); global_logger(logger); ## For suppressing GaussianMixtures output</code></pre><p>Differently from the <a href="../Regression - bike sharing/betaml_tutorial_regression_sharingBikes.html#regression_tutorial">regression tutorial</a>, we load the data here from [<code>RDatasets</code>](https://github.com/JuliaStats/RDatasets.jl](https://github.com/JuliaStats/RDatasets.jl), a package providing standard datasets.</p><pre><code class="language-julia hljs">iris = dataset(&quot;datasets&quot;, &quot;iris&quot;)
describe(iris)</code></pre><div><div style = "float: left;"><span>5×7 DataFrame</span></div><div style = "clear: both;"></div></div><div class = "data-frame" style = "overflow-x: scroll;"><table class = "data-frame" style = "margin-bottom: 6px;"><thead><tr class = "header"><th class = "rowNumber" style = "font-weight: bold; text-align: right;">Row</th><th style = "text-align: left;">variable</th><th style = "text-align: left;">mean</th><th style = "text-align: left;">min</th><th style = "text-align: left;">median</th><th style = "text-align: left;">max</th><th style = "text-align: left;">nmissing</th><th style = "text-align: left;">eltype</th></tr><tr class = "subheader headerLastRow"><th class = "rowNumber" style = "font-weight: bold; text-align: right;"></th><th title = "Symbol" style = "text-align: left;">Symbol</th><th title = "Union{Nothing, Float64}" style = "text-align: left;">Union…</th><th title = "Any" style = "text-align: left;">Any</th><th title = "Union{Nothing, Float64}" style = "text-align: left;">Union…</th><th title = "Any" style = "text-align: left;">Any</th><th title = "Int64" style = "text-align: left;">Int64</th><th title = "DataType" style = "text-align: left;">DataType</th></tr></thead><tbody><tr><td class = "rowNumber" style = "font-weight: bold; text-align: right;">1</td><td style = "text-align: left;">SepalLength</td><td style = "text-align: left;">5.84333</td><td style = "text-align: left;">4.3</td><td style = "text-align: left;">5.8</td><td style = "text-align: left;">7.9</td><td style = "text-align: right;">0</td><td style = "text-align: left;">Float64</td></tr><tr><td class = "rowNumber" style = "font-weight: bold; text-align: right;">2</td><td style = "text-align: left;">SepalWidth</td><td style = "text-align: left;">3.05733</td><td style = "text-align: left;">2.0</td><td style = "text-align: left;">3.0</td><td style = "text-align: left;">4.4</td><td style = "text-align: right;">0</td><td style = "text-align: left;">Float64</td></tr><tr><td class = "rowNumber" style = "font-weight: bold; text-align: right;">3</td><td style = "text-align: left;">PetalLength</td><td style = "text-align: left;">3.758</td><td style = "text-align: left;">1.0</td><td style = "text-align: left;">4.35</td><td style = "text-align: left;">6.9</td><td style = "text-align: right;">0</td><td style = "text-align: left;">Float64</td></tr><tr><td class = "rowNumber" style = "font-weight: bold; text-align: right;">4</td><td style = "text-align: left;">PetalWidth</td><td style = "text-align: left;">1.19933</td><td style = "text-align: left;">0.1</td><td style = "text-align: left;">1.3</td><td style = "text-align: left;">2.5</td><td style = "text-align: right;">0</td><td style = "text-align: left;">Float64</td></tr><tr><td class = "rowNumber" style = "font-weight: bold; text-align: right;">5</td><td style = "text-align: left;">Species</td><td style = "font-style: italic; text-align: left;"></td><td style = "text-align: left;">setosa</td><td style = "font-style: italic; text-align: left;"></td><td style = "text-align: left;">virginica</td><td style = "text-align: right;">0</td><td style = "text-align: left;">CategoricalValue{String, UInt8}</td></tr></tbody></table></div><p>The iris dataset  provides floreal measures in columns 1 to 4 and the assigned species name in column 5. There are no missing values</p><h2 id="Data-preparation"><a class="docs-heading-anchor" href="#Data-preparation">Data preparation</a><a id="Data-preparation-1"></a><a class="docs-heading-anchor-permalink" href="#Data-preparation" title="Permalink"></a></h2><p>The first step is to prepare the data for the analysis. We collect the first 4 columns as our <em>feature</em> <code>x</code> matrix and the last one as our <code>y</code> label vector. As we are using clustering algorithms, we are not actually using the labels to train the algorithms, we&#39;ll behave like we do not know them, we&#39;ll just let the algorithm &quot;learn&quot; from the structure of the data itself. We&#39;ll however use it to judge the accuracy that the various algorithms reach.</p><pre><code class="language-julia hljs">x       = Matrix{Float64}(iris[:,1:4]);
yLabels = unique(iris[:,5])</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">3-element Vector{String}:
 &quot;setosa&quot;
 &quot;versicolor&quot;
 &quot;virginica&quot;</code></pre><p>As the labels are expressed as strings, the first thing we do is encode them as integers for our analysis using the <a href="../../Utils.html#BetaML.Utils.OrdinalEncoder"><code>OrdinalEncoder</code></a> model (data isn&#39;t really needed to be actually ordered):</p><pre><code class="language-julia hljs">y  = fit!(OrdinalEncoder(categories=yLabels),iris[:,5])</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">150-element Vector{Int64}:
 1
 1
 1
 1
 1
 1
 1
 1
 1
 1
 ⋮
 3
 3
 3
 3
 3
 3
 3
 3
 3</code></pre><p>The dataset from RDatasets is ordered by species, so we need to shuffle it to avoid biases. Shuffling happens by default in cross<em>validation, but we are keeping here a copy of the shuffled version for later. Note that the version of [`consistent</em>shuffle`](@ref) that is included in BetaML accepts several n-dimensional arrays and shuffle them (by default on rows, by we can specify the dimension) keeping the association  between the various arrays in the shuffled output.</p><pre><code class="language-julia hljs">(xs,ys) = consistent_shuffle([x,y], rng=copy(AFIXEDRNG));</code></pre><h2 id="Main-analysis"><a class="docs-heading-anchor" href="#Main-analysis">Main analysis</a><a id="Main-analysis-1"></a><a class="docs-heading-anchor-permalink" href="#Main-analysis" title="Permalink"></a></h2><p>We will try 3 BetaML models (<a href="../../Clustering.html#BetaML.Clustering.KMeansClusterer"><code>KMeansClusterer</code></a>, <a href="../../Clustering.html#BetaML.Clustering.KMedoidsClusterer"><code>KMedoidsClusterer</code></a> and <a href="../../GMM.html#BetaML.GMM.GaussianMixtureClusterer"><code>GaussianMixtureClusterer</code></a>) and we compare them with <code>kmeans</code> from Clusterings.jl and <code>GMM</code> from GaussianMixtures.jl</p><p><code>KMeansClusterer</code> and <code>KMedoidsClusterer</code> works by first initialising the centers of the k-clusters (step a ). These centers, also known as the &quot;representatives&quot;, must be selected within the data for kmedoids, while for kmeans they are the geometrical centers.</p><p>Then ( step b ) the algorithms iterates toward each point to assign the point to the cluster of the closest representative (according with a user defined distance metric, default to Euclidean), and ( step c ) moves each representative at the center of its newly acquired cluster (where &quot;center&quot; depends again from the metric).</p><p>Steps <em>b</em> and <em>c</em> are reiterated until the algorithm converge, i.e. the tentative k representative points (and their relative clusters) don&#39;t move any more. The result (output of the algorithm) is that each point is assigned to one of the clusters (classes).</p><p>The algorithm in <code>GaussianMixtureClusterer</code> is similar in that it employs an iterative approach (the Expectation<em>Minimisation algorithm, &quot;em&quot;) but here we make the hipothesis that the data points are the observed outcomes of some _mixture</em> probabilistic models where we have first a k-categorical variables whose outcomes are the (unobservble) parameters of a probabilistic distribution from which the data is finally drawn. Because the parameters of each of the k-possible distributions is unobservable this is also called a model with latent variables.</p><p>Most <code>gmm</code> models use the Gaussain distribution as the family of the mixture components, so we can tought the <code>gmm</code> acronym to indicate <em>Gaussian Mixture Model</em>. In BetaML we have currently implemented only Gaussain components, but any distribution could be used by just subclassing <code>AbstractMixture</code> and implementing a couple of methids (you are invited to contribute or just ask for a distribution family you are interested), so I prefer to think &quot;gmm&quot; as an acronym for <em>Generative Mixture Model</em>.</p><p>The algorithm tries to find the mixture that maximises the likelihood that the data has been generated indeed from such mixture, where the &quot;E&quot; step refers to computing the probability that each point belongs to each of the k-composants (somehow similar to the step <em>b</em> in the kmeans/kmedoids algorithms), and the &quot;M&quot; step estimates, giving the association probabilities in step &quot;E&quot;, the parameters of the mixture and of the individual components (similar to step <em>c</em>).</p><p>The result here is that each point has a categorical distribution (PMF) representing the probabilities that it belongs to any of the k-components (our classes or clusters). This is interesting, as <code>gmm</code> can be used for many other things that clustering. It forms the backbone of the <a href="../../Imputation.html#BetaML.Imputation.GaussianMixtureImputer"><code>GaussianMixtureImputer</code></a> model to impute missing values (on some or all dimensions) based to how close the record seems to its pears. For the same reasons, <code>GaussianMixtureImputer</code> can also be used to predict user&#39;s behaviours (or users&#39; appreciation) according to the behaviour/ranking made by pears (&quot;collaborative filtering&quot;).</p><p>While the result of <code>GaussianMixtureClusterer</code> is a vector of PMFs (one for each record), error measures and reports with the true values (if known) can be directly applied, as in BetaML they internally call <code>mode()</code> to retrieve the class with the highest probability for each record.</p><p>As we are here, we also try different versions of the BetaML models, even if the default &quot;versions&quot; should be fine. For <code>KMeansClusterer</code> and <code>KMedoidsClusterer</code> we will try different initialisation strategies (&quot;gird&quot;, the default one, &quot;random&quot; and &quot;shuffle&quot;), while for the <code>GaussianMixtureClusterer</code> model we&#39;ll choose different distributions of the Gaussain family (<code>SphericalGaussian</code> - where the variance is a scalar, <code>DiagonalGaussian</code> - with a vector variance, and <code>FullGaussian</code>, where the covariance is a matrix).</p><p>As the result would depend on stochasticity both in the data selected and in the random initialisation, we use a cross-validation approach to run our models several times (with different data) and then we average their results. Cross-Validation in BetaML is very flexible and it is done using the <a href="../../Utils.html#BetaML.Utils.cross_validation"><code>cross_validation</code></a> function. It is used by default for hyperparameters autotuning of the BetaML supervised models. <code>cross_validation</code> works by calling the function <code>f</code>, defined by the user, passing to it the tuple <code>trainData</code>, <code>valData</code> and <code>rng</code> and collecting the result of the function f. The specific method for which <code>trainData</code>, and <code>valData</code> are selected at each iteration depends on the specific <code>sampler</code>.</p><p>We start by selectign a k-fold sampler that split our data in 5 different parts, it uses 4 for training and 1 part (not used here) for validation. We run the simulations twice and, to be sure to have replicable results, we fix the random seed (at the whole crossValidaiton level, not on each iteration).</p><pre><code class="language-julia hljs">sampler = KFold(nsplits=5,nrepeats=3,shuffle=true, rng=copy(AFIXEDRNG))</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">KFold(5, 3, true, StableRNGs.LehmerRNG(state=0x000000000000000000000000000000f7))</code></pre><p>We can now run the cross-validation with our models. Note that instead of defining the function <code>f</code> and then calling <code>cross_validation[f(trainData,testData,rng),[x,y],...)</code> we use the Julia <code>do</code> block syntax and we write directly the content of the <code>f</code> function in the <code>do</code> block. Also, by default cross<em>validation already returns the mean and the standard deviation of the output of the user-provided <code>f</code> function (or the <code>do</code> block). However this requires that the <code>f</code> function returns a single scalar. Here we are returning a vector of the accuracies of the different models (so we can run the cross-validation only once), and hence we indicate with `return</em>statistics=false` to cross_validation not to attempt to generate statistics but rather report the whole output. We&#39;ll compute the statistics ex-post.</p><p>Inside the <code>do</code> block we do 4 things:</p><ul><li>we recover from <code>trainData</code> (a tuple, as we passed a tuple to <code>cross_validation</code> too) the <code>xtrain</code> features and <code>ytrain</code> labels;</li><li>we run the various clustering algorithms</li><li>we use the real labels to compute the model accuracy. Note that the clustering algorithm know nothing about the specific label name or even their order. This is why <a href="../../Utils.html#BetaML.Utils.accuracy-Union{Tuple{T}, Tuple{AbstractVector{Int64}, AbstractMatrix{T}}} where T&lt;:Number"><code>accuracy</code></a> has the parameter <code>ignorelabels</code> to compute the accuracy oven any possible permutation of the classes found.</li><li>we return the various models&#39; accuracies</li></ul><pre><code class="language-julia hljs">cOut = cross_validation([x,y],sampler,return_statistics=false) do trainData,testData,rng
          # For unsupervised learning we use only the train data.
          # Also, we use the associated labels only to measure the performances
         (xtrain,ytrain)  = trainData;
         # We run the clustering algorithm and then and we compute the accuracy using the real labels:
         estcl = fit!(KMeansClusterer(n_classes=3,initialisation_strategy=&quot;grid&quot;,rng=rng),xtrain)
         kMeansGAccuracy    = accuracy(ytrain,estcl,ignorelabels=true)
         estcl = fit!(KMeansClusterer(n_classes=3,initialisation_strategy=&quot;random&quot;,rng=rng),xtrain)
         kMeansRAccuracy   = accuracy(ytrain,estcl,ignorelabels=true)
         estcl = fit!(KMeansClusterer(n_classes=3,initialisation_strategy=&quot;shuffle&quot;,rng=rng),xtrain)
         kMeansSAccuracy   = accuracy(ytrain,estcl,ignorelabels=true)
         estcl = fit!(KMedoidsClusterer(n_classes=3,initialisation_strategy=&quot;grid&quot;,rng=rng),xtrain)
         kMedoidsGAccuracy  = accuracy(ytrain,estcl,ignorelabels=true)
         estcl = fit!(KMedoidsClusterer(n_classes=3,initialisation_strategy=&quot;random&quot;,rng=rng),xtrain)
         kMedoidsRAccuracy = accuracy(ytrain,estcl,ignorelabels=true)
         estcl = fit!(KMedoidsClusterer(n_classes=3,initialisation_strategy=&quot;shuffle&quot;,rng=rng),xtrain)
         kMedoidsSAccuracy = accuracy(ytrain,estcl,ignorelabels=true)
         estcl = fit!(GaussianMixtureClusterer(n_classes=3,mixtures=SphericalGaussian,rng=rng,verbosity=NONE),xtrain)
         gmmSpherAccuracy  = accuracy(ytrain,estcl,ignorelabels=true, rng=rng)
         estcl = fit!(GaussianMixtureClusterer(n_classes=3,mixtures=DiagonalGaussian,rng=rng,verbosity=NONE),xtrain)
         gmmDiagAccuracy   = accuracy(ytrain,estcl,ignorelabels=true, rng=rng)
         estcl = fit!(GaussianMixtureClusterer(n_classes=3,mixtures=FullGaussian,rng=rng,verbosity=NONE),xtrain)
         gmmFullAccuracy   = accuracy(ytrain,estcl,ignorelabels=true, rng=rng)
         # For comparision with Clustering.jl
         clusteringOut     = Clustering.kmeans(xtrain&#39;, 3)
         kMeans2Accuracy   = accuracy(ytrain,clusteringOut.assignments,ignorelabels=true)
         # For comparision with GaussianMistures.jl - sometimes GaussianMistures.jl em! fails with a PosDefException
         dGMM              = GaussianMixtures.GMM(3, xtrain; method=:kmeans, kind=:diag)
         GaussianMixtures.em!(dGMM, xtrain)
         gmmDiag2Accuracy  = accuracy(ytrain,GaussianMixtures.gmmposterior(dGMM, xtrain)[1],ignorelabels=true)
         fGMM              = GaussianMixtures.GMM(3, xtrain; method=:kmeans, kind=:full)
         GaussianMixtures.em!(fGMM, xtrain)
         gmmFull2Accuracy  = accuracy(ytrain,GaussianMixtures.gmmposterior(fGMM, xtrain)[1],ignorelabels=true)
         # Returning the accuracies
         return kMeansGAccuracy,kMeansRAccuracy,kMeansSAccuracy,kMedoidsGAccuracy,kMedoidsRAccuracy,kMedoidsSAccuracy,gmmSpherAccuracy,gmmDiagAccuracy,gmmFullAccuracy,kMeans2Accuracy,gmmDiag2Accuracy,gmmFull2Accuracy
 end

# We transform the output in matrix for easier analysis
accuracies = fill(0.0,(length(cOut),length(cOut[1])))
[accuracies[r,c] = cOut[r][c] for r in 1:length(cOut),c in 1:length(cOut[1])]
μs = mean(accuracies,dims=1)
σs = std(accuracies,dims=1)


modelLabels=[&quot;kMeansG&quot;,&quot;kMeansR&quot;,&quot;kMeansS&quot;,&quot;kMedoidsG&quot;,&quot;kMedoidsR&quot;,&quot;kMedoidsS&quot;,&quot;gmmSpher&quot;,&quot;gmmDiag&quot;,&quot;gmmFull&quot;,&quot;kMeans (Clustering.jl)&quot;,&quot;gmmDiag (GaussianMixtures.jl)&quot;,&quot;gmmFull (GaussianMixtures.jl)&quot;]

report = DataFrame(mName = modelLabels, avgAccuracy = dropdims(round.(μs&#39;,digits=3),dims=2), stdAccuracy = dropdims(round.(σs&#39;,digits=3),dims=2))</code></pre><div><div style = "float: left;"><span>12×3 DataFrame</span></div><div style = "clear: both;"></div></div><div class = "data-frame" style = "overflow-x: scroll;"><table class = "data-frame" style = "margin-bottom: 6px;"><thead><tr class = "header"><th class = "rowNumber" style = "font-weight: bold; text-align: right;">Row</th><th style = "text-align: left;">mName</th><th style = "text-align: left;">avgAccuracy</th><th style = "text-align: left;">stdAccuracy</th></tr><tr class = "subheader headerLastRow"><th class = "rowNumber" style = "font-weight: bold; text-align: right;"></th><th title = "String" style = "text-align: left;">String</th><th title = "Float64" style = "text-align: left;">Float64</th><th title = "Float64" style = "text-align: left;">Float64</th></tr></thead><tbody><tr><td class = "rowNumber" style = "font-weight: bold; text-align: right;">1</td><td style = "text-align: left;">kMeansG</td><td style = "text-align: right;">0.892</td><td style = "text-align: right;">0.015</td></tr><tr><td class = "rowNumber" style = "font-weight: bold; text-align: right;">2</td><td style = "text-align: left;">kMeansR</td><td style = "text-align: right;">0.835</td><td style = "text-align: right;">0.098</td></tr><tr><td class = "rowNumber" style = "font-weight: bold; text-align: right;">3</td><td style = "text-align: left;">kMeansS</td><td style = "text-align: right;">0.825</td><td style = "text-align: right;">0.134</td></tr><tr><td class = "rowNumber" style = "font-weight: bold; text-align: right;">4</td><td style = "text-align: left;">kMedoidsG</td><td style = "text-align: right;">0.897</td><td style = "text-align: right;">0.014</td></tr><tr><td class = "rowNumber" style = "font-weight: bold; text-align: right;">5</td><td style = "text-align: left;">kMedoidsR</td><td style = "text-align: right;">0.817</td><td style = "text-align: right;">0.136</td></tr><tr><td class = "rowNumber" style = "font-weight: bold; text-align: right;">6</td><td style = "text-align: left;">kMedoidsS</td><td style = "text-align: right;">0.851</td><td style = "text-align: right;">0.124</td></tr><tr><td class = "rowNumber" style = "font-weight: bold; text-align: right;">7</td><td style = "text-align: left;">gmmSpher</td><td style = "text-align: right;">0.894</td><td style = "text-align: right;">0.015</td></tr><tr><td class = "rowNumber" style = "font-weight: bold; text-align: right;">8</td><td style = "text-align: left;">gmmDiag</td><td style = "text-align: right;">0.918</td><td style = "text-align: right;">0.019</td></tr><tr><td class = "rowNumber" style = "font-weight: bold; text-align: right;">9</td><td style = "text-align: left;">gmmFull</td><td style = "text-align: right;">0.974</td><td style = "text-align: right;">0.027</td></tr><tr><td class = "rowNumber" style = "font-weight: bold; text-align: right;">10</td><td style = "text-align: left;">kMeans (Clustering.jl)</td><td style = "text-align: right;">0.871</td><td style = "text-align: right;">0.09</td></tr><tr><td class = "rowNumber" style = "font-weight: bold; text-align: right;">11</td><td style = "text-align: left;">gmmDiag (GaussianMixtures.jl)</td><td style = "text-align: right;">0.881</td><td style = "text-align: right;">0.1</td></tr><tr><td class = "rowNumber" style = "font-weight: bold; text-align: right;">12</td><td style = "text-align: left;">gmmFull (GaussianMixtures.jl)</td><td style = "text-align: right;">0.909</td><td style = "text-align: right;">0.138</td></tr></tbody></table></div><p>Accuracies (mean and its standard dev.) running this scripts with different random seeds (<code>123</code>, <code>1000</code> and <code>10000</code>):</p><table><tr><th style="text-align: right">model</th><th style="text-align: right">μ 1</th><th style="text-align: right">σ² 1</th><th style="text-align: right">μ 2</th><th style="text-align: right">σ² 2</th><th style="text-align: right">μ 3</th><th style="text-align: right">σ² 3</th></tr><tr><td style="text-align: right">│ kMeansG</td><td style="text-align: right">0.891</td><td style="text-align: right">0.017</td><td style="text-align: right">0.892</td><td style="text-align: right">0.012</td><td style="text-align: right">0.893</td><td style="text-align: right">0.017</td></tr><tr><td style="text-align: right">│ kMeansR</td><td style="text-align: right">0.866</td><td style="text-align: right">0.083</td><td style="text-align: right">0.831</td><td style="text-align: right">0.127</td><td style="text-align: right">0.836</td><td style="text-align: right">0.114</td></tr><tr><td style="text-align: right">│ kMeansS</td><td style="text-align: right">0.764</td><td style="text-align: right">0.174</td><td style="text-align: right">0.822</td><td style="text-align: right">0.145</td><td style="text-align: right">0.779</td><td style="text-align: right">0.170</td></tr><tr><td style="text-align: right">│ kMedoidsG</td><td style="text-align: right">0.894</td><td style="text-align: right">0.015</td><td style="text-align: right">0.896</td><td style="text-align: right">0.012</td><td style="text-align: right">0.894</td><td style="text-align: right">0.017</td></tr><tr><td style="text-align: right">│ kMedoidsR</td><td style="text-align: right">0.804</td><td style="text-align: right">0.144</td><td style="text-align: right">0.841</td><td style="text-align: right">0.123</td><td style="text-align: right">0.825</td><td style="text-align: right">0.134</td></tr><tr><td style="text-align: right">│ kMedoidsS</td><td style="text-align: right">0.893</td><td style="text-align: right">0.018</td><td style="text-align: right">0.834</td><td style="text-align: right">0.130</td><td style="text-align: right">0.877</td><td style="text-align: right">0.085</td></tr><tr><td style="text-align: right">│ gmmSpher</td><td style="text-align: right">0.893</td><td style="text-align: right">0.016</td><td style="text-align: right">0.891</td><td style="text-align: right">0.016</td><td style="text-align: right">0.895</td><td style="text-align: right">0.017</td></tr><tr><td style="text-align: right">│ gmmDiag</td><td style="text-align: right">0.917</td><td style="text-align: right">0.022</td><td style="text-align: right">0.912</td><td style="text-align: right">0.016</td><td style="text-align: right">0.916</td><td style="text-align: right">0.014</td></tr><tr><td style="text-align: right">│ gmmFull</td><td style="text-align: right">0.970</td><td style="text-align: right">0.035</td><td style="text-align: right">0.982</td><td style="text-align: right">0.013</td><td style="text-align: right">0.981</td><td style="text-align: right">0.009</td></tr><tr><td style="text-align: right">│ kMeans (Clustering.jl)</td><td style="text-align: right">0.856</td><td style="text-align: right">0.112</td><td style="text-align: right">0.873</td><td style="text-align: right">0.083</td><td style="text-align: right">0.873</td><td style="text-align: right">0.089</td></tr><tr><td style="text-align: right">│ gmmDiag (GaussianMixtures.jl)</td><td style="text-align: right">0.865</td><td style="text-align: right">0.127</td><td style="text-align: right">0.872</td><td style="text-align: right">0.090</td><td style="text-align: right">0.833</td><td style="text-align: right">0.152</td></tr><tr><td style="text-align: right">│ gmmFull (GaussianMixtures.jl)</td><td style="text-align: right">0.907</td><td style="text-align: right">0.133</td><td style="text-align: right">0.914</td><td style="text-align: right">0.160</td><td style="text-align: right">0.917</td><td style="text-align: right">0.141</td></tr></table><p>We can see that running the script multiple times with different random seed confirm the estimated standard deviations collected with the cross_validation, with the BetaML GMM-based models and grid based ones being the most stable ones.</p><h3 id="BetaML-model-accuracies"><a class="docs-heading-anchor" href="#BetaML-model-accuracies">BetaML model accuracies</a><a id="BetaML-model-accuracies-1"></a><a class="docs-heading-anchor-permalink" href="#BetaML-model-accuracies" title="Permalink"></a></h3><p>From the output We see that the gmm models perform for this dataset generally better than kmeans or kmedoids algorithms, and they further have very low variances. In detail, it is the (default) <code>grid</code> initialisation that leads to the better results for <code>kmeans</code> and <code>kmedoids</code>, while for the <code>gmm</code> models it is the <code>FullGaussian</code> to perform better.</p><h3 id="Comparisions-with-Clustering.jl-and-GaussianMixtures.jl"><a class="docs-heading-anchor" href="#Comparisions-with-Clustering.jl-and-GaussianMixtures.jl">Comparisions with <code>Clustering.jl</code> and <code>GaussianMixtures.jl</code></a><a id="Comparisions-with-Clustering.jl-and-GaussianMixtures.jl-1"></a><a class="docs-heading-anchor-permalink" href="#Comparisions-with-Clustering.jl-and-GaussianMixtures.jl" title="Permalink"></a></h3><p>For this specific case, both <code>Clustering.jl</code> and <code>GaussianMixtures.jl</code> report substantially worst accuracies, and with very high variances. But we maintain the ranking that Full Gaussian gmm &gt; Diagonal Gaussian &gt; Kmeans accuracy. I suspect the reason that BetaML gmm works so well is in relation to the usage of kmeans algorithm for initialisation of the mixtures, itself initialized with a &quot;grid&quot; arpproach. The grid initialisation &quot;guarantee&quot; indeed that the initial means of the mixture components are well spread across the multidimensional space defined by the data, and it helps avoiding the EM algoritm to converge to a bad local optimus.</p><h2 id="Working-without-the-labels"><a class="docs-heading-anchor" href="#Working-without-the-labels">Working without the labels</a><a id="Working-without-the-labels-1"></a><a class="docs-heading-anchor-permalink" href="#Working-without-the-labels" title="Permalink"></a></h2><p>Up to now we used the real labels to compare the model accuracies. But in real clustering examples we don&#39;t have the true classes, or we wouln&#39;t need to do clustering in the first instance, so we don&#39;t know the number of classes to use. There are several methods to judge clusters algorithms goodness. For likelyhood based algorithms as <code>GaussianMixtureClusterer</code> we can use a information criteria that trade the goodness of the lickelyhood with the number of parameters used to do the fit. BetaML provides by default in the gmm clustering outputs both the <em>Bayesian information criterion</em>  (<a href="../../Utils.html#BetaML.Utils.bic-Tuple{Any, Any, Any}"><code>BIC</code></a>) and the <em>Akaike information criterion</em>  (<a href="../../Utils.html#BetaML.Utils.aic-Tuple{Any, Any}"><code>AIC</code></a>), where for both a lower value is better.</p><p>We can then run the model with different number of classes and see which one leads to the lower BIC or AIC. We run hence <code>cross_validation</code> again with the <code>FullGaussian</code> gmm model. Note that we use the BIC/AIC criteria here for establishing the &quot;best&quot; number of classes but we could have used it also to select the kind of Gaussain distribution to use. This is one example of hyper-parameter tuning that we developed more in detail using autotuning in the <a href="../Regression - bike sharing/betaml_tutorial_regression_sharingBikes.html#regression_tutorial">regression tutorial</a>.</p><p>Let&#39;s try up to 4 possible classes:</p><pre><code class="language-julia hljs">K = 4
sampler = KFold(nsplits=5,nrepeats=2,shuffle=true, rng=copy(AFIXEDRNG))
cOut = cross_validation([x,y],sampler,return_statistics=false) do trainData,testData,rng
    (xtrain,ytrain)  = trainData;
    BICS = []
    AICS = []
    for k in 1:K
        m = GaussianMixtureClusterer(n_classes=k,mixtures=FullGaussian,rng=rng,verbosity=NONE)
        fit!(m,xtrain)
        push!(BICS,info(m)[&quot;BIC&quot;])
        push!(AICS,info(m)[&quot;AIC&quot;])
    end
    return (BICS,AICS)
end

# Transforming the output in matrices for easier analysis
Nit = length(cOut)

BICS = fill(0.0,(Nit,K))
AICS = fill(0.0,(Nit,K))
[BICS[r,c] = cOut[r][1][c] for r in 1:Nit,c in 1:K]
[AICS[r,c] = cOut[r][2][c] for r in 1:Nit,c in 1:K]

μsBICS = mean(BICS,dims=1)</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">1×4 Matrix{Float64}:
 762.112  516.031  539.392  593.272</code></pre><pre><code class="language-julia hljs">σsBICS = std(BICS,dims=1)</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">1×4 Matrix{Float64}:
 12.2912  15.8085  17.7181  24.6026</code></pre><pre><code class="language-julia hljs">μsAICS = mean(AICS,dims=1)</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">1×4 Matrix{Float64}:
 723.087  435.194  416.743  428.81</code></pre><pre><code class="language-julia hljs">σsAICS = std(AICS,dims=1)</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">1×4 Matrix{Float64}:
 12.2912  15.8085  17.7181  24.6026</code></pre><pre><code class="language-julia hljs">plot(1:K,[μsBICS&#39; μsAICS&#39;], labels=[&quot;BIC&quot; &quot;AIC&quot;], title=&quot;Information criteria by number of classes&quot;, xlabel=&quot;number of classes&quot;, ylabel=&quot;lower is better&quot;)</code></pre><img src="betaml_tutorial_cluster_iris-c3bc1822.svg" alt="Example block output"/><p>We see that following the &quot;lowest AIC&quot; rule we would indeed choose three classes, while following the &quot;lowest BIC&quot; criteria we would have choosen only two classes. This means that there is two classes that, concerning the floreal measures used in the database, are very similar, and our models are unsure about them. Perhaps the biologists will end up one day with the conclusion that it is indeed only one specie :-).</p><p>We could study this issue more in detail by analysing the <a href="../../Utils.html#BetaML.Utils.ConfusionMatrix"><code>ConfusionMatrix</code></a>, but the one used in BetaML does not account for the ignorelabels option (yet).</p><h3 id="Analysing-the-silhouette-of-the-cluster"><a class="docs-heading-anchor" href="#Analysing-the-silhouette-of-the-cluster">Analysing the silhouette of the cluster</a><a id="Analysing-the-silhouette-of-the-cluster-1"></a><a class="docs-heading-anchor-permalink" href="#Analysing-the-silhouette-of-the-cluster" title="Permalink"></a></h3><p>A further metric to analyse cluster output is the so-called <a href="https://en.wikipedia.org/wiki/Silhouette_(clustering)">Sinhouette method</a></p><p>Silhouette is a distance-based metric and require as first argument a matrix of pairwise distances. This can be computed with the <a href="../../Utils.html#BetaML.Utils.pairwise-Tuple{AbstractArray}"><code>pairwise</code></a> function, that default to using <code>l2_distance</code> (i.e. Euclidean). Many other distance functions are available in the <a href="../../Clustering.html#BetaML.Clustering"><code>Clustering</code></a> sub-module or one can use the efficiently implemented distances from the <a href="https://github.com/JuliaStats/Distances.jl"><code>Distances</code></a> package, as in this example.</p><p>We&#39;ll use here the <a href="../../Utils.html#BetaML.Utils.silhouette-Tuple{Any, Any}"><code>silhouette</code></a> function over a simple loop:</p><pre><code class="language-julia hljs">x,y = consistent_shuffle([x,y],dims=1)
import Distances
pd = pairwise(x,distance=Distances.euclidean) # we compute the pairwise distances
nclasses = 2:6
models = [KMeansClusterer, KMedoidsClusterer, GaussianMixtureClusterer]
println(&quot;Silhouette score by model type and class number:&quot;)
for ncl in nclasses, mtype in models
    m = mtype(n_classes=ncl, verbosity=NONE)
    ŷ = fit!(m,x)
    if mtype == GaussianMixtureClusterer
        ŷ = mode(ŷ)
    end
    s = mean(silhouette(pd,ŷ))
    println(&quot;$mtype \t ($ncl classes): $s&quot;)
end</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">Silhouette score by model type and class number:
KMeansClusterer 	 (2 classes): 0.6810461692117465
KMedoidsClusterer 	 (2 classes): 0.6857881712617193
GaussianMixtureClusterer 	 (2 classes): 0.6867350732769778
KMeansClusterer 	 (3 classes): 0.5528190123564098
KMedoidsClusterer 	 (3 classes): 0.5528190123564098
GaussianMixtureClusterer 	 (3 classes): 0.5522806746748189
KMeansClusterer 	 (4 classes): 0.4962511348125098
KMedoidsClusterer 	 (4 classes): 0.4882419477378052
GaussianMixtureClusterer 	 (4 classes): 0.46095184656234467
KMeansClusterer 	 (5 classes): 0.48874888709310615
KMedoidsClusterer 	 (5 classes): 0.4584191432646477
GaussianMixtureClusterer 	 (5 classes): 0.4863408030950679
KMeansClusterer 	 (6 classes): 0.3674845748098317
KMedoidsClusterer 	 (6 classes): 0.34916011367198635
GaussianMixtureClusterer 	 (6 classes): 0.3543173617053886</code></pre><p>Highest levels are better. We see again that 2 classes have better scores !</p><h2 id="Conclusions"><a class="docs-heading-anchor" href="#Conclusions">Conclusions</a><a id="Conclusions-1"></a><a class="docs-heading-anchor-permalink" href="#Conclusions" title="Permalink"></a></h2><p>We have shown in this tutorial how we can easily run clustering algorithms in BetaML with just one line of code <code>fit!(ChoosenClusterer(),x)</code>, but also how can we use cross-validation in order to help the model or parameter selection, with or whithout knowing the real classes. We retrieve here what we observed with supervised models. Globally the accuracy of BetaML models are comparable to those of leading specialised packages (in this case they are even better), but there is a significant gap in computational efficiency that restricts the pratical usage of BetaML to datasets that fits in the pc memory. However we trade this relative inefficiency with very flexible model definition and utility functions (for example <code>GaussianMixtureClusterer</code> works with missing data, allowing it to be used as the backbone of the <a href="../../Imputation.html#BetaML.Imputation.GaussianMixtureImputer"><code>GaussianMixtureImputer</code></a> missing imputation function, or for collaborative reccomendation systems).</p><p><a href="betaml_tutorial_cluster_iris.jl">View this file on Github</a>.</p><hr/><p><em>This page was generated using <a href="https://github.com/fredrikekre/Literate.jl">Literate.jl</a>.</em></p></article><nav class="docs-footer"><a class="docs-footer-prevpage" href="../Regression - bike sharing/betaml_tutorial_regression_sharingBikes.html">« A regression task: the prediction of  bike  sharing demand</a><a class="docs-footer-nextpage" href="../Multi-branch neural network/betaml_tutorial_multibranch_nn.html">A deep neural network with multi-branch architecture »</a><div class="flexbox-break"></div><p class="footer-message">Powered by <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> and the <a href="https://julialang.org/">Julia Programming Language</a>.</p></nav></div><div class="modal" id="documenter-settings"><div class="modal-background"></div><div class="modal-card"><header class="modal-card-head"><p class="modal-card-title">Settings</p><button class="delete"></button></header><section class="modal-card-body"><p><label class="label">Theme</label><div class="select"><select id="documenter-themepicker"><option value="auto">Automatic (OS)</option><option value="documenter-light">documenter-light</option><option value="documenter-dark">documenter-dark</option></select></div></p><hr/><p>This document was generated with <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> version 1.4.1 on <span class="colophon-date" title="Wednesday 15 May 2024 20:02">Wednesday 15 May 2024</span>. Using Julia version 1.6.7.</p></section><footer class="modal-card-foot"></footer></div></div></div></body></html>
