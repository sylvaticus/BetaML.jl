<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>GMM · BetaML.jl Documentation</title><script async src="https://www.googletagmanager.com/gtag/js?id=G-JYKX8QY5JW"></script><script>  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());
  gtag('config', 'G-JYKX8QY5JW', {'page_path': location.pathname + location.search + location.hash});
</script><script data-outdated-warner src="assets/warner.js"></script><link href="https://cdnjs.cloudflare.com/ajax/libs/lato-font/3.0.0/css/lato-font.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/juliamono/0.045/juliamono.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.4/css/fontawesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.4/css/solid.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.4/css/brands.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.24/katex.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL="."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" data-main="assets/documenter.js"></script><script src="siteinfo.js"></script><script src="../versions.js"></script><link class="docs-theme-link" rel="stylesheet" type="text/css" href="assets/themes/documenter-dark.css" data-theme-name="documenter-dark" data-theme-primary-dark/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="assets/themes/documenter-light.css" data-theme-name="documenter-light" data-theme-primary/><script src="assets/themeswap.js"></script></head><body><div id="documenter"><nav class="docs-sidebar"><div class="docs-package-name"><span class="docs-autofit"><a href="index.html">BetaML.jl Documentation</a></span></div><form class="docs-search" action="search.html"><input class="docs-search-query" id="documenter-search-query" name="q" type="text" placeholder="Search docs"/></form><ul class="docs-menu"><li><a class="tocitem" href="index.html">Index</a></li><li><span class="tocitem">Tutorial</span><ul><li><a class="tocitem" href="tutorials/Betaml_tutorial_getting_started.html">Getting started</a></li><li><input class="collapse-toggle" id="menuitem-2-2" type="checkbox"/><label class="tocitem" for="menuitem-2-2"><span class="docs-label">Regression - bike sharing</span><i class="docs-chevron"></i></label><ul class="collapsed"><li><a class="tocitem" href="tutorials/Regression - bike sharing/betaml_tutorial_regression_sharingBikes.html">A regression task: the prediction of  bike  sharing demand</a></li></ul></li><li><input class="collapse-toggle" id="menuitem-2-3" type="checkbox"/><label class="tocitem" for="menuitem-2-3"><span class="docs-label">Classification - cars</span><i class="docs-chevron"></i></label><ul class="collapsed"><li><a class="tocitem" href="tutorials/Classification - cars/betaml_tutorial_classification_cars.html">A classification task when labels are known - determining the country of origin of cars given the cars characteristics</a></li></ul></li><li><input class="collapse-toggle" id="menuitem-2-4" type="checkbox"/><label class="tocitem" for="menuitem-2-4"><span class="docs-label">Clustering - Iris</span><i class="docs-chevron"></i></label><ul class="collapsed"><li><a class="tocitem" href="tutorials/Clustering - Iris/betaml_tutorial_cluster_iris.html">A clustering task: the prediction of  plant species from floreal measures (the iris dataset)</a></li></ul></li></ul></li><li><span class="tocitem">API (Reference manual)</span><ul><li><input class="collapse-toggle" id="menuitem-3-1" type="checkbox"/><label class="tocitem" for="menuitem-3-1"><span class="docs-label">API V2 (current)</span><i class="docs-chevron"></i></label><ul class="collapsed"><li><a class="tocitem" href="Api_v2_user.html">Introduction for user</a></li><li><input class="collapse-toggle" id="menuitem-3-1-2" type="checkbox"/><label class="tocitem" for="menuitem-3-1-2"><span class="docs-label">For developers</span><i class="docs-chevron"></i></label><ul class="collapsed"><li><a class="tocitem" href="Api_v2_developer.html">API implementation</a></li><li><a class="tocitem" href="StyleGuide_templates.html">Style guide</a></li></ul></li><li><a class="tocitem" href="Api.html">The Api module</a></li></ul></li><li><a class="tocitem" href="Perceptron.html">Perceptron</a></li><li><a class="tocitem" href="Trees.html">Trees</a></li><li><a class="tocitem" href="Nn.html">Nn</a></li><li><a class="tocitem" href="Clustering.html">Clustering</a></li><li class="is-active"><a class="tocitem" href="GMM.html">GMM</a><ul class="internal"><li><a class="tocitem" href="#Module-Index"><span>Module Index</span></a></li><li><a class="tocitem" href="#Detailed-API"><span>Detailed API</span></a></li></ul></li><li><a class="tocitem" href="Imputation.html">Imputation</a></li><li><a class="tocitem" href="Utils.html">Utils</a></li></ul></li></ul><div class="docs-version-selector field has-addons"><div class="control"><span class="docs-label button is-static is-size-7">Version</span></div><div class="docs-selector control is-expanded"><div class="select is-fullwidth is-size-7"><select id="documenter-version-selector"></select></div></div></div></nav><div class="docs-main"><header class="docs-navbar"><nav class="breadcrumb"><ul class="is-hidden-mobile"><li><a class="is-disabled">API (Reference manual)</a></li><li class="is-active"><a href="GMM.html">GMM</a></li></ul><ul class="is-hidden-tablet"><li class="is-active"><a href="GMM.html">GMM</a></li></ul></nav><div class="docs-right"><a class="docs-edit-link" href="https://github.com/sylvaticus/BetaML.jl/blob/master/docs/src/GMM.md" title="Edit on GitHub"><span class="docs-icon fab"></span><span class="docs-label is-hidden-touch">Edit on GitHub</span></a><a class="docs-settings-button fas fa-cog" id="documenter-settings-button" href="#" title="Settings"></a><a class="docs-sidebar-button fa fa-bars is-hidden-desktop" id="documenter-sidebar-button" href="#"></a></div></header><article class="content" id="documenter-page"><h1 id="gmm_module"><a class="docs-heading-anchor" href="#gmm_module">The BetaML.GMM Module</a><a id="gmm_module-1"></a><a class="docs-heading-anchor-permalink" href="#gmm_module" title="Permalink"></a></h1><article class="docstring"><header><a class="docstring-binding" id="BetaML.GMM" href="#BetaML.GMM"><code>BetaML.GMM</code></a> — <span class="docstring-category">Module</span></header><section><div><pre><code class="language-julia hljs">GMM module</code></pre><p>Generative (Gaussian) Mixed Model learners (supervised/unsupervised)</p><p>Provides clustering and regressors using  (Generative) Gaussiam Mixture Model (probabilistic).</p><p>Collaborative filtering / missing values imputation / reccomendation systems based on GMM is available in the <a href="@ref BetaML.Imputation"><code>Imputation</code></a> module.</p><p>The module provides the following models. Use <code>?[model]</code> to access their documentation:</p><ul><li><a href="GMM.html#BetaML.GMM.GMMClusterer"><code>GMMClusterer</code></a>: soft-clustering using GMM</li><li><a href="GMM.html#BetaML.GMM.GMMRegressor1"><code>GMMRegressor1</code></a>: regressor using GMM as back-end (first algorithm)</li><li><a href="GMM.html#BetaML.GMM.GMMRegressor1"><code>GMMRegressor1</code></a>: regressor using GMM as back-end (second algorithm)</li></ul><p>All the algorithms works with arbitrary mixture distribution, altought only {Spherical|Diagonal|Full} Gaussian mixtures has been implemented. User defined mixtures can be used defining a struct as subtype of <code>AbstractMixture</code> and implementing for that mixture the following functions:</p><ul><li><code>init_mixtures!(mixtures, X; minimum_variance, minimum_covariance, initialisation_strategy)</code></li><li><code>lpdf(m,x,mask)</code> (for the e-step)</li><li><code>update_parameters!(mixtures, X, pₙₖ; minimum_variance, minimum_covariance)</code> (the m-step)</li></ul><p>All the GMM-based algorithms works only with numerical data, but accepts also Missing one.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/sylvaticus/BetaML.jl/blob/be10a0b12b86ba83cbc3d611d51a1ce9b0f2f849/src/GMM/GMM.jl#L3-L25">source</a></section></article><h2 id="Module-Index"><a class="docs-heading-anchor" href="#Module-Index">Module Index</a><a id="Module-Index-1"></a><a class="docs-heading-anchor-permalink" href="#Module-Index" title="Permalink"></a></h2><ul><li><a href="GMM.html#BetaML.GMM.GMMClusterer"><code>BetaML.GMM.GMMClusterer</code></a></li><li><a href="GMM.html#BetaML.GMM.GMMHyperParametersSet"><code>BetaML.GMM.GMMHyperParametersSet</code></a></li><li><a href="GMM.html#BetaML.GMM.GMMRegressor1"><code>BetaML.GMM.GMMRegressor1</code></a></li><li><a href="GMM.html#BetaML.GMM.GMMRegressor2"><code>BetaML.GMM.GMMRegressor2</code></a></li><li><a href="GMM.html#BetaML.GMM.GaussianMixtureClusterer"><code>BetaML.GMM.GaussianMixtureClusterer</code></a></li><li><a href="GMM.html#BetaML.GMM.GaussianMixtureRegressor"><code>BetaML.GMM.GaussianMixtureRegressor</code></a></li><li><a href="GMM.html#BetaML.GMM.MultitargetGaussianMixtureRegressor"><code>BetaML.GMM.MultitargetGaussianMixtureRegressor</code></a></li><li><a href="GMM.html#BetaML.GMM.gmm-Tuple{Any, Any}"><code>BetaML.GMM.gmm</code></a></li><li><a href="GMM.html#BetaML.GMM.init_mixtures!-Union{Tuple{T}, Tuple{Vector{T}, Any}} where T&lt;:BetaML.GMM.AbstractGaussian"><code>BetaML.GMM.init_mixtures!</code></a></li><li><a href="GMM.html#BetaML.GMM.lpdf-Tuple{SphericalGaussian, Any, Any}"><code>BetaML.GMM.lpdf</code></a></li><li><a href="GMM.html#BetaML.GMM.lpdf-Tuple{DiagonalGaussian, Any, Any}"><code>BetaML.GMM.lpdf</code></a></li><li><a href="GMM.html#BetaML.GMM.lpdf-Tuple{FullGaussian, Any, Any}"><code>BetaML.GMM.lpdf</code></a></li></ul><h2 id="Detailed-API"><a class="docs-heading-anchor" href="#Detailed-API">Detailed API</a><a id="Detailed-API-1"></a><a class="docs-heading-anchor-permalink" href="#Detailed-API" title="Permalink"></a></h2><article class="docstring"><header><a class="docstring-binding" id="BetaML.GMM.GMMClusterer" href="#BetaML.GMM.GMMClusterer"><code>BetaML.GMM.GMMClusterer</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">mutable struct GMMClusterer &lt;: BetaMLUnsupervisedModel</code></pre><p>Assign class probabilities to records (i.e. <em>soft</em> clustering) assuming a probabilistic generative model of observed data using mixtures.</p><p>For the parameters see <a href="GMM.html#BetaML.GMM.GMMHyperParametersSet"><code>?GMMHyperParametersSet</code></a> and <a href="Api.html#BetaML.Api.BetaMLDefaultOptionsSet"><code>?BetaMLDefaultOptionsSet</code></a>.</p><p><strong>Notes:</strong></p><ul><li>Data must be numerical</li><li>Mixtures can be user defined: see the <a href="GMM.html#BetaML.GMM"><code>?GMM</code></a> module documentation for a discussion on provided vs custom mixtures.</li><li>Online fitting (re-fitting with new data) is supported by setting the old learned mixtrures as the starting values</li><li>The model is fitted using an Expectation-Minimisation (EM) algorithm that supports Missing data and is implemented in the log-domain for better numerical accuracy with many dimensions</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/sylvaticus/BetaML.jl/blob/be10a0b12b86ba83cbc3d611d51a1ce9b0f2f849/src/GMM/GMM_clustering.jl#L211">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="BetaML.GMM.GMMHyperParametersSet" href="#BetaML.GMM.GMMHyperParametersSet"><code>BetaML.GMM.GMMHyperParametersSet</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">mutable struct GMMHyperParametersSet &lt;: BetaMLHyperParametersSet</code></pre><p>Hyperparameters for GMM clusters and other GMM-based algorithms</p><p><strong>Parameters:</strong></p><ul><li><p><code>n_classes</code></p><p>Number of mixtures (latent classes) to consider [def: 3]</p></li><li><p><code>initial_probmixtures</code></p><p>Initial probabilities of the categorical distribution (n_classes x 1) [default: <code>[]</code>]</p></li><li><p><code>mixtures</code></p><p>An array (of length <code>n_classes</code><code>) of the mixtures to employ (see the [</code>?GMM<code>](@ref GMM) module). Each mixture object can be provided with or without its parameters (e.g. mean and variance for the gaussian ones). Fully qualified mixtures are useful only if the</code>initialisation<em>strategy<code>parameter is  set to &quot;gived&quot;</code> [def: `[DiagonalGaussian() for i in 1:n</em>classes]`]</p></li><li><p><code>tol</code></p><p>Tolerance to stop the algorithm [default: 10^(-6)]</p></li><li><p><code>minimum_variance</code></p><p>Minimum variance for the mixtures [default: 0.05]</p></li><li><p><code>minimum_covariance</code></p><p>Minimum covariance for the mixtures with full covariance matrix [default: 0]. This should be set different than minimum_variance (see notes).</p></li><li><p><code>initialisation_strategy</code></p><p>The computation method of the vector of the initial mixtures. One of the following:</p><ul><li>&quot;grid&quot;: using a grid approach</li><li>&quot;given&quot;: using the mixture provided in the fully qualified <code>mixtures</code> parameter</li><li>&quot;kmeans&quot;: use first kmeans (itself initialised with a &quot;grid&quot; strategy) to set the initial mixture centers [default]</li></ul><p>Note that currently &quot;random&quot; and &quot;shuffle&quot; initialisations are not supported in gmm-based algorithms.</p></li></ul><ul><li><p><code>maximum_iterations</code></p><p>Maximum number of iterations [def: <code>typemax(Int64)</code>, i.e. ∞]</p></li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/sylvaticus/BetaML.jl/blob/be10a0b12b86ba83cbc3d611d51a1ce9b0f2f849/src/GMM/GMM_clustering.jl#L168">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="BetaML.GMM.GMMRegressor1" href="#BetaML.GMM.GMMRegressor1"><code>BetaML.GMM.GMMRegressor1</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">mutable struct GMMRegressor1 &lt;: BetaMLUnsupervisedModel</code></pre><p>A multi-dimensional, missing data friendly non-linear regressor based on Generative (Gaussian) Mixture Model (strategy &quot;1&quot;).</p><p>The training data is used to fit a probabilistic model with latent mixtures (Gaussian distributions with different covariances are already implemented) and then predictions of new data is obtained by fitting the new data to the mixtures.</p><p>For hyperparameters see <a href="GMM.html#BetaML.GMM.GMMHyperParametersSet"><code>GMMHyperParametersSet</code></a> and <a href="Api.html#BetaML.Api.BetaMLDefaultOptionsSet"><code>BetaMLDefaultOptionsSet</code></a>.</p><p>This strategy (<code>GMMRegressor1</code>) works by fitting the EM algorithm on the feature matrix X. Once the data has been probabilistically assigned to the various classes, a mean value of fitting values Y is computed for each cluster (using the probabilities as weigths). At predict time, the new data is first fitted to the learned mixtures using the e-step part of the EM algorithm to obtain the probabilistic assignment of each record to the various mixtures. Then these probabilities are multiplied to the mixture averages for the Y dimensions learned at training time to obtain the predicted value(s) for each record. </p><p><strong>Notes:</strong></p><ul><li>the predicted values are always a matrix, even when a single variable is predicted (use <code>dropdims(ŷ,dims=2)</code> to get a single vector).</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/sylvaticus/BetaML.jl/blob/be10a0b12b86ba83cbc3d611d51a1ce9b0f2f849/src/GMM/GMM_regression.jl#L15">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="BetaML.GMM.GMMRegressor2" href="#BetaML.GMM.GMMRegressor2"><code>BetaML.GMM.GMMRegressor2</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">mutable struct GMMRegressor2 &lt;: BetaMLUnsupervisedModel</code></pre><p>A multi-dimensional, missing data friendly non-linear regressor based on Generative (Gaussian) Mixture Model.</p><p>The training data is used to fit a probabilistic model with latent mixtures (Gaussian distributions with different covariances are already implemented) and then predictions of new data is obtained by fitting the new data to the mixtures.</p><p>For hyperparameters see <a href="GMM.html#BetaML.GMM.GMMHyperParametersSet"><code>GMMHyperParametersSet</code></a> and <a href="Api.html#BetaML.Api.BetaMLDefaultOptionsSet"><code>BetaMLDefaultOptionsSet</code></a>.</p><p>Thsi strategy (<code>GMMRegressor2</code>) works by training the EM algorithm on a combined (hcat) matrix of X and Y. At predict time, the new data is first fitted to the learned mixtures using the e-step part of the EM algorithm (and using missing values for the dimensions belonging to Y) to obtain the probabilistic assignment of each record to the various mixtures. Then these probabilities are multiplied to the mixture averages for the Y dimensions to obtain the predicted value(s) for each record. </p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/sylvaticus/BetaML.jl/blob/be10a0b12b86ba83cbc3d611d51a1ce9b0f2f849/src/GMM/GMM_regression.jl#L157">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="BetaML.GMM.GaussianMixtureClusterer" href="#BetaML.GMM.GaussianMixtureClusterer"><code>BetaML.GMM.GaussianMixtureClusterer</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">mutable struct GaussianMixtureClusterer &lt;: MLJModelInterface.Unsupervised</code></pre><p>A Expectation-Maximisation clustering algorithm with customisable mixtures, from the Beta Machine Learning Toolkit (BetaML).</p><p><strong>Hyperparameters:</strong></p><ul><li><p><code>n_classes::Int64</code></p><p>Number of mixtures (latent classes) to consider [def: 3]</p></li><li><p><code>initial_probmixtures::AbstractVector{Float64}</code></p><p>Initial probabilities of the categorical distribution (n_classes x 1) [default: <code>[]</code>]</p></li><li><p><code>mixtures::Vector{AbstractMixture}</code></p><p>An array (of length <code>n_classes</code><code>) of the mixtures to employ (see the [</code>?GMM<code>](@ref GMM) module of BetaML for available mixtures). Each mixture object can be provided with or without its parameters (e.g. mean and variance for the gaussian ones). Fully qualified mixtures are useful only if the</code>initialisation<em>strategy<code>parameter is  set to &quot;given&quot;</code> [def: `[DiagonalGaussian() for i in 1:n</em>classes]`]</p></li><li><p><code>tol::Float64</code></p><p>Tolerance to stop the algorithm [default: 10^(-6)]</p></li><li><p><code>minimum_variance::Float64</code></p><p>Minimum variance for the mixtures [default: 0.05]</p></li><li><p><code>minimum_covariance::Float64</code></p><p>Minimum covariance for the mixtures with full covariance matrix [default: 0]. This should be set different than minimum_variance (see notes).</p></li><li><p><code>initialisation_strategy::String</code></p><p>The computation method of the vector of the initial mixtures. One of the following:</p><ul><li>&quot;grid&quot;: using a grid approach</li><li>&quot;given&quot;: using the mixture provided in the fully qualified <code>mixtures</code> parameter</li><li>&quot;kmeans&quot;: use first kmeans (itself initialised with a &quot;grid&quot; strategy) to set the initial mixture centers [default]</li></ul><p>Note that currently &quot;random&quot; and &quot;shuffle&quot; initialisations are not supported in gmm-based algorithms.</p></li><li><p><code>maximum_iterations::Int64</code></p><p>Maximum number of iterations [def: <code>typemax(Int64)</code>, i.e. ∞]</p></li><li><p><code>rng::Random.AbstractRNG</code></p><p>Random Number Generator [deafult: <code>Random.GLOBAL_RNG</code>]</p></li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/sylvaticus/BetaML.jl/blob/be10a0b12b86ba83cbc3d611d51a1ce9b0f2f849/src/GMM/GMM_MLJ.jl#L13">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="BetaML.GMM.GaussianMixtureRegressor" href="#BetaML.GMM.GaussianMixtureRegressor"><code>BetaML.GMM.GaussianMixtureRegressor</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">mutable struct GaussianMixtureRegressor &lt;: MLJModelInterface.Deterministic</code></pre><p>A non-linear regressor derived from fitting the data on a probabilistic model (Gaussian Mixture Model). Relatively fast.</p><p>This is the single-target version of the model. If you want to predict several labels (y) at once, use the MLJ model <a href="GMM.html#BetaML.GMM.MultitargetGaussianMixtureRegressor"><code>MultitargetGaussianMixtureRegressor</code></a>.</p><p><strong>Hyperparameters:</strong></p><ul><li><p><code>n_classes::Int64</code></p><p>Number of mixtures (latent classes) to consider [def: 3]</p></li><li><p><code>initial_probmixtures::Vector{Float64}</code></p><p>Initial probabilities of the categorical distribution (n_classes x 1) [default: <code>[]</code>]</p></li><li><p><code>mixtures::Vector{AbstractMixture}</code></p><p>An array (of length <code>n_classes</code><code>) of the mixtures to employ (see the [</code>?GMM<code>](@ref GMM) module). Each mixture object can be provided with or without its parameters (e.g. mean and variance for the gaussian ones). Fully qualified mixtures are useful only if the</code>initialisation<em>strategy<code>parameter is  set to &quot;given&quot;</code> [def: `[DiagonalGaussian() for i in 1:n</em>classes]`]</p></li><li><p><code>tol::Float64</code></p><p>Tolerance to stop the algorithm [default: 10^(-6)]</p></li><li><p><code>minimum_variance::Float64</code></p><p>Minimum variance for the mixtures [default: 0.05]</p></li><li><p><code>minimum_covariance::Float64</code></p><p>Minimum covariance for the mixtures with full covariance matrix [default: 0]. This should be set different than minimum_variance (see notes).</p></li><li><p><code>initialisation_strategy::String</code></p><p>The computation method of the vector of the initial mixtures. One of the following:</p><ul><li>&quot;grid&quot;: using a grid approach</li><li>&quot;given&quot;: using the mixture provided in the fully qualified <code>mixtures</code> parameter</li><li>&quot;kmeans&quot;: use first kmeans (itself initialised with a &quot;grid&quot; strategy) to set the initial mixture centers [default]</li></ul><p>Note that currently &quot;random&quot; and &quot;shuffle&quot; initialisations are not supported in gmm-based algorithms.</p></li></ul><ul><li><p><code>maximum_iterations::Int64</code></p><p>Maximum number of iterations [def: <code>typemax(Int64)</code>, i.e. ∞]</p></li><li><p><code>rng::Random.AbstractRNG</code></p><p>Random Number Generator [deafult: <code>Random.GLOBAL_RNG</code>]</p></li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/sylvaticus/BetaML.jl/blob/be10a0b12b86ba83cbc3d611d51a1ce9b0f2f849/src/GMM/GMM_MLJ.jl#L62">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="BetaML.GMM.MultitargetGaussianMixtureRegressor" href="#BetaML.GMM.MultitargetGaussianMixtureRegressor"><code>BetaML.GMM.MultitargetGaussianMixtureRegressor</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">mutable struct MultitargetGaussianMixtureRegressor &lt;: MLJModelInterface.Deterministic</code></pre><p>A non-linear regressor derived from fitting the data on a probabilistic model (Gaussian Mixture Model). Relatively fast.</p><p>This is the multi-target version of the model. If you want to predict a single label (y), use the MLJ model <a href="GMM.html#BetaML.GMM.GaussianMixtureRegressor"><code>GaussianMixtureRegressor</code></a>.</p><p><strong>Hyperparameters:</strong></p><ul><li><p><code>n_classes::Int64</code></p><p>Number of mixtures (latent classes) to consider [def: 3]</p></li><li><p><code>initial_probmixtures::Vector{Float64}</code></p><p>Initial probabilities of the categorical distribution (n_classes x 1) [default: <code>[]</code>]</p></li><li><p><code>mixtures::Vector{AbstractMixture}</code></p><p>An array (of length <code>n_classes</code><code>) of the mixtures to employ (see the [</code>?GMM<code>](@ref GMM) module). Each mixture object can be provided with or without its parameters (e.g. mean and variance for the gaussian ones). Fully qualified mixtures are useful only if the</code>initialisation<em>strategy<code>parameter is  set to &quot;given&quot;</code> [def: `[DiagonalGaussian() for i in 1:n</em>classes]`]</p></li><li><p><code>tol::Float64</code></p><p>Tolerance to stop the algorithm [default: 10^(-6)]</p></li><li><p><code>minimum_variance::Float64</code></p><p>Minimum variance for the mixtures [default: 0.05]</p></li><li><p><code>minimum_covariance::Float64</code></p><p>Minimum covariance for the mixtures with full covariance matrix [default: 0]. This should be set different than minimum_variance (see notes).</p></li><li><p><code>initialisation_strategy::String</code></p><p>The computation method of the vector of the initial mixtures. One of the following:</p><ul><li>&quot;grid&quot;: using a grid approach</li><li>&quot;given&quot;: using the mixture provided in the fully qualified <code>mixtures</code> parameter</li><li>&quot;kmeans&quot;: use first kmeans (itself initialised with a &quot;grid&quot; strategy) to set the initial mixture centers [default]</li></ul><p>Note that currently &quot;random&quot; and &quot;shuffle&quot; initialisations are not supported in gmm-based algorithms.</p></li></ul><ul><li><p><code>maximum_iterations::Int64</code></p><p>Maximum number of iterations [def: <code>typemax(Int64)</code>, i.e. ∞]</p></li><li><p><code>rng::Random.AbstractRNG</code></p><p>Random Number Generator [deafult: <code>Random.GLOBAL_RNG</code>]</p></li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/sylvaticus/BetaML.jl/blob/be10a0b12b86ba83cbc3d611d51a1ce9b0f2f849/src/GMM/GMM_MLJ.jl#L113">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="BetaML.GMM.gmm-Tuple{Any, Any}" href="#BetaML.GMM.gmm-Tuple{Any, Any}"><code>BetaML.GMM.gmm</code></a> — <span class="docstring-category">Method</span></header><section><div><p>gmm(X,K;initial<em>probmixtures,mixtures,tol,verbosity,minimum</em>variance,minimum<em>covariance,initialisation</em>strategy)</p><p>Compute Expectation-Maximisation algorithm to identify K clusters of X data, i.e. employ a Generative Mixture Model as the underlying probabilistic model.</p><div class="admonition is-warning"><header class="admonition-header">Warning</header><div class="admonition-body"><p>This function is deprecated and will possibly be removed in BetaML 0.9. Use one of the various models that use GMM as backend instead.</p></div></div><p>X can contain missing values in some or all of its dimensions. In such case the learning is done only with the available data. Implemented in the log-domain for better numerical accuracy with many dimensions.</p><p><strong>Parameters:</strong></p><ul><li><code>X</code>  :           A (n x d) data to clusterise</li><li><code>K</code>  :           Number of cluster wanted</li><li><code>initial_probmixtures</code> :           Initial probabilities of the categorical distribution (K x 1) [default: <code>[]</code>]</li><li><code>mixtures</code>:      An array (of length K) of the mixture to employ (see notes) [def: <code>[DiagonalGaussian() for i in 1:K]</code>]</li><li><code>tol</code>:           Tolerance to stop the algorithm [default: 10^(-6)]</li><li><code>verbosity</code>:     A verbosity parameter regulating the information messages frequency [def: <code>STD</code>]</li><li><code>minimum_variance</code>:   Minimum variance for the mixtures [default: 0.05]</li><li><code>minimum_covariance</code>: Minimum covariance for the mixtures with full covariance matrix [default: 0]. This should be set different than minimum_variance (see notes).</li><li><code>initialisation_strategy</code>:  Mixture initialisation algorithm [def: <code>kmeans</code>]</li><li><code>maximum_iterations</code>:       Maximum number of iterations [def: <code>typemax(Int64)</code>, i.e. ∞]</li><li><code>rng</code>:           Random Number Generator (see <a href="Api.html#BetaML.Api.FIXEDSEED"><code>FIXEDSEED</code></a>) [deafult: <code>Random.GLOBAL_RNG</code>]</li></ul><p><strong>Returns:</strong></p><ul><li>A named touple of:</li><li><code>pₙₖ</code>:      Matrix of size (N x K) of the probabilities of each point i to belong to cluster j</li><li><code>pₖ</code>:       Probabilities of the categorical distribution (K x 1)</li><li><code>mixtures</code>: Vector (K x 1) of the estimated underlying distributions</li><li><code>ϵ</code>:        Vector of the discrepancy (matrix norm) between pⱼₓ and the lagged pⱼₓ at each iteration</li><li><code>lL</code>:       The log-likelihood (without considering the last mixture optimisation)</li><li><code>BIC</code>:      The Bayesian Information Criterion (lower is better)</li><li><code>AIC</code>:      The Akaike Information Criterion (lower is better)</li></ul><p><strong>Notes:</strong></p><ul><li>The mixtures currently implemented are <code>SphericalGaussian(μ,σ²)</code>,<code>DiagonalGaussian(μ,σ²)</code> and <code>FullGaussian(μ,σ²)</code></li><li>Reasonable choices for the minimum_variance/Covariance depends on the mixture. For example 0.25 seems a reasonable value for the SphericalGaussian, 0.05 seems better for the DiagonalGaussian, and FullGaussian seems to prefer either very low values of variance/covariance (e.g. <code>(0.05,0.05)</code> ) or very big but similar ones (e.g. <code>(100,100)</code> ).</li><li>For <code>initialisation_strategy</code>, look at the documentation of <code>init_mixtures!</code> for the mixture you want. The provided gaussian mixtures support <code>grid</code>, <code>kmeans</code> or <code>given</code>. <code>grid</code> is faster (expecially if X contains missing values), but <code>kmeans</code> often provides better results.</li></ul><p><strong>Resources:</strong></p><ul><li><a href="https://doi.org/10.1016/j.csda.2006.10.002">Paper describing gmm with missing values</a></li><li><a href="https://stackedit.io/viewer#!url=https://github.com/sylvaticus/MITx_6.86x/raw/master/Unit 04 - Unsupervised Learning/Unit 04 - Unsupervised Learning.md">Class notes from MITx 6.86x (Sec 15.9)</a></li><li><a href="https://www.r-craft.org/r-news/when-not-to-use-gaussian-mixture-model-gmm-clustering/">Limitations of gmm</a></li></ul><p><strong>Example:</strong></p><pre><code class="language-julia hljs">julia&gt; clusters = gmm([1 10.5;1.5 0; 1.8 8; 1.7 15; 3.2 40; 0 0; 3.3 38; 0 -2.3; 5.2 -2.4],3,verbosity=HIGH)</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/sylvaticus/BetaML.jl/blob/be10a0b12b86ba83cbc3d611d51a1ce9b0f2f849/src/GMM/GMM_clustering.jl#L36-L85">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="BetaML.GMM.init_mixtures!-Union{Tuple{T}, Tuple{Vector{T}, Any}} where T&lt;:BetaML.GMM.AbstractGaussian" href="#BetaML.GMM.init_mixtures!-Union{Tuple{T}, Tuple{Vector{T}, Any}} where T&lt;:BetaML.GMM.AbstractGaussian"><code>BetaML.GMM.init_mixtures!</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">init_mixtures!(mixtures::Array{T,1}, X; minimum_variance=0.25, minimum_covariance=0.0, initialisation_strategy=&quot;grid&quot;,rng=Random.GLOBAL_RNG)</code></pre><p>The parameter <code>initialisation_strategy</code> can be <code>grid</code>, <code>kmeans</code> or <code>given</code>:</p><ul><li><code>grid</code>: Uniformly cover the space observed by the data</li><li><code>kmeans</code>: Use the kmeans algorithm. If the data contains missing values, a first run of <code>predictMissing</code> is done under init=<code>grid</code> to impute the missing values just to allow the kmeans algorithm. Then the em algorithm is used with the output of kmean as init values.</li><li><code>given</code>: Leave the provided set of initial mixtures</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/sylvaticus/BetaML.jl/blob/be10a0b12b86ba83cbc3d611d51a1ce9b0f2f849/src/GMM/Mixtures.jl#L96-L105">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="BetaML.GMM.lpdf-Tuple{DiagonalGaussian, Any, Any}" href="#BetaML.GMM.lpdf-Tuple{DiagonalGaussian, Any, Any}"><code>BetaML.GMM.lpdf</code></a> — <span class="docstring-category">Method</span></header><section><div><p>lpdf(m::DiagonalGaussian,x,mask) - Log PDF of the mixture given the observation <code>x</code></p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/sylvaticus/BetaML.jl/blob/be10a0b12b86ba83cbc3d611d51a1ce9b0f2f849/src/GMM/Mixtures.jl#L197">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="BetaML.GMM.lpdf-Tuple{FullGaussian, Any, Any}" href="#BetaML.GMM.lpdf-Tuple{FullGaussian, Any, Any}"><code>BetaML.GMM.lpdf</code></a> — <span class="docstring-category">Method</span></header><section><div><p>lpdf(m::FullGaussian,x,mask) - Log PDF of the mixture given the observation <code>x</code></p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/sylvaticus/BetaML.jl/blob/be10a0b12b86ba83cbc3d611d51a1ce9b0f2f849/src/GMM/Mixtures.jl#L206">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="BetaML.GMM.lpdf-Tuple{SphericalGaussian, Any, Any}" href="#BetaML.GMM.lpdf-Tuple{SphericalGaussian, Any, Any}"><code>BetaML.GMM.lpdf</code></a> — <span class="docstring-category">Method</span></header><section><div><p>lpdf(m::SphericalGaussian,x,mask) - Log PDF of the mixture given the observation <code>x</code></p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/sylvaticus/BetaML.jl/blob/be10a0b12b86ba83cbc3d611d51a1ce9b0f2f849/src/GMM/Mixtures.jl#L187">source</a></section></article></article><nav class="docs-footer"><a class="docs-footer-prevpage" href="Clustering.html">« Clustering</a><a class="docs-footer-nextpage" href="Imputation.html">Imputation »</a><div class="flexbox-break"></div><p class="footer-message">Powered by <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> and the <a href="https://julialang.org/">Julia Programming Language</a>.</p></nav></div><div class="modal" id="documenter-settings"><div class="modal-background"></div><div class="modal-card"><header class="modal-card-head"><p class="modal-card-title">Settings</p><button class="delete"></button></header><section class="modal-card-body"><p><label class="label">Theme</label><div class="select"><select id="documenter-themepicker"><option value="documenter-light">documenter-light</option><option value="documenter-dark">documenter-dark</option></select></div></p><hr/><p>This document was generated with <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> version 0.27.23 on <span class="colophon-date" title="Tuesday 20 September 2022 14:53">Tuesday 20 September 2022</span>. Using Julia version 1.6.7.</p></section><footer class="modal-card-foot"></footer></div></div></div></body></html>
