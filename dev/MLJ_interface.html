<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>MLJ interface · BetaML.jl Documentation</title><meta name="title" content="MLJ interface · BetaML.jl Documentation"/><meta property="og:title" content="MLJ interface · BetaML.jl Documentation"/><meta property="twitter:title" content="MLJ interface · BetaML.jl Documentation"/><meta name="description" content="Documentation for BetaML.jl Documentation."/><meta property="og:description" content="Documentation for BetaML.jl Documentation."/><meta property="twitter:description" content="Documentation for BetaML.jl Documentation."/><script async src="https://www.googletagmanager.com/gtag/js?id=G-JYKX8QY5JW"></script><script>  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());
  gtag('config', 'G-JYKX8QY5JW', {'page_path': location.pathname + location.search + location.hash});
</script><script data-outdated-warner src="assets/warner.js"></script><link href="https://cdnjs.cloudflare.com/ajax/libs/lato-font/3.0.0/css/lato-font.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/juliamono/0.050/juliamono.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/fontawesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/solid.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/brands.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.16.8/katex.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL="."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" data-main="assets/documenter.js"></script><script src="search_index.js"></script><script src="siteinfo.js"></script><script src="../versions.js"></script><link class="docs-theme-link" rel="stylesheet" type="text/css" href="assets/themes/documenter-dark.css" data-theme-name="documenter-dark" data-theme-primary-dark/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="assets/themes/documenter-light.css" data-theme-name="documenter-light" data-theme-primary/><script src="assets/themeswap.js"></script></head><body><div id="documenter"><nav class="docs-sidebar"><div class="docs-package-name"><span class="docs-autofit"><a href="index.html">BetaML.jl Documentation</a></span></div><button class="docs-search-query input is-rounded is-small is-clickable my-2 mx-auto py-1 px-2" id="documenter-search-query">Search docs (Ctrl + /)</button><ul class="docs-menu"><li><a class="tocitem" href="index.html">Index</a></li><li><span class="tocitem">Tutorial</span><ul><li><a class="tocitem" href="tutorials/Betaml_tutorial_getting_started.html">Getting started</a></li><li><input class="collapse-toggle" id="menuitem-2-2" type="checkbox"/><label class="tocitem" for="menuitem-2-2"><span class="docs-label">Classification - cars</span><i class="docs-chevron"></i></label><ul class="collapsed"><li><a class="tocitem" href="tutorials/Classification - cars/betaml_tutorial_classification_cars.html">A classification task when labels are known - determining the country of origin of cars given the cars characteristics</a></li></ul></li><li><input class="collapse-toggle" id="menuitem-2-3" type="checkbox"/><label class="tocitem" for="menuitem-2-3"><span class="docs-label">Regression - bike sharing</span><i class="docs-chevron"></i></label><ul class="collapsed"><li><a class="tocitem" href="tutorials/Regression - bike sharing/betaml_tutorial_regression_sharingBikes.html">A regression task: the prediction of  bike  sharing demand</a></li></ul></li><li><input class="collapse-toggle" id="menuitem-2-4" type="checkbox"/><label class="tocitem" for="menuitem-2-4"><span class="docs-label">Clustering - Iris</span><i class="docs-chevron"></i></label><ul class="collapsed"><li><a class="tocitem" href="tutorials/Clustering - Iris/betaml_tutorial_cluster_iris.html">A clustering task: the prediction of  plant species from floreal measures (the iris dataset)</a></li></ul></li><li><input class="collapse-toggle" id="menuitem-2-5" type="checkbox"/><label class="tocitem" for="menuitem-2-5"><span class="docs-label">Multi-branch neural network</span><i class="docs-chevron"></i></label><ul class="collapsed"><li><a class="tocitem" href="tutorials/Multi-branch neural network/betaml_tutorial_multibranch_nn.html">A deep neural network with multi-branch architecture</a></li></ul></li></ul></li><li><span class="tocitem">API (Reference manual)</span><ul><li><input class="collapse-toggle" id="menuitem-3-1" type="checkbox"/><label class="tocitem" for="menuitem-3-1"><span class="docs-label">API V2 (current)</span><i class="docs-chevron"></i></label><ul class="collapsed"><li><a class="tocitem" href="Api_v2_user.html">Introduction for user</a></li><li><input class="collapse-toggle" id="menuitem-3-1-2" type="checkbox"/><label class="tocitem" for="menuitem-3-1-2"><span class="docs-label">For developers</span><i class="docs-chevron"></i></label><ul class="collapsed"><li><a class="tocitem" href="Api_v2_developer.html">API implementation</a></li><li><a class="tocitem" href="StyleGuide_templates.html">Style guide</a></li></ul></li><li><a class="tocitem" href="Api.html">The Api module</a></li></ul></li><li><a class="tocitem" href="Perceptron.html">Perceptron</a></li><li><a class="tocitem" href="Trees.html">Trees</a></li><li><a class="tocitem" href="Nn.html">Nn</a></li><li><a class="tocitem" href="Clustering.html">Clustering</a></li><li><a class="tocitem" href="GMM.html">GMM</a></li><li><a class="tocitem" href="Imputation.html">Imputation</a></li><li><a class="tocitem" href="Utils.html">Utils</a></li><li class="is-active"><a class="tocitem" href="MLJ_interface.html">MLJ interface</a><ul class="internal"><li><a class="tocitem" href="#Models-available-through-MLJ"><span>Models available through MLJ</span></a></li><li><a class="tocitem" href="#Detailed-models-documentation"><span>Detailed models documentation</span></a></li></ul></li></ul></li></ul><div class="docs-version-selector field has-addons"><div class="control"><span class="docs-label button is-static is-size-7">Version</span></div><div class="docs-selector control is-expanded"><div class="select is-fullwidth is-size-7"><select id="documenter-version-selector"></select></div></div></div></nav><div class="docs-main"><header class="docs-navbar"><a class="docs-sidebar-button docs-navbar-link fa-solid fa-bars is-hidden-desktop" id="documenter-sidebar-button" href="#"></a><nav class="breadcrumb"><ul class="is-hidden-mobile"><li><a class="is-disabled">API (Reference manual)</a></li><li class="is-active"><a href="MLJ_interface.html">MLJ interface</a></li></ul><ul class="is-hidden-tablet"><li class="is-active"><a href="MLJ_interface.html">MLJ interface</a></li></ul></nav><div class="docs-right"><a class="docs-navbar-link" href="https://github.com/sylvaticus/BetaML.jl" title="View the repository on GitHub"><span class="docs-icon fa-brands"></span><span class="docs-label is-hidden-touch">GitHub</span></a><a class="docs-navbar-link" href="https://github.com/sylvaticus/BetaML.jl/blob/master/docs/src/MLJ_interface.md" title="Edit source on GitHub"><span class="docs-icon fa-solid"></span></a><a class="docs-settings-button docs-navbar-link fa-solid fa-gear" id="documenter-settings-button" href="#" title="Settings"></a><a class="docs-article-toggle-button fa-solid fa-chevron-up" id="documenter-article-toggle-button" href="javascript:;" title="Collapse all docstrings"></a></div></header><article class="content" id="documenter-page"><h1 id="bmlj_module"><a class="docs-heading-anchor" href="#bmlj_module">The MLJ interface to BetaML Models</a><a id="bmlj_module-1"></a><a class="docs-heading-anchor-permalink" href="#bmlj_module" title="Permalink"></a></h1><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="BetaML.Bmlj" href="#BetaML.Bmlj"><code>BetaML.Bmlj</code></a> — <span class="docstring-category">Module</span></header><section><div><p><strong>MLJ interface for BetaML models</strong></p><p>In this module we define the interface of several BetaML models. They can be used using the <a href="https://github.com/alan-turing-institute/MLJ.jl">MLJ framework</a>.</p><p>Note that MLJ models (whose name could be the same as the underlying BetaML model) are not exported. You can access them with <code>BetaML.Bmlj.ModelXYZ</code>.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/sylvaticus/BetaML.jl/blob/f8f3bf703fd327fbebd7d330505b6b6516418385/src/Bmlj/Bmlj.jl#L4-L11">source</a></section></article><h2 id="Models-available-through-MLJ"><a class="docs-heading-anchor" href="#Models-available-through-MLJ">Models available through MLJ</a><a id="Models-available-through-MLJ-1"></a><a class="docs-heading-anchor-permalink" href="#Models-available-through-MLJ" title="Permalink"></a></h2><ul><li><a href="MLJ_interface.html#BetaML.Bmlj.mljverbosity_to_betaml_verbosity-Tuple{Integer}"><code>BetaML.Bmlj.mljverbosity_to_betaml_verbosity</code></a></li><li><a href="MLJ_interface.html#BetaML.Bmlj.AutoEncoder"><code>BetaML.Bmlj.AutoEncoder</code></a></li><li><a href="MLJ_interface.html#BetaML.Bmlj.DecisionTreeClassifier"><code>BetaML.Bmlj.DecisionTreeClassifier</code></a></li><li><a href="MLJ_interface.html#BetaML.Bmlj.DecisionTreeRegressor"><code>BetaML.Bmlj.DecisionTreeRegressor</code></a></li><li><a href="MLJ_interface.html#BetaML.Bmlj.GaussianMixtureClusterer"><code>BetaML.Bmlj.GaussianMixtureClusterer</code></a></li><li><a href="MLJ_interface.html#BetaML.Bmlj.GaussianMixtureImputer"><code>BetaML.Bmlj.GaussianMixtureImputer</code></a></li><li><a href="MLJ_interface.html#BetaML.Bmlj.GaussianMixtureRegressor"><code>BetaML.Bmlj.GaussianMixtureRegressor</code></a></li><li><a href="MLJ_interface.html#BetaML.Bmlj.GeneralImputer"><code>BetaML.Bmlj.GeneralImputer</code></a></li><li><a href="MLJ_interface.html#BetaML.Bmlj.KMeansClusterer"><code>BetaML.Bmlj.KMeansClusterer</code></a></li><li><a href="MLJ_interface.html#BetaML.Bmlj.KMedoidsClusterer"><code>BetaML.Bmlj.KMedoidsClusterer</code></a></li><li><a href="MLJ_interface.html#BetaML.Bmlj.KernelPerceptronClassifier"><code>BetaML.Bmlj.KernelPerceptronClassifier</code></a></li><li><a href="MLJ_interface.html#BetaML.Bmlj.MultitargetGaussianMixtureRegressor"><code>BetaML.Bmlj.MultitargetGaussianMixtureRegressor</code></a></li><li><a href="MLJ_interface.html#BetaML.Bmlj.MultitargetNeuralNetworkRegressor"><code>BetaML.Bmlj.MultitargetNeuralNetworkRegressor</code></a></li><li><a href="MLJ_interface.html#BetaML.Bmlj.NeuralNetworkClassifier"><code>BetaML.Bmlj.NeuralNetworkClassifier</code></a></li><li><a href="MLJ_interface.html#BetaML.Bmlj.NeuralNetworkRegressor"><code>BetaML.Bmlj.NeuralNetworkRegressor</code></a></li><li><a href="MLJ_interface.html#BetaML.Bmlj.PegasosClassifier"><code>BetaML.Bmlj.PegasosClassifier</code></a></li><li><a href="MLJ_interface.html#BetaML.Bmlj.PerceptronClassifier"><code>BetaML.Bmlj.PerceptronClassifier</code></a></li><li><a href="MLJ_interface.html#BetaML.Bmlj.RandomForestClassifier"><code>BetaML.Bmlj.RandomForestClassifier</code></a></li><li><a href="MLJ_interface.html#BetaML.Bmlj.RandomForestImputer"><code>BetaML.Bmlj.RandomForestImputer</code></a></li><li><a href="MLJ_interface.html#BetaML.Bmlj.RandomForestRegressor"><code>BetaML.Bmlj.RandomForestRegressor</code></a></li><li><a href="MLJ_interface.html#BetaML.Bmlj.SimpleImputer"><code>BetaML.Bmlj.SimpleImputer</code></a></li></ul><h2 id="Detailed-models-documentation"><a class="docs-heading-anchor" href="#Detailed-models-documentation">Detailed models documentation</a><a id="Detailed-models-documentation-1"></a><a class="docs-heading-anchor-permalink" href="#Detailed-models-documentation" title="Permalink"></a></h2><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="BetaML.Bmlj.AutoEncoder" href="#BetaML.Bmlj.AutoEncoder"><code>BetaML.Bmlj.AutoEncoder</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">mutable struct AutoEncoder &lt;: MLJModelInterface.Unsupervised</code></pre><p>A ready-to use AutoEncoder, from the Beta Machine Learning Toolkit (BetaML) for ecoding and decoding of data using neural networks</p><p><strong>Parameters:</strong></p><ul><li><p><code>e_layers</code>: The layers (vector of <code>AbstractLayer</code>s) responsable of the encoding of the data [def: <code>nothing</code>, i.e. two dense layers with the inner one of <code>layers_size</code>]. See <code>subtypes(BetaML.AbstractLayer)</code> for supported layers</p></li><li><p><code>d_layers</code>: The layers (vector of <code>AbstractLayer</code>s) responsable of the decoding of the data [def: <code>nothing</code>, i.e. two dense layers with the inner one of <code>layers_size</code>]. See <code>subtypes(BetaML.AbstractLayer)</code> for supported layers</p></li><li><p><code>encoded_size</code>: The number of neurons (i.e. dimensions) of the encoded data. If the value is a float it is consiered a percentual (to be rounded) of the dimensionality of the data [def: <code>0.33</code>]</p></li><li><p><code>layers_size</code>: Inner layer dimension (i.e. number of neurons). If the value is a float it is considered a percentual (to be rounded) of the dimensionality of the data [def: <code>nothing</code> that applies a specific heuristic]. Consider that the underlying neural network is trying to predict multiple values at the same times. Normally this requires many more neurons than a scalar prediction. If <code>e_layers</code> or <code>d_layers</code> are specified, this parameter is ignored for the respective part.</p></li><li><p><code>loss</code>: Loss (cost) function [def: <code>BetaML.squared_cost</code>]. Should always assume y and ŷ as (n x d) matrices.</p><div class="admonition is-warning"><header class="admonition-header">Warning</header><div class="admonition-body"><p>If you change the parameter <code>loss</code>, you need to either provide its derivative on the parameter <code>dloss</code> or use autodiff with <code>dloss=nothing</code>.</p></div></div></li></ul><ul><li><p><code>dloss</code>: Derivative of the loss function [def: <code>BetaML.dsquared_cost</code> if <code>loss==squared_cost</code>, <code>nothing</code> otherwise, i.e. use the derivative of the squared cost or autodiff]</p></li><li><p><code>epochs</code>: Number of epochs, i.e. passages trough the whole training sample [def: <code>200</code>]</p></li><li><p><code>batch_size</code>: Size of each individual batch [def: <code>8</code>]</p></li><li><p><code>opt_alg</code>: The optimisation algorithm to update the gradient at each batch [def: <code>BetaML.ADAM()</code>] See <code>subtypes(BetaML.OptimisationAlgorithm)</code> for supported optimizers</p></li><li><p><code>shuffle</code>: Whether to randomly shuffle the data at each iteration (epoch) [def: <code>true</code>]</p></li><li><p><code>tunemethod</code>: The method - and its parameters - to employ for hyperparameters autotuning. See <a href="Utils.html#BetaML.Utils.SuccessiveHalvingSearch"><code>SuccessiveHalvingSearch</code></a> for the default method. To implement automatic hyperparameter tuning during the (first) <code>fit!</code> call simply set <code>autotune=true</code> and eventually change the default <code>tunemethod</code> options (including the parameter ranges, the resources to employ and the loss function to adopt).</p></li></ul><ul><li><p><code>descr</code>: An optional title and/or description for this model</p></li><li><p><code>rng</code>: Random Number Generator (see <a href="Api.html#BetaML.Api.FIXEDSEED"><code>FIXEDSEED</code></a>) [deafult: <code>Random.GLOBAL_RNG</code>]</p></li></ul><p><strong>Notes:</strong></p><ul><li>data must be numerical</li><li>use <code>transform</code> to obtain the encoded data, and <code>inverse_trasnform</code> to decode to the original data</li></ul><p><strong>Example:</strong></p><pre><code class="language-julia hljs">julia&gt; using MLJ

julia&gt; X, y        = @load_iris;

julia&gt; modelType   = @load AutoEncoder pkg = &quot;BetaML&quot; verbosity=0;

julia&gt; model       = modelType(encoded_size=2,layers_size=10);

julia&gt; mach        = machine(model, X)
untrained Machine; caches model-specific representations of data
  model: AutoEncoder(e_layers = nothing, …)
  args: 
    1:	Source @334 ⏎ Table{AbstractVector{Continuous}}

julia&gt; fit!(mach,verbosity=2)
[ Info: Training machine(AutoEncoder(e_layers = nothing, …), …).
***
*** Training  for 200 epochs with algorithm BetaML.Nn.ADAM.
Training.. 	 avg loss on epoch 1 (1): 	 35.48243542158747
Training.. 	 avg loss on epoch 20 (20): 	 0.07528042222678126
Training.. 	 avg loss on epoch 40 (40): 	 0.06293071729378613
Training.. 	 avg loss on epoch 60 (60): 	 0.057035588828991145
Training.. 	 avg loss on epoch 80 (80): 	 0.056313167754822875
Training.. 	 avg loss on epoch 100 (100): 	 0.055521461091809436
Training the Neural Network...  52%|██████████████████████████████████████                                   |  ETA: 0:00:01Training.. 	 avg loss on epoch 120 (120): 	 0.06015206472927942
Training.. 	 avg loss on epoch 140 (140): 	 0.05536835903285201
Training.. 	 avg loss on epoch 160 (160): 	 0.05877560142428245
Training.. 	 avg loss on epoch 180 (180): 	 0.05476302769966953
Training.. 	 avg loss on epoch 200 (200): 	 0.049240864053557445
Training the Neural Network... 100%|█████████████████████████████████████████████████████████████████████████| Time: 0:00:01
Training of 200 epoch completed. Final epoch error: 0.049240864053557445.
trained Machine; caches model-specific representations of data
  model: AutoEncoder(e_layers = nothing, …)
  args: 
    1:	Source @334 ⏎ Table{AbstractVector{Continuous}}


julia&gt; X_latent    = transform(mach, X)
150×2 Matrix{Float64}:
 7.01701   -2.77285
 6.50615   -2.9279
 6.5233    -2.60754
 ⋮        
 6.70196  -10.6059
 6.46369  -11.1117
 6.20212  -10.1323

julia&gt; X_recovered = inverse_transform(mach,X_latent)
150×4 Matrix{Float64}:
 5.04973  3.55838  1.43251  0.242215
 4.73689  3.19985  1.44085  0.295257
 4.65128  3.25308  1.30187  0.244354
 ⋮                          
 6.50077  2.93602  5.3303   1.87647
 6.38639  2.83864  5.54395  2.04117
 6.01595  2.67659  5.03669  1.83234

julia&gt; BetaML.relative_mean_error(MLJ.matrix(X),X_recovered)
0.03387721261716176

</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/sylvaticus/BetaML.jl/blob/f8f3bf703fd327fbebd7d330505b6b6516418385/src/Bmlj/Utils_mlj.jl#L19">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="BetaML.Bmlj.DecisionTreeClassifier" href="#BetaML.Bmlj.DecisionTreeClassifier"><code>BetaML.Bmlj.DecisionTreeClassifier</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">mutable struct DecisionTreeClassifier &lt;: MLJModelInterface.Probabilistic</code></pre><p>A simple Decision Tree model for classification with support for Missing data, from the Beta Machine Learning Toolkit (BetaML).</p><p><strong>Hyperparameters:</strong></p><ul><li><p><code>max_depth::Int64</code>: The maximum depth the tree is allowed to reach. When this is reached the node is forced to become a leaf [def: <code>0</code>, i.e. no limits]</p></li><li><p><code>min_gain::Float64</code>: The minimum information gain to allow for a node&#39;s partition [def: <code>0</code>]</p></li><li><p><code>min_records::Int64</code>: The minimum number of records a node must holds to consider for a partition of it [def: <code>2</code>]</p></li><li><p><code>max_features::Int64</code>: The maximum number of (random) features to consider at each partitioning [def: <code>0</code>, i.e. look at all features]</p></li><li><p><code>splitting_criterion::Function</code>: This is the name of the function to be used to compute the information gain of a specific partition. This is done by measuring the difference betwwen the &quot;impurity&quot; of the labels of the parent node with those of the two child nodes, weighted by the respective number of items. [def: <code>gini</code>]. Either <code>gini</code>, <code>entropy</code> or a custom function. It can also be an anonymous function.</p></li><li><p><code>rng::Random.AbstractRNG</code>: A Random Number Generator to be used in stochastic parts of the code [deafult: <code>Random.GLOBAL_RNG</code>]</p></li></ul><p><strong>Example:</strong></p><pre><code class="language-julia hljs">julia&gt; using MLJ

julia&gt; X, y        = @load_iris;

julia&gt; modelType   = @load DecisionTreeClassifier pkg = &quot;BetaML&quot; verbosity=0
BetaML.Trees.DecisionTreeClassifier

julia&gt; model       = modelType()
DecisionTreeClassifier(
  max_depth = 0, 
  min_gain = 0.0, 
  min_records = 2, 
  max_features = 0, 
  splitting_criterion = BetaML.Utils.gini, 
  rng = Random._GLOBAL_RNG())

julia&gt; mach        = machine(model, X, y);

julia&gt; fit!(mach);
[ Info: Training machine(DecisionTreeClassifier(max_depth = 0, …), …).

julia&gt; cat_est    = predict(mach, X)
150-element CategoricalDistributions.UnivariateFiniteVector{Multiclass{3}, String, UInt32, Float64}:
 UnivariateFinite{Multiclass{3}}(setosa=&gt;1.0, versicolor=&gt;0.0, virginica=&gt;0.0)
 UnivariateFinite{Multiclass{3}}(setosa=&gt;1.0, versicolor=&gt;0.0, virginica=&gt;0.0)
 ⋮
 UnivariateFinite{Multiclass{3}}(setosa=&gt;0.0, versicolor=&gt;0.0, virginica=&gt;1.0)
 UnivariateFinite{Multiclass{3}}(setosa=&gt;0.0, versicolor=&gt;0.0, virginica=&gt;1.0)
 UnivariateFinite{Multiclass{3}}(setosa=&gt;0.0, versicolor=&gt;0.0, virginica=&gt;1.0)</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/sylvaticus/BetaML.jl/blob/f8f3bf703fd327fbebd7d330505b6b6516418385/src/Bmlj/Trees_mlj.jl#L80">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="BetaML.Bmlj.DecisionTreeRegressor" href="#BetaML.Bmlj.DecisionTreeRegressor"><code>BetaML.Bmlj.DecisionTreeRegressor</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">mutable struct DecisionTreeRegressor &lt;: MLJModelInterface.Deterministic</code></pre><p>A simple Decision Tree model for regression with support for Missing data, from the Beta Machine Learning Toolkit (BetaML).</p><p><strong>Hyperparameters:</strong></p><ul><li><p><code>max_depth::Int64</code>: The maximum depth the tree is allowed to reach. When this is reached the node is forced to become a leaf [def: <code>0</code>, i.e. no limits]</p></li><li><p><code>min_gain::Float64</code>: The minimum information gain to allow for a node&#39;s partition [def: <code>0</code>]</p></li><li><p><code>min_records::Int64</code>: The minimum number of records a node must holds to consider for a partition of it [def: <code>2</code>]</p></li><li><p><code>max_features::Int64</code>: The maximum number of (random) features to consider at each partitioning [def: <code>0</code>, i.e. look at all features]</p></li><li><p><code>splitting_criterion::Function</code>: This is the name of the function to be used to compute the information gain of a specific partition. This is done by measuring the difference betwwen the &quot;impurity&quot; of the labels of the parent node with those of the two child nodes, weighted by the respective number of items. [def: <code>variance</code>]. Either <code>variance</code> or a custom function. It can also be an anonymous function.</p></li><li><p><code>rng::Random.AbstractRNG</code>: A Random Number Generator to be used in stochastic parts of the code [deafult: <code>Random.GLOBAL_RNG</code>]</p></li></ul><p><strong>Example:</strong></p><pre><code class="language-julia hljs">julia&gt; using MLJ

julia&gt; X, y        = @load_boston;

julia&gt; modelType   = @load DecisionTreeRegressor pkg = &quot;BetaML&quot; verbosity=0
BetaML.Trees.DecisionTreeRegressor

julia&gt; model       = modelType()
DecisionTreeRegressor(
  max_depth = 0, 
  min_gain = 0.0, 
  min_records = 2, 
  max_features = 0, 
  splitting_criterion = BetaML.Utils.variance, 
  rng = Random._GLOBAL_RNG())

julia&gt; mach        = machine(model, X, y);

julia&gt; fit!(mach);
[ Info: Training machine(DecisionTreeRegressor(max_depth = 0, …), …).

julia&gt; ŷ           = predict(mach, X);

julia&gt; hcat(y,ŷ)
506×2 Matrix{Float64}:
 24.0  26.35
 21.6  21.6
 34.7  34.8
  ⋮    
 23.9  23.75
 22.0  22.2
 11.9  13.2</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/sylvaticus/BetaML.jl/blob/f8f3bf703fd327fbebd7d330505b6b6516418385/src/Bmlj/Trees_mlj.jl#L11">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="BetaML.Bmlj.GaussianMixtureClusterer" href="#BetaML.Bmlj.GaussianMixtureClusterer"><code>BetaML.Bmlj.GaussianMixtureClusterer</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">mutable struct GaussianMixtureClusterer &lt;: MLJModelInterface.Unsupervised</code></pre><p>A Expectation-Maximisation clustering algorithm with customisable mixtures, from the Beta Machine Learning Toolkit (BetaML).</p><p><strong>Hyperparameters:</strong></p><ul><li><p><code>n_classes::Int64</code>: Number of mixtures (latent classes) to consider [def: 3]</p></li><li><p><code>initial_probmixtures::AbstractVector{Float64}</code>: Initial probabilities of the categorical distribution (n_classes x 1) [default: <code>[]</code>]</p></li><li><p><code>mixtures::Union{Type, Vector{var&quot;#s1264&quot;} where var&quot;#s1264&quot;&lt;:AbstractMixture}</code>: An array (of length <code>n_classes</code>) of the mixtures to employ (see the <a href="GMM.html#BetaML.GMM"><code>?GMM</code></a> module). Each mixture object can be provided with or without its parameters (e.g. mean and variance for the gaussian ones). Fully qualified mixtures are useful only if the <code>initialisation_strategy</code> parameter is set to &quot;gived&quot;. This parameter can also be given symply in term of a <em>type</em>. In this case it is automatically extended to a vector of <code>n_classes</code> mixtures of the specified type. Note that mixing of different mixture types is not currently supported. [def: <code>[DiagonalGaussian() for i in 1:n_classes]</code>]</p></li><li><p><code>tol::Float64</code>: Tolerance to stop the algorithm [default: 10^(-6)]</p></li><li><p><code>minimum_variance::Float64</code>: Minimum variance for the mixtures [default: 0.05]</p></li><li><p><code>minimum_covariance::Float64</code>: Minimum covariance for the mixtures with full covariance matrix [default: 0]. This should be set different than minimum_variance (see notes).</p></li><li><p><code>initialisation_strategy::String</code>: The computation method of the vector of the initial mixtures. One of the following:</p><ul><li>&quot;grid&quot;: using a grid approach</li><li>&quot;given&quot;: using the mixture provided in the fully qualified <code>mixtures</code> parameter</li><li>&quot;kmeans&quot;: use first kmeans (itself initialised with a &quot;grid&quot; strategy) to set the initial mixture centers [default]</li></ul><p>Note that currently &quot;random&quot; and &quot;shuffle&quot; initialisations are not supported in gmm-based algorithms.</p></li><li><p><code>maximum_iterations::Int64</code>: Maximum number of iterations [def: <code>typemax(Int64)</code>, i.e. ∞]</p></li><li><p><code>rng::Random.AbstractRNG</code>: Random Number Generator [deafult: <code>Random.GLOBAL_RNG</code>]</p></li></ul><p><strong>Example:</strong></p><pre><code class="language-julia hljs">
julia&gt; using MLJ

julia&gt; X, y        = @load_iris;

julia&gt; modelType   = @load GaussianMixtureClusterer pkg = &quot;BetaML&quot; verbosity=0
BetaML.GMM.GaussianMixtureClusterer

julia&gt; model       = modelType()
GaussianMixtureClusterer(
  n_classes = 3, 
  initial_probmixtures = Float64[], 
  mixtures = BetaML.GMM.DiagonalGaussian{Float64}[BetaML.GMM.DiagonalGaussian{Float64}(nothing, nothing), BetaML.GMM.DiagonalGaussian{Float64}(nothing, nothing), BetaML.GMM.DiagonalGaussian{Float64}(nothing, nothing)], 
  tol = 1.0e-6, 
  minimum_variance = 0.05, 
  minimum_covariance = 0.0, 
  initialisation_strategy = &quot;kmeans&quot;, 
  maximum_iterations = 9223372036854775807, 
  rng = Random._GLOBAL_RNG())

julia&gt; mach        = machine(model, X);

julia&gt; fit!(mach);
[ Info: Training machine(GaussianMixtureClusterer(n_classes = 3, …), …).
Iter. 1:        Var. of the post  10.800150114964184      Log-likelihood -650.0186451891216

julia&gt; classes_est = predict(mach, X)
150-element CategoricalDistributions.UnivariateFiniteVector{Multiclass{3}, Int64, UInt32, Float64}:
 UnivariateFinite{Multiclass{3}}(1=&gt;1.0, 2=&gt;4.17e-15, 3=&gt;2.1900000000000003e-31)
 UnivariateFinite{Multiclass{3}}(1=&gt;1.0, 2=&gt;1.25e-13, 3=&gt;5.87e-31)
 UnivariateFinite{Multiclass{3}}(1=&gt;1.0, 2=&gt;4.5e-15, 3=&gt;1.55e-32)
 UnivariateFinite{Multiclass{3}}(1=&gt;1.0, 2=&gt;6.93e-14, 3=&gt;3.37e-31)
 ⋮
 UnivariateFinite{Multiclass{3}}(1=&gt;5.39e-25, 2=&gt;0.0167, 3=&gt;0.983)
 UnivariateFinite{Multiclass{3}}(1=&gt;7.5e-29, 2=&gt;0.000106, 3=&gt;1.0)
 UnivariateFinite{Multiclass{3}}(1=&gt;1.6e-20, 2=&gt;0.594, 3=&gt;0.406)</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/sylvaticus/BetaML.jl/blob/f8f3bf703fd327fbebd7d330505b6b6516418385/src/Bmlj/GMM_mlj.jl#L10">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="BetaML.Bmlj.GaussianMixtureImputer" href="#BetaML.Bmlj.GaussianMixtureImputer"><code>BetaML.Bmlj.GaussianMixtureImputer</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">mutable struct GaussianMixtureImputer &lt;: MLJModelInterface.Unsupervised</code></pre><p>Impute missing values using a probabilistic approach (Gaussian Mixture Models) fitted using the Expectation-Maximisation algorithm, from the Beta Machine Learning Toolkit (BetaML).</p><p><strong>Hyperparameters:</strong></p><ul><li><p><code>n_classes::Int64</code>: Number of mixtures (latent classes) to consider [def: 3]</p></li><li><p><code>initial_probmixtures::Vector{Float64}</code>: Initial probabilities of the categorical distribution (n_classes x 1) [default: <code>[]</code>]</p></li><li><p><code>mixtures::Union{Type, Vector{var&quot;#s1264&quot;} where var&quot;#s1264&quot;&lt;:AbstractMixture}</code>: An array (of length <code>n_classes</code><code>) of the mixtures to employ (see the [</code>?GMM<code>](@ref GMM) module in BetaML). Each mixture object can be provided with or without its parameters (e.g. mean and variance for the gaussian ones). Fully qualified mixtures are useful only if the</code>initialisation<em>strategy<code>parameter is  set to &quot;gived&quot;</code> This parameter can also be given symply in term of a _type</em>. In this case it is automatically extended to a vector of <code>n_classes</code><code>mixtures of the specified type. Note that mixing of different mixture types is not currently supported and that currently implemented mixtures are</code>SphericalGaussian<code>,</code>DiagonalGaussian<code>and</code>FullGaussian<code>. [def:</code>DiagonalGaussian`]</p></li><li><p><code>tol::Float64</code>: Tolerance to stop the algorithm [default: 10^(-6)]</p></li><li><p><code>minimum_variance::Float64</code>: Minimum variance for the mixtures [default: 0.05]</p></li><li><p><code>minimum_covariance::Float64</code>: Minimum covariance for the mixtures with full covariance matrix [default: 0]. This should be set different than minimum_variance.</p></li><li><p><code>initialisation_strategy::String</code>: The computation method of the vector of the initial mixtures. One of the following:</p><ul><li>&quot;grid&quot;: using a grid approach</li><li>&quot;given&quot;: using the mixture provided in the fully qualified <code>mixtures</code> parameter</li><li>&quot;kmeans&quot;: use first kmeans (itself initialised with a &quot;grid&quot; strategy) to set the initial mixture centers [default]</li></ul><p>Note that currently &quot;random&quot; and &quot;shuffle&quot; initialisations are not supported in gmm-based algorithms.</p></li></ul><ul><li><code>rng::Random.AbstractRNG</code>: A Random Number Generator to be used in stochastic parts of the code [deafult: <code>Random.GLOBAL_RNG</code>]</li></ul><p><strong>Example :</strong></p><pre><code class="language-julia hljs">julia&gt; using MLJ

julia&gt; X = [1 10.5;1.5 missing; 1.8 8; 1.7 15; 3.2 40; missing missing; 3.3 38; missing -2.3; 5.2 -2.4] |&gt; table ;

julia&gt; modelType   = @load GaussianMixtureImputer  pkg = &quot;BetaML&quot; verbosity=0
BetaML.Imputation.GaussianMixtureImputer

julia&gt; model     = modelType(initialisation_strategy=&quot;grid&quot;)
GaussianMixtureImputer(
  n_classes = 3, 
  initial_probmixtures = Float64[], 
  mixtures = BetaML.GMM.DiagonalGaussian{Float64}[BetaML.GMM.DiagonalGaussian{Float64}(nothing, nothing), BetaML.GMM.DiagonalGaussian{Float64}(nothing, nothing), BetaML.GMM.DiagonalGaussian{Float64}(nothing, nothing)], 
  tol = 1.0e-6, 
  minimum_variance = 0.05, 
  minimum_covariance = 0.0, 
  initialisation_strategy = &quot;grid&quot;, 
  rng = Random._GLOBAL_RNG())

julia&gt; mach      = machine(model, X);

julia&gt; fit!(mach);
[ Info: Training machine(GaussianMixtureImputer(n_classes = 3, …), …).
Iter. 1:        Var. of the post  2.0225921341714286      Log-likelihood -42.96100103213314

julia&gt; X_full       = transform(mach) |&gt; MLJ.matrix
9×2 Matrix{Float64}:
 1.0      10.5
 1.5      14.7366
 1.8       8.0
 1.7      15.0
 3.2      40.0
 2.51842  15.1747
 3.3      38.0
 2.47412  -2.3
 5.2      -2.4</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/sylvaticus/BetaML.jl/blob/f8f3bf703fd327fbebd7d330505b6b6516418385/src/Bmlj/Imputation_mlj.jl#L58">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="BetaML.Bmlj.GaussianMixtureRegressor" href="#BetaML.Bmlj.GaussianMixtureRegressor"><code>BetaML.Bmlj.GaussianMixtureRegressor</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">mutable struct GaussianMixtureRegressor &lt;: MLJModelInterface.Deterministic</code></pre><p>A non-linear regressor derived from fitting the data on a probabilistic model (Gaussian Mixture Model). Relatively fast but generally not very precise, except for data with a structure matching the chosen underlying mixture.</p><p>This is the single-target version of the model. If you want to predict several labels (y) at once, use the MLJ model <a href="MLJ_interface.html#BetaML.Bmlj.MultitargetGaussianMixtureRegressor"><code>MultitargetGaussianMixtureRegressor</code></a>.</p><p><strong>Hyperparameters:</strong></p><ul><li><p><code>n_classes::Int64</code>: Number of mixtures (latent classes) to consider [def: 3]</p></li><li><p><code>initial_probmixtures::Vector{Float64}</code>: Initial probabilities of the categorical distribution (n_classes x 1) [default: <code>[]</code>]</p></li><li><p><code>mixtures::Union{Type, Vector{var&quot;#s1264&quot;} where var&quot;#s1264&quot;&lt;:AbstractMixture}</code>: An array (of length <code>n_classes</code><code>) of the mixtures to employ (see the [</code>?GMM<code>](@ref GMM) module). Each mixture object can be provided with or without its parameters (e.g. mean and variance for the gaussian ones). Fully qualified mixtures are useful only if the</code>initialisation<em>strategy<code>parameter is  set to &quot;gived&quot;</code> This parameter can also be given symply in term of a _type</em>. In this case it is automatically extended to a vector of <code>n_classes</code><code>mixtures of the specified type. Note that mixing of different mixture types is not currently supported. [def:</code>[DiagonalGaussian() for i in 1:n_classes]`]</p></li><li><p><code>tol::Float64</code>: Tolerance to stop the algorithm [default: 10^(-6)]</p></li><li><p><code>minimum_variance::Float64</code>: Minimum variance for the mixtures [default: 0.05]</p></li><li><p><code>minimum_covariance::Float64</code>: Minimum covariance for the mixtures with full covariance matrix [default: 0]. This should be set different than minimum_variance (see notes).</p></li><li><p><code>initialisation_strategy::String</code>: The computation method of the vector of the initial mixtures. One of the following:</p><ul><li>&quot;grid&quot;: using a grid approach</li><li>&quot;given&quot;: using the mixture provided in the fully qualified <code>mixtures</code> parameter</li><li>&quot;kmeans&quot;: use first kmeans (itself initialised with a &quot;grid&quot; strategy) to set the initial mixture centers [default]</li></ul><p>Note that currently &quot;random&quot; and &quot;shuffle&quot; initialisations are not supported in gmm-based algorithms.</p></li></ul><ul><li><p><code>maximum_iterations::Int64</code>: Maximum number of iterations [def: <code>typemax(Int64)</code>, i.e. ∞]</p></li><li><p><code>rng::Random.AbstractRNG</code>: Random Number Generator [deafult: <code>Random.GLOBAL_RNG</code>]</p></li></ul><p><strong>Example:</strong></p><pre><code class="language-julia hljs">julia&gt; using MLJ

julia&gt; X, y      = @load_boston;

julia&gt; modelType = @load GaussianMixtureRegressor pkg = &quot;BetaML&quot; verbosity=0
BetaML.GMM.GaussianMixtureRegressor

julia&gt; model     = modelType()
GaussianMixtureRegressor(
  n_classes = 3, 
  initial_probmixtures = Float64[], 
  mixtures = BetaML.GMM.DiagonalGaussian{Float64}[BetaML.GMM.DiagonalGaussian{Float64}(nothing, nothing), BetaML.GMM.DiagonalGaussian{Float64}(nothing, nothing), BetaML.GMM.DiagonalGaussian{Float64}(nothing, nothing)], 
  tol = 1.0e-6, 
  minimum_variance = 0.05, 
  minimum_covariance = 0.0, 
  initialisation_strategy = &quot;kmeans&quot;, 
  maximum_iterations = 9223372036854775807, 
  rng = Random._GLOBAL_RNG())

julia&gt; mach      = machine(model, X, y);

julia&gt; fit!(mach);
[ Info: Training machine(GaussianMixtureRegressor(n_classes = 3, …), …).
Iter. 1:        Var. of the post  21.74887448784976       Log-likelihood -21687.09917379566

julia&gt; ŷ         = predict(mach, X)
506-element Vector{Float64}:
 24.703442835305577
 24.70344283512716
  ⋮
 17.172486989759676
 17.172486989759644</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/sylvaticus/BetaML.jl/blob/f8f3bf703fd327fbebd7d330505b6b6516418385/src/Bmlj/GMM_mlj.jl#L106">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="BetaML.Bmlj.GeneralImputer" href="#BetaML.Bmlj.GeneralImputer"><code>BetaML.Bmlj.GeneralImputer</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">mutable struct GeneralImputer &lt;: MLJModelInterface.Unsupervised</code></pre><p>Impute missing values using arbitrary learning models, from the Beta Machine Learning Toolkit (BetaML).</p><p>Impute missing values using a vector (one per column) of arbitrary learning models (classifiers/regressors, not necessarily from BetaML) that implement the interface <code>m = Model([options])</code>, <code>train!(m,X,Y)</code> and <code>predict(m,X)</code>.</p><p><strong>Hyperparameters:</strong></p><ul><li><p><code>cols_to_impute::Union{String, Vector{Int64}}</code>: Columns in the matrix for which to create an imputation model, i.e. to impute. It can be a vector of columns IDs (positions), or the keywords &quot;auto&quot; (default) or &quot;all&quot;. With &quot;auto&quot; the model automatically detects the columns with missing data and impute only them. You may manually specify the columns or use &quot;all&quot; if you want to create a imputation model for that columns during training even if all training data are non-missing to apply then the training model to further data with possibly missing values.</p></li><li><p><code>estimator::Any</code>: An entimator model (regressor or classifier), with eventually its options (hyper-parameters), to be used to impute the various columns of the matrix. It can also be a <code>cols_to_impute</code>-length vector of different estimators to consider a different estimator for each column (dimension) to impute, for example when some columns are categorical (and will hence require a classifier) and some others are numerical (hence requiring a regressor). [default: <code>nothing</code>, i.e. use BetaML random forests, handling classification and regression jobs automatically].</p></li><li><p><code>missing_supported::Union{Bool, Vector{Bool}}</code>: Wheter the estimator(s) used to predict the missing data support itself missing data in the training features (X). If not, when the model for a certain dimension is fitted, dimensions with missing data in the same rows of those where imputation is needed are dropped and then only non-missing rows in the other remaining dimensions are considered. It can be a vector of boolean values to specify this property for each individual estimator or a single booleann value to apply to all the estimators [default: <code>false</code>]</p></li><li><p><code>fit_function::Union{Function, Vector{Function}}</code>: The function used by the estimator(s) to fit the model. It should take as fist argument the model itself, as second argument a matrix representing the features, and as third argument a vector representing the labels. This parameter is mandatory for non-BetaML estimators and can be a single value or a vector (one per estimator) in case of different estimator packages used. [default: <code>BetaML.fit!</code>]</p></li><li><p><code>predict_function::Union{Function, Vector{Function}}</code>: The function used by the estimator(s) to predict the labels. It should take as fist argument the model itself and as second argument a matrix representing the features. This parameter is mandatory for non-BetaML estimators and can be a single value or a vector (one per estimator) in case of different estimator packages used. [default: <code>BetaML.predict</code>]</p></li><li><p><code>recursive_passages::Int64</code>: Define the number of times to go trough the various columns to impute their data. Useful when there are data to impute on multiple columns. The order of the first passage is given by the decreasing number of missing values per column, the other passages are random [default: <code>1</code>].</p></li><li><p><code>rng::Random.AbstractRNG</code>: A Random Number Generator to be used in stochastic parts of the code [deafult: <code>Random.GLOBAL_RNG</code>]. Note that this influence only the specific GeneralImputer code, the individual estimators may have their own rng (or similar) parameter.</p></li></ul><p><strong>Examples :</strong></p><ul><li><em>Using BetaML models</em>:</li></ul><pre><code class="language-julia hljs">julia&gt; using MLJ;
julia&gt; import BetaML # The library from which to get the individual estimators to be used for each column imputation
julia&gt; X = [&quot;a&quot;         8.2;
            &quot;a&quot;     missing;
            &quot;a&quot;         7.8;
            &quot;b&quot;          21;
            &quot;b&quot;          18;
            &quot;c&quot;        -0.9;
            missing      20;
            &quot;c&quot;        -1.8;
            missing    -2.3;
            &quot;c&quot;        -2.4] |&gt; table ;
julia&gt; modelType = @load GeneralImputer  pkg = &quot;BetaML&quot; verbosity=0
BetaML.Imputation.GeneralImputer
julia&gt; model     = modelType(estimator=BetaML.DecisionTreeEstimator(),recursive_passages=2);
julia&gt; mach      = machine(model, X);
julia&gt; fit!(mach);
[ Info: Training machine(GeneralImputer(cols_to_impute = auto, …), …).
julia&gt; X_full       = transform(mach) |&gt; MLJ.matrix
10×2 Matrix{Any}:
 &quot;a&quot;   8.2
 &quot;a&quot;   8.0
 &quot;a&quot;   7.8
 &quot;b&quot;  21
 &quot;b&quot;  18
 &quot;c&quot;  -0.9
 &quot;b&quot;  20
 &quot;c&quot;  -1.8
 &quot;c&quot;  -2.3
 &quot;c&quot;  -2.4</code></pre><ul><li><em>Using third party packages</em> (in this example <code>DecisionTree</code>):</li></ul><pre><code class="language-julia hljs">julia&gt; using MLJ;
julia&gt; import DecisionTree # An example of external estimators to be used for each column imputation
julia&gt; X = [&quot;a&quot;         8.2;
            &quot;a&quot;     missing;
            &quot;a&quot;         7.8;
            &quot;b&quot;          21;
            &quot;b&quot;          18;
            &quot;c&quot;        -0.9;
            missing      20;
            &quot;c&quot;        -1.8;
            missing    -2.3;
            &quot;c&quot;        -2.4] |&gt; table ;
julia&gt; modelType   = @load GeneralImputer  pkg = &quot;BetaML&quot; verbosity=0
BetaML.Imputation.GeneralImputer
julia&gt; model     = modelType(estimator=[DecisionTree.DecisionTreeClassifier(),DecisionTree.DecisionTreeRegressor()], fit_function=DecisionTree.fit!,predict_function=DecisionTree.predict,recursive_passages=2);
julia&gt; mach      = machine(model, X);
julia&gt; fit!(mach);
[ Info: Training machine(GeneralImputer(cols_to_impute = auto, …), …).
julia&gt; X_full       = transform(mach) |&gt; MLJ.matrix
10×2 Matrix{Any}:
 &quot;a&quot;   8.2
 &quot;a&quot;   7.51111
 &quot;a&quot;   7.8
 &quot;b&quot;  21
 &quot;b&quot;  18
 &quot;c&quot;  -0.9
 &quot;b&quot;  20
 &quot;c&quot;  -1.8
 &quot;c&quot;  -2.3
 &quot;c&quot;  -2.4</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/sylvaticus/BetaML.jl/blob/f8f3bf703fd327fbebd7d330505b6b6516418385/src/Bmlj/Imputation_mlj.jl#L230">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="BetaML.Bmlj.KMeansClusterer" href="#BetaML.Bmlj.KMeansClusterer"><code>BetaML.Bmlj.KMeansClusterer</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">mutable struct KMeansClusterer &lt;: MLJModelInterface.Unsupervised</code></pre><p>The classical KMeansClusterer clustering algorithm, from the Beta Machine Learning Toolkit (BetaML).</p><p><strong>Parameters:</strong></p><ul><li><p><code>n_classes::Int64</code>: Number of classes to discriminate the data [def: 3]</p></li><li><p><code>dist::Function</code>: Function to employ as distance. Default to the Euclidean distance. Can be one of the predefined distances (<code>l1_distance</code>, <code>l2_distance</code>, <code>l2squared_distance</code>),  <code>cosine_distance</code>), any user defined function accepting two vectors and returning a scalar or an anonymous function with the same characteristics. Attention that, contrary to <code>KMedoidsClusterer</code>, the <code>KMeansClusterer</code> algorithm is not guaranteed to converge with other distances than the Euclidean one.</p></li><li><p><code>initialisation_strategy::String</code>: The computation method of the vector of the initial representatives. One of the following:</p><ul><li>&quot;random&quot;: randomly in the X space</li><li>&quot;grid&quot;: using a grid approach</li><li>&quot;shuffle&quot;: selecting randomly within the available points [default]</li><li>&quot;given&quot;: using a provided set of initial representatives provided in the <code>initial_representatives</code> parameter</li></ul></li></ul><ul><li><p><code>initial_representatives::Union{Nothing, Matrix{Float64}}</code>: Provided (K x D) matrix of initial representatives (useful only with <code>initialisation_strategy=&quot;given&quot;</code>) [default: <code>nothing</code>]</p></li><li><p><code>rng::Random.AbstractRNG</code>: Random Number Generator [deafult: <code>Random.GLOBAL_RNG</code>]</p></li></ul><p><strong>Notes:</strong></p><ul><li>data must be numerical</li><li>online fitting (re-fitting with new data) is supported</li></ul><p><strong>Example:</strong></p><pre><code class="language-julia hljs">julia&gt; using MLJ

julia&gt; X, y        = @load_iris;

julia&gt; modelType   = @load KMeansClusterer pkg = &quot;BetaML&quot; verbosity=0
BetaML.Clustering.KMeansClusterer

julia&gt; model       = modelType()
KMeansClusterer(
  n_classes = 3, 
  dist = BetaML.Clustering.var&quot;#34#36&quot;(), 
  initialisation_strategy = &quot;shuffle&quot;, 
  initial_representatives = nothing, 
  rng = Random._GLOBAL_RNG())

julia&gt; mach        = machine(model, X);

julia&gt; fit!(mach);
[ Info: Training machine(KMeansClusterer(n_classes = 3, …), …).

julia&gt; classes_est = predict(mach, X);

julia&gt; hcat(y,classes_est)
150×2 CategoricalArrays.CategoricalArray{Union{Int64, String},2,UInt32}:
 &quot;setosa&quot;     2
 &quot;setosa&quot;     2
 &quot;setosa&quot;     2
 ⋮            
 &quot;virginica&quot;  3
 &quot;virginica&quot;  3
 &quot;virginica&quot;  1</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/sylvaticus/BetaML.jl/blob/f8f3bf703fd327fbebd7d330505b6b6516418385/src/Bmlj/Clustering_mlj.jl#L9">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="BetaML.Bmlj.KMedoidsClusterer" href="#BetaML.Bmlj.KMedoidsClusterer"><code>BetaML.Bmlj.KMedoidsClusterer</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">mutable struct KMedoidsClusterer &lt;: MLJModelInterface.Unsupervised</code></pre><p><strong>Parameters:</strong></p><ul><li><p><code>n_classes::Int64</code>: Number of classes to discriminate the data [def: 3]</p></li><li><p><code>dist::Function</code>: Function to employ as distance. Default to the Euclidean distance. Can be one of the predefined distances (<code>l1_distance</code>, <code>l2_distance</code>, <code>l2squared_distance</code>),  <code>cosine_distance</code>), any user defined function accepting two vectors and returning a scalar or an anonymous function with the same characteristics.</p></li><li><p><code>initialisation_strategy::String</code>: The computation method of the vector of the initial representatives. One of the following:</p><ul><li>&quot;random&quot;: randomly in the X space</li><li>&quot;grid&quot;: using a grid approach</li><li>&quot;shuffle&quot;: selecting randomly within the available points [default]</li><li>&quot;given&quot;: using a provided set of initial representatives provided in the <code>initial_representatives</code> parameter</li></ul></li></ul><ul><li><p><code>initial_representatives::Union{Nothing, Matrix{Float64}}</code>: Provided (K x D) matrix of initial representatives (useful only with <code>initialisation_strategy=&quot;given&quot;</code>) [default: <code>nothing</code>]</p></li><li><p><code>rng::Random.AbstractRNG</code>: Random Number Generator [deafult: <code>Random.GLOBAL_RNG</code>]</p></li></ul><p>The K-medoids clustering algorithm with customisable distance function, from the Beta Machine Learning Toolkit (BetaML).</p><p>Similar to K-Means, but the &quot;representatives&quot; (the cetroids) are guaranteed to be one of the training points. The algorithm work with any arbitrary distance measure.</p><p><strong>Notes:</strong></p><ul><li>data must be numerical</li><li>online fitting (re-fitting with new data) is supported</li></ul><p><strong>Example:</strong></p><pre><code class="language-julia hljs">julia&gt; using MLJ

julia&gt; X, y        = @load_iris;

julia&gt; modelType   = @load KMedoidsClusterer pkg = &quot;BetaML&quot; verbosity=0
BetaML.Clustering.KMedoidsClusterer

julia&gt; model       = modelType()
KMedoidsClusterer(
  n_classes = 3, 
  dist = BetaML.Clustering.var&quot;#39#41&quot;(), 
  initialisation_strategy = &quot;shuffle&quot;, 
  initial_representatives = nothing, 
  rng = Random._GLOBAL_RNG())

julia&gt; mach        = machine(model, X);

julia&gt; fit!(mach);
[ Info: Training machine(KMedoidsClusterer(n_classes = 3, …), …).

julia&gt; classes_est = predict(mach, X);

julia&gt; hcat(y,classes_est)
150×2 CategoricalArrays.CategoricalArray{Union{Int64, String},2,UInt32}:
 &quot;setosa&quot;     3
 &quot;setosa&quot;     3
 &quot;setosa&quot;     3
 ⋮            
 &quot;virginica&quot;  1
 &quot;virginica&quot;  1
 &quot;virginica&quot;  2</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/sylvaticus/BetaML.jl/blob/f8f3bf703fd327fbebd7d330505b6b6516418385/src/Bmlj/Clustering_mlj.jl#L83">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="BetaML.Bmlj.KernelPerceptronClassifier" href="#BetaML.Bmlj.KernelPerceptronClassifier"><code>BetaML.Bmlj.KernelPerceptronClassifier</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">mutable struct KernelPerceptronClassifier &lt;: MLJModelInterface.Probabilistic</code></pre><p>The kernel perceptron algorithm using one-vs-one for multiclass, from the Beta Machine Learning Toolkit (BetaML).</p><p><strong>Hyperparameters:</strong></p><ul><li><p><code>kernel::Function</code>: Kernel function to employ. See <code>?radial_kernel</code> or <code>?polynomial_kernel</code> (once loaded the BetaML package) for details or check <code>?BetaML.Utils</code> to verify if other kernels are defined (you can alsways define your own kernel) [def: <a href="Utils.html#BetaML.Utils.radial_kernel-Tuple{Any, Any}"><code>radial_kernel</code></a>]</p></li><li><p><code>epochs::Int64</code>: Maximum number of epochs, i.e. passages trough the whole training sample [def: <code>100</code>]</p></li><li><p><code>initial_errors::Union{Nothing, Vector{Vector{Int64}}}</code>: Initial distribution of the number of errors errors [def: <code>nothing</code>, i.e. zeros]. If provided, this should be a nModels-lenght vector of nRecords integer values vectors , where nModels is computed as <code>(n_classes  * (n_classes - 1)) / 2</code></p></li><li><p><code>shuffle::Bool</code>: Whether to randomly shuffle the data at each iteration (epoch) [def: <code>true</code>]</p></li><li><p><code>rng::Random.AbstractRNG</code>: A Random Number Generator to be used in stochastic parts of the code [deafult: <code>Random.GLOBAL_RNG</code>]</p></li></ul><p><strong>Example:</strong></p><pre><code class="language-julia hljs">julia&gt; using MLJ

julia&gt; X, y        = @load_iris;

julia&gt; modelType   = @load KernelPerceptronClassifier pkg = &quot;BetaML&quot;
[ Info: For silent loading, specify `verbosity=0`. 
import BetaML ✔
BetaML.Perceptron.KernelPerceptronClassifier

julia&gt; model       = modelType()
KernelPerceptronClassifier(
  kernel = BetaML.Utils.radial_kernel, 
  epochs = 100, 
  initial_errors = nothing, 
  shuffle = true, 
  rng = Random._GLOBAL_RNG())

julia&gt; mach        = machine(model, X, y);

julia&gt; fit!(mach);

julia&gt; est_classes = predict(mach, X)
150-element CategoricalDistributions.UnivariateFiniteVector{Multiclass{3}, String, UInt8, Float64}:
 UnivariateFinite{Multiclass{3}}(setosa=&gt;0.665, versicolor=&gt;0.245, virginica=&gt;0.09)
 UnivariateFinite{Multiclass{3}}(setosa=&gt;0.665, versicolor=&gt;0.245, virginica=&gt;0.09)
 ⋮
 UnivariateFinite{Multiclass{3}}(setosa=&gt;0.09, versicolor=&gt;0.245, virginica=&gt;0.665)
 UnivariateFinite{Multiclass{3}}(setosa=&gt;0.09, versicolor=&gt;0.665, virginica=&gt;0.245)</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/sylvaticus/BetaML.jl/blob/f8f3bf703fd327fbebd7d330505b6b6516418385/src/Bmlj/Perceptron_mlj.jl#L80">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="BetaML.Bmlj.MultitargetGaussianMixtureRegressor" href="#BetaML.Bmlj.MultitargetGaussianMixtureRegressor"><code>BetaML.Bmlj.MultitargetGaussianMixtureRegressor</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">mutable struct MultitargetGaussianMixtureRegressor &lt;: MLJModelInterface.Deterministic</code></pre><p>A non-linear regressor derived from fitting the data on a probabilistic model (Gaussian Mixture Model). Relatively fast but generally not very precise, except for data with a structure matching the chosen underlying mixture.</p><p>This is the multi-target version of the model. If you want to predict a single label (y), use the MLJ model <a href="MLJ_interface.html#BetaML.Bmlj.GaussianMixtureRegressor"><code>GaussianMixtureRegressor</code></a>.</p><p><strong>Hyperparameters:</strong></p><ul><li><p><code>n_classes::Int64</code>: Number of mixtures (latent classes) to consider [def: 3]</p></li><li><p><code>initial_probmixtures::Vector{Float64}</code>: Initial probabilities of the categorical distribution (n_classes x 1) [default: <code>[]</code>]</p></li><li><p><code>mixtures::Union{Type, Vector{var&quot;#s1264&quot;} where var&quot;#s1264&quot;&lt;:AbstractMixture}</code>: An array (of length <code>n_classes</code><code>) of the mixtures to employ (see the [</code>?GMM<code>](@ref GMM) module). Each mixture object can be provided with or without its parameters (e.g. mean and variance for the gaussian ones). Fully qualified mixtures are useful only if the</code>initialisation<em>strategy<code>parameter is  set to &quot;gived&quot;</code> This parameter can also be given symply in term of a _type</em>. In this case it is automatically extended to a vector of <code>n_classes</code><code>mixtures of the specified type. Note that mixing of different mixture types is not currently supported. [def:</code>[DiagonalGaussian() for i in 1:n_classes]`]</p></li><li><p><code>tol::Float64</code>: Tolerance to stop the algorithm [default: 10^(-6)]</p></li><li><p><code>minimum_variance::Float64</code>: Minimum variance for the mixtures [default: 0.05]</p></li><li><p><code>minimum_covariance::Float64</code>: Minimum covariance for the mixtures with full covariance matrix [default: 0]. This should be set different than minimum_variance (see notes).</p></li><li><p><code>initialisation_strategy::String</code>: The computation method of the vector of the initial mixtures. One of the following:</p><ul><li>&quot;grid&quot;: using a grid approach</li><li>&quot;given&quot;: using the mixture provided in the fully qualified <code>mixtures</code> parameter</li><li>&quot;kmeans&quot;: use first kmeans (itself initialised with a &quot;grid&quot; strategy) to set the initial mixture centers [default]</li></ul><p>Note that currently &quot;random&quot; and &quot;shuffle&quot; initialisations are not supported in gmm-based algorithms.</p></li></ul><ul><li><p><code>maximum_iterations::Int64</code>: Maximum number of iterations [def: <code>typemax(Int64)</code>, i.e. ∞]</p></li><li><p><code>rng::Random.AbstractRNG</code>: Random Number Generator [deafult: <code>Random.GLOBAL_RNG</code>]</p></li></ul><p><strong>Example:</strong></p><pre><code class="language-julia hljs">julia&gt; using MLJ

julia&gt; X, y        = @load_boston;

julia&gt; ydouble     = hcat(y, y .*2  .+5);

julia&gt; modelType   = @load MultitargetGaussianMixtureRegressor pkg = &quot;BetaML&quot; verbosity=0
BetaML.GMM.MultitargetGaussianMixtureRegressor

julia&gt; model       = modelType()
MultitargetGaussianMixtureRegressor(
  n_classes = 3, 
  initial_probmixtures = Float64[], 
  mixtures = BetaML.GMM.DiagonalGaussian{Float64}[BetaML.GMM.DiagonalGaussian{Float64}(nothing, nothing), BetaML.GMM.DiagonalGaussian{Float64}(nothing, nothing), BetaML.GMM.DiagonalGaussian{Float64}(nothing, nothing)], 
  tol = 1.0e-6, 
  minimum_variance = 0.05, 
  minimum_covariance = 0.0, 
  initialisation_strategy = &quot;kmeans&quot;, 
  maximum_iterations = 9223372036854775807, 
  rng = Random._GLOBAL_RNG())

julia&gt; mach        = machine(model, X, ydouble);

julia&gt; fit!(mach);
[ Info: Training machine(MultitargetGaussianMixtureRegressor(n_classes = 3, …), …).
Iter. 1:        Var. of the post  20.46947926187522       Log-likelihood -23662.72770575145

julia&gt; ŷdouble    = predict(mach, X)
506×2 Matrix{Float64}:
 23.3358  51.6717
 23.3358  51.6717
  ⋮       
 16.6843  38.3686
 16.6843  38.3686</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/sylvaticus/BetaML.jl/blob/f8f3bf703fd327fbebd7d330505b6b6516418385/src/Bmlj/GMM_mlj.jl#L200">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="BetaML.Bmlj.MultitargetNeuralNetworkRegressor" href="#BetaML.Bmlj.MultitargetNeuralNetworkRegressor"><code>BetaML.Bmlj.MultitargetNeuralNetworkRegressor</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">mutable struct MultitargetNeuralNetworkRegressor &lt;: MLJModelInterface.Deterministic</code></pre><p>A simple but flexible Feedforward Neural Network, from the Beta Machine Learning Toolkit (BetaML) for regression of multiple dimensional targets.</p><p><strong>Parameters:</strong></p><ul><li><p><code>layers</code>: Array of layer objects [def: <code>nothing</code>, i.e. basic network]. See <code>subtypes(BetaML.AbstractLayer)</code> for supported layers</p></li><li><p><code>loss</code>: Loss (cost) function [def: <code>BetaML.squared_cost</code>].  Should always assume y and ŷ as matrices.</p><div class="admonition is-warning"><header class="admonition-header">Warning</header><div class="admonition-body"><p>If you change the parameter <code>loss</code>, you need to either provide its derivative on the parameter <code>dloss</code> or use autodiff with <code>dloss=nothing</code>.</p></div></div></li></ul><ul><li><p><code>dloss</code>: Derivative of the loss function [def: <code>BetaML.dsquared_cost</code>, i.e. use the derivative of the squared cost]. Use <code>nothing</code> for autodiff.</p></li><li><p><code>epochs</code>: Number of epochs, i.e. passages trough the whole training sample [def: <code>300</code>]</p></li><li><p><code>batch_size</code>: Size of each individual batch [def: <code>16</code>]</p></li><li><p><code>opt_alg</code>: The optimisation algorithm to update the gradient at each batch [def: <code>BetaML.ADAM()</code>]. See <code>subtypes(BetaML.OptimisationAlgorithm)</code> for supported optimizers</p></li><li><p><code>shuffle</code>: Whether to randomly shuffle the data at each iteration (epoch) [def: <code>true</code>]</p></li><li><p><code>descr</code>: An optional title and/or description for this model</p></li><li><p><code>cb</code>: A call back function to provide information during training [def: <code>BetaML.fitting_info</code>]</p></li><li><p><code>rng</code>: Random Number Generator (see <a href="Api.html#BetaML.Api.FIXEDSEED"><code>FIXEDSEED</code></a>) [deafult: <code>Random.GLOBAL_RNG</code>]</p></li></ul><p><strong>Notes:</strong></p><ul><li>data must be numerical</li><li>the label should be a <em>n-records</em> by <em>n-dimensions</em> matrix </li></ul><p><strong>Example:</strong></p><pre><code class="language-julia hljs">julia&gt; using MLJ

julia&gt; X, y        = @load_boston;

julia&gt; ydouble     = hcat(y, y .*2  .+5);

julia&gt; modelType   = @load MultitargetNeuralNetworkRegressor pkg = &quot;BetaML&quot; verbosity=0
BetaML.Nn.MultitargetNeuralNetworkRegressor

julia&gt; layers                      = [BetaML.DenseLayer(12,50,f=BetaML.relu),BetaML.DenseLayer(50,50,f=BetaML.relu),BetaML.DenseLayer(50,50,f=BetaML.relu),BetaML.DenseLayer(50,2,f=BetaML.relu)];

julia&gt; model       = modelType(layers=layers,opt_alg=BetaML.ADAM(),epochs=500)
MultitargetNeuralNetworkRegressor(
  layers = BetaML.Nn.AbstractLayer[BetaML.Nn.DenseLayer([-0.2591582523441157 -0.027962845131416225 … 0.16044535560124418 -0.12838827994676857; -0.30381834909561184 0.2405495243851402 … -0.2588144861880588 0.09538577909777807; … ; -0.017320292924711156 -0.14042266424603767 … 0.06366999105841187 -0.13419651752478906; 0.07393079961409338 0.24521350531110264 … 0.04256867886217541 -0.0895506802948175], [0.14249427336553644, 0.24719379413682485, -0.25595911822556566, 0.10034088778965933, -0.017086404878505712, 0.21932184025609347, -0.031413516834861266, -0.12569076082247596, -0.18080140982481183, 0.14551901873323253  …  -0.13321995621967364, 0.2436582233332092, 0.0552222336976439, 0.07000814133633904, 0.2280064379660025, -0.28885681475734193, -0.07414214246290696, -0.06783184733650621, -0.055318068046308455, -0.2573488383282579], BetaML.Utils.relu, BetaML.Utils.drelu), BetaML.Nn.DenseLayer([-0.0395424111703751 -0.22531232360829911 … -0.04341228943744482 0.024336206858365517; -0.16481887432946268 0.17798073384748508 … -0.18594039305095766 0.051159225856547474; … ; -0.011639475293705043 -0.02347011206244673 … 0.20508869536159186 -0.1158382446274592; -0.19078069527757857 -0.007487540070740484 … -0.21341165344291158 -0.24158671316310726], [-0.04283623889330032, 0.14924461547060602, -0.17039563392959683, 0.00907774027816255, 0.21738885963113852, -0.06308040225941691, -0.14683286822101105, 0.21726892197970937, 0.19784321784707126, -0.0344988665714947  …  -0.23643089430602846, -0.013560425201427584, 0.05323948910726356, -0.04644175812567475, -0.2350400292671211, 0.09628312383424742, 0.07016420995205697, -0.23266392927140334, -0.18823664451487, 0.2304486691429084], BetaML.Utils.relu, BetaML.Utils.drelu), BetaML.Nn.DenseLayer([-0.11504184627266828 0.08601794194664503 … 0.03843129724045469 -0.18417305624127284; 0.10181551438831654 0.13459759904443674 … 0.11094951365942118 -0.1549466590355218; … ; 0.15279817525427697 0.0846661196058916 … -0.07993619892911122 0.07145402617285884; -0.1614160186346092 -0.13032002335149 … -0.12310552194729624 -0.15915773071049827], [-0.03435885900946367, -0.1198543931290306, 0.008454985905194445, -0.17980887188986966, -0.03557204910359624, 0.19125847393334877, -0.10949700778538696, -0.09343206702591, -0.12229583511781811, -0.09123969069220564  …  0.22119233518322862, 0.2053873143308657, 0.12756489387198222, 0.11567243705173319, -0.20982445664020496, 0.1595157838386987, -0.02087331046544119, -0.20556423263489765, -0.1622837764237961, -0.019220998739847395], BetaML.Utils.relu, BetaML.Utils.drelu), BetaML.Nn.DenseLayer([-0.25796717031347993 0.17579536633402948 … -0.09992960168785256 -0.09426177454620635; -0.026436330246675632 0.18070899284865127 … -0.19310119102392206 -0.06904005900252091], [0.16133004882307822, -0.3061228721091248], BetaML.Utils.relu, BetaML.Utils.drelu)], 
  loss = BetaML.Utils.squared_cost, 
  dloss = BetaML.Utils.dsquared_cost, 
  epochs = 500, 
  batch_size = 32, 
  opt_alg = BetaML.Nn.ADAM(BetaML.Nn.var&quot;#90#93&quot;(), 1.0, 0.9, 0.999, 1.0e-8, BetaML.Nn.Learnable[], BetaML.Nn.Learnable[]), 
  shuffle = true, 
  descr = &quot;&quot;, 
  cb = BetaML.Nn.fitting_info, 
  rng = Random._GLOBAL_RNG())

julia&gt; mach        = machine(model, X, ydouble);

julia&gt; fit!(mach);

julia&gt; ŷdouble    = predict(mach, X);

julia&gt; hcat(ydouble,ŷdouble)
506×4 Matrix{Float64}:
 24.0  53.0  28.4624  62.8607
 21.6  48.2  22.665   49.7401
 34.7  74.4  31.5602  67.9433
 33.4  71.8  33.0869  72.4337
  ⋮                   
 23.9  52.8  23.3573  50.654
 22.0  49.0  22.1141  48.5926
 11.9  28.8  19.9639  45.5823</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/sylvaticus/BetaML.jl/blob/f8f3bf703fd327fbebd7d330505b6b6516418385/src/Bmlj/Nn_mlj.jl#L121">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="BetaML.Bmlj.NeuralNetworkClassifier" href="#BetaML.Bmlj.NeuralNetworkClassifier"><code>BetaML.Bmlj.NeuralNetworkClassifier</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">mutable struct NeuralNetworkClassifier &lt;: MLJModelInterface.Probabilistic</code></pre><p>A simple but flexible Feedforward Neural Network, from the Beta Machine Learning Toolkit (BetaML) for classification  problems.</p><p><strong>Parameters:</strong></p><ul><li><p><code>layers</code>: Array of layer objects [def: <code>nothing</code>, i.e. basic network]. See <code>subtypes(BetaML.AbstractLayer)</code> for supported layers. The last &quot;softmax&quot; layer is automatically added.</p></li><li><p><code>loss</code>: Loss (cost) function [def: <code>BetaML.crossentropy</code>]. Should always assume y and ŷ as matrices.</p><div class="admonition is-warning"><header class="admonition-header">Warning</header><div class="admonition-body"><p>If you change the parameter <code>loss</code>, you need to either provide its derivative on the parameter <code>dloss</code> or use autodiff with <code>dloss=nothing</code>.</p></div></div></li></ul><ul><li><p><code>dloss</code>: Derivative of the loss function [def: <code>BetaML.dcrossentropy</code>, i.e. the derivative of the cross-entropy]. Use <code>nothing</code> for autodiff.</p></li><li><p><code>epochs</code>: Number of epochs, i.e. passages trough the whole training sample [def: <code>200</code>]</p></li><li><p><code>batch_size</code>: Size of each individual batch [def: <code>16</code>]</p></li><li><p><code>opt_alg</code>: The optimisation algorithm to update the gradient at each batch [def: <code>BetaML.ADAM()</code>]. See <code>subtypes(BetaML.OptimisationAlgorithm)</code> for supported optimizers</p></li><li><p><code>shuffle</code>: Whether to randomly shuffle the data at each iteration (epoch) [def: <code>true</code>]</p></li><li><p><code>descr</code>: An optional title and/or description for this model</p></li><li><p><code>cb</code>: A call back function to provide information during training [def: <code>BetaML.fitting_info</code>]</p></li><li><p><code>categories</code>: The categories to represent as columns. [def: <code>nothing</code>, i.e. unique training values].</p></li><li><p><code>handle_unknown</code>: How to handle categories not seens in training or not present in the provided <code>categories</code> array? &quot;error&quot; (default) rises an error, &quot;infrequent&quot; adds a specific column for these categories.</p></li><li><p><code>other_categories_name</code>: Which value during prediction to assign to this &quot;other&quot; category (i.e. categories not seen on training or not present in the provided <code>categories</code> array? [def: <code>nothing</code>, i.e. typemax(Int64) for integer vectors and &quot;other&quot; for other types]. This setting is active only if <code>handle_unknown=&quot;infrequent&quot;</code> and in that case it MUST be specified if Y is neither integer or strings</p></li><li><p><code>rng</code>: Random Number Generator [deafult: <code>Random.GLOBAL_RNG</code>]</p></li></ul><p><strong>Notes:</strong></p><ul><li>data must be numerical</li><li>the label should be a <em>n-records</em> by <em>n-dimensions</em> matrix (e.g. a one-hot-encoded data for classification), where the output columns should be interpreted as the probabilities for each categories.</li></ul><p><strong>Example:</strong></p><pre><code class="language-julia hljs">julia&gt; using MLJ

julia&gt; X, y        = @load_iris;

julia&gt; modelType   = @load NeuralNetworkClassifier pkg = &quot;BetaML&quot; verbosity=0
BetaML.Nn.NeuralNetworkClassifier

julia&gt; layers      = [BetaML.DenseLayer(4,8,f=BetaML.relu),BetaML.DenseLayer(8,8,f=BetaML.relu),BetaML.DenseLayer(8,3,f=BetaML.relu),BetaML.VectorFunctionLayer(3,f=BetaML.softmax)];

julia&gt; model       = modelType(layers=layers,opt_alg=BetaML.ADAM())
NeuralNetworkClassifier(
  layers = BetaML.Nn.AbstractLayer[BetaML.Nn.DenseLayer([-0.376173352338049 0.7029289511758696 -0.5589563304592478 -0.21043274001651874; 0.044758889527899415 0.6687689636685921 0.4584331114653877 0.6820506583840453; … ; -0.26546358457167507 -0.28469736227283804 -0.164225549922154 -0.516785639164486; -0.5146043550684141 -0.0699113265130964 0.14959906603941908 -0.053706860039406834], [0.7003943613125758, -0.23990840466587576, -0.23823126271387746, 0.4018101580410387, 0.2274483050356888, -0.564975060667734, 0.1732063297031089, 0.11880299829896945], BetaML.Utils.relu, BetaML.Utils.drelu), BetaML.Nn.DenseLayer([-0.029467850439546583 0.4074661266592745 … 0.36775675246760053 -0.595524555448422; 0.42455597698371306 -0.2458082732997091 … -0.3324220683462514 0.44439454998610595; … ; -0.2890883863364267 -0.10109249362508033 … -0.0602680568207582 0.18177278845097555; -0.03432587226449335 -0.4301192922760063 … 0.5646018168286626 0.47269177680892693], [0.13777442835428688, 0.5473306726675433, 0.3781939472904011, 0.24021813428130567, -0.0714779477402877, -0.020386373530818958, 0.5465466618404464, -0.40339790713616525], BetaML.Utils.relu, BetaML.Utils.drelu), BetaML.Nn.DenseLayer([0.6565120540082393 0.7139211611842745 … 0.07809812467915389 -0.49346311403373844; -0.4544472987041656 0.6502667641568863 … 0.43634608676548214 0.7213049952968921; 0.41212264783075303 -0.21993289366360613 … 0.25365007887755064 -0.5664469566269569], [-0.6911986792747682, -0.2149343209329364, -0.6347727539063817], BetaML.Utils.relu, BetaML.Utils.drelu), BetaML.Nn.VectorFunctionLayer{0}(fill(NaN), 3, 3, BetaML.Utils.softmax, BetaML.Utils.dsoftmax, nothing)], 
  loss = BetaML.Utils.crossentropy, 
  dloss = BetaML.Utils.dcrossentropy, 
  epochs = 100, 
  batch_size = 32, 
  opt_alg = BetaML.Nn.ADAM(BetaML.Nn.var&quot;#90#93&quot;(), 1.0, 0.9, 0.999, 1.0e-8, BetaML.Nn.Learnable[], BetaML.Nn.Learnable[]), 
  shuffle = true, 
  descr = &quot;&quot;, 
  cb = BetaML.Nn.fitting_info, 
  categories = nothing, 
  handle_unknown = &quot;error&quot;, 
  other_categories_name = nothing, 
  rng = Random._GLOBAL_RNG())

julia&gt; mach        = machine(model, X, y);

julia&gt; fit!(mach);

julia&gt; classes_est = predict(mach, X)
150-element CategoricalDistributions.UnivariateFiniteVector{Multiclass{3}, String, UInt8, Float64}:
 UnivariateFinite{Multiclass{3}}(setosa=&gt;0.575, versicolor=&gt;0.213, virginica=&gt;0.213)
 UnivariateFinite{Multiclass{3}}(setosa=&gt;0.573, versicolor=&gt;0.213, virginica=&gt;0.213)
 ⋮
 UnivariateFinite{Multiclass{3}}(setosa=&gt;0.236, versicolor=&gt;0.236, virginica=&gt;0.529)
 UnivariateFinite{Multiclass{3}}(setosa=&gt;0.254, versicolor=&gt;0.254, virginica=&gt;0.492)</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/sylvaticus/BetaML.jl/blob/f8f3bf703fd327fbebd7d330505b6b6516418385/src/Bmlj/Nn_mlj.jl#L234">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="BetaML.Bmlj.NeuralNetworkRegressor" href="#BetaML.Bmlj.NeuralNetworkRegressor"><code>BetaML.Bmlj.NeuralNetworkRegressor</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">mutable struct NeuralNetworkRegressor &lt;: MLJModelInterface.Deterministic</code></pre><p>A simple but flexible Feedforward Neural Network, from the Beta Machine Learning Toolkit (BetaML) for regression of a single dimensional target.</p><p><strong>Parameters:</strong></p><ul><li><p><code>layers</code>: Array of layer objects [def: <code>nothing</code>, i.e. basic network]. See <code>subtypes(BetaML.AbstractLayer)</code> for supported layers</p></li><li><p><code>loss</code>: Loss (cost) function [def: <code>BetaML.squared_cost</code>]. Should always assume y and ŷ as matrices, even if the regression task is 1-D</p><div class="admonition is-warning"><header class="admonition-header">Warning</header><div class="admonition-body"><p>If you change the parameter <code>loss</code>, you need to either provide its derivative on the parameter <code>dloss</code> or use autodiff with <code>dloss=nothing</code>.</p></div></div></li></ul><ul><li><p><code>dloss</code>: Derivative of the loss function [def: <code>BetaML.dsquared_cost</code>, i.e. use the derivative of the squared cost]. Use <code>nothing</code> for autodiff.</p></li><li><p><code>epochs</code>: Number of epochs, i.e. passages trough the whole training sample [def: <code>200</code>]</p></li><li><p><code>batch_size</code>: Size of each individual batch [def: <code>16</code>]</p></li><li><p><code>opt_alg</code>: The optimisation algorithm to update the gradient at each batch [def: <code>BetaML.ADAM()</code>]. See <code>subtypes(BetaML.OptimisationAlgorithm)</code> for supported optimizers</p></li><li><p><code>shuffle</code>: Whether to randomly shuffle the data at each iteration (epoch) [def: <code>true</code>]</p></li><li><p><code>descr</code>: An optional title and/or description for this model</p></li><li><p><code>cb</code>: A call back function to provide information during training [def: <code>fitting_info</code>]</p></li><li><p><code>rng</code>: Random Number Generator (see <a href="Api.html#BetaML.Api.FIXEDSEED"><code>FIXEDSEED</code></a>) [deafult: <code>Random.GLOBAL_RNG</code>]</p></li></ul><p><strong>Notes:</strong></p><ul><li>data must be numerical</li><li>the label should be be a <em>n-records</em> vector.</li></ul><p><strong>Example:</strong></p><pre><code class="language-julia hljs">julia&gt; using MLJ

julia&gt; X, y        = @load_boston;

julia&gt; modelType   = @load NeuralNetworkRegressor pkg = &quot;BetaML&quot; verbosity=0
BetaML.Nn.NeuralNetworkRegressor

julia&gt; layers                      = [BetaML.DenseLayer(12,20,f=BetaML.relu),BetaML.DenseLayer(20,20,f=BetaML.relu),BetaML.DenseLayer(20,1,f=BetaML.relu)];

julia&gt; model       = modelType(layers=layers,opt_alg=BetaML.ADAM());
NeuralNetworkRegressor(
  layers = BetaML.Nn.AbstractLayer[BetaML.Nn.DenseLayer([-0.23249759178069676 -0.4125090172711131 … 0.41401934928739 -0.33017881111237535; -0.27912169279319965 0.270551221249931 … 0.19258414323473344 0.1703002982374256; … ; 0.31186742456482447 0.14776438287394805 … 0.3624993442655036 0.1438885872964824; 0.24363744610286758 -0.3221033024934767 … 0.14886090419299408 0.038411663101909355], [-0.42360286004241765, -0.34355377040029594, 0.11510963232946697, 0.29078650404397893, -0.04940236502546075, 0.05142849152316714, -0.177685375947775, 0.3857630523957018, -0.25454667127064756, -0.1726731848206195, 0.29832456225553444, -0.21138505291162835, -0.15763643112604903, -0.08477044513587562, -0.38436681165349196, 0.20538016429104916, -0.25008157754468335, 0.268681800562054, 0.10600581996650865, 0.4262194464325672], BetaML.Utils.relu, BetaML.Utils.drelu), BetaML.Nn.DenseLayer([-0.08534180387478185 0.19659398307677617 … -0.3413633217504578 -0.0484925247381256; 0.0024419192794883915 -0.14614102508129 … -0.21912059923003044 0.2680725396694708; … ; 0.25151545823147886 -0.27532269951606037 … 0.20739970895058063 0.2891938885916349; -0.1699020711688904 -0.1350423717084296 … 0.16947589410758873 0.3629006047373296], [0.2158116357688406, -0.3255582642532289, -0.057314442103850394, 0.29029696770539953, 0.24994080694366455, 0.3624239027782297, -0.30674318230919984, -0.3854738338935017, 0.10809721838554087, 0.16073511121016176, -0.005923262068960489, 0.3157147976348795, -0.10938918304264739, -0.24521229198853187, -0.307167732178712, 0.0808907777008302, -0.014577497150872254, -0.0011287181458157214, 0.07522282588658086, 0.043366500526073104], BetaML.Utils.relu, BetaML.Utils.drelu), BetaML.Nn.DenseLayer([-0.021367697115938555 -0.28326652172347155 … 0.05346175368370165 -0.26037328415871647], [-0.2313659199724562], BetaML.Utils.relu, BetaML.Utils.drelu)], 
  loss = BetaML.Utils.squared_cost, 
  dloss = BetaML.Utils.dsquared_cost, 
  epochs = 100, 
  batch_size = 32, 
  opt_alg = BetaML.Nn.ADAM(BetaML.Nn.var&quot;#90#93&quot;(), 1.0, 0.9, 0.999, 1.0e-8, BetaML.Nn.Learnable[], BetaML.Nn.Learnable[]), 
  shuffle = true, 
  descr = &quot;&quot;, 
  cb = BetaML.Nn.fitting_info, 
  rng = Random._GLOBAL_RNG())

julia&gt; mach        = machine(model, X, y);

julia&gt; fit!(mach);

julia&gt; ŷ    = predict(mach, X);

julia&gt; hcat(y,ŷ)
506×2 Matrix{Float64}:
 24.0  30.7726
 21.6  28.0811
 34.7  31.3194
  ⋮    
 23.9  30.9032
 22.0  29.49
 11.9  27.2438</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/sylvaticus/BetaML.jl/blob/f8f3bf703fd327fbebd7d330505b6b6516418385/src/Bmlj/Nn_mlj.jl#L11">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="BetaML.Bmlj.PegasosClassifier" href="#BetaML.Bmlj.PegasosClassifier"><code>BetaML.Bmlj.PegasosClassifier</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">mutable struct PegasosClassifier &lt;: MLJModelInterface.Probabilistic</code></pre><p>The gradient-based linear &quot;pegasos&quot; classifier using one-vs-all for multiclass, from the Beta Machine Learning Toolkit (BetaML).</p><p><strong>Hyperparameters:</strong></p><ul><li><p><code>initial_coefficients::Union{Nothing, Matrix{Float64}}</code>: N-classes by D-dimensions matrix of initial linear coefficients [def: <code>nothing</code>, i.e. zeros]</p></li><li><p><code>initial_constant::Union{Nothing, Vector{Float64}}</code>: N-classes vector of initial contant terms [def: <code>nothing</code>, i.e. zeros]</p></li><li><p><code>learning_rate::Function</code>: Learning rate [def: (epoch -&gt; 1/sqrt(epoch))]</p></li><li><p><code>learning_rate_multiplicative::Float64</code>: Multiplicative term of the learning rate [def: <code>0.5</code>]</p></li><li><p><code>epochs::Int64</code>: Maximum number of epochs, i.e. passages trough the whole training sample [def: <code>1000</code>]</p></li><li><p><code>shuffle::Bool</code>: Whether to randomly shuffle the data at each iteration (epoch) [def: <code>true</code>]</p></li><li><p><code>force_origin::Bool</code>: Whether to force the parameter associated with the constant term to remain zero [def: <code>false</code>]</p></li><li><p><code>return_mean_hyperplane::Bool</code>: Whether to return the average hyperplane coefficients instead of the final ones  [def: <code>false</code>]</p></li><li><p><code>rng::Random.AbstractRNG</code>: A Random Number Generator to be used in stochastic parts of the code [deafult: <code>Random.GLOBAL_RNG</code>]</p></li></ul><p><strong>Example:</strong></p><pre><code class="language-julia hljs">julia&gt; using MLJ

julia&gt; X, y        = @load_iris;

julia&gt; modelType   = @load PegasosClassifier pkg = &quot;BetaML&quot; verbosity=0
BetaML.Perceptron.PegasosClassifier

julia&gt; model       = modelType()
PegasosClassifier(
  initial_coefficients = nothing, 
  initial_constant = nothing, 
  learning_rate = BetaML.Perceptron.var&quot;#71#73&quot;(), 
  learning_rate_multiplicative = 0.5, 
  epochs = 1000, 
  shuffle = true, 
  force_origin = false, 
  return_mean_hyperplane = false, 
  rng = Random._GLOBAL_RNG())

julia&gt; mach        = machine(model, X, y);

julia&gt; fit!(mach);

julia&gt; est_classes = predict(mach, X)
150-element CategoricalDistributions.UnivariateFiniteVector{Multiclass{3}, String, UInt8, Float64}:
 UnivariateFinite{Multiclass{3}}(setosa=&gt;0.817, versicolor=&gt;0.153, virginica=&gt;0.0301)
 UnivariateFinite{Multiclass{3}}(setosa=&gt;0.791, versicolor=&gt;0.177, virginica=&gt;0.0318)
 ⋮
 UnivariateFinite{Multiclass{3}}(setosa=&gt;0.254, versicolor=&gt;0.5, virginica=&gt;0.246)
 UnivariateFinite{Multiclass{3}}(setosa=&gt;0.283, versicolor=&gt;0.51, virginica=&gt;0.207)</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/sylvaticus/BetaML.jl/blob/f8f3bf703fd327fbebd7d330505b6b6516418385/src/Bmlj/Perceptron_mlj.jl#L140">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="BetaML.Bmlj.PerceptronClassifier" href="#BetaML.Bmlj.PerceptronClassifier"><code>BetaML.Bmlj.PerceptronClassifier</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">mutable struct PerceptronClassifier &lt;: MLJModelInterface.Probabilistic</code></pre><p>The classical perceptron algorithm using one-vs-all for multiclass, from the Beta Machine Learning Toolkit (BetaML).</p><p><strong>Hyperparameters:</strong></p><ul><li><p><code>initial_coefficients::Union{Nothing, Matrix{Float64}}</code>: N-classes by D-dimensions matrix of initial linear coefficients [def: <code>nothing</code>, i.e. zeros]</p></li><li><p><code>initial_constant::Union{Nothing, Vector{Float64}}</code>: N-classes vector of initial contant terms [def: <code>nothing</code>, i.e. zeros]</p></li><li><p><code>epochs::Int64</code>: Maximum number of epochs, i.e. passages trough the whole training sample [def: <code>1000</code>]</p></li><li><p><code>shuffle::Bool</code>: Whether to randomly shuffle the data at each iteration (epoch) [def: <code>true</code>]</p></li><li><p><code>force_origin::Bool</code>: Whether to force the parameter associated with the constant term to remain zero [def: <code>false</code>]</p></li><li><p><code>return_mean_hyperplane::Bool</code>: Whether to return the average hyperplane coefficients instead of the final ones  [def: <code>false</code>]</p></li><li><p><code>rng::Random.AbstractRNG</code>: A Random Number Generator to be used in stochastic parts of the code [deafult: <code>Random.GLOBAL_RNG</code>]</p></li></ul><p><strong>Example:</strong></p><pre><code class="language-julia hljs">julia&gt; using MLJ

julia&gt; X, y        = @load_iris;

julia&gt; modelType   = @load PerceptronClassifier pkg = &quot;BetaML&quot;
[ Info: For silent loading, specify `verbosity=0`. 
import BetaML ✔
BetaML.Perceptron.PerceptronClassifier

julia&gt; model       = modelType()
PerceptronClassifier(
  initial_coefficients = nothing, 
  initial_constant = nothing, 
  epochs = 1000, 
  shuffle = true, 
  force_origin = false, 
  return_mean_hyperplane = false, 
  rng = Random._GLOBAL_RNG())

julia&gt; mach        = machine(model, X, y);

julia&gt; fit!(mach);
[ Info: Training machine(PerceptronClassifier(initial_coefficients = nothing, …), …).
*** Avg. error after epoch 2 : 0.0 (all elements of the set has been correctly classified)
julia&gt; est_classes = predict(mach, X)
150-element CategoricalDistributions.UnivariateFiniteVector{Multiclass{3}, String, UInt8, Float64}:
 UnivariateFinite{Multiclass{3}}(setosa=&gt;1.0, versicolor=&gt;2.53e-34, virginica=&gt;0.0)
 UnivariateFinite{Multiclass{3}}(setosa=&gt;1.0, versicolor=&gt;1.27e-18, virginica=&gt;1.86e-310)
 ⋮
 UnivariateFinite{Multiclass{3}}(setosa=&gt;2.77e-57, versicolor=&gt;1.1099999999999999e-82, virginica=&gt;1.0)
 UnivariateFinite{Multiclass{3}}(setosa=&gt;3.09e-22, versicolor=&gt;4.03e-25, virginica=&gt;1.0)</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/sylvaticus/BetaML.jl/blob/f8f3bf703fd327fbebd7d330505b6b6516418385/src/Bmlj/Perceptron_mlj.jl#L10">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="BetaML.Bmlj.RandomForestClassifier" href="#BetaML.Bmlj.RandomForestClassifier"><code>BetaML.Bmlj.RandomForestClassifier</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">mutable struct RandomForestClassifier &lt;: MLJModelInterface.Probabilistic</code></pre><p>A simple Random Forest model for classification with support for Missing data, from the Beta Machine Learning Toolkit (BetaML).</p><p><strong>Hyperparameters:</strong></p><ul><li><p><code>n_trees::Int64</code></p></li><li><p><code>max_depth::Int64</code>: The maximum depth the tree is allowed to reach. When this is reached the node is forced to become a leaf [def: <code>0</code>, i.e. no limits]</p></li><li><p><code>min_gain::Float64</code>: The minimum information gain to allow for a node&#39;s partition [def: <code>0</code>]</p></li><li><p><code>min_records::Int64</code>: The minimum number of records a node must holds to consider for a partition of it [def: <code>2</code>]</p></li><li><p><code>max_features::Int64</code>: The maximum number of (random) features to consider at each partitioning [def: <code>0</code>, i.e. square root of the data dimensions]</p></li><li><p><code>splitting_criterion::Function</code>: This is the name of the function to be used to compute the information gain of a specific partition. This is done by measuring the difference betwwen the &quot;impurity&quot; of the labels of the parent node with those of the two child nodes, weighted by the respective number of items. [def: <code>gini</code>]. Either <code>gini</code>, <code>entropy</code> or a custom function. It can also be an anonymous function.</p></li><li><p><code>β::Float64</code>: Parameter that regulate the weights of the scoring of each tree, to be (optionally) used in prediction based on the error of the individual trees computed on the records on which trees have not been trained. Higher values favour &quot;better&quot; trees, but too high values will cause overfitting [def: <code>0</code>, i.e. uniform weigths]</p></li><li><p><code>rng::Random.AbstractRNG</code>: A Random Number Generator to be used in stochastic parts of the code [deafult: <code>Random.GLOBAL_RNG</code>]</p></li></ul><p><strong>Example :</strong></p><pre><code class="language-julia hljs">julia&gt; using MLJ

julia&gt; X, y        = @load_iris;

julia&gt; modelType   = @load RandomForestClassifier pkg = &quot;BetaML&quot; verbosity=0
BetaML.Trees.RandomForestClassifier

julia&gt; model       = modelType()
RandomForestClassifier(
  n_trees = 30, 
  max_depth = 0, 
  min_gain = 0.0, 
  min_records = 2, 
  max_features = 0, 
  splitting_criterion = BetaML.Utils.gini, 
  β = 0.0, 
  rng = Random._GLOBAL_RNG())

julia&gt; mach        = machine(model, X, y);

julia&gt; fit!(mach);
[ Info: Training machine(RandomForestClassifier(n_trees = 30, …), …).

julia&gt; cat_est    = predict(mach, X)
150-element CategoricalDistributions.UnivariateFiniteVector{Multiclass{3}, String, UInt32, Float64}:
 UnivariateFinite{Multiclass{3}}(setosa=&gt;1.0, versicolor=&gt;0.0, virginica=&gt;0.0)
 UnivariateFinite{Multiclass{3}}(setosa=&gt;1.0, versicolor=&gt;0.0, virginica=&gt;0.0)
 ⋮
 UnivariateFinite{Multiclass{3}}(setosa=&gt;0.0, versicolor=&gt;0.0, virginica=&gt;1.0)
 UnivariateFinite{Multiclass{3}}(setosa=&gt;0.0, versicolor=&gt;0.0667, virginica=&gt;0.933)</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/sylvaticus/BetaML.jl/blob/f8f3bf703fd327fbebd7d330505b6b6516418385/src/Bmlj/Trees_mlj.jl#L221">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="BetaML.Bmlj.RandomForestImputer" href="#BetaML.Bmlj.RandomForestImputer"><code>BetaML.Bmlj.RandomForestImputer</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">mutable struct RandomForestImputer &lt;: MLJModelInterface.Unsupervised</code></pre><p>Impute missing values using Random Forests, from the Beta Machine Learning Toolkit (BetaML).</p><p><strong>Hyperparameters:</strong></p><ul><li><p><code>n_trees::Int64</code>: Number of (decision) trees in the forest [def: <code>30</code>]</p></li><li><p><code>max_depth::Union{Nothing, Int64}</code>: The maximum depth the tree is allowed to reach. When this is reached the node is forced to become a leaf [def: <code>nothing</code>, i.e. no limits]</p></li><li><p><code>min_gain::Float64</code>: The minimum information gain to allow for a node&#39;s partition [def: <code>0</code>]</p></li><li><p><code>min_records::Int64</code>: The minimum number of records a node must holds to consider for a partition of it [def: <code>2</code>]</p></li><li><p><code>max_features::Union{Nothing, Int64}</code>: The maximum number of (random) features to consider at each partitioning [def: <code>nothing</code>, i.e. square root of the data dimension]</p></li><li><p><code>forced_categorical_cols::Vector{Int64}</code>: Specify the positions of the integer columns to treat as categorical instead of cardinal. [Default: empty vector (all numerical cols are treated as cardinal by default and the others as categorical)]</p></li><li><p><code>splitting_criterion::Union{Nothing, Function}</code>: Either <code>gini</code>, <code>entropy</code> or <code>variance</code>. This is the name of the function to be used to compute the information gain of a specific partition. This is done by measuring the difference betwwen the &quot;impurity&quot; of the labels of the parent node with those of the two child nodes, weighted by the respective number of items. [def: <code>nothing</code>, i.e. <code>gini</code> for categorical labels (classification task) and <code>variance</code> for numerical labels(regression task)]. It can be an anonymous function.</p></li><li><p><code>recursive_passages::Int64</code>: Define the times to go trough the various columns to impute their data. Useful when there are data to impute on multiple columns. The order of the first passage is given by the decreasing number of missing values per column, the other passages are random [default: <code>1</code>].</p></li><li><p><code>rng::Random.AbstractRNG</code>: A Random Number Generator to be used in stochastic parts of the code [deafult: <code>Random.GLOBAL_RNG</code>]</p></li></ul><p><strong>Example:</strong></p><pre><code class="language-julia hljs">julia&gt; using MLJ

julia&gt; X = [1 10.5;1.5 missing; 1.8 8; 1.7 15; 3.2 40; missing missing; 3.3 38; missing -2.3; 5.2 -2.4] |&gt; table ;

julia&gt; modelType   = @load RandomForestImputer  pkg = &quot;BetaML&quot; verbosity=0
BetaML.Imputation.RandomForestImputer

julia&gt; model     = modelType(n_trees=40)
RandomForestImputer(
  n_trees = 40, 
  max_depth = nothing, 
  min_gain = 0.0, 
  min_records = 2, 
  max_features = nothing, 
  forced_categorical_cols = Int64[], 
  splitting_criterion = nothing, 
  recursive_passages = 1, 
  rng = Random._GLOBAL_RNG())

julia&gt; mach      = machine(model, X);

julia&gt; fit!(mach);
[ Info: Training machine(RandomForestImputer(n_trees = 40, …), …).

julia&gt; X_full       = transform(mach) |&gt; MLJ.matrix
9×2 Matrix{Float64}:
 1.0      10.5
 1.5      10.3909
 1.8       8.0
 1.7      15.0
 3.2      40.0
 2.88375   8.66125
 3.3      38.0
 3.98125  -2.3
 5.2      -2.4</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/sylvaticus/BetaML.jl/blob/f8f3bf703fd327fbebd7d330505b6b6516418385/src/Bmlj/Imputation_mlj.jl#L150">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="BetaML.Bmlj.RandomForestRegressor" href="#BetaML.Bmlj.RandomForestRegressor"><code>BetaML.Bmlj.RandomForestRegressor</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">mutable struct RandomForestRegressor &lt;: MLJModelInterface.Deterministic</code></pre><p>A simple Random Forest model for regression with support for Missing data, from the Beta Machine Learning Toolkit (BetaML).</p><p><strong>Hyperparameters:</strong></p><ul><li><p><code>n_trees::Int64</code>: Number of (decision) trees in the forest [def: <code>30</code>]</p></li><li><p><code>max_depth::Int64</code>: The maximum depth the tree is allowed to reach. When this is reached the node is forced to become a leaf [def: <code>0</code>, i.e. no limits]</p></li><li><p><code>min_gain::Float64</code>: The minimum information gain to allow for a node&#39;s partition [def: <code>0</code>]</p></li><li><p><code>min_records::Int64</code>: The minimum number of records a node must holds to consider for a partition of it [def: <code>2</code>]</p></li><li><p><code>max_features::Int64</code>: The maximum number of (random) features to consider at each partitioning [def: <code>0</code>, i.e. square root of the data dimension]</p></li><li><p><code>splitting_criterion::Function</code>: This is the name of the function to be used to compute the information gain of a specific partition. This is done by measuring the difference betwwen the &quot;impurity&quot; of the labels of the parent node with those of the two child nodes, weighted by the respective number of items. [def: <code>variance</code>]. Either <code>variance</code> or a custom function. It can also be an anonymous function.</p></li><li><p><code>β::Float64</code>: Parameter that regulate the weights of the scoring of each tree, to be (optionally) used in prediction based on the error of the individual trees computed on the records on which trees have not been trained. Higher values favour &quot;better&quot; trees, but too high values will cause overfitting [def: <code>0</code>, i.e. uniform weigths]</p></li><li><p><code>rng::Random.AbstractRNG</code>: A Random Number Generator to be used in stochastic parts of the code [deafult: <code>Random.GLOBAL_RNG</code>]</p></li></ul><p><strong>Example:</strong></p><pre><code class="language-julia hljs">julia&gt; using MLJ

julia&gt; X, y        = @load_boston;

julia&gt; modelType   = @load RandomForestRegressor pkg = &quot;BetaML&quot; verbosity=0
BetaML.Trees.RandomForestRegressor

julia&gt; model       = modelType()
RandomForestRegressor(
  n_trees = 30, 
  max_depth = 0, 
  min_gain = 0.0, 
  min_records = 2, 
  max_features = 0, 
  splitting_criterion = BetaML.Utils.variance, 
  β = 0.0, 
  rng = Random._GLOBAL_RNG())

julia&gt; mach        = machine(model, X, y);

julia&gt; fit!(mach);
[ Info: Training machine(RandomForestRegressor(n_trees = 30, …), …).

julia&gt; ŷ           = predict(mach, X);

julia&gt; hcat(y,ŷ)
506×2 Matrix{Float64}:
 24.0  25.8433
 21.6  22.4317
 34.7  35.5742
 33.4  33.9233
  ⋮    
 23.9  24.42
 22.0  22.4433
 11.9  15.5833</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/sylvaticus/BetaML.jl/blob/f8f3bf703fd327fbebd7d330505b6b6516418385/src/Bmlj/Trees_mlj.jl#L145">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="BetaML.Bmlj.SimpleImputer" href="#BetaML.Bmlj.SimpleImputer"><code>BetaML.Bmlj.SimpleImputer</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">mutable struct SimpleImputer &lt;: MLJModelInterface.Unsupervised</code></pre><p>Impute missing values using feature (column) mean, with optional record normalisation (using l-<code>norm</code> norms), from the Beta Machine Learning Toolkit (BetaML).</p><p><strong>Hyperparameters:</strong></p><ul><li><p><code>statistic::Function</code>: The descriptive statistic of the column (feature) to use as imputed value [def: <code>mean</code>]</p></li><li><p><code>norm::Union{Nothing, Int64}</code>: Normalise the feature mean by l-<code>norm</code> norm of the records [default: <code>nothing</code>]. Use it (e.g. <code>norm=1</code> to use the l-1 norm) if the records are highly heterogeneus (e.g. quantity exports of different countries).</p></li></ul><p><strong>Example:</strong></p><pre><code class="language-julia hljs">julia&gt; using MLJ

julia&gt; X = [1 10.5;1.5 missing; 1.8 8; 1.7 15; 3.2 40; missing missing; 3.3 38; missing -2.3; 5.2 -2.4] |&gt; table ;

julia&gt; modelType   = @load SimpleImputer  pkg = &quot;BetaML&quot; verbosity=0
BetaML.Imputation.SimpleImputer

julia&gt; model     = modelType(norm=1)
SimpleImputer(
  statistic = Statistics.mean, 
  norm = 1)

julia&gt; mach      = machine(model, X);

julia&gt; fit!(mach);
[ Info: Training machine(SimpleImputer(statistic = mean, …), …).

julia&gt; X_full       = transform(mach) |&gt; MLJ.matrix
9×2 Matrix{Float64}:
 1.0        10.5
 1.5         0.295466
 1.8         8.0
 1.7        15.0
 3.2        40.0
 0.280952    1.69524
 3.3        38.0
 0.0750839  -2.3
 5.2        -2.4</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/sylvaticus/BetaML.jl/blob/f8f3bf703fd327fbebd7d330505b6b6516418385/src/Bmlj/Imputation_mlj.jl#L7">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="BetaML.Bmlj.mljverbosity_to_betaml_verbosity-Tuple{Integer}" href="#BetaML.Bmlj.mljverbosity_to_betaml_verbosity-Tuple{Integer}"><code>BetaML.Bmlj.mljverbosity_to_betaml_verbosity</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">mljverbosity_to_betaml_verbosity(i::Integer) -&gt; Verbosity
</code></pre><p>Convert any integer (short scale) to one of the defined betaml verbosity levels Currently &quot;steps&quot; are 0, 1, 2 and 3</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/sylvaticus/BetaML.jl/blob/f8f3bf703fd327fbebd7d330505b6b6516418385/src/Bmlj/Bmlj.jl#L35">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="MLJModelInterface.fit-Tuple{BetaML.Bmlj.AutoEncoder, Any, Any}" href="#MLJModelInterface.fit-Tuple{BetaML.Bmlj.AutoEncoder, Any, Any}"><code>MLJModelInterface.fit</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">fit(
    m::BetaML.Bmlj.AutoEncoder,
    verbosity,
    X
) -&gt; Tuple{AutoEncoder, Nothing, Nothing}
</code></pre><p>For the <code>verbosity</code> parameter see <a href="Api.html#BetaML.Api.Verbosity"><code>Verbosity</code></a>)</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/sylvaticus/BetaML.jl/blob/f8f3bf703fd327fbebd7d330505b6b6516418385/src/Bmlj/Utils_mlj.jl#L133">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="MLJModelInterface.fit-Tuple{BetaML.Bmlj.MultitargetNeuralNetworkRegressor, Any, Any, Any}" href="#MLJModelInterface.fit-Tuple{BetaML.Bmlj.MultitargetNeuralNetworkRegressor, Any, Any, Any}"><code>MLJModelInterface.fit</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">fit(
    m::BetaML.Bmlj.MultitargetNeuralNetworkRegressor,
    verbosity,
    X,
    y
) -&gt; Tuple{NeuralNetworkEstimator, Nothing, Nothing}
</code></pre><p>For the <code>verbosity</code> parameter see <a href="Api.html#BetaML.Api.Verbosity"><code>Verbosity</code></a>)</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/sylvaticus/BetaML.jl/blob/f8f3bf703fd327fbebd7d330505b6b6516418385/src/Bmlj/Nn_mlj.jl#L204">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="MLJModelInterface.fit-Tuple{BetaML.Bmlj.NeuralNetworkClassifier, Any, Any, Any}" href="#MLJModelInterface.fit-Tuple{BetaML.Bmlj.NeuralNetworkClassifier, Any, Any, Any}"><code>MLJModelInterface.fit</code></a> — <span class="docstring-category">Method</span></header><section><div><p>MMI.fit(model::NeuralNetworkClassifier, verbosity, X, y)</p><p>For the <code>verbosity</code> parameter see <a href="Api.html#BetaML.Api.Verbosity"><code>Verbosity</code></a>)</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/sylvaticus/BetaML.jl/blob/f8f3bf703fd327fbebd7d330505b6b6516418385/src/Bmlj/Nn_mlj.jl#L318-L323">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="MLJModelInterface.fit-Tuple{BetaML.Bmlj.NeuralNetworkRegressor, Any, Any, Any}" href="#MLJModelInterface.fit-Tuple{BetaML.Bmlj.NeuralNetworkRegressor, Any, Any, Any}"><code>MLJModelInterface.fit</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">fit(
    m::BetaML.Bmlj.NeuralNetworkRegressor,
    verbosity,
    X,
    y
) -&gt; Tuple{NeuralNetworkEstimator, Nothing, Nothing}
</code></pre><p>For the <code>verbosity</code> parameter see <a href="Api.html#BetaML.Api.Verbosity"><code>Verbosity</code></a>)</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/sylvaticus/BetaML.jl/blob/f8f3bf703fd327fbebd7d330505b6b6516418385/src/Bmlj/Nn_mlj.jl#L91">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="MLJModelInterface.predict-Tuple{Union{BetaML.Bmlj.KMeansClusterer, BetaML.Bmlj.KMedoidsClusterer}, Any, Any}" href="#MLJModelInterface.predict-Tuple{Union{BetaML.Bmlj.KMeansClusterer, BetaML.Bmlj.KMedoidsClusterer}, Any, Any}"><code>MLJModelInterface.predict</code></a> — <span class="docstring-category">Method</span></header><section><div><p>predict(m::KMeansClusterer, fitResults, X) - Given a fitted clustering model and some observations, predict the class of the observation</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/sylvaticus/BetaML.jl/blob/f8f3bf703fd327fbebd7d330505b6b6516418385/src/Bmlj/Clustering_mlj.jl#L197">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="MLJModelInterface.transform-Tuple{BetaML.Bmlj.GeneralImputer, Any, Any}" href="#MLJModelInterface.transform-Tuple{BetaML.Bmlj.GeneralImputer, Any, Any}"><code>MLJModelInterface.transform</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">transform(m, fitResults, X)</code></pre><p>Given a trained imputator model fill the missing data of some new observations. Note that with multiple recursive imputations and inner estimators that don&#39;t support missing data, this function works only for X for which th model has been trained with, i.e. this function can not be applied to new matrices with empty values using model trained on other matrices.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/sylvaticus/BetaML.jl/blob/f8f3bf703fd327fbebd7d330505b6b6516418385/src/Bmlj/Imputation_mlj.jl#L461-L466">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="MLJModelInterface.transform-Tuple{Union{BetaML.Bmlj.GaussianMixtureImputer, BetaML.Bmlj.RandomForestImputer, BetaML.Bmlj.SimpleImputer}, Any, Any}" href="#MLJModelInterface.transform-Tuple{Union{BetaML.Bmlj.GaussianMixtureImputer, BetaML.Bmlj.RandomForestImputer, BetaML.Bmlj.SimpleImputer}, Any, Any}"><code>MLJModelInterface.transform</code></a> — <span class="docstring-category">Method</span></header><section><div><p>transform(m, fitResults, X) - Given a trained imputator model fill the missing data of some new observations</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/sylvaticus/BetaML.jl/blob/f8f3bf703fd327fbebd7d330505b6b6516418385/src/Bmlj/Imputation_mlj.jl#L453">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="MLJModelInterface.transform-Tuple{Union{BetaML.Bmlj.KMeansClusterer, BetaML.Bmlj.KMedoidsClusterer}, Any, Any}" href="#MLJModelInterface.transform-Tuple{Union{BetaML.Bmlj.KMeansClusterer, BetaML.Bmlj.KMedoidsClusterer}, Any, Any}"><code>MLJModelInterface.transform</code></a> — <span class="docstring-category">Method</span></header><section><div><p>fit(m::KMeansClusterer, fitResults, X) - Given a fitted clustering model and some observations, return the distances to each centroids </p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/sylvaticus/BetaML.jl/blob/f8f3bf703fd327fbebd7d330505b6b6516418385/src/Bmlj/Clustering_mlj.jl#L180">source</a></section></article></article><nav class="docs-footer"><a class="docs-footer-prevpage" href="Utils.html">« Utils</a><div class="flexbox-break"></div><p class="footer-message">Powered by <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> and the <a href="https://julialang.org/">Julia Programming Language</a>.</p></nav></div><div class="modal" id="documenter-settings"><div class="modal-background"></div><div class="modal-card"><header class="modal-card-head"><p class="modal-card-title">Settings</p><button class="delete"></button></header><section class="modal-card-body"><p><label class="label">Theme</label><div class="select"><select id="documenter-themepicker"><option value="documenter-light">documenter-light</option><option value="documenter-dark">documenter-dark</option><option value="auto">Automatic (OS)</option></select></div></p><hr/><p>This document was generated with <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> version 1.2.1 on <span class="colophon-date" title="Friday 26 January 2024 10:56">Friday 26 January 2024</span>. Using Julia version 1.6.7.</p></section><footer class="modal-card-foot"></footer></div></div></div></body></html>
