<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>Nn · BetaML.jl Documentation</title><script async src="https://www.googletagmanager.com/gtag/js?id=G-JYKX8QY5JW"></script><script>  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());
  gtag('config', 'G-JYKX8QY5JW', {'page_path': location.pathname + location.search + location.hash});
</script><script data-outdated-warner src="assets/warner.js"></script><link href="https://cdnjs.cloudflare.com/ajax/libs/lato-font/3.0.0/css/lato-font.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/juliamono/0.045/juliamono.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.4/css/fontawesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.4/css/solid.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.4/css/brands.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.24/katex.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL="."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" data-main="assets/documenter.js"></script><script src="siteinfo.js"></script><script src="../versions.js"></script><link class="docs-theme-link" rel="stylesheet" type="text/css" href="assets/themes/documenter-dark.css" data-theme-name="documenter-dark" data-theme-primary-dark/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="assets/themes/documenter-light.css" data-theme-name="documenter-light" data-theme-primary/><script src="assets/themeswap.js"></script></head><body><div id="documenter"><nav class="docs-sidebar"><div class="docs-package-name"><span class="docs-autofit"><a href="index.html">BetaML.jl Documentation</a></span></div><form class="docs-search" action="search.html"><input class="docs-search-query" id="documenter-search-query" name="q" type="text" placeholder="Search docs"/></form><ul class="docs-menu"><li><a class="tocitem" href="index.html">Index</a></li><li><span class="tocitem">Tutorial</span><ul><li><a class="tocitem" href="tutorials/Betaml_tutorial_getting_started.html">Getting started</a></li><li><input class="collapse-toggle" id="menuitem-2-2" type="checkbox"/><label class="tocitem" for="menuitem-2-2"><span class="docs-label">Classification - cars</span><i class="docs-chevron"></i></label><ul class="collapsed"><li><a class="tocitem" href="tutorials/Classification - cars/betaml_tutorial_classification_cars.html">A classification task when labels are known - determining the country of origin of cars given the cars characteristics</a></li></ul></li><li><input class="collapse-toggle" id="menuitem-2-3" type="checkbox"/><label class="tocitem" for="menuitem-2-3"><span class="docs-label">Regression - bike sharing</span><i class="docs-chevron"></i></label><ul class="collapsed"><li><a class="tocitem" href="tutorials/Regression - bike sharing/betaml_tutorial_regression_sharingBikes.html">A regression task: the prediction of  bike  sharing demand</a></li></ul></li><li><input class="collapse-toggle" id="menuitem-2-4" type="checkbox"/><label class="tocitem" for="menuitem-2-4"><span class="docs-label">Clustering - Iris</span><i class="docs-chevron"></i></label><ul class="collapsed"><li><a class="tocitem" href="tutorials/Clustering - Iris/betaml_tutorial_cluster_iris.html">A clustering task: the prediction of  plant species from floreal measures (the iris dataset)</a></li></ul></li></ul></li><li><span class="tocitem">API (Reference manual)</span><ul><li><input class="collapse-toggle" id="menuitem-3-1" type="checkbox"/><label class="tocitem" for="menuitem-3-1"><span class="docs-label">API V2 (current)</span><i class="docs-chevron"></i></label><ul class="collapsed"><li><a class="tocitem" href="Api_v2_user.html">Introduction for user</a></li><li><input class="collapse-toggle" id="menuitem-3-1-2" type="checkbox"/><label class="tocitem" for="menuitem-3-1-2"><span class="docs-label">For developers</span><i class="docs-chevron"></i></label><ul class="collapsed"><li><a class="tocitem" href="Api_v2_developer.html">API implementation</a></li><li><a class="tocitem" href="StyleGuide_templates.html">Style guide</a></li></ul></li><li><a class="tocitem" href="Api.html">The Api module</a></li></ul></li><li><a class="tocitem" href="Perceptron.html">Perceptron</a></li><li><a class="tocitem" href="Trees.html">Trees</a></li><li class="is-active"><a class="tocitem" href="Nn.html">Nn</a><ul class="internal"><li><a class="tocitem" href="#Module-Index"><span>Module Index</span></a></li><li><a class="tocitem" href="#Detailed-API"><span>Detailed API</span></a></li></ul></li><li><a class="tocitem" href="Clustering.html">Clustering</a></li><li><a class="tocitem" href="GMM.html">GMM</a></li><li><a class="tocitem" href="Imputation.html">Imputation</a></li><li><a class="tocitem" href="Utils.html">Utils</a></li></ul></li></ul><div class="docs-version-selector field has-addons"><div class="control"><span class="docs-label button is-static is-size-7">Version</span></div><div class="docs-selector control is-expanded"><div class="select is-fullwidth is-size-7"><select id="documenter-version-selector"></select></div></div></div></nav><div class="docs-main"><header class="docs-navbar"><nav class="breadcrumb"><ul class="is-hidden-mobile"><li><a class="is-disabled">API (Reference manual)</a></li><li class="is-active"><a href="Nn.html">Nn</a></li></ul><ul class="is-hidden-tablet"><li class="is-active"><a href="Nn.html">Nn</a></li></ul></nav><div class="docs-right"><a class="docs-edit-link" href="https://github.com/sylvaticus/BetaML.jl/blob/master/docs/src/Nn.md" title="Edit on GitHub"><span class="docs-icon fab"></span><span class="docs-label is-hidden-touch">Edit on GitHub</span></a><a class="docs-settings-button fas fa-cog" id="documenter-settings-button" href="#" title="Settings"></a><a class="docs-sidebar-button fa fa-bars is-hidden-desktop" id="documenter-sidebar-button" href="#"></a></div></header><article class="content" id="documenter-page"><h1 id="nn_module"><a class="docs-heading-anchor" href="#nn_module">The BetaML.Nn Module</a><a id="nn_module-1"></a><a class="docs-heading-anchor-permalink" href="#nn_module" title="Permalink"></a></h1><article class="docstring"><header><a class="docstring-binding" id="BetaML.Nn" href="#BetaML.Nn"><code>BetaML.Nn</code></a> — <span class="docstring-category">Module</span></header><section><div><pre><code class="language-julia hljs">BetaML.Nn module</code></pre><p>Implement the functionality required to define an artificial Neural Network, train it with data, forecast data and assess its performances.</p><p>Common type of layers and optimisation algorithms are already provided, but you can define your own ones subclassing respectively the <code>AbstractLayer</code> and <code>OptimisationAlgorithm</code> abstract types.</p><p>The module provide the following types or functions. Use <code>?[type or function]</code> to access their full signature and detailed documentation:</p><p><strong>Model definition:</strong></p><ul><li><code>DenseLayer</code>: Classical feed-forward layer with user-defined activation function</li><li><code>DenseNoBiasLayer</code>: Classical layer without the bias parameter</li><li><code>VectorFunctionLayer</code>: Parameterless layer whose activation function run over the ensable of its nodes rather than on each one individually</li><li><code>NeuralNetworkEstimator</code>: Build the chained network and define a cost function</li></ul><p>Each layer can use a default activation function, one of the functions provided in the <code>Utils</code> module (<code>relu</code>, <code>tanh</code>, <code>softmax</code>,...) or you can specify your own function. The derivative of the activation function can be optionally be provided, in such case training will be quicker, altought this difference tends to vanish with bigger datasets. You can alternativly implement your own layer defining a new type as subtype of the abstract type <code>AbstractLayer</code>. Each user-implemented layer must define the following methods:</p><ul><li>A suitable constructor</li><li><code>forward(layer,x)</code></li><li><code>backward(layer,x,next_gradient)</code></li><li><code>get_params(layer)</code></li><li><code>get_gradient(layer,x,next_gradient)</code></li><li><code>set_params!(layer,w)</code></li><li><code>size(layer)</code></li></ul><p><strong>Model fitting:</strong></p><ul><li><code>fit!(nn,X,Y)</code>:  fitting function</li><li><code>fitting_info(nn)</code>: Default callback function during fitting</li><li><code>SGD</code>:  The classical optimisation algorithm</li><li><code>ADAM</code>: A faster moment-based optimisation algorithm </li></ul><p>To define your own optimisation algorithm define a subtype of <code>OptimisationAlgorithm</code> and implement the function <code>single_update!(θ,▽;opt_alg)</code> and eventually <code>init_optalg!(⋅)</code> specific for it.</p><p><strong>Model predictions and assessment:</strong></p><ul><li><code>predict(nn)</code> or <code>predict(nn,X)</code>: Return the output given the data</li></ul><p>While high-level functions operating on the dataset expect it to be in the standard format (n<em>records × n</em>dimensions matrices) it is customary to represent the chain of a neural network as a flow of column vectors, so all low-level operations (operating on a single datapoint) expect both the input and the output as a column vector.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/sylvaticus/BetaML.jl/blob/a758127e0ec2c69cc6d6bf1f2b37a4db462b4b58/src/Nn/Nn.jl#L4-L47">source</a></section></article><h2 id="Module-Index"><a class="docs-heading-anchor" href="#Module-Index">Module Index</a><a id="Module-Index-1"></a><a class="docs-heading-anchor-permalink" href="#Module-Index" title="Permalink"></a></h2><ul><li><a href="Nn.html#BetaML.Nn.ADAM"><code>BetaML.Nn.ADAM</code></a></li><li><a href="Nn.html#BetaML.Nn.ConvLayer-Tuple{Any, Any, Any}"><code>BetaML.Nn.ConvLayer</code></a></li><li><a href="Nn.html#BetaML.Nn.ConvLayer"><code>BetaML.Nn.ConvLayer</code></a></li><li><a href="Nn.html#BetaML.Nn.DenseLayer"><code>BetaML.Nn.DenseLayer</code></a></li><li><a href="Nn.html#BetaML.Nn.DenseNoBiasLayer"><code>BetaML.Nn.DenseNoBiasLayer</code></a></li><li><a href="Nn.html#BetaML.Nn.Learnable"><code>BetaML.Nn.Learnable</code></a></li><li><a href="Nn.html#BetaML.Nn.MultitargetNeuralNetworkRegressor"><code>BetaML.Nn.MultitargetNeuralNetworkRegressor</code></a></li><li><a href="Nn.html#BetaML.Nn.NNHyperParametersSet"><code>BetaML.Nn.NNHyperParametersSet</code></a></li><li><a href="Nn.html#BetaML.Nn.NeuralNetworkClassifier"><code>BetaML.Nn.NeuralNetworkClassifier</code></a></li><li><a href="Nn.html#BetaML.Nn.NeuralNetworkEstimator"><code>BetaML.Nn.NeuralNetworkEstimator</code></a></li><li><a href="Nn.html#BetaML.Nn.NeuralNetworkEstimatorOptionsSet"><code>BetaML.Nn.NeuralNetworkEstimatorOptionsSet</code></a></li><li><a href="Nn.html#BetaML.Nn.NeuralNetworkRegressor"><code>BetaML.Nn.NeuralNetworkRegressor</code></a></li><li><a href="Nn.html#BetaML.Nn.PoolingLayer"><code>BetaML.Nn.PoolingLayer</code></a></li><li><a href="Nn.html#BetaML.Nn.PoolingLayer-Tuple{Any, Any}"><code>BetaML.Nn.PoolingLayer</code></a></li><li><a href="Nn.html#BetaML.Nn.ReshaperLayer"><code>BetaML.Nn.ReshaperLayer</code></a></li><li><a href="Nn.html#BetaML.Nn.SGD"><code>BetaML.Nn.SGD</code></a></li><li><a href="Nn.html#BetaML.Nn.ScalarFunctionLayer"><code>BetaML.Nn.ScalarFunctionLayer</code></a></li><li><a href="Nn.html#BetaML.Nn.VectorFunctionLayer"><code>BetaML.Nn.VectorFunctionLayer</code></a></li><li><a href="Nn.html#BetaML.Nn.backward-Tuple{AbstractLayer, Any, Any}"><code>BetaML.Nn.backward</code></a></li><li><a href="Nn.html#BetaML.Nn.fitting_info-Tuple{Any, Any, Any}"><code>BetaML.Nn.fitting_info</code></a></li><li><a href="Nn.html#BetaML.Nn.forward-Tuple{PoolingLayer, Any}"><code>BetaML.Nn.forward</code></a></li><li><a href="Nn.html#BetaML.Nn.forward-Tuple{AbstractLayer, Any}"><code>BetaML.Nn.forward</code></a></li><li><a href="Nn.html#BetaML.Nn.forward-Tuple{ConvLayer, Any}"><code>BetaML.Nn.forward</code></a></li><li><a href="Nn.html#BetaML.Nn.get_gradient-Union{Tuple{N2}, Tuple{N1}, Tuple{T2}, Tuple{T}, Tuple{BetaML.Nn.NN, Union{AbstractArray{T, N1}, T}, Union{AbstractArray{T2, N2}, T2}}} where {T&lt;:Number, T2&lt;:Number, N1, N2}"><code>BetaML.Nn.get_gradient</code></a></li><li><a href="Nn.html#BetaML.Nn.get_gradient-Tuple{AbstractLayer, Any, Any}"><code>BetaML.Nn.get_gradient</code></a></li><li><a href="Nn.html#BetaML.Nn.get_params-Tuple{BetaML.Nn.NN}"><code>BetaML.Nn.get_params</code></a></li><li><a href="Nn.html#BetaML.Nn.get_params-Tuple{AbstractLayer}"><code>BetaML.Nn.get_params</code></a></li><li><a href="Nn.html#BetaML.Nn.init_optalg!-Tuple{ADAM}"><code>BetaML.Nn.init_optalg!</code></a></li><li><a href="Nn.html#BetaML.Nn.init_optalg!-Tuple{BetaML.Nn.OptimisationAlgorithm}"><code>BetaML.Nn.init_optalg!</code></a></li><li><a href="Nn.html#BetaML.Nn.preprocess!-Tuple{AbstractLayer}"><code>BetaML.Nn.preprocess!</code></a></li><li><a href="Nn.html#BetaML.Nn.set_params!-Tuple{AbstractLayer, Any}"><code>BetaML.Nn.set_params!</code></a></li><li><a href="Nn.html#BetaML.Nn.set_params!-Tuple{BetaML.Nn.NN, Any}"><code>BetaML.Nn.set_params!</code></a></li><li><a href="Nn.html#BetaML.Nn.single_update!-Tuple{Any, Any}"><code>BetaML.Nn.single_update!</code></a></li></ul><h2 id="Detailed-API"><a class="docs-heading-anchor" href="#Detailed-API">Detailed API</a><a id="Detailed-API-1"></a><a class="docs-heading-anchor-permalink" href="#Detailed-API" title="Permalink"></a></h2><article class="docstring"><header><a class="docstring-binding" id="BetaML.Nn.ADAM" href="#BetaML.Nn.ADAM"><code>BetaML.Nn.ADAM</code></a> — <span class="docstring-category">Type</span></header><section><div><p>ADAM(;η, λ, β₁, β₂, ϵ)</p><p>The <a href="https://arxiv.org/pdf/1412.6980.pdf">ADAM</a> algorithm, an adaptive moment estimation optimiser.</p><p><strong>Fields:</strong></p><ul><li><code>η</code>:  Learning rate (stepsize, α in the paper), as a function of the current epoch [def: t -&gt; 0.001 (i.e. fixed)]</li><li><code>λ</code>:  Multiplicative constant to the learning rate [def: 1]</li><li><code>β₁</code>: Exponential decay rate for the first moment estimate [range: ∈ [0,1], def: 0.9]</li><li><code>β₂</code>: Exponential decay rate for the second moment estimate [range: ∈ [0,1], def: 0.999]</li><li><code>ϵ</code>:  Epsilon value to avoid division by zero [def: 10^-8]</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/sylvaticus/BetaML.jl/blob/a758127e0ec2c69cc6d6bf1f2b37a4db462b4b58/src/Nn/Nn_default_optalgs.jl#L47-L58">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="BetaML.Nn.ConvLayer" href="#BetaML.Nn.ConvLayer"><code>BetaML.Nn.ConvLayer</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">mutable struct ConvLayer{ND, NDPLUS1, NDPLUS2} &lt;: AbstractLayer</code></pre><p>Representation of a convolutional layer in the network</p><p><strong>Fields:</strong></p><ul><li><p><code>input_size::StaticArraysCore.SVector{NDPLUS1, Int64} where NDPLUS1</code>: Input size (including nchannel_in as last dimension)</p></li><li><p><code>weight::Array{Float64, NDPLUS2} where NDPLUS2</code>: Weight tensor (aka &quot;filter&quot; or &quot;kernel&quot;) with respect to the input from previous layer or data (kernel<em>size array augmented by the nchannels</em>in and nchannels_out dimensions)</p></li><li><p><code>usebias::Bool</code>: Wether to use (and learn) a bias weigth [def: true]</p></li><li><p><code>bias::Vector{Float64}</code>: Bias (nchannels_out array)</p></li><li><p><code>padding_start::StaticArraysCore.SVector{ND, Int64} where ND</code>: Padding (initial)</p></li><li><p><code>padding_end::StaticArraysCore.SVector{ND, Int64} where ND</code>: Padding (ending)</p></li><li><p><code>stride::StaticArraysCore.SVector{ND, Int64} where ND</code>: Stride</p></li><li><p><code>ndims::Int64</code>: Number of dimensions (excluding input and output channels)</p></li><li><p><code>f::Function</code>: Activation function</p></li><li><p><code>df::Union{Nothing, Function}</code>: Derivative of the activation function</p></li><li><p><code>x_ids::Array{Tuple{Vararg{Int32, NDPLUS1}}, 1} where NDPLUS1</code>: x ids of the convolution (computed in <code>preprocessing</code><code>- itself at the beginning of</code>train`</p></li><li><p><code>y_ids::Array{Tuple{Vararg{Int32, NDPLUS1}}, 1} where NDPLUS1</code>: y ids of the convolution (computed in <code>preprocessing</code><code>- itself at the beginning of</code>train`</p></li><li><p><code>w_ids::Array{Tuple{Vararg{Int32, NDPLUS2}}, 1} where NDPLUS2</code>: w ids of the convolution (computed in <code>preprocessing</code><code>- itself at the beginning of</code>train`</p></li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/sylvaticus/BetaML.jl/blob/a758127e0ec2c69cc6d6bf1f2b37a4db462b4b58/src/Nn/default_layers/ConvLayer.jl#L5">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="BetaML.Nn.ConvLayer-Tuple{Any, Any, Any}" href="#BetaML.Nn.ConvLayer-Tuple{Any, Any, Any}"><code>BetaML.Nn.ConvLayer</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">ConvLayer(
    input_size_with_channel,
    kernel_size,
    nchannels_out;
    stride,
    rng,
    padding,
    kernel_init,
    usebias,
    bias_init,
    f,
    df
) -&gt; ConvLayer{_A, _B, _C} where {_A, _B, _C}
</code></pre><p>Alternative constructor for a <code>ConvLayer</code> where the number of channels in input is specified as a further dimension in the input size instead of as a separate parameter, so to use <code>size(previous_layer)[2]</code> if one wish.</p><p>For arguments and default values see the documentation of the main constructor.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/sylvaticus/BetaML.jl/blob/a758127e0ec2c69cc6d6bf1f2b37a4db462b4b58/src/Nn/default_layers/ConvLayer.jl#L117">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="BetaML.Nn.DenseLayer" href="#BetaML.Nn.DenseLayer"><code>BetaML.Nn.DenseLayer</code></a> — <span class="docstring-category">Type</span></header><section><div><p>DenseLayer</p><p>Representation of a layer in the network</p><p><strong>Fields:</strong></p><ul><li><code>w</code>:  Weigths matrix with respect to the input from previous layer or data (n x n pr. layer)</li><li><code>wb</code>: Biases (n)</li><li><code>f</code>:  Activation function</li><li><code>df</code>: Derivative of the activation function</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/sylvaticus/BetaML.jl/blob/a758127e0ec2c69cc6d6bf1f2b37a4db462b4b58/src/Nn/default_layers/DenseLayer.jl#L4-L14">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="BetaML.Nn.DenseNoBiasLayer" href="#BetaML.Nn.DenseNoBiasLayer"><code>BetaML.Nn.DenseNoBiasLayer</code></a> — <span class="docstring-category">Type</span></header><section><div><p>DenseNoBiasLayer</p><p>Representation of a layer without bias in the network</p><p><strong>Fields:</strong></p><ul><li><code>w</code>:  Weigths matrix with respect to the input from previous layer or data (n x n pr. layer)</li><li><code>f</code>:  Activation function</li><li><code>df</code>: Derivative of the activation function</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/sylvaticus/BetaML.jl/blob/a758127e0ec2c69cc6d6bf1f2b37a4db462b4b58/src/Nn/default_layers/DenseNoBiasLayer.jl#L4-L13">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="BetaML.Nn.Learnable" href="#BetaML.Nn.Learnable"><code>BetaML.Nn.Learnable</code></a> — <span class="docstring-category">Type</span></header><section><div><p>Learnable(data)</p><p>Structure representing the learnable parameters of a layer or its gradient.</p><p>The learnable parameters of a layers are given in the form of a N-tuple of Array{Float64,N2} where N2 can change (e.g. we can have a layer with the first parameter being a matrix, and the second one being a scalar). We wrap the tuple on its own structure a bit for some efficiency gain, but above all to define standard mathematic operations on the gradients without doing &quot;type piracy&quot; with respect to Base tuples.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/sylvaticus/BetaML.jl/blob/a758127e0ec2c69cc6d6bf1f2b37a4db462b4b58/src/Nn/Nn.jl#L76-L83">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="BetaML.Nn.MultitargetNeuralNetworkRegressor" href="#BetaML.Nn.MultitargetNeuralNetworkRegressor"><code>BetaML.Nn.MultitargetNeuralNetworkRegressor</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">mutable struct MultitargetNeuralNetworkRegressor &lt;: MLJModelInterface.Deterministic</code></pre><p>A simple but flexible Feedforward Neural Network, from the Beta Machine Learning Toolkit (BetaML) for regression of multiple dimensional targets.</p><p><strong>Parameters:</strong></p><ul><li><p><code>layers</code>: Array of layer objects [def: <code>nothing</code>, i.e. basic network]. See <code>subtypes(BetaML.AbstractLayer)</code> for supported layers</p></li><li><p><code>loss</code>: Loss (cost) function [def: <code>squared_cost</code>].  Should always assume y and ŷ as matrices.</p><div class="admonition is-warning"><header class="admonition-header">Warning</header><div class="admonition-body"><p>If you change the parameter <code>loss</code>, you need to either provide its derivative on the parameter <code>dloss</code> or use autodiff with <code>dloss=nothing</code>.</p></div></div></li></ul><ul><li><p><code>dloss</code>: Derivative of the loss function [def: <code>dsquared_cost</code>, i.e. use the derivative of the squared cost]. Use <code>nothing</code> for autodiff.</p></li><li><p><code>epochs</code>: Number of epochs, i.e. passages trough the whole training sample [def: <code>1000</code>]</p></li><li><p><code>batch_size</code>: Size of each individual batch [def: <code>32</code>]</p></li><li><p><code>opt_alg</code>: The optimisation algorithm to update the gradient at each batch [def: <code>ADAM()</code>]</p></li><li><p><code>shuffle</code>: Whether to randomly shuffle the data at each iteration (epoch) [def: <code>true</code>]</p></li><li><p><code>descr</code>: An optional title and/or description for this model</p></li><li><p><code>cb</code>: A call back function to provide information during training [def: <code>fitting_info</code></p></li><li><p><code>rng</code>: Random Number Generator (see <a href="Api.html#BetaML.Api.FIXEDSEED"><code>FIXEDSEED</code></a>) [deafult: <code>Random.GLOBAL_RNG</code>]</p></li></ul><p><strong>Notes:</strong></p><ul><li>data must be numerical</li><li>the label should be a <em>n-records</em> by <em>n-dimensions</em> matrix </li></ul><p><strong>Example:</strong></p><pre><code class="language-julia hljs">julia&gt; using MLJ

julia&gt; X, y                        = @load_boston;

julia&gt; ydouble                     = hcat(y,y);

julia&gt; modelType                   = @load MultitargetNeuralNetworkRegressor pkg = &quot;BetaML&quot;
[ Info: For silent loading, specify `verbosity=0`. 
import BetaML ✔
BetaML.Nn.MultitargetNeuralNetworkRegressor

julia&gt; layers                      = [BetaML.DenseLayer(12,50,f=BetaML.relu),BetaML.DenseLayer(50,50,f=BetaML.relu),BetaML.DenseLayer(50,2,f=BetaML.relu)];

julia&gt; model                       = modelType(layers=layers,opt_alg=BetaML.ADAM())
MultitargetNeuralNetworkRegressor(
  layers = BetaML.Nn.AbstractLayer[BetaML.Nn.DenseLayer([-0.14268958168480084 0.1556430517823459 … -0.08125686623988268 -0.2544570399728793; 0.28423814923214763 -0.1372659640176363 … 0.2264470618518154 -0.06631320101636362; … ; 0.02789179672476405 0.28348513690171906 … 0.2871912147350063 0.11554385516710886; -0.06320205436628074 -0.10694711454519892 … -0.10253686449899962 -0.26585990317571573], [0.09338448989761905, 0.2718624735230576, 0.023797261385177626, -0.17917031167475778, 0.15385702004431373, 0.012842680042847276, -0.10232304504376691, -0.13099353498374394, -0.11649189067696844, 0.30591295324151513  …  -0.2972600758671511, -0.177382174249729, -0.26266997240771395, 0.20268565473608047, 0.014804452498253184, 0.24784415091647882, 0.27962551308477157, -0.2880952267241536, 0.26057211923117796, -0.044009535090302976], BetaML.Utils.relu, nothing), BetaML.Nn.DenseLayer([-0.10136741184492606 -0.13038485207770573 … 0.1165162505227173 -0.025817955934162834; -0.20802525780664402 0.15425857417999556 … -0.19434363128519133 0.17652319228668767; … ; -0.10027182894787812 -0.16280219623873593 … -0.16389190054287556 -0.16859625236026915; 0.03561207609341421 -0.05272100409252414 … 0.18362700621532496 -0.11053112518410535], [0.2049701239390826, 0.04727896759708039, 0.22583290172299525, 0.13866713565359567, -0.032397509451043055, 0.041099957445332624, -0.2401413229195337, -0.022035553374859435, -0.2420707290337102, -0.0007123143227169282  …  -0.04350755341649204, 0.13228009527783768, -0.1313043131118029, -0.09176750039253359, 0.17829147060531736, -0.22431760512441942, 0.022861161675965136, -0.022343912739403338, -0.15410438565251305, -0.16252399721019406], BetaML.Utils.relu, nothing), BetaML.Nn.DenseLayer([-0.1865482260376025 -0.12501419399141886 … 0.1502899731849523 0.26034732010433115; -0.2829352616445401 -0.13834226908657268 … -0.016410622720088086 0.0022255074057040414], [0.1750612378638422, 0.16520212643140864], BetaML.Utils.relu, nothing)], 
  loss = BetaML.Utils.squared_cost, 
  dloss = BetaML.Utils.dsquared_cost, 
  epochs = 100, 
  batch_size = 32, 
  opt_alg = BetaML.Nn.ADAM(BetaML.Nn.var&quot;#69#72&quot;(), 1.0, 0.9, 0.999, 1.0e-8, BetaML.Nn.Learnable[], BetaML.Nn.Learnable[]), 
  shuffle = true, 
  descr = &quot;&quot;, 
  cb = BetaML.Nn.fitting_info, 
  rng = Random._GLOBAL_RNG())

julia&gt; (fitResults, cache, report) = MLJ.fit(model, -1, X, ydouble);

julia&gt; y_est                       = predict(model, fitResults, X)
506×2 Matrix{Float64}:
 29.7411  28.8886
 25.8501  26.5058
 29.3501  29.9779
  ⋮       
 30.3606  30.6514
 28.2101  28.3246
 24.1113  23.9118</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/sylvaticus/BetaML.jl/blob/a758127e0ec2c69cc6d6bf1f2b37a4db462b4b58/src/Nn/Nn_MLJ.jl#L116">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="BetaML.Nn.NNHyperParametersSet" href="#BetaML.Nn.NNHyperParametersSet"><code>BetaML.Nn.NNHyperParametersSet</code></a> — <span class="docstring-category">Type</span></header><section><div><p>**`</p><pre><code class="language-julia hljs">mutable struct NNHyperParametersSet &lt;: BetaMLHyperParametersSet</code></pre><p>`**</p><p>Hyperparameters for the <code>Feedforward</code> neural network model</p><p><strong>Parameters:</strong></p><ul><li><p><code>layers</code>: Array of layer objects [def: <code>nothing</code>, i.e. basic network]. See <code>subtypes(BetaML.AbstractLayer)</code> for supported layers</p></li><li><p><code>loss</code>: Loss (cost) function [def: <code>squared_cost</code>] It must always assume y and ŷ as (n x d) matrices, eventually using <code>dropdims</code> inside.</p></li></ul><ul><li><p><code>dloss</code>: Derivative of the loss function [def: <code>dsquared_cost</code> if <code>loss==squared_cost</code>, <code>nothing</code> otherwise, i.e. use the derivative of the squared cost or autodiff]</p></li><li><p><code>epochs</code>: Number of epochs, i.e. passages trough the whole training sample [def: <code>1000</code>]</p></li><li><p><code>batch_size</code>: Size of each individual batch [def: <code>32</code>]</p></li><li><p><code>opt_alg</code>: The optimisation algorithm to update the gradient at each batch [def: <code>ADAM()</code>]</p></li><li><p><code>shuffle</code>: Whether to randomly shuffle the data at each iteration (epoch) [def: <code>true</code>]</p></li><li><p><code>tunemethod</code>: The method - and its parameters - to employ for hyperparameters autotuning. See <a href="Utils.html#BetaML.Utils.SuccessiveHalvingSearch"><code>SuccessiveHalvingSearch</code></a> for the default method. To implement automatic hyperparameter tuning during the (first) <code>fit!</code> call simply set <code>autotune=true</code> and eventually change the default <code>tunemethod</code> options (including the parameter ranges, the resources to employ and the loss function to adopt).</p></li></ul><p>To know the available layers type <code>subtypes(AbstractLayer)</code>) and then type <code>?LayerName</code> for information on how to use each layer.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/sylvaticus/BetaML.jl/blob/a758127e0ec2c69cc6d6bf1f2b37a4db462b4b58/src/Nn/Nn.jl#L789-L799">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="BetaML.Nn.NeuralNetworkClassifier" href="#BetaML.Nn.NeuralNetworkClassifier"><code>BetaML.Nn.NeuralNetworkClassifier</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">mutable struct NeuralNetworkClassifier &lt;: MLJModelInterface.Probabilistic</code></pre><p>A simple but flexible Feedforward Neural Network, from the Beta Machine Learning Toolkit (BetaML) for classification  problems.</p><p><strong>Parameters:</strong></p><ul><li><p><code>layers</code>: Array of layer objects [def: <code>nothing</code>, i.e. basic network]. See <code>subtypes(BetaML.AbstractLayer)</code> for supported layers. The last &quot;softmax&quot; layer is automatically added.</p></li><li><p><code>loss</code>: Loss (cost) function [def: <code>crossentropy</code>]. Should always assume y and ŷ as matrices.</p><div class="admonition is-warning"><header class="admonition-header">Warning</header><div class="admonition-body"><p>If you change the parameter <code>loss</code>, you need to either provide its derivative on the parameter <code>dloss</code> or use autodiff with <code>dloss=nothing</code>.</p></div></div></li></ul><ul><li><p><code>dloss</code>: Derivative of the loss function [def: <code>dcrossentropy</code>, i.e. the derivative of the cross-entropy]. Use <code>nothing</code> for autodiff.</p></li><li><p><code>epochs</code>: Number of epochs, i.e. passages trough the whole training sample [def: <code>1000</code>]</p></li><li><p><code>batch_size</code>: Size of each individual batch [def: <code>32</code>]</p></li><li><p><code>opt_alg</code>: The optimisation algorithm to update the gradient at each batch [def: <code>BetaML.ADAM()</code>]</p></li><li><p><code>shuffle</code>: Whether to randomly shuffle the data at each iteration (epoch) [def: <code>true</code>]</p></li><li><p><code>descr</code>: An optional title and/or description for this model</p></li><li><p><code>cb</code>: A call back function to provide information during training [def: <code>BetaML.fitting_info</code></p></li><li><p><code>categories</code>: The categories to represent as columns. [def: <code>nothing</code>, i.e. unique training values].</p></li><li><p><code>handle_unknown</code>: How to handle categories not seens in training or not present in the provided <code>categories</code> array? &quot;error&quot; (default) rises an error, &quot;infrequent&quot; adds a specific column for these categories.</p></li><li><p><code>other_categories_name</code>: Which value during prediction to assign to this &quot;other&quot; category (i.e. categories not seen on training or not present in the provided <code>categories</code> array? [def: <code>nothing</code>, i.e. typemax(Int64) for integer vectors and &quot;other&quot; for other types]. This setting is active only if <code>handle_unknown=&quot;infrequent&quot;</code> and in that case it MUST be specified if Y is neither integer or strings</p></li><li><p><code>rng</code>: Random Number Generator [deafult: <code>Random.GLOBAL_RNG</code>]</p></li></ul><p><strong>Notes:</strong></p><ul><li>data must be numerical</li><li>the label should be a <em>n-records</em> by <em>n-dimensions</em> matrix (e.g. a one-hot-encoded data for classification), where the output columns should be interpreted as the probabilities for each categories.</li></ul><p><strong>Example:</strong></p><pre><code class="language-julia hljs">julia&gt; using MLJ

julia&gt; X, y                        = @load_iris;

julia&gt; modelType                   = @load NeuralNetworkClassifier pkg = &quot;BetaML&quot;
[ Info: For silent loading, specify `verbosity=0`. 
import BetaML ✔
BetaML.Nn.NeuralNetworkClassifier

julia&gt; layers                      = [BetaML.DenseLayer(4,8,f=BetaML.relu),BetaML.DenseLayer(8,8,f=BetaML.relu),BetaML.DenseLayer(8,3,f=BetaML.relu),BetaML.VectorFunctionLayer(3,f=BetaML.softmax)];

julia&gt; model                       = modelType(layers=layers,opt_alg=BetaML.ADAM())
NeuralNetworkClassifier(
  layers = BetaML.Nn.AbstractLayer[BetaML.Nn.DenseLayer([-0.13065425957999977 0.3006718045454293 -0.14208182654389845 -0.010396909703178414; 0.048520032692515036 -0.015206389893573924 0.10185996867206404 0.3322496808168578; … ; -0.35259614611009477 0.6482620436066895 0.008337847389667918 -0.12305204287019345; 0.4658422589725906 0.6934957957952972 -0.3085357878320247 0.20222661286207866], [0.36174111580772195, -0.35269496628536656, 0.26811746239579826, 0.5528187653581791, -0.3510634981562191, 0.10825967870150688, 0.3022797568475024, 0.4981155176339185], BetaML.Utils.relu, nothing), BetaML.Nn.DenseLayer([-0.10421417572899494 -0.35499611903472195 … -0.3335182269175171 -0.3985778486065036; 0.23543572035878935 0.59952318489473 … 0.2795331413389591 -0.5720523377542953; … ; 0.2647745208772335 -0.3248093104701972 … 0.3974038426324087 -0.08540125672267229; 0.5192880535413722 0.484381279307307 … 0.5908202412047914 0.3565865691496263], [-0.43847147676332937, -0.0792557647479405, 0.28527379769156247, 0.472161396182901, 0.5499454540456155, -0.24120815998677952, 0.07292491907243237, 0.6046011380800786], BetaML.Utils.relu, nothing), BetaML.Nn.DenseLayer([0.07404458231451261 -0.6297338418338474 … -0.5203349840135756 0.2659245561353357; -0.03739230431842255 -0.7175051212845613 … 0.7131622720546834 -0.6340542706678468; -0.14453639566110688 0.38900994015838364 … 0.5074513955919556 0.34154609716155104], [-0.39346454660088837, -0.3091008284310222, -0.03586152622920202], BetaML.Utils.relu, nothing), BetaML.Nn.VectorFunctionLayer{0}(fill(NaN), 3, 3, BetaML.Utils.softmax, nothing, nothing)], 
  loss = BetaML.Utils.crossentropy, 
  dloss = BetaML.Utils.dcrossentropy, 
  epochs = 100, 
  batch_size = 32, 
  opt_alg = BetaML.Nn.ADAM(BetaML.Nn.var&quot;#69#72&quot;(), 1.0, 0.9, 0.999, 1.0e-8, BetaML.Nn.Learnable[], BetaML.Nn.Learnable[]), 
  shuffle = true, 
  descr = &quot;&quot;, 
  cb = BetaML.Nn.fitting_info, 
  categories = nothing, 
  handle_unknown = &quot;error&quot;, 
  other_categories_name = nothing, 
  rng = Random._GLOBAL_RNG())

julia&gt; (fitResults, cache, report) = MLJ.fit(model, 0, X, y);

julia&gt; est_classes                 = predict(model, fitResults, X)
150-element CategoricalDistributions.UnivariateFiniteVector{Multiclass{3}, String, UInt8, Float64}:
 UnivariateFinite{Multiclass{3}}(setosa=&gt;0.57, versicolor=&gt;0.215, virginica=&gt;0.215)
 UnivariateFinite{Multiclass{3}}(setosa=&gt;0.565, versicolor=&gt;0.217, virginica=&gt;0.217)
 ⋮
 UnivariateFinite{Multiclass{3}}(setosa=&gt;0.255, versicolor=&gt;0.255, virginica=&gt;0.49)
 UnivariateFinite{Multiclass{3}}(setosa=&gt;0.254, versicolor=&gt;0.254, virginica=&gt;0.492)
 UnivariateFinite{Multiclass{3}}(setosa=&gt;0.263, versicolor=&gt;0.263, virginica=&gt;0.473)</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/sylvaticus/BetaML.jl/blob/a758127e0ec2c69cc6d6bf1f2b37a4db462b4b58/src/Nn/Nn_MLJ.jl#L226">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="BetaML.Nn.NeuralNetworkEstimator" href="#BetaML.Nn.NeuralNetworkEstimator"><code>BetaML.Nn.NeuralNetworkEstimator</code></a> — <span class="docstring-category">Type</span></header><section><div><p><strong><code>NeuralNetworkEstimator</code></strong></p><p>A &quot;feedforward&quot; neural network (supervised).</p><p>For the parameters see <a href="Nn.html#BetaML.Nn.NNHyperParametersSet"><code>NNHyperParametersSet</code></a>.</p><p><strong>Notes:</strong></p><ul><li>data must be numerical</li><li>the label can be a <em>n-records</em> vector or a <em>n-records</em> by <em>n-dimensions</em> matrix, but the result is always a matrix.<ul><li>For one-dimension regressions drop the unnecessary dimension with <code>dropdims(ŷ,dims=2)</code></li><li>For classification tasks the columns should normally be interpreted as the probabilities for each categories</li></ul></li></ul><p><strong>Examples:</strong></p><ul><li>Classification...</li></ul><pre><code class="language-julia hljs">julia&gt; using BetaML

julia&gt; X = [1.8 2.5; 0.5 20.5; 0.6 18; 0.7 22.8; 0.4 31; 1.7 3.7];

julia&gt; y = [&quot;a&quot;,&quot;b&quot;,&quot;b&quot;,&quot;b&quot;,&quot;b&quot;,&quot;a&quot;];

julia&gt; ohmod = OneHotEncoder()
A OneHotEncoder BetaMLModel (unfitted)

julia&gt; y_oh  = fit!(ohmod,y)
6×2 Matrix{Bool}:
 1  0
 0  1
 0  1
 0  1
 0  1
 1  0

julia&gt; layers = [DenseLayer(2,6),DenseLayer(6,2),VectorFunctionLayer(2,f=softmax)];

julia&gt; m      = NeuralNetworkEstimator(layers=layers,opt_alg=ADAM(),epochs=300,verbosity=LOW)
NeuralNetworkEstimator - A Feed-forward neural network (unfitted)

julia&gt; ŷ_prob = fit!(m,X,y_oh)
***
*** Training  for 300 epochs with algorithm ADAM.
Training..       avg ϵ on (Epoch 1 Batch 1):     0.4116936481380642
Training of 300 epoch completed. Final epoch error: 0.44308719831108734.
6×2 Matrix{Float64}:
 0.853198    0.146802
 0.0513715   0.948629
 0.0894273   0.910573
 0.0367079   0.963292
 0.00548038  0.99452
 0.808334    0.191666

julia&gt; ŷ      = inverse_predict(ohmod,ŷ_prob)
6-element Vector{String}:
 &quot;a&quot;
 &quot;b&quot;
 &quot;b&quot;
 &quot;b&quot;
 &quot;b&quot;
 &quot;a&quot;</code></pre><ul><li>Regression...</li></ul><pre><code class="language-julia hljs">julia&gt; using BetaML

julia&gt; X = [1.8 2.5; 0.5 20.5; 0.6 18; 0.7 22.8; 0.4 31; 1.7 3.7];

julia&gt; y = 2 .* X[:,1] .- X[:,2] .+ 3;

julia&gt; layers = [DenseLayer(2,6),DenseLayer(6,6),DenseLayer(6,1)];

julia&gt; m      = NeuralNetworkEstimator(layers=layers,opt_alg=ADAM(),epochs=3000,verbosity=LOW)
NeuralNetworkEstimator - A Feed-forward neural network (unfitted)

julia&gt; ŷ      = fit!(m,X,y);
***
*** Training  for 3000 epochs with algorithm ADAM.
Training..       avg ϵ on (Epoch 1 Batch 1):     33.30063874270561
Training of 3000 epoch completed. Final epoch error: 34.61265465430473.

julia&gt; hcat(y,ŷ)
6×2 Matrix{Float64}:
   4.1    4.11015
 -16.5  -16.5329
 -13.8  -13.8381
 -18.4  -18.3876
 -27.2  -27.1667
   2.7    2.70542</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/sylvaticus/BetaML.jl/blob/a758127e0ec2c69cc6d6bf1f2b37a4db462b4b58/src/Nn/Nn.jl#L854-L947">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="BetaML.Nn.NeuralNetworkEstimatorOptionsSet" href="#BetaML.Nn.NeuralNetworkEstimatorOptionsSet"><code>BetaML.Nn.NeuralNetworkEstimatorOptionsSet</code></a> — <span class="docstring-category">Type</span></header><section><div><p>NeuralNetworkEstimatorOptionsSet</p><p>A struct defining the options used by the Feedforward neural network model</p><p><strong>Parameters:</strong></p><ul><li><p><code>cache</code>: Cache the results of the fitting stage, as to allow predict(mod) [default: <code>true</code>]. Set it to <code>false</code> to save memory for large data.</p></li><li><p><code>descr</code>: An optional title and/or description for this model</p></li><li><p><code>verbosity</code>: The verbosity level to be used in training or prediction (see <a href="Api.html#BetaML.Api.Verbosity"><code>Verbosity</code></a>) [deafult: <code>STD</code>]</p></li><li><p><code>cb</code>: A call back function to provide information during training [def: <code>fitting_info</code></p></li><li><p><code>autotune</code>: 0ption for hyper-parameters autotuning [def: <code>false</code>, i.e. not autotuning performed]. If activated, autotuning is performed on the first <code>fit!()</code> call. Controll auto-tuning trough the option <code>tunemethod</code> (see the model hyper-parameters)</p></li><li><p><code>rng</code>: Random Number Generator (see <a href="Api.html#BetaML.Api.FIXEDSEED"><code>FIXEDSEED</code></a>) [deafult: <code>Random.GLOBAL_RNG</code>]</p></li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/sylvaticus/BetaML.jl/blob/a758127e0ec2c69cc6d6bf1f2b37a4db462b4b58/src/Nn/Nn.jl#L825-L833">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="BetaML.Nn.NeuralNetworkRegressor" href="#BetaML.Nn.NeuralNetworkRegressor"><code>BetaML.Nn.NeuralNetworkRegressor</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">mutable struct NeuralNetworkRegressor &lt;: MLJModelInterface.Deterministic</code></pre><p>A simple but flexible Feedforward Neural Network, from the Beta Machine Learning Toolkit (BetaML) for regression of a single dimensional target.</p><p><strong>Parameters:</strong></p><ul><li><p><code>layers</code>: Array of layer objects [def: <code>nothing</code>, i.e. basic network]. See <code>subtypes(BetaML.AbstractLayer)</code> for supported layers</p></li><li><p><code>loss</code>: Loss (cost) function [def: <code>squared_cost</code>]. Should always assume y and ŷ as matrices, even if the regression task is 1-D</p><div class="admonition is-warning"><header class="admonition-header">Warning</header><div class="admonition-body"><p>If you change the parameter <code>loss</code>, you need to either provide its derivative on the parameter <code>dloss</code> or use autodiff with <code>dloss=nothing</code>.</p></div></div></li></ul><ul><li><p><code>dloss</code>: Derivative of the loss function [def: <code>dsquared_cost</code>, i.e. use the derivative of the squared cost]. Use <code>nothing</code> for autodiff.</p></li><li><p><code>epochs</code>: Number of epochs, i.e. passages trough the whole training sample [def: <code>1000</code>]</p></li><li><p><code>batch_size</code>: Size of each individual batch [def: <code>32</code>]</p></li><li><p><code>opt_alg</code>: The optimisation algorithm to update the gradient at each batch [def: <code>ADAM()</code>]</p></li><li><p><code>shuffle</code>: Whether to randomly shuffle the data at each iteration (epoch) [def: <code>true</code>]</p></li><li><p><code>descr</code>: An optional title and/or description for this model</p></li><li><p><code>cb</code>: A call back function to provide information during training [def: <code>fitting_info</code></p></li><li><p><code>rng</code>: Random Number Generator (see <a href="Api.html#BetaML.Api.FIXEDSEED"><code>FIXEDSEED</code></a>) [deafult: <code>Random.GLOBAL_RNG</code>]</p></li></ul><p><strong>Notes:</strong></p><ul><li>data must be numerical</li><li>the label should be be a <em>n-records</em> vector.</li></ul><p><strong>Example:</strong></p><pre><code class="language-julia hljs">julia&gt; using MLJ

julia&gt; modelType                   = @load NeuralNetworkRegressor pkg = &quot;BetaML&quot;
[ Info: For silent loading, specify `verbosity=0`. 
import BetaML ✔
BetaML.Nn.NeuralNetworkRegressor

julia&gt; layers                      = [BetaML.DenseLayer(12,20,f=BetaML.relu),BetaML.DenseLayer(20,20,f=BetaML.relu),BetaML.DenseLayer(20,1,f=BetaML.relu)];

julia&gt; model                       = modelType(layers=layers,opt_alg=BetaML.ADAM())
NeuralNetworkRegressor(
  layers = BetaML.Nn.AbstractLayer[BetaML.Nn.DenseLayer([-0.32801116352654236 0.19721617381409956 … 0.17423147551933688 -0.3203352184325144; 0.20325978849525422 0.2753359303406094 … -0.054177947724910136 0.040744621813733006; … ; 0.3614670391623493 0.4184392845285712 … -0.14577760559119207 -0.12430574279080464; -0.04477463648956215 -0.04575413793278821 … 0.2586292045719249 -0.4146332506686543], [0.386016400524039, -0.4120960765923787, -0.37660375260656787, 0.3754674172848425, 0.3933763861297827, -0.09574612456235942, 0.28147281593639867, -0.11333754049443168, -0.19680033976399594, -0.24747338342736486, 0.022885791740458516, -0.34253183385897484, 0.22126071792632201, -0.3539779424727334, -0.37335255502088455, -0.2462814314064721, 0.01620706528968724, -0.3724728631729394, 0.21311037493715396, -0.20613597904524303], BetaML.Utils.relu, nothing), BetaML.Nn.DenseLayer([0.37603456115187256 0.3542546426240723 … -0.0024023384912328916 0.1834672226168586; -0.1535424198342724 -0.07672083294894799 … -0.1433915698536904 -0.1633699269469485; … ; -0.16189872793833512 0.32683924051358165 … -0.08638288054654059 -0.3802058507922781; -0.19558165681593773 0.16664095708205845 … 0.2503794347207368 -0.031688833520039705], [0.021102385823098146, 0.22228546967483392, 0.1300959971946743, -0.20976715493972442, 0.04091175703653677, 0.023810417350970836, 0.2781644696873053, -0.3057357809062001, 0.10103624908600595, 0.12700817756236799, 0.08642857384856573, -0.1675652351991456, -0.17329950695590257, 0.12896500307404696, -0.1484448116427858, -0.24124008136893604, -0.08216916194774915, 0.33079670478470163, 0.19806334350809457, 0.32549757061401846], BetaML.Utils.relu, nothing), BetaML.Nn.DenseLayer([-0.035318774804408926 0.2774737129427495 … 0.07256585990736009 0.229332566953939], [0.39178172498331654], BetaML.Utils.relu, nothing)], 
  loss = BetaML.Utils.squared_cost, 
  dloss = BetaML.Utils.dsquared_cost, 
  epochs = 100, 
  batch_size = 32, 
  opt_alg = BetaML.Nn.ADAM(BetaML.Nn.var&quot;#69#72&quot;(), 1.0, 0.9, 0.999, 1.0e-8, BetaML.Nn.Learnable[], BetaML.Nn.Learnable[]), 
  shuffle = true, 
  descr = &quot;&quot;, 
  cb = BetaML.Nn.fitting_info, 
  rng = Random._GLOBAL_RNG())

julia&gt; (fitResults, cache, report) = MLJ.fit(model, 0, X, y);

julia&gt; y_est                       = predict(model, fitResults, X)
506-element Vector{Float64}:
 29.67359458542452
 27.72073250260763
  ⋮
 27.259757923962265</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/sylvaticus/BetaML.jl/blob/a758127e0ec2c69cc6d6bf1f2b37a4db462b4b58/src/Nn/Nn_MLJ.jl#L13">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="BetaML.Nn.PoolingLayer" href="#BetaML.Nn.PoolingLayer"><code>BetaML.Nn.PoolingLayer</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">mutable struct PoolingLayer{ND, NDPLUS1, NDPLUS2} &lt;: AbstractLayer</code></pre><p>Representation of a pooling layer in the network</p><p><strong>Fields:</strong></p><ul><li><p><code>input_size::StaticArraysCore.SVector{NDPLUS1, Int64} where NDPLUS1</code>: Input size (including nchannel_in as last dimension)</p></li><li><p><code>output_size::StaticArraysCore.SVector{NDPLUS1, Int64} where NDPLUS1</code>: Output size (including nchannel_out as last dimension)</p></li><li><p><code>kernel_size::StaticArraysCore.SVector{NDPLUS2, Int64} where NDPLUS2</code>: kernel<em>size augmented by the nchannels</em>in and nchannels_out dimensions</p></li><li><p><code>padding_start::StaticArraysCore.SVector{ND, Int64} where ND</code>: Padding (initial)</p></li><li><p><code>padding_end::StaticArraysCore.SVector{ND, Int64} where ND</code>: Padding (ending)</p></li><li><p><code>stride::StaticArraysCore.SVector{ND, Int64} where ND</code>: Stride</p></li><li><p><code>ndims::Int64</code>: Number of dimensions (excluding input and output channels)</p></li><li><p><code>f::Function</code>: Activation function</p></li><li><p><code>df::Union{Nothing, Function}</code>: Derivative of the activation function</p></li><li><p><code>y_to_x_ids::Array{Array{Tuple{Vararg{Int32, NDPLUS1}}, 1}, NDPLUS1} where NDPLUS1</code>: A y-dims array of vectors of ids of x(s) contributing to the giving y</p></li><li><p><code>preprocessed::Bool</code>: Wheather this layer has already been preprocessed</p></li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/sylvaticus/BetaML.jl/blob/a758127e0ec2c69cc6d6bf1f2b37a4db462b4b58/src/Nn/default_layers/PoolingLayer.jl#L5">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="BetaML.Nn.PoolingLayer-Tuple{Any, Any}" href="#BetaML.Nn.PoolingLayer-Tuple{Any, Any}"><code>BetaML.Nn.PoolingLayer</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">PoolingLayer(
    input_size_with_channel,
    kernel_size;
    stride,
    padding,
    f,
    df
) -&gt; PoolingLayer{_A, _B, _C} where {_A, _B, _C}
</code></pre><p>Alternative constructor for a <code>PoolingLayer</code> where the number of channels in input is specified as a further dimension in the input size instead of as a separate parameter, so to use <code>size(previous_layer)[2]</code> if one wish.</p><p>For arguments and default values see the documentation of the main constructor.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/sylvaticus/BetaML.jl/blob/a758127e0ec2c69cc6d6bf1f2b37a4db462b4b58/src/Nn/default_layers/PoolingLayer.jl#L123">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="BetaML.Nn.ReshaperLayer" href="#BetaML.Nn.ReshaperLayer"><code>BetaML.Nn.ReshaperLayer</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">mutable struct ReshaperLayer{NDIN, NDOUT} &lt;: AbstractLayer</code></pre><p>Representation of a &quot;reshaper&quot; (weigthless) layer in the network</p><p><strong>Fields:</strong></p><ul><li><p><code>input_size::StaticArraysCore.SVector{NDIN, Int64} where NDIN</code>: Input size</p></li><li><p><code>output_size::StaticArraysCore.SVector{NDOUT, Int64} where NDOUT</code>: Output size</p></li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/sylvaticus/BetaML.jl/blob/a758127e0ec2c69cc6d6bf1f2b37a4db462b4b58/src/Nn/default_layers/ReshaperLayer.jl#L4">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="BetaML.Nn.SGD" href="#BetaML.Nn.SGD"><code>BetaML.Nn.SGD</code></a> — <span class="docstring-category">Type</span></header><section><div><p>SGD(;η=t -&gt; 1/(1+t), λ=2)</p><p>Stochastic Gradient Descent algorithm (default)</p><p><strong>Fields:</strong></p><ul><li><code>η</code>: Learning rate, as a function of the current epoch [def: t -&gt; 1/(1+t)]</li><li><code>λ</code>: Multiplicative constant to the learning rate [def: 2]</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/sylvaticus/BetaML.jl/blob/a758127e0ec2c69cc6d6bf1f2b37a4db462b4b58/src/Nn/Nn_default_optalgs.jl#L8-L16">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="BetaML.Nn.ScalarFunctionLayer" href="#BetaML.Nn.ScalarFunctionLayer"><code>BetaML.Nn.ScalarFunctionLayer</code></a> — <span class="docstring-category">Type</span></header><section><div><p>ScalarFunctionLayer</p><p>Representation of a ScalarFunction layer in the network. ScalarFunctionLayer applies the activation function directly to the output of the previous layer (i.e., without passing for a weigth matrix), but using an  optional learnable parameter (an array) used as second argument, similarly to [<code>VectorFunctionLayer</code>(@ref). Differently from <code>VectorFunctionLayer</code>, the function is applied scalarwise to each node. </p><p>The number of nodes in input must be set to the same as in the previous layer</p><p><strong>Fields:</strong></p><ul><li><code>w</code>:   Weigths (parameter) array passes as second argument to the activation        function (if not empty)</li><li><code>n</code>:   Number of nodes in output (≡ number of nodes in input )</li><li><code>f</code>:   Activation function (vector)</li><li><code>dfx</code>: Derivative of the (vector) activation function with respect to the        layer inputs (x)</li><li><code>dfw</code>: Derivative of the (vector) activation function with respect to the        optional learnable weigths (w)         </li></ul><p><strong>Notes:</strong></p><ul><li>The output <code>size</code> of this layer is the same as those of the previous layers.</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/sylvaticus/BetaML.jl/blob/a758127e0ec2c69cc6d6bf1f2b37a4db462b4b58/src/Nn/default_layers/ScalarFunctionLayer.jl#L4-L30">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="BetaML.Nn.VectorFunctionLayer" href="#BetaML.Nn.VectorFunctionLayer"><code>BetaML.Nn.VectorFunctionLayer</code></a> — <span class="docstring-category">Type</span></header><section><div><p>VectorFunctionLayer</p><p>Representation of a VectorFunction layer in the network. Vector function layer expects a vector activation function, i.e. a function taking the whole output of the previous layer an input rather than working on a single node as &quot;normal&quot; activation functions would do. Useful for example with the SoftMax function in classification or with the <code>pool1D</code> function to implement a &quot;pool&quot; layer in 1 dimensions. By default it is weightless, i.e. it doesn&#39;t apply any transformation to the output coming from the previous layer except the activation function. However, by passing the parameter <code>wsize</code> (a touple or array - tested only 1D) you can pass the learnable parameter to the activation function too. It is your responsability to be sure the activation function accept only X or also this  learnable array (as second argument).    The number of nodes in input must be set to the same as in the previous layer (and if you are using this for classification, to the number of classes, i.e. the <em>previous</em> layer must be set equal to the number of classes in the predictions).</p><p><strong>Fields:</strong></p><ul><li><code>w</code>:   Weigths (parameter) array passes as second argument to the activation        function (if not empty)</li><li><code>nₗ</code>:  Number of nodes in input (i.e. length of previous layer)</li><li><code>n</code>:   Number of nodes in output (automatically inferred in the constructor)</li><li><code>f</code>:   Activation function (vector)</li><li><code>dfx</code>: Derivative of the (vector) activation function with respect to the        layer inputs (x)</li><li><code>dfw</code>: Derivative of the (vector) activation function with respect to the        optional learnable weigths (w)         </li></ul><p><strong>Notes:</strong></p><ul><li>The output <code>size</code> of this layer is given by the size of the output function,</li></ul><p>that not necessarily is the same as the previous layers.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/sylvaticus/BetaML.jl/blob/a758127e0ec2c69cc6d6bf1f2b37a4db462b4b58/src/Nn/default_layers/VectorFunctionLayer.jl#L4-L38">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Base.size-Tuple{AbstractLayer}" href="#Base.size-Tuple{AbstractLayer}"><code>Base.size</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">size(layer)</code></pre><p>Get the size of the layers in terms of (size in input, size in output) - both as tuples</p><p><strong>Notes:</strong></p><ul><li>You need to use <code>import Base.size</code> before defining this function for your layer</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/sylvaticus/BetaML.jl/blob/a758127e0ec2c69cc6d6bf1f2b37a4db462b4b58/src/Nn/Nn.jl#L248-L255">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Base.size-Tuple{ConvLayer}" href="#Base.size-Tuple{ConvLayer}"><code>Base.size</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">size(layer::ConvLayer) -&gt; Tuple{Tuple, Tuple}
</code></pre><p>Get the dimensions of the layers in terms of (dimensions in input, dimensions in output) including channels as last dimension</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/sylvaticus/BetaML.jl/blob/a758127e0ec2c69cc6d6bf1f2b37a4db462b4b58/src/Nn/default_layers/ConvLayer.jl#L319">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Base.size-Tuple{PoolingLayer}" href="#Base.size-Tuple{PoolingLayer}"><code>Base.size</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">size(layer::PoolingLayer) -&gt; Tuple{Tuple, Tuple}
</code></pre><p>Get the dimensions of the layers in terms of (dimensions in input, dimensions in output) including channels as last dimension</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/sylvaticus/BetaML.jl/blob/a758127e0ec2c69cc6d6bf1f2b37a4db462b4b58/src/Nn/default_layers/PoolingLayer.jl#L266">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="BetaML.Nn.backward-Tuple{AbstractLayer, Any, Any}" href="#BetaML.Nn.backward-Tuple{AbstractLayer, Any, Any}"><code>BetaML.Nn.backward</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">backward(layer,x,next_gradient)</code></pre><p>Compute backpropagation for this layer with respect to its inputs</p><p><strong>Parameters:</strong></p><ul><li><code>layer</code>:        Worker layer</li><li><code>x</code>:            Input to the layer</li><li><code>next_gradient</code>: Derivative of the overal loss with respect to the input of the next layer (output of this layer)</li></ul><p><strong>Return:</strong></p><ul><li>The evaluated gradient of the loss with respect to this layer inputs</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/sylvaticus/BetaML.jl/blob/a758127e0ec2c69cc6d6bf1f2b37a4db462b4b58/src/Nn/Nn.jl#L181-L194">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="BetaML.Nn.fitting_info-Tuple{Any, Any, Any}" href="#BetaML.Nn.fitting_info-Tuple{Any, Any, Any}"><code>BetaML.Nn.fitting_info</code></a> — <span class="docstring-category">Method</span></header><section><div><p>fitting<em>info(nn,x,y;n,batch</em>size,epochs,verbosity,n<em>epoch,n</em>batch)</p><p>Default callback funtion to display information during training, depending on the verbosity level</p><p><strong>Parameters:</strong></p><ul><li><code>nn</code>: Worker network</li><li><code>x</code>:  Batch input to the network (batch_size,d)</li><li><code>y</code>:  Batch label input (batch_size,d)</li><li><code>n</code>: Size of the full training set</li><li><code>n_batches</code> : Number of baches per epoch</li><li><code>epochs</code>: Number of epochs defined for the training</li><li><code>verbosity</code>: Verbosity level defined for the training (NONE,LOW,STD,HIGH,FULL)</li><li><code>n_epoch</code>: Counter of the current epoch</li><li><code>n_batch</code>: Counter of the current batch</li></ul><p>#Notes:</p><ul><li>Reporting of the error (loss of the network) is expensive. Use <code>verbosity=NONE</code> for better performances</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/sylvaticus/BetaML.jl/blob/a758127e0ec2c69cc6d6bf1f2b37a4db462b4b58/src/Nn/Nn.jl#L536-L554">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="BetaML.Nn.forward-Tuple{AbstractLayer, Any}" href="#BetaML.Nn.forward-Tuple{AbstractLayer, Any}"><code>BetaML.Nn.forward</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">forward(layer,x)</code></pre><p>Predict the output of the layer given the input</p><p><strong>Parameters:</strong></p><ul><li><code>layer</code>:  Worker layer</li><li><code>x</code>:      Input to the layer</li></ul><p><strong>Return:</strong></p><ul><li>An Array{T,1} of the prediction (even for a scalar)</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/sylvaticus/BetaML.jl/blob/a758127e0ec2c69cc6d6bf1f2b37a4db462b4b58/src/Nn/Nn.jl#L165-L176">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="BetaML.Nn.forward-Tuple{ConvLayer, Any}" href="#BetaML.Nn.forward-Tuple{ConvLayer, Any}"><code>BetaML.Nn.forward</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">forward(layer::ConvLayer, x) -&gt; Any
</code></pre><p>Compute forward pass of a ConvLayer</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/sylvaticus/BetaML.jl/blob/a758127e0ec2c69cc6d6bf1f2b37a4db462b4b58/src/Nn/default_layers/ConvLayer.jl#L238">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="BetaML.Nn.forward-Tuple{PoolingLayer, Any}" href="#BetaML.Nn.forward-Tuple{PoolingLayer, Any}"><code>BetaML.Nn.forward</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">forward(layer::PoolingLayer, x) -&gt; Any
</code></pre><p>Compute forward pass of a ConvLayer</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/sylvaticus/BetaML.jl/blob/a758127e0ec2c69cc6d6bf1f2b37a4db462b4b58/src/Nn/default_layers/PoolingLayer.jl#L211">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="BetaML.Nn.get_gradient-Tuple{AbstractLayer, Any, Any}" href="#BetaML.Nn.get_gradient-Tuple{AbstractLayer, Any, Any}"><code>BetaML.Nn.get_gradient</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">get_gradient(layer,x,next_gradient)</code></pre><p>Compute backpropagation for this layer with respect to the layer weigths</p><p><strong>Parameters:</strong></p><ul><li><code>layer</code>:        Worker layer</li><li><code>x</code>:            Input to the layer</li><li><code>next_gradient</code>: Derivative of the overaall loss with respect to the input of the next layer (output of this layer)</li></ul><p><strong>Return:</strong></p><ul><li>The evaluated gradient of the loss with respect to this layer&#39;s trainable parameters as tuple of matrices. It is up to you to decide how to organise this tuple, as long you are consistent with the <code>get_params()</code> and <code>set_params()</code> functions. Note that starting from BetaML 0.2.2 this tuple needs to be wrapped in its <code>Learnable</code> type.</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/sylvaticus/BetaML.jl/blob/a758127e0ec2c69cc6d6bf1f2b37a4db462b4b58/src/Nn/Nn.jl#L214-L226">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="BetaML.Nn.get_gradient-Union{Tuple{N2}, Tuple{N1}, Tuple{T2}, Tuple{T}, Tuple{BetaML.Nn.NN, Union{AbstractArray{T, N1}, T}, Union{AbstractArray{T2, N2}, T2}}} where {T&lt;:Number, T2&lt;:Number, N1, N2}" href="#BetaML.Nn.get_gradient-Union{Tuple{N2}, Tuple{N1}, Tuple{T2}, Tuple{T}, Tuple{BetaML.Nn.NN, Union{AbstractArray{T, N1}, T}, Union{AbstractArray{T2, N2}, T2}}} where {T&lt;:Number, T2&lt;:Number, N1, N2}"><code>BetaML.Nn.get_gradient</code></a> — <span class="docstring-category">Method</span></header><section><div><p>get_gradient(nn,x,y)</p><p>Low level function that retrieve the current gradient of the weigthts (i.e. derivative of the cost with respect to the weigths). Unexported in BetaML &gt;= v0.9</p><p><strong>Parameters:</strong></p><ul><li><code>nn</code>: Worker network</li><li><code>x</code>:   Input to the network (d,1)</li><li><code>y</code>:   Label input (d,1)</li></ul><p>#Notes:</p><ul><li>The output is a vector of tuples of each layer&#39;s input weigths and bias weigths</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/sylvaticus/BetaML.jl/blob/a758127e0ec2c69cc6d6bf1f2b37a4db462b4b58/src/Nn/Nn.jl#L400-L412">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="BetaML.Nn.get_params-Tuple{AbstractLayer}" href="#BetaML.Nn.get_params-Tuple{AbstractLayer}"><code>BetaML.Nn.get_params</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">get_params(layer)</code></pre><p>Get the layers current value of its trainable parameters</p><p><strong>Parameters:</strong></p><ul><li><code>layer</code>:  Worker layer</li></ul><p><strong>Return:</strong></p><ul><li>The current value of the layer&#39;s trainable parameters as tuple of matrices. It is up to you to decide how to organise this tuple, as long you are consistent with the <code>get_gradient()</code> and <code>set_params()</code> functions. Note that starting from BetaML 0.2.2 this tuple needs to be wrapped in its <code>Learnable</code> type.</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/sylvaticus/BetaML.jl/blob/a758127e0ec2c69cc6d6bf1f2b37a4db462b4b58/src/Nn/Nn.jl#L199-L209">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="BetaML.Nn.get_params-Tuple{BetaML.Nn.NN}" href="#BetaML.Nn.get_params-Tuple{BetaML.Nn.NN}"><code>BetaML.Nn.get_params</code></a> — <span class="docstring-category">Method</span></header><section><div><p>get_params(nn)</p><p>Retrieve current weigthts</p><p><strong>Parameters:</strong></p><ul><li><code>nn</code>: Worker network</li></ul><p><strong>Notes:</strong></p><ul><li>The output is a vector of tuples of each layer&#39;s input weigths and bias weigths</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/sylvaticus/BetaML.jl/blob/a758127e0ec2c69cc6d6bf1f2b37a4db462b4b58/src/Nn/Nn.jl#L384-L394">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="BetaML.Nn.init_optalg!-Tuple{ADAM}" href="#BetaML.Nn.init_optalg!-Tuple{ADAM}"><code>BetaML.Nn.init_optalg!</code></a> — <span class="docstring-category">Method</span></header><section><div><p>init<em>optalg!(opt</em>alg::ADAM;θ,batch_size,x,y,rng)</p><p>Initialize the ADAM algorithm with the parameters m and v as zeros and check parameter bounds</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/sylvaticus/BetaML.jl/blob/a758127e0ec2c69cc6d6bf1f2b37a4db462b4b58/src/Nn/Nn_default_optalgs.jl#L72-L76">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="BetaML.Nn.init_optalg!-Tuple{BetaML.Nn.OptimisationAlgorithm}" href="#BetaML.Nn.init_optalg!-Tuple{BetaML.Nn.OptimisationAlgorithm}"><code>BetaML.Nn.init_optalg!</code></a> — <span class="docstring-category">Method</span></header><section><div><p>init<em>optalg!(opt</em>alg;θ,batch_size,x,y)</p><p>Initialize the optimisation algorithm</p><p><strong>Parameters:</strong></p><ul><li><code>opt_alg</code>:    The Optimisation algorithm to use</li><li><code>θ</code>:         Current parameters</li><li><code>batch_size</code>:    The size of the batch</li><li><code>x</code>:   The training (input) data</li><li><code>y</code>:   The training &quot;labels&quot; to match</li><li><code>rng</code>: Random Number Generator (see <a href="Api.html#BetaML.Api.FIXEDSEED"><code>FIXEDSEED</code></a>) [deafult: <code>Random.GLOBAL_RNG</code>]</li></ul><p><strong>Notes:</strong></p><ul><li>Only a few optimizers need this function and consequently ovverride it. By default it does nothing, so if you want write your own optimizer and don&#39;t need to initialise it, you don&#39;t have to override this method</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/sylvaticus/BetaML.jl/blob/a758127e0ec2c69cc6d6bf1f2b37a4db462b4b58/src/Nn/Nn.jl#L723-L738">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="BetaML.Nn.preprocess!-Tuple{AbstractLayer}" href="#BetaML.Nn.preprocess!-Tuple{AbstractLayer}"><code>BetaML.Nn.preprocess!</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">preprocess!(layer::AbstractLayer)
</code></pre><p>Preprocess the layer with information known at layer creation (i.e. no data info used)</p><p>This function is used for some layers to cache some computation that doesn&#39;t require the data and it is called at the beginning of <code>fit!</code>. For example, it is used in ConvLayer to store the ids of the convolution.</p><p><strong>Notes:</strong></p><ul><li>as it doesn&#39;t depend on data, it is not reset by <code>reset!</code></li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/sylvaticus/BetaML.jl/blob/a758127e0ec2c69cc6d6bf1f2b37a4db462b4b58/src/Nn/Nn.jl#L275">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="BetaML.Nn.set_params!-Tuple{AbstractLayer, Any}" href="#BetaML.Nn.set_params!-Tuple{AbstractLayer, Any}"><code>BetaML.Nn.set_params!</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">set_params!(layer,w)</code></pre><p>Set the trainable parameters of the layer with the given values</p><p><strong>Parameters:</strong></p><ul><li><code>layer</code>: Worker layer</li><li><code>w</code>:     The new parameters to set (Learnable)</li></ul><p><strong>Notes:</strong></p><ul><li>The format of the tuple wrapped by Learnable must be consistent with those of the <code>get_params()</code> and <code>get_gradient()</code> functions.</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/sylvaticus/BetaML.jl/blob/a758127e0ec2c69cc6d6bf1f2b37a4db462b4b58/src/Nn/Nn.jl#L231-L242">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="BetaML.Nn.set_params!-Tuple{BetaML.Nn.NN, Any}" href="#BetaML.Nn.set_params!-Tuple{BetaML.Nn.NN, Any}"><code>BetaML.Nn.set_params!</code></a> — <span class="docstring-category">Method</span></header><section><div><p>set_params!(nn,w)</p><p>Update weigths of the network</p><p><strong>Parameters:</strong></p><ul><li><code>nn</code>: Worker network</li><li><code>w</code>:  The new weights to set</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/sylvaticus/BetaML.jl/blob/a758127e0ec2c69cc6d6bf1f2b37a4db462b4b58/src/Nn/Nn.jl#L482-L490">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="BetaML.Nn.single_update!-Tuple{Any, Any}" href="#BetaML.Nn.single_update!-Tuple{Any, Any}"><code>BetaML.Nn.single_update!</code></a> — <span class="docstring-category">Method</span></header><section><div><p>single<em>update!(θ,▽;n</em>epoch,n<em>batch,batch</em>size,xbatch,ybatch,opt_alg)</p><p>Perform the parameters update based on the average batch gradient.</p><p><strong>Parameters:</strong></p><ul><li><code>θ</code>:         Current parameters</li><li><code>▽</code>:         Average gradient of the batch</li><li><code>n_epoch</code>:    Count of current epoch</li><li><code>n_batch</code>:    Count of current batch</li><li><code>n_batches</code>:  Number of batches per epoch</li><li><code>xbatch</code>:    Data associated to the current batch</li><li><code>ybatch</code>:    Labels associated to the current batch</li><li><code>opt_alg</code>:    The Optimisation algorithm to use for the update</li></ul><p><strong>Notes:</strong></p><ul><li>This function is overridden so that each optimisation algorithm implement their</li></ul><p>own version</p><ul><li>Most parameters are not used by any optimisation algorithm. They are provided</li></ul><p>to support the largest possible class of optimisation algorithms</p><ul><li>Some optimisation algorithms may change their internal structure in this function</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/sylvaticus/BetaML.jl/blob/a758127e0ec2c69cc6d6bf1f2b37a4db462b4b58/src/Nn/Nn.jl#L693-L714">source</a></section></article></article><nav class="docs-footer"><a class="docs-footer-prevpage" href="Trees.html">« Trees</a><a class="docs-footer-nextpage" href="Clustering.html">Clustering »</a><div class="flexbox-break"></div><p class="footer-message">Powered by <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> and the <a href="https://julialang.org/">Julia Programming Language</a>.</p></nav></div><div class="modal" id="documenter-settings"><div class="modal-background"></div><div class="modal-card"><header class="modal-card-head"><p class="modal-card-title">Settings</p><button class="delete"></button></header><section class="modal-card-body"><p><label class="label">Theme</label><div class="select"><select id="documenter-themepicker"><option value="documenter-light">documenter-light</option><option value="documenter-dark">documenter-dark</option></select></div></p><hr/><p>This document was generated with <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> version 0.27.23 on <span class="colophon-date" title="Tuesday 21 February 2023 14:41">Tuesday 21 February 2023</span>. Using Julia version 1.6.7.</p></section><footer class="modal-card-foot"></footer></div></div></div></body></html>
