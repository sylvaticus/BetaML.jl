<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>Trees · BetaML.jl Documentation</title><script async src="https://www.googletagmanager.com/gtag/js?id=G-JYKX8QY5JW"></script><script>  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());
  gtag('config', 'G-JYKX8QY5JW', {'page_path': location.pathname + location.search + location.hash});
</script><script data-outdated-warner src="assets/warner.js"></script><link href="https://cdnjs.cloudflare.com/ajax/libs/lato-font/3.0.0/css/lato-font.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/juliamono/0.045/juliamono.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.4/css/fontawesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.4/css/solid.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.4/css/brands.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.24/katex.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL="."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" data-main="assets/documenter.js"></script><script src="siteinfo.js"></script><script src="../versions.js"></script><link class="docs-theme-link" rel="stylesheet" type="text/css" href="assets/themes/documenter-dark.css" data-theme-name="documenter-dark" data-theme-primary-dark/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="assets/themes/documenter-light.css" data-theme-name="documenter-light" data-theme-primary/><script src="assets/themeswap.js"></script></head><body><div id="documenter"><nav class="docs-sidebar"><div class="docs-package-name"><span class="docs-autofit"><a href="index.html">BetaML.jl Documentation</a></span></div><form class="docs-search" action="search.html"><input class="docs-search-query" id="documenter-search-query" name="q" type="text" placeholder="Search docs"/></form><ul class="docs-menu"><li><a class="tocitem" href="index.html">Index</a></li><li><span class="tocitem">Tutorial</span><ul><li><a class="tocitem" href="tutorials/Betaml_tutorial_getting_started.html">Getting started</a></li><li><input class="collapse-toggle" id="menuitem-2-2" type="checkbox"/><label class="tocitem" for="menuitem-2-2"><span class="docs-label">Classification - cars</span><i class="docs-chevron"></i></label><ul class="collapsed"><li><a class="tocitem" href="tutorials/Classification - cars/betaml_tutorial_classification_cars.html">A classification task when labels are known - determining the country of origin of cars given the cars characteristics</a></li></ul></li><li><input class="collapse-toggle" id="menuitem-2-3" type="checkbox"/><label class="tocitem" for="menuitem-2-3"><span class="docs-label">Regression - bike sharing</span><i class="docs-chevron"></i></label><ul class="collapsed"><li><a class="tocitem" href="tutorials/Regression - bike sharing/betaml_tutorial_regression_sharingBikes.html">A regression task: the prediction of  bike  sharing demand</a></li></ul></li><li><input class="collapse-toggle" id="menuitem-2-4" type="checkbox"/><label class="tocitem" for="menuitem-2-4"><span class="docs-label">Clustering - Iris</span><i class="docs-chevron"></i></label><ul class="collapsed"><li><a class="tocitem" href="tutorials/Clustering - Iris/betaml_tutorial_cluster_iris.html">A clustering task: the prediction of  plant species from floreal measures (the iris dataset)</a></li></ul></li></ul></li><li><span class="tocitem">API (Reference manual)</span><ul><li><input class="collapse-toggle" id="menuitem-3-1" type="checkbox"/><label class="tocitem" for="menuitem-3-1"><span class="docs-label">API V2 (current)</span><i class="docs-chevron"></i></label><ul class="collapsed"><li><a class="tocitem" href="Api_v2_user.html">Introduction for user</a></li><li><input class="collapse-toggle" id="menuitem-3-1-2" type="checkbox"/><label class="tocitem" for="menuitem-3-1-2"><span class="docs-label">For developers</span><i class="docs-chevron"></i></label><ul class="collapsed"><li><a class="tocitem" href="Api_v2_developer.html">API implementation</a></li><li><a class="tocitem" href="StyleGuide_templates.html">Style guide</a></li></ul></li><li><a class="tocitem" href="Api.html">The Api module</a></li></ul></li><li><a class="tocitem" href="Perceptron.html">Perceptron</a></li><li class="is-active"><a class="tocitem" href="Trees.html">Trees</a><ul class="internal"><li><a class="tocitem" href="#Module-Index"><span>Module Index</span></a></li><li><a class="tocitem" href="#Detailed-API"><span>Detailed API</span></a></li></ul></li><li><a class="tocitem" href="Nn.html">Nn</a></li><li><a class="tocitem" href="Clustering.html">Clustering</a></li><li><a class="tocitem" href="GMM.html">GMM</a></li><li><a class="tocitem" href="Imputation.html">Imputation</a></li><li><a class="tocitem" href="Utils.html">Utils</a></li></ul></li></ul><div class="docs-version-selector field has-addons"><div class="control"><span class="docs-label button is-static is-size-7">Version</span></div><div class="docs-selector control is-expanded"><div class="select is-fullwidth is-size-7"><select id="documenter-version-selector"></select></div></div></div></nav><div class="docs-main"><header class="docs-navbar"><nav class="breadcrumb"><ul class="is-hidden-mobile"><li><a class="is-disabled">API (Reference manual)</a></li><li class="is-active"><a href="Trees.html">Trees</a></li></ul><ul class="is-hidden-tablet"><li class="is-active"><a href="Trees.html">Trees</a></li></ul></nav><div class="docs-right"><a class="docs-edit-link" href="https://github.com/sylvaticus/BetaML.jl/blob/master/docs/src/Trees.md" title="Edit on GitHub"><span class="docs-icon fab"></span><span class="docs-label is-hidden-touch">Edit on GitHub</span></a><a class="docs-settings-button fas fa-cog" id="documenter-settings-button" href="#" title="Settings"></a><a class="docs-sidebar-button fa fa-bars is-hidden-desktop" id="documenter-sidebar-button" href="#"></a></div></header><article class="content" id="documenter-page"><h1 id="trees_module"><a class="docs-heading-anchor" href="#trees_module">The BetaML.Trees Module</a><a id="trees_module-1"></a><a class="docs-heading-anchor-permalink" href="#trees_module" title="Permalink"></a></h1><article class="docstring"><header><a class="docstring-binding" id="BetaML.Trees" href="#BetaML.Trees"><code>BetaML.Trees</code></a> — <span class="docstring-category">Module</span></header><section><div><pre><code class="language-julia hljs">BetaML.Trees module</code></pre><p>Implement the <a href="Trees.html#BetaML.Trees.DecisionTreeEstimator"><code>DecisionTreeEstimator</code></a> and <a href="Trees.html#BetaML.Trees.RandomForestEstimator"><code>RandomForestEstimator</code></a> models (Decision Trees and Random Forests).</p><p>Both Decision Trees and Random Forests can be used for regression or classification problems, based on the type of the labels (numerical or not). The automatic selection can be overridden with the parameter <code>force_classification=true</code>, typically if labels are integer representing some categories rather than numbers. For classification problems the output of <code>predict</code> is a dictionary with the key being the labels with non-zero probabilitity and the corresponding value its probability; for regression it is a numerical value.</p><p>Please be aware that, differently from most other implementations, the Random Forest algorithm collects and averages the probabilities from the trees, rather than just repording the mode, i.e. no information is lost and the output of the forest classifier is still a PMF.</p><p>To retrieve the prediction with the highest probability use <a href="Utils.html#BetaML.Utils.mode-Union{Tuple{AbstractArray{Dict{T, Float64}, N} where N}, Tuple{T}} where T"><code>mode</code></a> over the prediciton returned by the model. Most error/accuracy measures in the <a href="Utils.html#BetaML.Utils"><code>Utils</code></a> BetaML module works diretly with this format.</p><p>Missing data and trully unordered types are supported on the features, both on training and on prediction.</p><p>The module provide the following functions. Use <code>?[type or function]</code> to access their full signature and detailed documentation:</p><p>Features are expected to be in the standard format (nRecords × nDimensions matrices) and the labels (either categorical or numerical) as a nRecords column vector.</p><p>Acknowlegdments: originally based on the <a href="https://www.youtube.com/watch?v=LDRbO9a6XPU">Josh Gordon&#39;s code</a></p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/sylvaticus/BetaML.jl/blob/a758127e0ec2c69cc6d6bf1f2b37a4db462b4b58/src/Trees/Trees.jl#L4-L22">source</a></section></article><h2 id="Module-Index"><a class="docs-heading-anchor" href="#Module-Index">Module Index</a><a id="Module-Index-1"></a><a class="docs-heading-anchor-permalink" href="#Module-Index" title="Permalink"></a></h2><ul><li><a href="Trees.html#BetaML.Trees.DTHyperParametersSet"><code>BetaML.Trees.DTHyperParametersSet</code></a></li><li><a href="Trees.html#BetaML.Trees.DecisionNode"><code>BetaML.Trees.DecisionNode</code></a></li><li><a href="Trees.html#BetaML.Trees.DecisionTreeClassifier"><code>BetaML.Trees.DecisionTreeClassifier</code></a></li><li><a href="Trees.html#BetaML.Trees.DecisionTreeEstimator"><code>BetaML.Trees.DecisionTreeEstimator</code></a></li><li><a href="Trees.html#BetaML.Trees.DecisionTreeRegressor"><code>BetaML.Trees.DecisionTreeRegressor</code></a></li><li><a href="Trees.html#BetaML.Trees.InfoNode"><code>BetaML.Trees.InfoNode</code></a></li><li><a href="Trees.html#BetaML.Trees.Leaf"><code>BetaML.Trees.Leaf</code></a></li><li><a href="Trees.html#BetaML.Trees.RFHyperParametersSet"><code>BetaML.Trees.RFHyperParametersSet</code></a></li><li><a href="Trees.html#BetaML.Trees.RandomForestClassifier"><code>BetaML.Trees.RandomForestClassifier</code></a></li><li><a href="Trees.html#BetaML.Trees.RandomForestEstimator"><code>BetaML.Trees.RandomForestEstimator</code></a></li><li><a href="Trees.html#BetaML.Trees.RandomForestRegressor"><code>BetaML.Trees.RandomForestRegressor</code></a></li></ul><h2 id="Detailed-API"><a class="docs-heading-anchor" href="#Detailed-API">Detailed API</a><a id="Detailed-API-1"></a><a class="docs-heading-anchor-permalink" href="#Detailed-API" title="Permalink"></a></h2><article class="docstring"><header><a class="docstring-binding" id="BetaML.Trees.DTHyperParametersSet" href="#BetaML.Trees.DTHyperParametersSet"><code>BetaML.Trees.DTHyperParametersSet</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">mutable struct DTHyperParametersSet &lt;: BetaMLHyperParametersSet</code></pre><p>Hyperparameters for <a href="Trees.html#BetaML.Trees.DecisionTreeEstimator"><code>DecisionTreeEstimator</code></a> (Decision Tree).</p><p><strong>Parameters:</strong></p><ul><li><p><code>max_depth::Union{Nothing, Int64}</code>: The maximum depth the tree is allowed to reach. When this is reached the node is forced to become a leaf [def: <code>nothing</code>, i.e. no limits]</p></li><li><p><code>min_gain::Float64</code>: The minimum information gain to allow for a node&#39;s partition [def: <code>0</code>]</p></li><li><p><code>min_records::Int64</code>: The minimum number of records a node must holds to consider for a partition of it [def: <code>2</code>]</p></li><li><p><code>max_features::Union{Nothing, Int64}</code>: The maximum number of (random) features to consider at each partitioning [def: <code>nothing</code>, i.e. look at all features]</p></li><li><p><code>force_classification::Bool</code>: Whether to force a classification task even if the labels are numerical (typically when labels are integers encoding some feature rather than representing a real cardinal measure) [def: <code>false</code>]</p></li><li><p><code>splitting_criterion::Union{Nothing, Function}</code>: This is the name of the function to be used to compute the information gain of a specific partition. This is done by measuring the difference betwwen the &quot;impurity&quot; of the labels of the parent node with those of the two child nodes, weighted by the respective number of items. [def: <code>nothing</code>, i.e. <code>gini</code> for categorical labels (classification task) and <code>variance</code> for numerical labels(regression task)]. Either <code>gini</code>, <code>entropy</code>, <code>variance</code> or a custom function. It can also be an anonymous function.</p></li><li><p><code>fast_algorithm::Bool</code>: Use an experimental faster algoritm for looking up the best split in ordered fields (colums). Currently it brings down the fitting time of an order of magnitude, but predictions are sensibly affected. If used, control the meaning of integer fields with <code>integer_encoded_cols</code>.</p></li><li><p><code>integer_encoded_cols::Union{Nothing, Vector{Int64}}</code>: A vector of columns positions to specify which integer columns should be treated as encoding of categorical variables insteads of ordered classes/values. [def: <code>nothing</code>, integer columns with less than 20 unique values are considered categorical]. Useful in conjunction with <code>fast_algorithm</code>, little difference otherwise.</p></li><li><p><code>tunemethod::AutoTuneMethod</code>: The method - and its parameters - to employ for hyperparameters autotuning. See <a href="Utils.html#BetaML.Utils.SuccessiveHalvingSearch"><code>SuccessiveHalvingSearch</code></a> for the default method. To implement automatic hyperparameter tuning during the (first) <code>fit!</code> call simply set <code>autotune=true</code> and eventually change the default <code>tunemethod</code> options (including the parameter ranges, the resources to employ and the loss function to adopt).</p></li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/sylvaticus/BetaML.jl/blob/a758127e0ec2c69cc6d6bf1f2b37a4db462b4b58/src/Trees/DecisionTrees.jl#L111-L119">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="BetaML.Trees.DecisionNode" href="#BetaML.Trees.DecisionNode"><code>BetaML.Trees.DecisionNode</code></a> — <span class="docstring-category">Type</span></header><section><div><p>DecisionNode(question,trueBranch,falseBranch, depth)</p><p>A tree&#39;s non-terminal node.</p><p><strong>Constructor&#39;s arguments and struct members:</strong></p><ul><li><code>question</code>: The question asked in this node</li><li><code>trueBranch</code>: A reference to the &quot;true&quot; branch of the trees</li><li><code>falseBranch</code>: A reference to the &quot;false&quot; branch of the trees</li><li><code>depth</code>: The nodes&#39;s depth in the tree</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/sylvaticus/BetaML.jl/blob/a758127e0ec2c69cc6d6bf1f2b37a4db462b4b58/src/Trees/DecisionTrees.jl#L87-L97">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="BetaML.Trees.DecisionTreeClassifier" href="#BetaML.Trees.DecisionTreeClassifier"><code>BetaML.Trees.DecisionTreeClassifier</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">mutable struct DecisionTreeClassifier &lt;: MLJModelInterface.Probabilistic</code></pre><p>A simple Decision Tree model for classification with support for Missing data, from the Beta Machine Learning Toolkit (BetaML).</p><p><strong>Hyperparameters:</strong></p><ul><li><p><code>max_depth::Int64</code>: The maximum depth the tree is allowed to reach. When this is reached the node is forced to become a leaf [def: <code>0</code>, i.e. no limits]</p></li><li><p><code>min_gain::Float64</code>: The minimum information gain to allow for a node&#39;s partition [def: <code>0</code>]</p></li><li><p><code>min_records::Int64</code>: The minimum number of records a node must holds to consider for a partition of it [def: <code>2</code>]</p></li><li><p><code>max_features::Int64</code>: The maximum number of (random) features to consider at each partitioning [def: <code>0</code>, i.e. look at all features]</p></li><li><p><code>splitting_criterion::Function</code>: This is the name of the function to be used to compute the information gain of a specific partition. This is done by measuring the difference betwwen the &quot;impurity&quot; of the labels of the parent node with those of the two child nodes, weighted by the respective number of items. [def: <code>gini</code>]. Either <code>gini</code>, <code>entropy</code> or a custom function. It can also be an anonymous function.</p></li><li><p><code>rng::Random.AbstractRNG</code>: A Random Number Generator to be used in stochastic parts of the code [deafult: <code>Random.GLOBAL_RNG</code>]</p></li></ul><p><strong>Example:</strong></p><pre><code class="language-julia hljs">
julia&gt; using MLJ

julia&gt; X, y                        = @load_iris;

julia&gt; modelType                   = @load DecisionTreeClassifier pkg = &quot;BetaML&quot; verbosity=0
BetaML.Trees.DecisionTreeClassifier

julia&gt; model                       = modelType()
DecisionTreeClassifier(
  max_depth = 0, 
  min_gain = 0.0, 
  min_records = 2, 
  max_features = 0, 
  splitting_criterion = BetaML.Utils.gini, 
  rng = Random._GLOBAL_RNG())

julia&gt; (fitResults, cache, report) = MLJ.fit(model, 0, X, y);

julia&gt; class_est                   = predict(model, fitResults, X)
150-element CategoricalDistributions.UnivariateFiniteVector{Multiclass{3}, String, UInt32, Float64}:
 UnivariateFinite{Multiclass{3}}(setosa=&gt;1.0, versicolor=&gt;0.0, virginica=&gt;0.0)
 UnivariateFinite{Multiclass{3}}(setosa=&gt;1.0, versicolor=&gt;0.0, virginica=&gt;0.0)
 UnivariateFinite{Multiclass{3}}(setosa=&gt;1.0, versicolor=&gt;0.0, virginica=&gt;0.0)
 ⋮
 UnivariateFinite{Multiclass{3}}(setosa=&gt;0.0, versicolor=&gt;0.0, virginica=&gt;1.0)
 UnivariateFinite{Multiclass{3}}(setosa=&gt;0.0, versicolor=&gt;0.0, virginica=&gt;1.0)
 UnivariateFinite{Multiclass{3}}(setosa=&gt;0.0, versicolor=&gt;0.0, virginica=&gt;1.0)</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/sylvaticus/BetaML.jl/blob/a758127e0ec2c69cc6d6bf1f2b37a4db462b4b58/src/Trees/Trees_MLJ.jl#L78">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="BetaML.Trees.DecisionTreeEstimator" href="#BetaML.Trees.DecisionTreeEstimator"><code>BetaML.Trees.DecisionTreeEstimator</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">mutable struct DecisionTreeEstimator &lt;: BetaMLSupervisedModel</code></pre><p>A Decision Tree classifier and regressor (supervised).</p><p>Decision Tree works by finding the &quot;best&quot; question to split the fitting data (according to the metric specified by the parameter <code>splitting_criterion</code> on the associated labels) untill either all the dataset is separated or a terminal condition is reached. </p><p>For the parameters see <a href="Trees.html#BetaML.Trees.DTHyperParametersSet"><code>?DTHyperParametersSet</code></a> and <a href="Api.html#BetaML.Api.BetaMLDefaultOptionsSet"><code>?BetaMLDefaultOptionsSet</code></a>.</p><p><strong>Notes:</strong></p><ul><li>Online fitting (re-fitting with new data) is not supported</li><li>Missing data (in the feature dataset) is supported.</li></ul><p><strong>Examples:</strong></p><ul><li>Classification...</li></ul><pre><code class="language-julia hljs">julia&gt; using BetaML

julia&gt; X   = [1.8 2.5; 0.5 20.5; 0.6 18; 0.7 22.8; 0.4 31; 1.7 3.7];

julia&gt; y   = [&quot;a&quot;,&quot;b&quot;,&quot;b&quot;,&quot;b&quot;,&quot;b&quot;,&quot;a&quot;];

julia&gt; mod = DecisionTreeEstimator(max_depth=5)
DecisionTreeEstimator - A Decision Tree model (unfitted)

julia&gt; ŷ   = fit!(mod,X,y) |&gt; mode
6-element Vector{String}:
 &quot;a&quot;
 &quot;b&quot;
 &quot;b&quot;
 &quot;b&quot;
 &quot;b&quot;
 &quot;a&quot;

julia&gt; println(mod)
DecisionTreeEstimator - A Decision Tree classifier (fitted on 6 records)
Dict{String, Any}(&quot;job_is_regression&quot; =&gt; 0, &quot;fitted_records&quot; =&gt; 6, &quot;max_reached_depth&quot; =&gt; 2, &quot;avg_depth&quot; =&gt; 2.0, &quot;xndims&quot; =&gt; 2)
*** Printing Decision Tree: ***

1. Is col 2 &gt;= 18.0 ?
--&gt; True :  Dict(&quot;b&quot; =&gt; 1.0)
--&gt; False:  Dict(&quot;a&quot; =&gt; 1.0)</code></pre><ul><li>Regression...</li></ul><pre><code class="language-julia hljs">julia&gt; using BetaML

julia&gt; X = [1.8 2.5; 0.5 20.5; 0.6 18; 0.7 22.8; 0.4 31; 1.7 3.7];

julia&gt; y = 2 .* X[:,1] .- X[:,2] .+ 3;

julia&gt; mod = DecisionTreeEstimator(max_depth=10)
DecisionTreeEstimator - A Decision Tree model (unfitted)

julia&gt; ŷ   = fit!(mod,X,y);

julia&gt; hcat(y,ŷ)
6×2 Matrix{Float64}:
   4.1    3.4
 -16.5  -17.45
 -13.8  -13.8
 -18.4  -17.45
 -27.2  -27.2
   2.7    3.4

julia&gt; println(mod)
DecisionTreeEstimator - A Decision Tree regressor (fitted on 6 records)
Dict{String, Any}(&quot;job_is_regression&quot; =&gt; 1, &quot;fitted_records&quot; =&gt; 6, &quot;max_reached_depth&quot; =&gt; 4, &quot;avg_depth&quot; =&gt; 3.25, &quot;xndims&quot; =&gt; 2)
*** Printing Decision Tree: ***

1. Is col 2 &gt;= 18.0 ?
--&gt; True :
                1.2. Is col 2 &gt;= 31.0 ?
                --&gt; True :  -27.2
                --&gt; False:
                        1.2.3. Is col 2 &gt;= 20.5 ?
                        --&gt; True :  -17.450000000000003
                        --&gt; False:  -13.8
--&gt; False:  3.3999999999999995</code></pre><ul><li>Visualisation...</li></ul><p>You can either text-print or plot a decision tree using the <code>AbstractTree</code> and <code>TreeRecipe</code> package..</p><p>```julia julia&gt; println(mod) DecisionTreeEstimator - A Decision Tree regressor (fitted on 6 records) Dict{String, Any}(&quot;job<em>is</em>regression&quot; =&gt; 1, &quot;fitted<em>records&quot; =&gt; 6, &quot;max</em>reached<em>depth&quot; =&gt; 4, &quot;avg</em>depth&quot; =&gt; 3.25, &quot;xndims&quot; =&gt; 2) *** Printing Decision Tree: ***</p><ol><li>Is col 2 &gt;= 18.0 ?</li></ol><p>–&gt; True :                 1.2. Is col 2 &gt;= 31.0 ?                 –&gt; True :  -27.2                 –&gt; False:                         1.2.3. Is col 2 &gt;= 20.5 ?                         –&gt; True :  -17.450000000000003                         –&gt; False:  -13.8 –&gt; False:  3.3999999999999995</p><p>julia&gt; using Plots, TreeRecipe, AbstractTrees julia&gt; featurenames = [&quot;Something&quot;, &quot;Som else&quot;]; julia&gt; wrapped<em>tree   = wrap(dtree, featurenames = featurenames); # featurenames is otional julia&gt; print</em>tree(wrapped<em>tree) Som else &gt;= 18.0? ├─ Som else &gt;= 31.0? │  ├─ -27.2 │  │   │  └─ Som else &gt;= 20.5? │     ├─ -17.450000000000003 │     │   │     └─ -13.8 │         └─ 3.3999999999999995 julia&gt; plot(wrapped</em>tree)     ```` <img src="assets/dtplot.png" alt="DT plot"/> </p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/sylvaticus/BetaML.jl/blob/a758127e0ec2c69cc6d6bf1f2b37a4db462b4b58/src/Trees/DecisionTrees.jl#L151">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="BetaML.Trees.DecisionTreeRegressor" href="#BetaML.Trees.DecisionTreeRegressor"><code>BetaML.Trees.DecisionTreeRegressor</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">mutable struct DecisionTreeRegressor &lt;: MLJModelInterface.Deterministic</code></pre><p>A simple Decision Tree model for regression with support for Missing data, from the Beta Machine Learning Toolkit (BetaML).</p><p><strong>Hyperparameters:</strong></p><ul><li><p><code>max_depth::Int64</code>: The maximum depth the tree is allowed to reach. When this is reached the node is forced to become a leaf [def: <code>0</code>, i.e. no limits]</p></li><li><p><code>min_gain::Float64</code>: The minimum information gain to allow for a node&#39;s partition [def: <code>0</code>]</p></li><li><p><code>min_records::Int64</code>: The minimum number of records a node must holds to consider for a partition of it [def: <code>2</code>]</p></li><li><p><code>max_features::Int64</code>: The maximum number of (random) features to consider at each partitioning [def: <code>0</code>, i.e. look at all features]</p></li><li><p><code>splitting_criterion::Function</code>: This is the name of the function to be used to compute the information gain of a specific partition. This is done by measuring the difference betwwen the &quot;impurity&quot; of the labels of the parent node with those of the two child nodes, weighted by the respective number of items. [def: <code>variance</code>]. Either <code>variance</code> or a custom function. It can also be an anonymous function.</p></li><li><p><code>rng::Random.AbstractRNG</code>: A Random Number Generator to be used in stochastic parts of the code [deafult: <code>Random.GLOBAL_RNG</code>]</p></li></ul><p><strong>Example:</strong></p><pre><code class="language-julia hljs">julia&gt; using MLJ

julia&gt; X, y                        = @load_boston;

julia&gt; modelType                   = @load DecisionTreeRegressor pkg = &quot;BetaML&quot; verbosity=0
BetaML.Trees.DecisionTreeRegressor

julia&gt; model                       = modelType()
DecisionTreeRegressor(
  max_depth = 0, 
  min_gain = 0.0, 
  min_records = 2, 
  max_features = 0, 
  splitting_criterion = BetaML.Utils.variance, 
  rng = Random._GLOBAL_RNG())

julia&gt; (fitResults, cache, report) = MLJ.fit(model, 0, X, y);

julia&gt; y_est                       = predict(model, fitResults, X)
506-element Vector{Float64}:
 26.35
 21.6
 34.8
  ⋮
 23.75
 22.2
 13.2</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/sylvaticus/BetaML.jl/blob/a758127e0ec2c69cc6d6bf1f2b37a4db462b4b58/src/Trees/Trees_MLJ.jl#L14">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="BetaML.Trees.InfoNode" href="#BetaML.Trees.InfoNode"><code>BetaML.Trees.InfoNode</code></a> — <span class="docstring-category">Type</span></header><section><div><p>These types are introduced so that additional information currently not present in  a <code>DecisionTree</code>-structure – namely the feature names  –  can be used for visualization.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/sylvaticus/BetaML.jl/blob/a758127e0ec2c69cc6d6bf1f2b37a4db462b4b58/src/Trees/AbstractTrees_BetaML_interface.jl#L16-L20">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="BetaML.Trees.Leaf" href="#BetaML.Trees.Leaf"><code>BetaML.Trees.Leaf</code></a> — <span class="docstring-category">Type</span></header><section><div><p>Leaf(y,depth)</p><p>A tree&#39;s leaf (terminal) node.</p><p><strong>Constructor&#39;s arguments:</strong></p><ul><li><code>y</code>: The labels assorciated to each record (either numerical or categorical)</li><li><code>depth</code>: The nodes&#39;s depth in the tree</li></ul><p><strong>Struct members:</strong></p><ul><li><code>predictions</code>: Either the relative label&#39;s count (i.e. a PMF) or the mean</li><li><code>depth</code>: The nodes&#39;s depth in the tree</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/sylvaticus/BetaML.jl/blob/a758127e0ec2c69cc6d6bf1f2b37a4db462b4b58/src/Trees/DecisionTrees.jl#L41-L53">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="BetaML.Trees.RFHyperParametersSet" href="#BetaML.Trees.RFHyperParametersSet"><code>BetaML.Trees.RFHyperParametersSet</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">mutable struct RFHyperParametersSet &lt;: BetaMLHyperParametersSet</code></pre><p>Hyperparameters for <a href="Trees.html#BetaML.Trees.RandomForestEstimator"><code>RandomForestEstimator</code></a> (Random Forest).</p><p><strong>Parameters:</strong></p><ul><li><p><code>n_trees::Int64</code>: Number of (decision) trees in the forest [def: <code>30</code>]</p></li><li><p><code>max_depth::Union{Nothing, Int64}</code>: The maximum depth the tree is allowed to reach. When this is reached the node is forced to become a leaf [def: <code>nothing</code>, i.e. no limits]</p></li><li><p><code>min_gain::Float64</code>: The minimum information gain to allow for a node&#39;s partition [def: <code>0</code>]</p></li><li><p><code>min_records::Int64</code>: The minimum number of records a node must holds to consider for a partition of it [def: <code>2</code>]</p></li><li><p><code>max_features::Union{Nothing, Int64}</code>: The maximum number of (random) features to consider when choosing the optimal partition of the dataset [def: <code>nothing</code>, i.e. square root of the dimensions of the training data`]</p></li><li><p><code>force_classification::Bool</code>: Whether to force a classification task even if the labels are numerical (typically when labels are integers encoding some feature rather than representing a real cardinal measure) [def: <code>false</code>]</p></li><li><p><code>splitting_criterion::Union{Nothing, Function}</code>: Either <code>gini</code>, <code>entropy</code> or <code>variance</code>. This is the name of the function to be used to compute the information gain of a specific partition. This is done by measuring the difference betwwen the &quot;impurity&quot; of the labels of the parent node with those of the two child nodes, weighted by the respective number of items. [def: <code>nothing</code>, i.e. <code>gini</code> for categorical labels (classification task) and <code>variance</code> for numerical labels(regression task)]. It can be an anonymous function.</p></li><li><p><code>fast_algorithm::Bool</code>: Use an experimental faster algoritm for looking up the best split in ordered fields (colums). Currently it brings down the fitting time of an order of magnitude, but predictions are sensibly affected. If used, control the meaning of integer fields with <code>integer_encoded_cols</code>.</p></li><li><p><code>integer_encoded_cols::Union{Nothing, Vector{Int64}}</code>: A vector of columns positions to specify which integer columns should be treated as encoding of categorical variables insteads of ordered classes/values. [def: <code>nothing</code>, integer columns with less than 20 unique values are considered categorical]. Useful in conjunction with <code>fast_algorithm</code>, little difference otherwise.</p></li><li><p><code>beta::Float64</code>: Parameter that regulate the weights of the scoring of each tree, to be (optionally) used in prediction based on the error of the individual trees computed on the records on which trees have not been trained. Higher values favour &quot;better&quot; trees, but too high values will cause overfitting [def: <code>0</code>, i.e. uniform weigths]</p></li><li><p><code>oob::Bool</code>: Wheter to compute the <em>Out-Of-Bag</em> error, an estimation of the validation error (the mismatching error for classification and the relative mean error for regression jobs).</p></li><li><p><code>tunemethod::AutoTuneMethod</code>: The method - and its parameters - to employ for hyperparameters autotuning. See <a href="Utils.html#BetaML.Utils.SuccessiveHalvingSearch"><code>SuccessiveHalvingSearch</code></a> for the default method. To implement automatic hyperparameter tuning during the (first) <code>fit!</code> call simply set <code>autotune=true</code> and eventually change the default <code>tunemethod</code> options (including the parameter ranges, the resources to employ and the loss function to adopt).</p></li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/sylvaticus/BetaML.jl/blob/a758127e0ec2c69cc6d6bf1f2b37a4db462b4b58/src/Trees/RandomForests.jl#L30-L38">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="BetaML.Trees.RandomForestClassifier" href="#BetaML.Trees.RandomForestClassifier"><code>BetaML.Trees.RandomForestClassifier</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">mutable struct RandomForestClassifier &lt;: MLJModelInterface.Probabilistic</code></pre><p>A simple Random Forest model for classification with support for Missing data, from the Beta Machine Learning Toolkit (BetaML).</p><p><strong>Hyperparameters:</strong></p><ul><li><p><code>n_trees::Int64</code></p></li><li><p><code>max_depth::Int64</code>: The maximum depth the tree is allowed to reach. When this is reached the node is forced to become a leaf [def: <code>0</code>, i.e. no limits]</p></li><li><p><code>min_gain::Float64</code>: The minimum information gain to allow for a node&#39;s partition [def: <code>0</code>]</p></li><li><p><code>min_records::Int64</code>: The minimum number of records a node must holds to consider for a partition of it [def: <code>2</code>]</p></li><li><p><code>max_features::Int64</code>: The maximum number of (random) features to consider at each partitioning [def: <code>0</code>, i.e. square root of the data dimensions]</p></li><li><p><code>splitting_criterion::Function</code>: This is the name of the function to be used to compute the information gain of a specific partition. This is done by measuring the difference betwwen the &quot;impurity&quot; of the labels of the parent node with those of the two child nodes, weighted by the respective number of items. [def: <code>gini</code>]. Either <code>gini</code>, <code>entropy</code> or a custom function. It can also be an anonymous function.</p></li><li><p><code>β::Float64</code>: Parameter that regulate the weights of the scoring of each tree, to be (optionally) used in prediction based on the error of the individual trees computed on the records on which trees have not been trained. Higher values favour &quot;better&quot; trees, but too high values will cause overfitting [def: <code>0</code>, i.e. uniform weigths]</p></li><li><p><code>rng::Random.AbstractRNG</code>: A Random Number Generator to be used in stochastic parts of the code [deafult: <code>Random.GLOBAL_RNG</code>]</p></li></ul><p><strong>Example :</strong></p><pre><code class="language-julia hljs">julia&gt; using MLJ

julia&gt; X, y                        = @load_iris;

julia&gt; modelType                   = @load RandomForestClassifier pkg = &quot;BetaML&quot; verbosity=0
BetaML.Trees.RandomForestClassifier

julia&gt; model                       = modelType()
RandomForestClassifier(
  n_trees = 30, 
  max_depth = 0, 
  min_gain = 0.0, 
  min_records = 2, 
  max_features = 0, 
  splitting_criterion = BetaML.Utils.gini, 
  β = 0.0, 
  rng = Random._GLOBAL_RNG())

julia&gt; (fitResults, cache, report) = MLJ.fit(model, 0, X, y);

julia&gt; class_est                   = predict(model, fitResults, X)
150-element CategoricalDistributions.UnivariateFiniteVector{Multiclass{3}, String, UInt32, Float64}:
 UnivariateFinite{Multiclass{3}}(setosa=&gt;1.0, versicolor=&gt;0.0, virginica=&gt;0.0)
 UnivariateFinite{Multiclass{3}}(setosa=&gt;1.0, versicolor=&gt;0.0, virginica=&gt;0.0)
 UnivariateFinite{Multiclass{3}}(setosa=&gt;1.0, versicolor=&gt;0.0, virginica=&gt;0.0)
 ⋮
 UnivariateFinite{Multiclass{3}}(setosa=&gt;0.0, versicolor=&gt;0.0, virginica=&gt;1.0)
 UnivariateFinite{Multiclass{3}}(setosa=&gt;0.0, versicolor=&gt;0.0, virginica=&gt;1.0)
 UnivariateFinite{Multiclass{3}}(setosa=&gt;0.0, versicolor=&gt;0.0, virginica=&gt;1.0)</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/sylvaticus/BetaML.jl/blob/a758127e0ec2c69cc6d6bf1f2b37a4db462b4b58/src/Trees/Trees_MLJ.jl#L213">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="BetaML.Trees.RandomForestEstimator" href="#BetaML.Trees.RandomForestEstimator"><code>BetaML.Trees.RandomForestEstimator</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">mutable struct RandomForestEstimator &lt;: BetaMLSupervisedModel</code></pre><p>A Random Forest classifier and regressor (supervised).</p><p>Random forests are <em>ensemble</em> of Decision Trees models (see <a href="Trees.html#BetaML.Trees.DecisionTreeEstimator"><code>?DecisionTreeEstimator</code></a>).</p><p>For the parameters see <a href="Trees.html#BetaML.Trees.RFHyperParametersSet"><code>?RFHyperParametersSet</code></a> and <a href="Api.html#BetaML.Api.BetaMLDefaultOptionsSet"><code>?BetaMLDefaultOptionsSet</code></a>.</p><p><strong>Notes :</strong></p><ul><li>Each individual decision tree is built using bootstrap over the data, i.e. &quot;sampling N records with replacement&quot; (hence, some records appear multiple times and some records do not appear in the specific tree training). The <code>maxx_feature</code> injects further variability and reduces the correlation between the forest trees.</li><li>The predictions of the &quot;forest&quot; (using the function <code>predict()</code>) are then the aggregated predictions of the individual trees (from which the name &quot;bagging&quot;: <strong>b</strong>oostrap <strong>agg</strong>regat<strong>ing</strong>).</li><li>The performances of each individual trees,  as measured using the records they have not being trained with, can then be (optionally) used as weights in the <code>predict</code> function. The parameter <code>beta ≥ 0</code> regulate the distribution of these weights: larger is <code>β</code>, the greater the importance (hence the weights) attached to the best-performing trees compared to the low-performing ones. Using these weights can significantly improve the forest performances (especially using small forests), however the correct value of <code>beta</code> depends on the problem under exam (and the chosen caratteristics of the random forest estimator) and should be cross-validated to avoid over-fitting.</li><li>Note that training <code>RandomForestEstimator</code> uses multiple threads if these are available. You can check the number of threads available with <code>Threads.nthreads()</code>. To set the number of threads in Julia either set the environmental variable <code>JULIA_NUM_THREADS</code> (before starting Julia) or start Julia with the command line option <code>--threads</code> (most integrated development editors for Julia already set the number of threads to 4).</li><li>Online fitting (re-fitting with new data) is not supported</li><li>Missing data (in the feature dataset) is supported.</li></ul><p><strong>Examples:</strong></p><ul><li>Classification...</li></ul><pre><code class="language-julia hljs">julia&gt; using BetaML

julia&gt; X   = [1.8 2.5; 0.5 20.5; 0.6 18; 0.7 22.8; 0.4 31; 1.7 3.7];

julia&gt; y   = [&quot;a&quot;,&quot;b&quot;,&quot;b&quot;,&quot;b&quot;,&quot;b&quot;,&quot;a&quot;];

julia&gt; mod = RandomForestEstimator(n_trees=5)
RandomForestEstimator - A 5 trees Random Forest model (unfitted)

julia&gt; ŷ   = fit!(mod,X,y) |&gt; mode
6-element Vector{String}:
 &quot;a&quot;
 &quot;b&quot;
 &quot;b&quot;
 &quot;b&quot;
 &quot;b&quot;
 &quot;a&quot;

julia&gt; println(mod)
RandomForestEstimator - A 5 trees Random Forest classifier (fitted on 6 records)
Dict{String, Any}(&quot;job_is_regression&quot; =&gt; 0, &quot;avg_avg_depth&quot; =&gt; 1.8, &quot;fitted_records&quot; =&gt; 6, &quot;avg_mmax_reached_depth&quot; =&gt; 1.8, &quot;oob_errors&quot; =&gt; Inf, &quot;xndims&quot; =&gt; 2)</code></pre><ul><li>Regression...</li></ul><pre><code class="language-julia hljs">julia&gt; using BetaML

julia&gt; X = [1.8 2.5; 0.5 20.5; 0.6 18; 0.7 22.8; 0.4 31; 1.7 3.7];

julia&gt; y = 2 .* X[:,1] .- X[:,2] .+ 3;

julia&gt; mod = RandomForestEstimator(n_trees=5)
RandomForestEstimator - A 5 trees Random Forest model (unfitted)

julia&gt; ŷ   = fit!(mod,X,y);

julia&gt; hcat(y,ŷ)
6×2 Matrix{Float64}:
   4.1    2.98
 -16.5  -18.37
 -13.8  -14.61
 -18.4  -17.37
 -27.2  -20.78
   2.7    2.98

julia&gt; println(mod)
RandomForestEstimator - A 5 trees Random Forest regressor (fitted on 6 records)
Dict{String, Any}(&quot;job_is_regression&quot; =&gt; 1, &quot;fitted_records&quot; =&gt; 6, &quot;avg_avg_depth&quot; =&gt; 2.8833333333333333, &quot;oob_errors&quot; =&gt; Inf, &quot;avg_max_reached_depth&quot; =&gt; 3.4, &quot;xndims&quot; =&gt; 2)</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/sylvaticus/BetaML.jl/blob/a758127e0ec2c69cc6d6bf1f2b37a4db462b4b58/src/Trees/RandomForests.jl#L76">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="BetaML.Trees.RandomForestRegressor" href="#BetaML.Trees.RandomForestRegressor"><code>BetaML.Trees.RandomForestRegressor</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">mutable struct RandomForestRegressor &lt;: MLJModelInterface.Deterministic</code></pre><p>A simple Random Forest model for regression with support for Missing data, from the Beta Machine Learning Toolkit (BetaML).</p><p><strong>Hyperparameters:</strong></p><ul><li><p><code>n_trees::Int64</code>: Number of (decision) trees in the forest [def: <code>30</code>]</p></li><li><p><code>max_depth::Int64</code>: The maximum depth the tree is allowed to reach. When this is reached the node is forced to become a leaf [def: <code>0</code>, i.e. no limits]</p></li><li><p><code>min_gain::Float64</code>: The minimum information gain to allow for a node&#39;s partition [def: <code>0</code>]</p></li><li><p><code>min_records::Int64</code>: The minimum number of records a node must holds to consider for a partition of it [def: <code>2</code>]</p></li><li><p><code>max_features::Int64</code>: The maximum number of (random) features to consider at each partitioning [def: <code>0</code>, i.e. square root of the data dimension]</p></li><li><p><code>splitting_criterion::Function</code>: This is the name of the function to be used to compute the information gain of a specific partition. This is done by measuring the difference betwwen the &quot;impurity&quot; of the labels of the parent node with those of the two child nodes, weighted by the respective number of items. [def: <code>variance</code>]. Either <code>variance</code> or a custom function. It can also be an anonymous function.</p></li><li><p><code>β::Float64</code>: Parameter that regulate the weights of the scoring of each tree, to be (optionally) used in prediction based on the error of the individual trees computed on the records on which trees have not been trained. Higher values favour &quot;better&quot; trees, but too high values will cause overfitting [def: <code>0</code>, i.e. uniform weigths]</p></li><li><p><code>rng::Random.AbstractRNG</code>: A Random Number Generator to be used in stochastic parts of the code [deafult: <code>Random.GLOBAL_RNG</code>]</p></li></ul><p><strong>Example:</strong></p><pre><code class="language-julia hljs">julia&gt; using MLJ

julia&gt; X, y                        = @load_boston;

julia&gt; modelType                   = @load RandomForestRegressor pkg = &quot;BetaML&quot; verbosity=0
BetaML.Trees.RandomForestRegressor

julia&gt; model                       = modelType()
RandomForestRegressor(
  n_trees = 30, 
  max_depth = 0, 
  min_gain = 0.0, 
  min_records = 2, 
  max_features = 0, 
  splitting_criterion = BetaML.Utils.variance, 
  β = 0.0, 
  rng = Random._GLOBAL_RNG())

julia&gt; (fitResults, cache, report) = MLJ.fit(model, 0, X, y);

julia&gt; y_est                       = predict(model, fitResults, X)
506-element Vector{Float64}:
 25.283333333333335
 22.700999999999997
 36.67500000000002
  ⋮
 19.378333333333334
 24.191666666666663
 23.726666666666674
 15.393333333333327</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/sylvaticus/BetaML.jl/blob/a758127e0ec2c69cc6d6bf1f2b37a4db462b4b58/src/Trees/Trees_MLJ.jl#L142">source</a></section></article></article><nav class="docs-footer"><a class="docs-footer-prevpage" href="Perceptron.html">« Perceptron</a><a class="docs-footer-nextpage" href="Nn.html">Nn »</a><div class="flexbox-break"></div><p class="footer-message">Powered by <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> and the <a href="https://julialang.org/">Julia Programming Language</a>.</p></nav></div><div class="modal" id="documenter-settings"><div class="modal-background"></div><div class="modal-card"><header class="modal-card-head"><p class="modal-card-title">Settings</p><button class="delete"></button></header><section class="modal-card-body"><p><label class="label">Theme</label><div class="select"><select id="documenter-themepicker"><option value="documenter-light">documenter-light</option><option value="documenter-dark">documenter-dark</option></select></div></p><hr/><p>This document was generated with <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> version 0.27.23 on <span class="colophon-date" title="Tuesday 21 February 2023 14:41">Tuesday 21 February 2023</span>. Using Julia version 1.6.7.</p></section><footer class="modal-card-foot"></footer></div></div></div></body></html>
