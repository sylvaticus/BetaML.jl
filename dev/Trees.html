<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>Trees · BetaML.jl Documentation</title><script async src="https://www.googletagmanager.com/gtag/js?id=G-JYKX8QY5JW"></script><script>  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());
  gtag('config', 'G-JYKX8QY5JW', {'page_path': location.pathname + location.search + location.hash});
</script><script data-outdated-warner src="assets/warner.js"></script><link href="https://cdnjs.cloudflare.com/ajax/libs/lato-font/3.0.0/css/lato-font.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/juliamono/0.045/juliamono.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.4/css/fontawesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.4/css/solid.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.4/css/brands.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.24/katex.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL="."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" data-main="assets/documenter.js"></script><script src="siteinfo.js"></script><script src="../versions.js"></script><link class="docs-theme-link" rel="stylesheet" type="text/css" href="assets/themes/documenter-dark.css" data-theme-name="documenter-dark" data-theme-primary-dark/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="assets/themes/documenter-light.css" data-theme-name="documenter-light" data-theme-primary/><script src="assets/themeswap.js"></script></head><body><div id="documenter"><nav class="docs-sidebar"><div class="docs-package-name"><span class="docs-autofit"><a href="index.html">BetaML.jl Documentation</a></span></div><form class="docs-search" action="search.html"><input class="docs-search-query" id="documenter-search-query" name="q" type="text" placeholder="Search docs"/></form><ul class="docs-menu"><li><a class="tocitem" href="index.html">Index</a></li><li><span class="tocitem">Tutorial</span><ul><li><a class="tocitem" href="tutorials/Betaml_tutorial_getting_started.html">Getting started</a></li><li><input class="collapse-toggle" id="menuitem-2-2" type="checkbox"/><label class="tocitem" for="menuitem-2-2"><span class="docs-label">Regression - bike sharing</span><i class="docs-chevron"></i></label><ul class="collapsed"><li><a class="tocitem" href="tutorials/Regression - bike sharing/betaml_tutorial_regression_sharingBikes.html">A regression task: the prediction of  bike  sharing demand</a></li></ul></li><li><input class="collapse-toggle" id="menuitem-2-3" type="checkbox"/><label class="tocitem" for="menuitem-2-3"><span class="docs-label">Classification - cars</span><i class="docs-chevron"></i></label><ul class="collapsed"><li><a class="tocitem" href="tutorials/Classification - cars/betaml_tutorial_classification_cars.html">A classification task when labels are known - determining the country of origin of cars given the cars characteristics</a></li></ul></li><li><input class="collapse-toggle" id="menuitem-2-4" type="checkbox"/><label class="tocitem" for="menuitem-2-4"><span class="docs-label">Clustering - Iris</span><i class="docs-chevron"></i></label><ul class="collapsed"><li><a class="tocitem" href="tutorials/Clustering - Iris/betaml_tutorial_cluster_iris.html">A clustering task: the prediction of  plant species from floreal measures (the iris dataset)</a></li></ul></li></ul></li><li><span class="tocitem">API (Reference manual)</span><ul><li><input class="collapse-toggle" id="menuitem-3-1" type="checkbox"/><label class="tocitem" for="menuitem-3-1"><span class="docs-label">API V2 (current)</span><i class="docs-chevron"></i></label><ul class="collapsed"><li><a class="tocitem" href="Api_v2_user.html">Introduction for user</a></li><li><input class="collapse-toggle" id="menuitem-3-1-2" type="checkbox"/><label class="tocitem" for="menuitem-3-1-2"><span class="docs-label">For developers</span><i class="docs-chevron"></i></label><ul class="collapsed"><li><a class="tocitem" href="Api_v2_developer.html">API implementation</a></li><li><a class="tocitem" href="StyleGuide_templates.html">Style guide</a></li></ul></li><li><a class="tocitem" href="Api.html">The Api module</a></li></ul></li><li><a class="tocitem" href="Perceptron.html">Perceptron</a></li><li class="is-active"><a class="tocitem" href="Trees.html">Trees</a><ul class="internal"><li><a class="tocitem" href="#Module-Index"><span>Module Index</span></a></li><li><a class="tocitem" href="#Detailed-API"><span>Detailed API</span></a></li></ul></li><li><a class="tocitem" href="Nn.html">Nn</a></li><li><a class="tocitem" href="Clustering.html">Clustering</a></li><li><a class="tocitem" href="GMM.html">GMM</a></li><li><a class="tocitem" href="Imputation.html">Imputation</a></li><li><a class="tocitem" href="Utils.html">Utils</a></li></ul></li></ul><div class="docs-version-selector field has-addons"><div class="control"><span class="docs-label button is-static is-size-7">Version</span></div><div class="docs-selector control is-expanded"><div class="select is-fullwidth is-size-7"><select id="documenter-version-selector"></select></div></div></div></nav><div class="docs-main"><header class="docs-navbar"><nav class="breadcrumb"><ul class="is-hidden-mobile"><li><a class="is-disabled">API (Reference manual)</a></li><li class="is-active"><a href="Trees.html">Trees</a></li></ul><ul class="is-hidden-tablet"><li class="is-active"><a href="Trees.html">Trees</a></li></ul></nav><div class="docs-right"><a class="docs-edit-link" href="https://github.com/sylvaticus/BetaML.jl/blob/master/docs/src/Trees.md" title="Edit on GitHub"><span class="docs-icon fab"></span><span class="docs-label is-hidden-touch">Edit on GitHub</span></a><a class="docs-settings-button fas fa-cog" id="documenter-settings-button" href="#" title="Settings"></a><a class="docs-sidebar-button fa fa-bars is-hidden-desktop" id="documenter-sidebar-button" href="#"></a></div></header><article class="content" id="documenter-page"><h1 id="trees_module"><a class="docs-heading-anchor" href="#trees_module">The BetaML.Trees Module</a><a id="trees_module-1"></a><a class="docs-heading-anchor-permalink" href="#trees_module" title="Permalink"></a></h1><article class="docstring"><header><a class="docstring-binding" id="BetaML.Trees" href="#BetaML.Trees"><code>BetaML.Trees</code></a> — <span class="docstring-category">Module</span></header><section><div><pre><code class="language-julia hljs">BetaML.Trees module</code></pre><p>Implement the <a href="Trees.html#BetaML.Trees.DecisionTreeEstimator"><code>DecisionTreeEstimator</code></a> and <a href="Trees.html#BetaML.Trees.RandomForestEstimator"><code>RandomForestEstimator</code></a> models (Decision Trees and Random Forests).</p><p>Both Decision Trees and Random Forests can be used for regression or classification problems, based on the type of the labels (numerical or not). The automatic selection can be overridden with the parameter <code>force_classification=true</code>, typically if labels are integer representing some categories rather than numbers. For classification problems the output of <code>predict</code> is a dictionary with the key being the labels with non-zero probabilitity and the corresponding value its probability; for regression it is a numerical value.</p><p>Please be aware that, differently from most other implementations, the Random Forest algorithm collects and averages the probabilities from the trees, rather than just repording the mode, i.e. no information is lost and the output of the forest classifier is still a PMF.</p><p>To retrieve the prediction with the highest probability use <a href="Utils.html#BetaML.Utils.mode-Union{Tuple{AbstractArray{Dict{T, Float64}, N} where N}, Tuple{T}} where T"><code>mode</code></a> over the prediciton returned by the model. Most error/accuracy measures in the <a href="@ref"><code>Util</code></a> BetaML module works diretly with this format.</p><p>Missing data and trully unordered types are supported on the features, both on training and on prediction.</p><p>The module provide the following functions. Use <code>?[type or function]</code> to access their full signature and detailed documentation:</p><p>Features are expected to be in the standard format (nRecords × nDimensions matrices) and the labels (either categorical or numerical) as a nRecords column vector.</p><p>Acknowlegdments: originally based on the <a href="https://www.youtube.com/watch?v=LDRbO9a6XPU">Josh Gordon&#39;s code</a></p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/sylvaticus/BetaML.jl/blob/a0e46177de858fa06c6b7194ba9b13449b2f0683/src/Trees/Trees.jl#L4-L22">source</a></section></article><h2 id="Module-Index"><a class="docs-heading-anchor" href="#Module-Index">Module Index</a><a id="Module-Index-1"></a><a class="docs-heading-anchor-permalink" href="#Module-Index" title="Permalink"></a></h2><ul><li><a href="Trees.html#BetaML.Trees.DTHyperParametersSet"><code>BetaML.Trees.DTHyperParametersSet</code></a></li><li><a href="Trees.html#BetaML.Trees.DecisionTreeClassifier"><code>BetaML.Trees.DecisionTreeClassifier</code></a></li><li><a href="Trees.html#BetaML.Trees.DecisionTreeEstimator"><code>BetaML.Trees.DecisionTreeEstimator</code></a></li><li><a href="Trees.html#BetaML.Trees.DecisionTreeRegressor"><code>BetaML.Trees.DecisionTreeRegressor</code></a></li><li><a href="Trees.html#BetaML.Trees.InfoNode"><code>BetaML.Trees.InfoNode</code></a></li><li><a href="Trees.html#BetaML.Trees.RFHyperParametersSet"><code>BetaML.Trees.RFHyperParametersSet</code></a></li><li><a href="Trees.html#BetaML.Trees.RandomForestClassifier"><code>BetaML.Trees.RandomForestClassifier</code></a></li><li><a href="Trees.html#BetaML.Trees.RandomForestEstimator"><code>BetaML.Trees.RandomForestEstimator</code></a></li><li><a href="Trees.html#BetaML.Trees.RandomForestRegressor"><code>BetaML.Trees.RandomForestRegressor</code></a></li><li><a href="Trees.html#BetaML.Trees.buildForest-Union{Tuple{Ty}, Tuple{Any, AbstractVector{Ty}}, Tuple{Any, AbstractVector{Ty}, Any}} where Ty"><code>BetaML.Trees.buildForest</code></a></li><li><a href="Trees.html#BetaML.Trees.buildTree-Union{Tuple{Ty}, Tuple{Any, AbstractVector{Ty}}} where Ty"><code>BetaML.Trees.buildTree</code></a></li><li><a href="Trees.html#BetaML.Trees.wrap"><code>BetaML.Trees.wrap</code></a></li></ul><h2 id="Detailed-API"><a class="docs-heading-anchor" href="#Detailed-API">Detailed API</a><a id="Detailed-API-1"></a><a class="docs-heading-anchor-permalink" href="#Detailed-API" title="Permalink"></a></h2><article class="docstring"><header><a class="docstring-binding" id="BetaML.Trees.DTHyperParametersSet" href="#BetaML.Trees.DTHyperParametersSet"><code>BetaML.Trees.DTHyperParametersSet</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">mutable struct DTHyperParametersSet &lt;: BetaMLHyperParametersSet</code></pre><p>Hyperparameters for <a href="Trees.html#BetaML.Trees.DecisionTreeEstimator"><code>DecisionTreeEstimator</code></a> (Decision Tree).</p><p><strong>Parameters:</strong></p><ul><li><p><code>max_depth::Union{Nothing, Int64}</code></p><p>The maximum depth the tree is allowed to reach. When this is reached the node is forced to become a leaf [def: <code>nothing</code>, i.e. no limits]</p></li><li><p><code>min_gain::Float64</code></p><p>The minimum information gain to allow for a node&#39;s partition [def: <code>0</code>]</p></li><li><p><code>min_records::Int64</code></p><p>The minimum number of records a node must holds to consider for a partition of it [def: <code>2</code>]</p></li><li><p><code>max_features::Union{Nothing, Int64}</code></p><p>The maximum number of (random) features to consider at each partitioning [def: <code>nothing</code>, i.e. look at all features]</p></li><li><p><code>force_classification::Bool</code></p><p>Whether to force a classification task even if the labels are numerical (typically when labels are integers encoding some feature rather than representing a real cardinal measure) [def: <code>false</code>]</p></li><li><p><code>splitting_criterion::Union{Nothing, Function}</code></p><p>This is the name of the function to be used to compute the information gain of a specific partition. This is done by measuring the difference betwwen the &quot;impurity&quot; of the labels of the parent node with those of the two child nodes, weighted by the respective number of items. [def: <code>nothing</code>, i.e. <code>gini</code> for categorical labels (classification task) and <code>variance</code> for numerical labels(regression task)]. Either <code>gini</code>, <code>entropy</code>, <code>variance</code> or a custom function. It can also be an anonymous function.</p></li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/sylvaticus/BetaML.jl/blob/a0e46177de858fa06c6b7194ba9b13449b2f0683/src/Trees/DecisionTrees.jl#L93-L101">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="BetaML.Trees.DecisionTreeClassifier" href="#BetaML.Trees.DecisionTreeClassifier"><code>BetaML.Trees.DecisionTreeClassifier</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">mutable struct DecisionTreeClassifier &lt;: MLJModelInterface.Probabilistic</code></pre><p>A simple Decision Tree for classification with support for Missing data, from the Beta Machine Learning Toolkit (BetaML).</p><p><strong>Hyperparameters:</strong></p><ul><li><p><code>max_depth::Int64</code></p><p>The maximum depth the tree is allowed to reach. When this is reached the node is forced to become a leaf [def: <code>0</code>, i.e. no limits]</p></li><li><p><code>min_gain::Float64</code></p><p>The minimum information gain to allow for a node&#39;s partition [def: <code>0</code>]</p></li><li><p><code>min_records::Int64</code></p><p>The minimum number of records a node must holds to consider for a partition of it [def: <code>2</code>]</p></li><li><p><code>max_features::Int64</code></p><p>The maximum number of (random) features to consider at each partitioning [def: <code>0</code>, i.e. look at all features]</p></li><li><p><code>splitting_criterion::Function</code></p><p>This is the name of the function to be used to compute the information gain of a specific partition. This is done by measuring the difference betwwen the &quot;impurity&quot; of the labels of the parent node with those of the two child nodes, weighted by the respective number of items. [def: <code>gini</code>]. Either <code>gini</code>, <code>entropy</code> or a custom function. It can also be an anonymous function.</p></li><li><p><code>rng::Random.AbstractRNG</code></p><p>A Random Number Generator to be used in stochastic parts of the code [deafult: <code>Random.GLOBAL_RNG</code>]</p></li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/sylvaticus/BetaML.jl/blob/a0e46177de858fa06c6b7194ba9b13449b2f0683/src/Trees/Trees_MLJ.jl#L46">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="BetaML.Trees.DecisionTreeEstimator" href="#BetaML.Trees.DecisionTreeEstimator"><code>BetaML.Trees.DecisionTreeEstimator</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">mutable struct DecisionTreeEstimator &lt;: BetaMLSupervisedModel</code></pre><p>A Decision Tree classifier and regressor (supervised).</p><p>Decision Tree works by finding the &quot;best&quot; question to split the fitting data (according to the metric specified by the parameter <code>splitting_criterion</code> on the associated labels) untill either all the dataset is separated or a terminal condition is reached. </p><p>For the parameters see <a href="Trees.html#BetaML.Trees.DTHyperParametersSet"><code>?DTHyperParametersSet</code></a> and <a href="Api.html#BetaML.Api.BetaMLDefaultOptionsSet"><code>?BetaMLDefaultOptionsSet</code></a>.</p><p><strong>Notes:</strong></p><ul><li>Online fitting (re-fitting with new data) is not supported</li><li>Missing data (in the feature dataset) is supported.</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/sylvaticus/BetaML.jl/blob/a0e46177de858fa06c6b7194ba9b13449b2f0683/src/Trees/DecisionTrees.jl#L122">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="BetaML.Trees.DecisionTreeRegressor" href="#BetaML.Trees.DecisionTreeRegressor"><code>BetaML.Trees.DecisionTreeRegressor</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">mutable struct DecisionTreeRegressor &lt;: MLJModelInterface.Deterministic</code></pre><p>A simple Decision Tree for regression with support for Missing data, from the Beta Machine Learning Toolkit (BetaML).</p><p><strong>Hyperparameters:</strong></p><ul><li><p><code>max_depth::Int64</code></p><p>The maximum depth the tree is allowed to reach. When this is reached the node is forced to become a leaf [def: <code>0</code>, i.e. no limits]</p></li><li><p><code>min_gain::Float64</code></p><p>The minimum information gain to allow for a node&#39;s partition [def: <code>0</code>]</p></li><li><p><code>min_records::Int64</code></p><p>The minimum number of records a node must holds to consider for a partition of it [def: <code>2</code>]</p></li><li><p><code>max_features::Int64</code></p><p>The maximum number of (random) features to consider at each partitioning [def: <code>0</code>, i.e. look at all features]</p></li><li><p><code>splitting_criterion::Function</code></p><p>This is the name of the function to be used to compute the information gain of a specific partition. This is done by measuring the difference betwwen the &quot;impurity&quot; of the labels of the parent node with those of the two child nodes, weighted by the respective number of items. [def: <code>variance</code>]. Either <code>variance</code> or a custom function. It can also be an anonymous function.</p></li><li><p><code>rng::Random.AbstractRNG</code></p><p>A Random Number Generator to be used in stochastic parts of the code [deafult: <code>Random.GLOBAL_RNG</code>]</p></li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/sylvaticus/BetaML.jl/blob/a0e46177de858fa06c6b7194ba9b13449b2f0683/src/Trees/Trees_MLJ.jl#L14">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="BetaML.Trees.InfoNode" href="#BetaML.Trees.InfoNode"><code>BetaML.Trees.InfoNode</code></a> — <span class="docstring-category">Type</span></header><section><div><p>These types are introduced so that additional information currently not present in  a <code>DecisionTree</code>-structure – namely the feature names  –  can be used for visualization.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/sylvaticus/BetaML.jl/blob/a0e46177de858fa06c6b7194ba9b13449b2f0683/src/Trees/AbstractTrees.jl#L21-L25">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="BetaML.Trees.RFHyperParametersSet" href="#BetaML.Trees.RFHyperParametersSet"><code>BetaML.Trees.RFHyperParametersSet</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">mutable struct RFHyperParametersSet &lt;: BetaMLHyperParametersSet</code></pre><p>Hyperparameters for <a href="Trees.html#BetaML.Trees.RandomForestEstimator"><code>RandomForestEstimator</code></a> (Random Forest).</p><p><strong>Parameters:</strong></p><ul><li><p><code>n_trees::Int64</code></p><p>Number of (decision) trees in the forest [def: <code>30</code>]</p></li><li><p><code>max_depth::Union{Nothing, Int64}</code></p><p>The maximum depth the tree is allowed to reach. When this is reached the node is forced to become a leaf [def: <code>nothing</code>, i.e. no limits]</p></li><li><p><code>min_gain::Float64</code></p><p>The minimum information gain to allow for a node&#39;s partition [def: <code>0</code>]</p></li><li><p><code>min_records::Int64</code></p><p>The minimum number of records a node must holds to consider for a partition of it [def: <code>2</code>]</p></li><li><p><code>max_features::Union{Nothing, Int64}</code></p><p>The maximum number of (random) features to consider when choosing the optimal partition of the dataset [def: <code>nothing</code>, i.e. square root of the dimensions of the training data`]</p></li><li><p><code>force_classification::Bool</code></p><p>Whether to force a classification task even if the labels are numerical (typically when labels are integers encoding some feature rather than representing a real cardinal measure) [def: <code>false</code>]</p></li><li><p><code>splitting_criterion::Union{Nothing, Function}</code></p><p>Either <code>gini</code>, <code>entropy</code> or <code>variance</code>. This is the name of the function to be used to compute the information gain of a specific partition. This is done by measuring the difference betwwen the &quot;impurity&quot; of the labels of the parent node with those of the two child nodes, weighted by the respective number of items. [def: <code>nothing</code>, i.e. <code>gini</code> for categorical labels (classification task) and <code>variance</code> for numerical labels(regression task)]. It can be an anonymous function.</p></li><li><p><code>beta::Float64</code></p><p>Parameter that regulate the weights of the scoring of each tree, to be (optionally) used in prediction based on the error of the individual trees computed on the records on which trees have not been trained. Higher values favour &quot;better&quot; trees, but too high values will cause overfitting [def: <code>0</code>, i.e. uniform weigths]</p></li><li><p><code>oob::Bool</code></p><p>Wheter to compute the <em>Out-Of-Bag</em> error, an estimation of the validation error (the mismatching error for classification and the relative mean error for regression jobs).</p></li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/sylvaticus/BetaML.jl/blob/a0e46177de858fa06c6b7194ba9b13449b2f0683/src/Trees/RandomForests.jl#L30-L38">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="BetaML.Trees.RandomForestClassifier" href="#BetaML.Trees.RandomForestClassifier"><code>BetaML.Trees.RandomForestClassifier</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">mutable struct RandomForestClassifier &lt;: MLJModelInterface.Probabilistic</code></pre><p>A simple Random Forest for classification with support for Missing data, from the Beta Machine Learning Toolkit (BetaML).</p><p><strong>Hyperparameters:</strong></p><ul><li><p><code>n_trees::Int64</code></p></li><li><p><code>max_depth::Int64</code></p><p>The maximum depth the tree is allowed to reach. When this is reached the node is forced to become a leaf [def: <code>0</code>, i.e. no limits]</p></li><li><p><code>min_gain::Float64</code></p><p>The minimum information gain to allow for a node&#39;s partition [def: <code>0</code>]</p></li><li><p><code>min_records::Int64</code></p><p>The minimum number of records a node must holds to consider for a partition of it [def: <code>2</code>]</p></li><li><p><code>max_features::Int64</code></p><p>The maximum number of (random) features to consider at each partitioning [def: <code>0</code>, i.e. square root of the data dimensions]</p></li><li><p><code>splitting_criterion::Function</code></p><p>This is the name of the function to be used to compute the information gain of a specific partition. This is done by measuring the difference betwwen the &quot;impurity&quot; of the labels of the parent node with those of the two child nodes, weighted by the respective number of items. [def: <code>gini</code>]. Either <code>gini</code>, <code>entropy</code> or a custom function. It can also be an anonymous function.</p></li><li><p><code>β::Float64</code></p><p>Parameter that regulate the weights of the scoring of each tree, to be (optionally) used in prediction based on the error of the individual trees computed on the records on which trees have not been trained. Higher values favour &quot;better&quot; trees, but too high values will cause overfitting [def: <code>0</code>, i.e. uniform weigths]</p></li><li><p><code>rng::Random.AbstractRNG</code></p><p>A Random Number Generator to be used in stochastic parts of the code [deafult: <code>Random.GLOBAL_RNG</code>]</p></li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/sylvaticus/BetaML.jl/blob/a0e46177de858fa06c6b7194ba9b13449b2f0683/src/Trees/Trees_MLJ.jl#L115">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="BetaML.Trees.RandomForestEstimator" href="#BetaML.Trees.RandomForestEstimator"><code>BetaML.Trees.RandomForestEstimator</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">mutable struct RandomForestEstimator &lt;: BetaMLSupervisedModel</code></pre><p>A Random Forest classifier and regressor (supervised).</p><p>Random forests are <em>ensemble</em> of Decision Trees models (see <a href="Trees.html#BetaML.Trees.DecisionTreeEstimator"><code>?DecisionTreeEstimator</code></a>).</p><p>For the parameters see <a href="Trees.html#BetaML.Trees.RFHyperParametersSet"><code>?RFHyperParametersSet</code></a> and <a href="Api.html#BetaML.Api.BetaMLDefaultOptionsSet"><code>?BetaMLDefaultOptionsSet</code></a>.</p><p><strong>Notes :</strong></p><ul><li>Each individual decision tree is built using bootstrap over the data, i.e. &quot;sampling N records with replacement&quot; (hence, some records appear multiple times and some records do not appear in the specific tree training). The <code>maxx_feature</code> injects further variability and reduces the correlation between the forest trees.</li><li>The predictions of the &quot;forest&quot; (using the function <code>predict()</code>) are then the aggregated predictions of the individual trees (from which the name &quot;bagging&quot;: <strong>b</strong>oostrap <strong>agg</strong>regat<strong>ing</strong>).</li><li>The performances of each individual trees,  as measured using the records they have not being trained with, can then be (optionally) used as weights in the <code>predict</code> function. The parameter <code>beta ≥ 0</code> regulate the distribution of these weights: larger is <code>β</code>, the greater the importance (hence the weights) attached to the best-performing trees compared to the low-performing ones. Using these weights can significantly improve the forest performances (especially using small forests), however the correct value of <code>beta</code> depends on the problem under exam (and the chosen caratteristics of the random forest estimator) and should be cross-validated to avoid over-fitting.</li><li>Note that training <code>RandomForestEstimator</code> uses multiple threads if these are available. You can check the number of threads available with <code>Threads.nthreads()</code>. To set the number of threads in Julia either set the environmental variable <code>JULIA_NUM_THREADS</code> (before starting Julia) or start Julia with the command line option <code>--threads</code> (most integrated development editors for Julia already set the number of threads to 4).</li><li>Online fitting (re-fitting with new data) is not supported</li><li>Missing data (in the feature dataset) is supported.</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/sylvaticus/BetaML.jl/blob/a0e46177de858fa06c6b7194ba9b13449b2f0683/src/Trees/RandomForests.jl#L60">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="BetaML.Trees.RandomForestRegressor" href="#BetaML.Trees.RandomForestRegressor"><code>BetaML.Trees.RandomForestRegressor</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">mutable struct RandomForestRegressor &lt;: MLJModelInterface.Deterministic</code></pre><p>A simple Random Forest for regression with support for Missing data, from the Beta Machine Learning Toolkit (BetaML).</p><p><strong>Hyperparameters:</strong></p><ul><li><p><code>n_trees::Int64</code></p></li><li><p><code>max_depth::Int64</code></p><p>The maximum depth the tree is allowed to reach. When this is reached the node is forced to become a leaf [def: <code>0</code>, i.e. no limits]</p></li><li><p><code>min_gain::Float64</code></p><p>The minimum information gain to allow for a node&#39;s partition [def: <code>0</code>]</p></li><li><p><code>min_records::Int64</code></p><p>The minimum number of records a node must holds to consider for a partition of it [def: <code>2</code>]</p></li><li><p><code>max_features::Int64</code></p><p>The maximum number of (random) features to consider at each partitioning [def: <code>0</code>, i.e. square root of the data dimension]</p></li><li><p><code>splitting_criterion::Function</code></p><p>This is the name of the function to be used to compute the information gain of a specific partition. This is done by measuring the difference betwwen the &quot;impurity&quot; of the labels of the parent node with those of the two child nodes, weighted by the respective number of items. [def: <code>variance</code>]. Either <code>variance</code> or a custom function. It can also be an anonymous function.</p></li><li><p><code>β::Float64</code></p><p>Parameter that regulate the weights of the scoring of each tree, to be (optionally) used in prediction based on the error of the individual trees computed on the records on which trees have not been trained. Higher values favour &quot;better&quot; trees, but too high values will cause overfitting [def: <code>0</code>, i.e. uniform weigths]</p></li><li><p><code>rng::Random.AbstractRNG</code></p><p>A Random Number Generator to be used in stochastic parts of the code [deafult: <code>Random.GLOBAL_RNG</code>]</p></li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/sylvaticus/BetaML.jl/blob/a0e46177de858fa06c6b7194ba9b13449b2f0683/src/Trees/Trees_MLJ.jl#L78">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="BetaML.Trees.buildForest-Union{Tuple{Ty}, Tuple{Any, AbstractVector{Ty}}, Tuple{Any, AbstractVector{Ty}, Any}} where Ty" href="#BetaML.Trees.buildForest-Union{Tuple{Ty}, Tuple{Any, AbstractVector{Ty}}, Tuple{Any, AbstractVector{Ty}, Any}} where Ty"><code>BetaML.Trees.buildForest</code></a> — <span class="docstring-category">Method</span></header><section><div><p>buildForest(x, y, n<em>trees; max</em>depth, min<em>gain, min</em>records, max<em>features, splitting</em>criterion, force_classification)</p><p>Builds (define and train) a &quot;forest&quot; of Decision Trees.</p><div class="admonition is-warning"><header class="admonition-header">Warning</header><div class="admonition-body"><p>This function is deprecated and will possibly be removed in BetaML 0.9. Use <a href="Trees.html#BetaML.Trees.RandomForestEstimator"><code>RandomForestEstimator</code></a> instead. </p></div></div><p><strong>Parameters:</strong></p><p>See <a href="Trees.html#BetaML.Trees.buildTree-Union{Tuple{Ty}, Tuple{Any, AbstractVector{Ty}}} where Ty"><code>buildTree</code></a>. The function has all the parameters of <code>bildTree</code> (with the <code>max_features</code> defaulting to <code>√D</code> instead of <code>D</code>) plus the following parameters:</p><ul><li><code>n_trees</code>: Number of trees in the forest [def: <code>30</code>]</li><li><code>β</code>: Parameter that regulate the weights of the scoring of each tree, to be (optionally) used in prediction (see later) [def: <code>0</code>, i.e. uniform weigths]</li><li><code>oob</code>: Whether to coompute the out-of-bag error, an estimation of the generalization accuracy [def: <code>false</code>]</li><li><code>rng</code>: Random Number Generator (see <a href="Api.html#BetaML.Api.FIXEDSEED"><code>FIXEDSEED</code></a>) [deafult: <code>Random.GLOBAL_RNG</code>]</li></ul><p><strong>Output:</strong></p><ul><li>The function returns a Forest object (see <a href="@ref"><code>Forest</code></a>).</li><li>The forest weights default to array of ones if <code>β ≤ 0</code> and the oob error to <code>+Inf</code> if <code>oob</code> == <code>false</code>.</li></ul><p><strong>Notes :</strong></p><ul><li>Each individual decision tree is built using bootstrap over the data, i.e. &quot;sampling N records with replacement&quot; (hence, some records appear multiple times and some records do not appear in the specific tree training). The <code>maxFeature</code> injects further variability and reduces the correlation between the forest trees.</li><li>The predictions of the &quot;forest&quot; (using the function <code>predict()</code>) are then the aggregated predictions of the individual trees (from which the name &quot;bagging&quot;: <strong>b</strong>oostrap <strong>agg</strong>regat<strong>ing</strong>).</li><li>This function optionally reports a weight distribution of the performances of eanch individual trees, as measured using the records he has not being trained with. These weights can then be (optionally) used in the <code>predict</code> function. The parameter <code>β ≥ 0</code> regulate the distribution of these weights: larger is <code>β</code>, the greater the importance (hence the weights) attached to the best-performing trees compared to the low-performing ones. Using these weights can significantly improve the forest performances (especially using small forests), however the correct value of β depends on the problem under exam (and the chosen caratteristics of the random forest estimator) and should be cross-validated to avoid over-fitting.</li><li>Note that this function uses multiple threads if these are available. You can check the number of threads available with <code>Threads.nthreads()</code>. To set the number of threads in Julia either set the environmental variable <code>JULIA_NUM_THREADS</code> (before starting Julia) or start Julia with the command line option <code>--threads</code> (most integrated development editors for Julia already set the number of threads to 4).</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/sylvaticus/BetaML.jl/blob/a0e46177de858fa06c6b7194ba9b13449b2f0683/src/Trees/RandomForests.jl#L107-L132">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="BetaML.Trees.buildTree-Union{Tuple{Ty}, Tuple{Any, AbstractVector{Ty}}} where Ty" href="#BetaML.Trees.buildTree-Union{Tuple{Ty}, Tuple{Any, AbstractVector{Ty}}} where Ty"><code>BetaML.Trees.buildTree</code></a> — <span class="docstring-category">Method</span></header><section><div><p>buildTree(x, y, depth; max<em>depth, min</em>gain, min<em>records, max</em>features, splitting<em>criterion, force</em>classification)</p><p>Builds (define and train) a Decision Tree.</p><div class="admonition is-warning"><header class="admonition-header">Warning</header><div class="admonition-body"><p>This function is deprecated and will possibly be removed in BetaML 0.9. Use <a href="Trees.html#BetaML.Trees.DecisionTreeEstimator"><code>DecisionTreeEstimator</code></a> instead. </p></div></div><p>Given a dataset of features <code>x</code> and the corresponding dataset of labels <code>y</code>, recursivelly build a decision tree by finding at each node the best question to split the data untill either all the dataset is separated or a terminal condition is reached. The given tree is then returned.</p><p><strong>Parameters:</strong></p><ul><li><code>x</code>: The dataset&#39;s features (N × D)</li><li><code>y</code>: The dataset&#39;s labels (N × 1)</li><li><code>max_depth</code>: The maximum depth the tree is allowed to reach. When this is reached the node is forced to become a leaf [def: <code>N</code>, i.e. no limits]</li><li><code>min_gain</code>: The minimum information gain to allow for a node&#39;s partition [def: <code>0</code>]</li><li><code>min_records</code>:  The minimum number of records a node must holds to consider for a partition of it [def: <code>2</code>]</li><li><code>max_features</code>: The maximum number of (random) features to consider at each partitioning [def: <code>D</code>, i.e. look at all features]</li><li><code>splitting_criterion</code>: Either <code>gini</code>, <code>entropy</code> or <code>variance</code> (see <a href="@ref"><code>infoGain</code></a> ) [def: <code>gini</code> for categorical labels (classification task) and <code>variance</code> for numerical labels(regression task)]</li><li><code>force_classification</code>: Whether to force a classification task even if the labels are numerical (typically when labels are integers encoding some feature rather than representing a real cardinal measure) [def: <code>false</code>]</li><li><code>rng</code>: Random Number Generator ((see <a href="Api.html#BetaML.Api.FIXEDSEED"><code>FIXEDSEED</code></a>)) [deafult: <code>Random.GLOBAL_RNG</code>]</li></ul><p><strong>Notes:</strong></p><p>Missing data (in the feature dataset) are supported.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/sylvaticus/BetaML.jl/blob/a0e46177de858fa06c6b7194ba9b13449b2f0683/src/Trees/DecisionTrees.jl#L324-L351">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="BetaML.Trees.wrap" href="#BetaML.Trees.wrap"><code>BetaML.Trees.wrap</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">wrap(node:: DecisionNode, ...)</code></pre><p>Called on the root node of a <code>DecsionTree</code> <code>dc</code> in order to add visualization information. In case of a <code>BetaML/DecisionTree</code> this is typically a list of feature names as follows:</p><p><code>wdc = wrap(dc, (featurenames = feature_names, ))</code></p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/sylvaticus/BetaML.jl/blob/a0e46177de858fa06c6b7194ba9b13449b2f0683/src/Trees/AbstractTrees.jl#L36-L43">source</a></section></article></article><nav class="docs-footer"><a class="docs-footer-prevpage" href="Perceptron.html">« Perceptron</a><a class="docs-footer-nextpage" href="Nn.html">Nn »</a><div class="flexbox-break"></div><p class="footer-message">Powered by <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> and the <a href="https://julialang.org/">Julia Programming Language</a>.</p></nav></div><div class="modal" id="documenter-settings"><div class="modal-background"></div><div class="modal-card"><header class="modal-card-head"><p class="modal-card-title">Settings</p><button class="delete"></button></header><section class="modal-card-body"><p><label class="label">Theme</label><div class="select"><select id="documenter-themepicker"><option value="documenter-light">documenter-light</option><option value="documenter-dark">documenter-dark</option></select></div></p><hr/><p>This document was generated with <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> version 0.27.23 on <span class="colophon-date" title="Monday 19 September 2022 14:43">Monday 19 September 2022</span>. Using Julia version 1.6.7.</p></section><footer class="modal-card-foot"></footer></div></div></div></body></html>
