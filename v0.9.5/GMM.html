<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>GMM · BetaML.jl Documentation</title><script async src="https://www.googletagmanager.com/gtag/js?id=G-JYKX8QY5JW"></script><script>  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());
  gtag('config', 'G-JYKX8QY5JW', {'page_path': location.pathname + location.search + location.hash});
</script><script data-outdated-warner src="assets/warner.js"></script><link href="https://cdnjs.cloudflare.com/ajax/libs/lato-font/3.0.0/css/lato-font.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/juliamono/0.045/juliamono.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.4/css/fontawesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.4/css/solid.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.4/css/brands.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.24/katex.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL="."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" data-main="assets/documenter.js"></script><script src="siteinfo.js"></script><script src="../versions.js"></script><link class="docs-theme-link" rel="stylesheet" type="text/css" href="assets/themes/documenter-dark.css" data-theme-name="documenter-dark" data-theme-primary-dark/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="assets/themes/documenter-light.css" data-theme-name="documenter-light" data-theme-primary/><script src="assets/themeswap.js"></script></head><body><div id="documenter"><nav class="docs-sidebar"><div class="docs-package-name"><span class="docs-autofit"><a href="index.html">BetaML.jl Documentation</a></span></div><form class="docs-search" action="search.html"><input class="docs-search-query" id="documenter-search-query" name="q" type="text" placeholder="Search docs"/></form><ul class="docs-menu"><li><a class="tocitem" href="index.html">Index</a></li><li><span class="tocitem">Tutorial</span><ul><li><a class="tocitem" href="tutorials/Betaml_tutorial_getting_started.html">Getting started</a></li><li><input class="collapse-toggle" id="menuitem-2-2" type="checkbox"/><label class="tocitem" for="menuitem-2-2"><span class="docs-label">Classification - cars</span><i class="docs-chevron"></i></label><ul class="collapsed"><li><a class="tocitem" href="tutorials/Classification - cars/betaml_tutorial_classification_cars.html">A classification task when labels are known - determining the country of origin of cars given the cars characteristics</a></li></ul></li><li><input class="collapse-toggle" id="menuitem-2-3" type="checkbox"/><label class="tocitem" for="menuitem-2-3"><span class="docs-label">Regression - bike sharing</span><i class="docs-chevron"></i></label><ul class="collapsed"><li><a class="tocitem" href="tutorials/Regression - bike sharing/betaml_tutorial_regression_sharingBikes.html">A regression task: the prediction of  bike  sharing demand</a></li></ul></li><li><input class="collapse-toggle" id="menuitem-2-4" type="checkbox"/><label class="tocitem" for="menuitem-2-4"><span class="docs-label">Clustering - Iris</span><i class="docs-chevron"></i></label><ul class="collapsed"><li><a class="tocitem" href="tutorials/Clustering - Iris/betaml_tutorial_cluster_iris.html">A clustering task: the prediction of  plant species from floreal measures (the iris dataset)</a></li></ul></li></ul></li><li><span class="tocitem">API (Reference manual)</span><ul><li><input class="collapse-toggle" id="menuitem-3-1" type="checkbox"/><label class="tocitem" for="menuitem-3-1"><span class="docs-label">API V2 (current)</span><i class="docs-chevron"></i></label><ul class="collapsed"><li><a class="tocitem" href="Api_v2_user.html">Introduction for user</a></li><li><input class="collapse-toggle" id="menuitem-3-1-2" type="checkbox"/><label class="tocitem" for="menuitem-3-1-2"><span class="docs-label">For developers</span><i class="docs-chevron"></i></label><ul class="collapsed"><li><a class="tocitem" href="Api_v2_developer.html">API implementation</a></li><li><a class="tocitem" href="StyleGuide_templates.html">Style guide</a></li></ul></li><li><a class="tocitem" href="Api.html">The Api module</a></li></ul></li><li><a class="tocitem" href="Perceptron.html">Perceptron</a></li><li><a class="tocitem" href="Trees.html">Trees</a></li><li><a class="tocitem" href="Nn.html">Nn</a></li><li><a class="tocitem" href="Clustering.html">Clustering</a></li><li class="is-active"><a class="tocitem" href="GMM.html">GMM</a><ul class="internal"><li><a class="tocitem" href="#Module-Index"><span>Module Index</span></a></li><li><a class="tocitem" href="#Detailed-API"><span>Detailed API</span></a></li></ul></li><li><a class="tocitem" href="Imputation.html">Imputation</a></li><li><a class="tocitem" href="Utils.html">Utils</a></li></ul></li></ul><div class="docs-version-selector field has-addons"><div class="control"><span class="docs-label button is-static is-size-7">Version</span></div><div class="docs-selector control is-expanded"><div class="select is-fullwidth is-size-7"><select id="documenter-version-selector"></select></div></div></div></nav><div class="docs-main"><header class="docs-navbar"><nav class="breadcrumb"><ul class="is-hidden-mobile"><li><a class="is-disabled">API (Reference manual)</a></li><li class="is-active"><a href="GMM.html">GMM</a></li></ul><ul class="is-hidden-tablet"><li class="is-active"><a href="GMM.html">GMM</a></li></ul></nav><div class="docs-right"><a class="docs-edit-link" href="https://github.com/sylvaticus/BetaML.jl/blob/master/docs/src/GMM.md" title="Edit on GitHub"><span class="docs-icon fab"></span><span class="docs-label is-hidden-touch">Edit on GitHub</span></a><a class="docs-settings-button fas fa-cog" id="documenter-settings-button" href="#" title="Settings"></a><a class="docs-sidebar-button fa fa-bars is-hidden-desktop" id="documenter-sidebar-button" href="#"></a></div></header><article class="content" id="documenter-page"><h1 id="gmm_module"><a class="docs-heading-anchor" href="#gmm_module">The BetaML.GMM Module</a><a id="gmm_module-1"></a><a class="docs-heading-anchor-permalink" href="#gmm_module" title="Permalink"></a></h1><article class="docstring"><header><a class="docstring-binding" id="BetaML.GMM" href="#BetaML.GMM"><code>BetaML.GMM</code></a> — <span class="docstring-category">Module</span></header><section><div><pre><code class="language-julia hljs">GMM module</code></pre><p>Generative (Gaussian) Mixed Model learners (supervised/unsupervised)</p><p>Provides clustering and regressors using  (Generative) Gaussiam Mixture Model (probabilistic).</p><p>Collaborative filtering / missing values imputation / reccomendation systems based on GMM is available in the <a href="@ref BetaML.Imputation"><code>Imputation</code></a> module.</p><p>The module provides the following models. Use <code>?[model]</code> to access their documentation:</p><ul><li><a href="GMM.html#BetaML.GMM.GMMClusterer"><code>GMMClusterer</code></a>: soft-clustering using GMM</li><li><a href="GMM.html#BetaML.GMM.GMMRegressor1"><code>GMMRegressor1</code></a>: regressor using GMM as back-end (first algorithm)</li><li><a href="GMM.html#BetaML.GMM.GMMRegressor1"><code>GMMRegressor1</code></a>: regressor using GMM as back-end (second algorithm)</li></ul><p>All the algorithms works with arbitrary mixture distribution, altought only {Spherical|Diagonal|Full} Gaussian mixtures has been implemented. User defined mixtures can be used defining a struct as subtype of <code>AbstractMixture</code> and implementing for that mixture the following functions:</p><ul><li><code>init_mixtures!(mixtures, X; minimum_variance, minimum_covariance, initialisation_strategy)</code></li><li><code>lpdf(m,x,mask)</code> (for the e-step)</li><li><code>update_parameters!(mixtures, X, pₙₖ; minimum_variance, minimum_covariance)</code> (the m-step)</li><li><code>npar(mixtures::Array{T,1})</code> (for the BIC/AIC computation)</li></ul><p>All the GMM-based algorithms works only with numerical data, but accepts also Missing one.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/sylvaticus/BetaML.jl/blob/cd5245a06ffcf35225cc9b50c11daf5a3695435f/src/GMM/GMM.jl#L3-L26">source</a></section></article><h2 id="Module-Index"><a class="docs-heading-anchor" href="#Module-Index">Module Index</a><a id="Module-Index-1"></a><a class="docs-heading-anchor-permalink" href="#Module-Index" title="Permalink"></a></h2><ul><li><a href="GMM.html#BetaML.GMM.GMMClusterer"><code>BetaML.GMM.GMMClusterer</code></a></li><li><a href="GMM.html#BetaML.GMM.GMMHyperParametersSet"><code>BetaML.GMM.GMMHyperParametersSet</code></a></li><li><a href="GMM.html#BetaML.GMM.GMMRegressor1"><code>BetaML.GMM.GMMRegressor1</code></a></li><li><a href="GMM.html#BetaML.GMM.GMMRegressor2"><code>BetaML.GMM.GMMRegressor2</code></a></li><li><a href="GMM.html#BetaML.GMM.GaussianMixtureClusterer"><code>BetaML.GMM.GaussianMixtureClusterer</code></a></li><li><a href="GMM.html#BetaML.GMM.GaussianMixtureRegressor"><code>BetaML.GMM.GaussianMixtureRegressor</code></a></li><li><a href="GMM.html#BetaML.GMM.MultitargetGaussianMixtureRegressor"><code>BetaML.GMM.MultitargetGaussianMixtureRegressor</code></a></li><li><a href="GMM.html#BetaML.GMM.init_mixtures!-Union{Tuple{T}, Tuple{Vector{T}, Any}} where T&lt;:BetaML.GMM.AbstractGaussian"><code>BetaML.GMM.init_mixtures!</code></a></li><li><a href="GMM.html#BetaML.GMM.lpdf-Tuple{DiagonalGaussian, Any, Any}"><code>BetaML.GMM.lpdf</code></a></li><li><a href="GMM.html#BetaML.GMM.lpdf-Tuple{SphericalGaussian, Any, Any}"><code>BetaML.GMM.lpdf</code></a></li><li><a href="GMM.html#BetaML.GMM.lpdf-Tuple{FullGaussian, Any, Any}"><code>BetaML.GMM.lpdf</code></a></li></ul><h2 id="Detailed-API"><a class="docs-heading-anchor" href="#Detailed-API">Detailed API</a><a id="Detailed-API-1"></a><a class="docs-heading-anchor-permalink" href="#Detailed-API" title="Permalink"></a></h2><article class="docstring"><header><a class="docstring-binding" id="BetaML.GMM.GMMClusterer" href="#BetaML.GMM.GMMClusterer"><code>BetaML.GMM.GMMClusterer</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">mutable struct GMMClusterer &lt;: BetaMLUnsupervisedModel</code></pre><p>Assign class probabilities to records (i.e. <em>soft</em> clustering) assuming a probabilistic generative model of observed data using mixtures.</p><p>For the parameters see <a href="GMM.html#BetaML.GMM.GMMHyperParametersSet"><code>?GMMHyperParametersSet</code></a> and <a href="Api.html#BetaML.Api.BetaMLDefaultOptionsSet"><code>?BetaMLDefaultOptionsSet</code></a>.</p><p><strong>Notes:</strong></p><ul><li>Data must be numerical</li><li>Mixtures can be user defined: see the <a href="GMM.html#BetaML.GMM"><code>?GMM</code></a> module documentation for a discussion on provided vs custom mixtures.</li><li>Online fitting (re-fitting with new data) is supported by setting the old learned mixtrures as the starting values</li><li>The model is fitted using an Expectation-Minimisation (EM) algorithm that supports Missing data and is implemented in the log-domain for better numerical accuracy with many dimensions</li></ul><p><strong>Example:</strong></p><pre><code class="language-julia hljs">julia&gt; using BetaML

julia&gt; X = [1.1 10.1; 0.9 9.8; 10.0 1.1; 12.1 0.8; 0.8 9.8];

julia&gt; mod = GMMClusterer(n_classes=2)
GMMClusterer - A Generative Mixture Model (unfitted)

julia&gt; prob_belong_classes = fit!(mod,X)
Iter. 1:        Var. of the post  2.15612140465882        Log-likelihood -29.06452054772657
5×2 Matrix{Float64}:
 1.0  0.0
 1.0  0.0
 0.0  1.0
 0.0  1.0
 1.0  0.0

julia&gt; new_probs = fit!(mod,[11 0.9])
Iter. 1:        Var. of the post  1.0     Log-likelihood -1.3312256125240092
1×2 Matrix{Float64}:
 0.0  1.0

julia&gt; info(mod)
Dict{String, Any} with 6 entries:
  &quot;xndims&quot;         =&gt; 2
  &quot;error&quot;          =&gt; [1.0, 0.0, 0.0]
  &quot;AIC&quot;            =&gt; 15.7843
  &quot;fitted_records&quot; =&gt; 6
  &quot;lL&quot;             =&gt; 1.10786
  &quot;BIC&quot;            =&gt; -2.21571

julia&gt; parameters(mod)
BetaML.GMM.GMMClusterLearnableParameters (a BetaMLLearnableParametersSet struct)
- mixtures: DiagonalGaussian{Float64}[DiagonalGaussian{Float64}([0.9333333333333332, 9.9], [0.05, 0.05]), DiagonalGaussian{Float64}([11.05, 0.9500000000000001], [0.05, 0.05])]
- initial_probmixtures: [0.0, 1.0]</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/sylvaticus/BetaML.jl/blob/cd5245a06ffcf35225cc9b50c11daf5a3695435f/src/GMM/GMM_clustering.jl#L255">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="BetaML.GMM.GMMHyperParametersSet" href="#BetaML.GMM.GMMHyperParametersSet"><code>BetaML.GMM.GMMHyperParametersSet</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">mutable struct GMMHyperParametersSet &lt;: BetaMLHyperParametersSet</code></pre><p>Hyperparameters for GMM clusters and other GMM-based algorithms</p><p><strong>Parameters:</strong></p><ul><li><code>n_classes</code></li></ul><p>: Number of mixtures (latent classes) to consider [def: 3]</p><ul><li><code>initial_probmixtures</code></li></ul><p>: Initial probabilities of the categorical distribution (n_classes x 1) [default: <code>[]</code>]</p><ul><li><code>mixtures</code></li></ul><p>: An array (of length <code>n_classes</code><code>) of the mixtures to employ (see the [</code>?GMM<code>](@ref GMM) module).     Each mixture object can be provided with or without its parameters (e.g. mean and variance for the gaussian ones). Fully qualified mixtures are useful only if the</code>initialisation<em>strategy<code>parameter is  set to &quot;gived&quot;</code>     This parameter can also be given symply in term of a _type</em>. In this case it is automatically extended to a vector of <code>n_classes</code><code>mixtures of the specified type.     Note that mixing of different mixture types is not currently supported and that currently implemented mixtures are</code>SphericalGaussian<code>,</code>DiagonalGaussian<code>and</code>FullGaussian<code>.     [def:</code>DiagonalGaussian`]</p><ul><li><code>tol</code></li></ul><p>: Tolerance to stop the algorithm [default: 10^(-6)]</p><ul><li><code>minimum_variance</code></li></ul><p>: Minimum variance for the mixtures [default: 0.05]</p><ul><li><code>minimum_covariance</code></li></ul><p>: Minimum covariance for the mixtures with full covariance matrix [default: 0]. This should be set different than minimum_variance.</p><ul><li><code>initialisation_strategy</code></li></ul><p>: The computation method of the vector of the initial mixtures.     One of the following:     - &quot;grid&quot;: using a grid approach     - &quot;given&quot;: using the mixture provided in the fully qualified <code>mixtures</code> parameter     - &quot;kmeans&quot;: use first kmeans (itself initialised with a &quot;grid&quot; strategy) to set the initial mixture centers [default]     Note that currently &quot;random&quot; and &quot;shuffle&quot; initialisations are not supported in gmm-based algorithms.</p><ul><li><code>maximum_iterations</code></li></ul><p>: Maximum number of iterations [def: 5000]</p><ul><li><code>tunemethod</code></li></ul><p>: The method - and its parameters - to employ for hyperparameters autotuning.     See <a href="@ref">`SuccessiveHalvingSearch</a> for the default method (suitable for the GMM-based regressors)     To implement automatic hyperparameter tuning during the (first) <code>fit!</code> call simply set <code>autotune=true</code> and eventually change the default <code>tunemethod</code> options (including the parameter ranges, the resources to employ and the loss function to adopt).</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/sylvaticus/BetaML.jl/blob/cd5245a06ffcf35225cc9b50c11daf5a3695435f/src/GMM/GMM_clustering.jl#L167">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="BetaML.GMM.GMMRegressor1" href="#BetaML.GMM.GMMRegressor1"><code>BetaML.GMM.GMMRegressor1</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">mutable struct GMMRegressor1 &lt;: BetaMLUnsupervisedModel</code></pre><p>A multi-dimensional, missing data friendly non-linear regressor based on Generative (Gaussian) Mixture Model (strategy &quot;1&quot;).</p><p>The training data is used to fit a probabilistic model with latent mixtures (Gaussian distributions with different covariances are already implemented) and then predictions of new data is obtained by fitting the new data to the mixtures.</p><p>For hyperparameters see <a href="GMM.html#BetaML.GMM.GMMHyperParametersSet"><code>GMMHyperParametersSet</code></a> and <a href="Api.html#BetaML.Api.BetaMLDefaultOptionsSet"><code>BetaMLDefaultOptionsSet</code></a>.</p><p>This strategy (<code>GMMRegressor1</code>) works by fitting the EM algorithm on the feature matrix X. Once the data has been probabilistically assigned to the various classes, a mean value of fitting values Y is computed for each cluster (using the probabilities as weigths). At predict time, the new data is first fitted to the learned mixtures using the e-step part of the EM algorithm to obtain the probabilistic assignment of each record to the various mixtures. Then these probabilities are multiplied to the mixture averages for the Y dimensions learned at training time to obtain the predicted value(s) for each record. </p><p><strong>Notes:</strong></p><ul><li>Predicted values are always a matrix, even when a single variable is predicted (use <code>dropdims(ŷ,dims=2)</code> to get a single vector).</li></ul><p><strong>Example:</strong></p><pre><code class="language-julia hljs">julia&gt; using BetaML

julia&gt; X = [1.1 10.1; 0.9 9.8; 10.0 1.1; 12.1 0.8; 0.8 9.8];

julia&gt; Y = X[:,1] .* 2 - X[:,2]
5-element Vector{Float64}:
 -7.8999999999999995
 -8.0
 18.9
 23.4
 -8.200000000000001

julia&gt; mod = GMMRegressor1(n_classes=2)
GMMRegressor1 - A regressor based on Generative Mixture Model (unfitted)

julia&gt; ŷ = fit!(mod,X,Y)
Iter. 1:        Var. of the post  2.15612140465882        Log-likelihood -29.06452054772657
5×1 Matrix{Float64}:
 -8.033333333333333
 -8.033333333333333
 21.15
 21.15
 -8.033333333333333

julia&gt; new_probs = predict(mod,[11 0.9])
1×1 Matrix{Float64}:
 21.15

julia&gt; info(mod)
Dict{String, Any} with 6 entries:
  &quot;xndims&quot;         =&gt; 2
  &quot;error&quot;          =&gt; [2.15612, 0.118848, 4.19495e-7, 0.0, 0.0]
  &quot;AIC&quot;            =&gt; 32.7605
  &quot;fitted_records&quot; =&gt; 5
  &quot;lL&quot;             =&gt; -7.38023
  &quot;BIC&quot;            =&gt; 29.2454</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/sylvaticus/BetaML.jl/blob/cd5245a06ffcf35225cc9b50c11daf5a3695435f/src/GMM/GMM_regression.jl#L15">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="BetaML.GMM.GMMRegressor2" href="#BetaML.GMM.GMMRegressor2"><code>BetaML.GMM.GMMRegressor2</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">mutable struct GMMRegressor2 &lt;: BetaMLUnsupervisedModel</code></pre><p>A multi-dimensional, missing data friendly non-linear regressor based on Generative (Gaussian) Mixture Model.</p><p>The training data is used to fit a probabilistic model with latent mixtures (Gaussian distributions with different covariances are already implemented) and then predictions of new data is obtained by fitting the new data to the mixtures.</p><p>For hyperparameters see <a href="GMM.html#BetaML.GMM.GMMHyperParametersSet"><code>GMMHyperParametersSet</code></a> and <a href="Api.html#BetaML.Api.BetaMLDefaultOptionsSet"><code>BetaMLDefaultOptionsSet</code></a>.</p><p>Thsi strategy (<code>GMMRegressor2</code>) works by training the EM algorithm on a combined (hcat) matrix of X and Y. At predict time, the new data is first fitted to the learned mixtures using the e-step part of the EM algorithm (and using missing values for the dimensions belonging to Y) to obtain the probabilistic assignment of each record to the various mixtures. Then these probabilities are multiplied to the mixture averages for the Y dimensions to obtain the predicted value(s) for each record. </p><p><strong>Example:</strong></p><pre><code class="language-julia hljs">julia&gt; using BetaML

julia&gt; X = [1.1 10.1; 0.9 9.8; 10.0 1.1; 12.1 0.8; 0.8 9.8];

julia&gt; Y = X[:,1] .* 2 - X[:,2]
5-element Vector{Float64}:
 -7.8999999999999995
 -8.0
 18.9
 23.4
 -8.200000000000001

julia&gt; mod = GMMRegressor2(n_classes=2)
GMMRegressor2 - A regressor based on Generative Mixture Model (unfitted)

julia&gt; ŷ = fit!(mod,X,Y)
Iter. 1:        Var. of the post  2.2191120060614065      Log-likelihood -47.70971887023561
5×1 Matrix{Float64}:
 -8.033333333333333
 -8.033333333333333
 21.15
 21.15
 -8.033333333333333

julia&gt; new_probs = predict(mod,[11 0.9])
1×1 Matrix{Float64}:
 21.15

julia&gt; info(mod)
Dict{String, Any} with 6 entries:
  &quot;xndims&quot;         =&gt; 3
  &quot;error&quot;          =&gt; [2.21911, 0.0260833, 3.19141e-39, 0.0]
  &quot;AIC&quot;            =&gt; 60.0684
  &quot;fitted_records&quot; =&gt; 5
  &quot;lL&quot;             =&gt; -17.0342
  &quot;BIC&quot;            =&gt; 54.9911

julia&gt; parameters(mod)
BetaML.GMM.GMMClusterLearnableParameters (a BetaMLLearnableParametersSet struct)
- mixtures: DiagonalGaussian{Float64}[DiagonalGaussian{Float64}([0.9333333333333332, 9.9, -8.033333333333333], [1.1024999999999996, 0.05, 5.0625]), DiagonalGaussian{Float64}([11.05, 0.9500000000000001, 21.15], [1.1024999999999996, 0.05, 5.0625])]
- initial_probmixtures: [0.6, 0.4]</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/sylvaticus/BetaML.jl/blob/cd5245a06ffcf35225cc9b50c11daf5a3695435f/src/GMM/GMM_regression.jl#L213">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="BetaML.GMM.GaussianMixtureClusterer" href="#BetaML.GMM.GaussianMixtureClusterer"><code>BetaML.GMM.GaussianMixtureClusterer</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">mutable struct GaussianMixtureClusterer &lt;: MLJModelInterface.Unsupervised</code></pre><p>A Expectation-Maximisation clustering algorithm with customisable mixtures, from the Beta Machine Learning Toolkit (BetaML).</p><p><strong>Hyperparameters:</strong></p><ul><li><code>n_classes::Int64</code></li></ul><p>: Number of mixtures (latent classes) to consider [def: 3]</p><ul><li><code>initial_probmixtures::AbstractVector{Float64}</code></li></ul><p>: Initial probabilities of the categorical distribution (n_classes x 1) [default: <code>[]</code>]</p><ul><li><code>mixtures::Union{Type, Vector{var&quot;#s792&quot;} where var&quot;#s792&quot;&lt;:AbstractMixture}</code></li></ul><p>: An array (of length <code>n_classes</code><code>) of the mixtures to employ (see the [</code>?GMM<code>](@ref GMM) module).     Each mixture object can be provided with or without its parameters (e.g. mean and variance for the gaussian ones). Fully qualified mixtures are useful only if the</code>initialisation<em>strategy<code>parameter is  set to &quot;gived&quot;</code>     This parameter can also be given symply in term of a _type</em>. In this case it is automatically extended to a vector of <code>n_classes</code><code>mixtures of the specified type.     Note that mixing of different mixture types is not currently supported.     [def:</code>[DiagonalGaussian() for i in 1:n_classes]`]</p><ul><li><code>tol::Float64</code></li></ul><p>: Tolerance to stop the algorithm [default: 10^(-6)]</p><ul><li><code>minimum_variance::Float64</code></li></ul><p>: Minimum variance for the mixtures [default: 0.05]</p><ul><li><code>minimum_covariance::Float64</code></li></ul><p>: Minimum covariance for the mixtures with full covariance matrix [default: 0]. This should be set different than minimum_variance (see notes).</p><ul><li><code>initialisation_strategy::String</code></li></ul><p>: The computation method of the vector of the initial mixtures.     One of the following:     - &quot;grid&quot;: using a grid approach     - &quot;given&quot;: using the mixture provided in the fully qualified <code>mixtures</code> parameter     - &quot;kmeans&quot;: use first kmeans (itself initialised with a &quot;grid&quot; strategy) to set the initial mixture centers [default]     Note that currently &quot;random&quot; and &quot;shuffle&quot; initialisations are not supported in gmm-based algorithms.</p><ul><li><code>maximum_iterations::Int64</code></li></ul><p>: Maximum number of iterations [def: <code>typemax(Int64)</code>, i.e. ∞]</p><ul><li><code>rng::Random.AbstractRNG</code></li></ul><p>: Random Number Generator [deafult: <code>Random.GLOBAL_RNG</code>]</p><p><strong>Example:</strong></p><pre><code class="language-julia hljs">julia&gt; using MLJ

julia&gt; modelType                   = @load GaussianMixtureClusterer pkg = &quot;BetaML&quot;
[ Info: For silent loading, specify `verbosity=0`. 
import BetaML ✔
BetaML.GMM.GaussianMixtureClusterer

julia&gt; model                       = modelType()
GaussianMixtureClusterer(
  n_classes = 3, 
  initial_probmixtures = Float64[], 
  mixtures = BetaML.GMM.DiagonalGaussian{Float64}[BetaML.GMM.DiagonalGaussian{Float64}(nothing, nothing), BetaML.GMM.DiagonalGaussian{Float64}(nothing, nothing), BetaML.GMM.DiagonalGaussian{Float64}(nothing, nothing)], 
  tol = 1.0e-6, 
  minimum_variance = 0.05, 
  minimum_covariance = 0.0, 
  initialisation_strategy = &quot;kmeans&quot;, 
  maximum_iterations = 9223372036854775807, 
  rng = Random._GLOBAL_RNG())

julia&gt; X, y                        = @load_iris;

julia&gt; (fitResults, cache, report) = MLJ.fit(model, 0, X);

julia&gt; est_classes                 = predict(model, fitResults, X)
150-element CategoricalDistributions.UnivariateFiniteVector{Multiclass{3}, Int64, UInt32, Float64}:
 UnivariateFinite{Multiclass{3}}(1=&gt;1.0, 2=&gt;4.17e-15, 3=&gt;2.1900000000000003e-31)
 UnivariateFinite{Multiclass{3}}(1=&gt;1.0, 2=&gt;1.25e-13, 3=&gt;5.87e-31)
 UnivariateFinite{Multiclass{3}}(1=&gt;1.0, 2=&gt;4.5e-15, 3=&gt;1.55e-32)
 ⋮
 UnivariateFinite{Multiclass{3}}(1=&gt;5.39e-25, 2=&gt;0.0167, 3=&gt;0.983)
 UnivariateFinite{Multiclass{3}}(1=&gt;7.5e-29, 2=&gt;0.000106, 3=&gt;1.0)
 UnivariateFinite{Multiclass{3}}(1=&gt;1.6e-20, 2=&gt;0.594, 3=&gt;0.406)

julia&gt; </code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/sylvaticus/BetaML.jl/blob/cd5245a06ffcf35225cc9b50c11daf5a3695435f/src/GMM/GMM_MLJ.jl#L13">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="BetaML.GMM.GaussianMixtureRegressor" href="#BetaML.GMM.GaussianMixtureRegressor"><code>BetaML.GMM.GaussianMixtureRegressor</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">mutable struct GaussianMixtureRegressor &lt;: MLJModelInterface.Deterministic</code></pre><p>A non-linear regressor derived from fitting the data on a probabilistic model (Gaussian Mixture Model). Relatively fast.</p><p>This is the single-target version of the model. If you want to predict several labels (y) at once, use the MLJ model <a href="GMM.html#BetaML.GMM.MultitargetGaussianMixtureRegressor"><code>MultitargetGaussianMixtureRegressor</code></a>.</p><p><strong>Hyperparameters:</strong></p><ul><li><code>n_classes::Int64</code></li></ul><p>: Number of mixtures (latent classes) to consider [def: 3]</p><ul><li><code>initial_probmixtures::Vector{Float64}</code></li></ul><p>: Initial probabilities of the categorical distribution (n_classes x 1) [default: <code>[]</code>]</p><ul><li><code>mixtures::Union{Type, Vector{var&quot;#s792&quot;} where var&quot;#s792&quot;&lt;:AbstractMixture}</code></li></ul><p>: An array (of length <code>n_classes</code><code>) of the mixtures to employ (see the [</code>?GMM<code>](@ref GMM) module).     Each mixture object can be provided with or without its parameters (e.g. mean and variance for the gaussian ones). Fully qualified mixtures are useful only if the</code>initialisation<em>strategy<code>parameter is  set to &quot;gived&quot;</code>     This parameter can also be given symply in term of a _type</em>. In this case it is automatically extended to a vector of <code>n_classes</code><code>mixtures of the specified type.     Note that mixing of different mixture types is not currently supported.     [def:</code>[DiagonalGaussian() for i in 1:n_classes]`]</p><ul><li><code>tol::Float64</code></li></ul><p>: Tolerance to stop the algorithm [default: 10^(-6)]</p><ul><li><code>minimum_variance::Float64</code></li></ul><p>: Minimum variance for the mixtures [default: 0.05]</p><ul><li><code>minimum_covariance::Float64</code></li></ul><p>: Minimum covariance for the mixtures with full covariance matrix [default: 0]. This should be set different than minimum_variance (see notes).</p><ul><li><code>initialisation_strategy::String</code></li></ul><p>: The computation method of the vector of the initial mixtures.     One of the following:     - &quot;grid&quot;: using a grid approach     - &quot;given&quot;: using the mixture provided in the fully qualified <code>mixtures</code> parameter     - &quot;kmeans&quot;: use first kmeans (itself initialised with a &quot;grid&quot; strategy) to set the initial mixture centers [default]     Note that currently &quot;random&quot; and &quot;shuffle&quot; initialisations are not supported in gmm-based algorithms.</p><ul><li><code>maximum_iterations::Int64</code></li></ul><p>: Maximum number of iterations [def: <code>typemax(Int64)</code>, i.e. ∞]</p><ul><li><code>rng::Random.AbstractRNG</code></li></ul><p>: Random Number Generator [deafult: <code>Random.GLOBAL_RNG</code>]</p><p><strong>Example:</strong></p><pre><code class="language-julia hljs">julia&gt; using MLJ

julia&gt; modelType                   = @load GaussianMixtureRegressor pkg = &quot;BetaML&quot;
[ Info: For silent loading, specify `verbosity=0`. 
import BetaML ✔
BetaML.GMM.GaussianMixtureRegressor

julia&gt; model                       = modelType()
GaussianMixtureRegressor(
n_classes = 3, 
initial_probmixtures = Float64[], 
mixtures = BetaML.GMM.DiagonalGaussian{Float64}[BetaML.GMM.DiagonalGaussian{Float64}(nothing, nothing), BetaML.GMM.DiagonalGaussian{Float64}(nothing, nothing), BetaML.GMM.DiagonalGaussian{Float64}(nothing, nothing)], 
tol = 1.0e-6, 
minimum_variance = 0.05, 
minimum_covariance = 0.0, 
initialisation_strategy = &quot;kmeans&quot;, 
maximum_iterations = 9223372036854775807, 
rng = Random._GLOBAL_RNG())

julia&gt; X, y                        = @load_boston;

julia&gt; (fitResults, cache, report) = MLJ.fit(model, 0, X, y);

julia&gt; y_est                       = predict(model, fitResults, X)
506-element Vector{Float64}:
24.703442835305577
24.70344283512716
24.70344283528249
⋮
17.172486989759676
17.172486989759676
17.172486989759644</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/sylvaticus/BetaML.jl/blob/cd5245a06ffcf35225cc9b50c11daf5a3695435f/src/GMM/GMM_MLJ.jl#L108">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="BetaML.GMM.MultitargetGaussianMixtureRegressor" href="#BetaML.GMM.MultitargetGaussianMixtureRegressor"><code>BetaML.GMM.MultitargetGaussianMixtureRegressor</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">mutable struct MultitargetGaussianMixtureRegressor &lt;: MLJModelInterface.Deterministic</code></pre><p>A non-linear regressor derived from fitting the data on a probabilistic model (Gaussian Mixture Model). Relatively fast.</p><p>This is the multi-target version of the model. If you want to predict a single label (y), use the MLJ model <a href="GMM.html#BetaML.GMM.GaussianMixtureRegressor"><code>GaussianMixtureRegressor</code></a>.</p><p><strong>Hyperparameters:</strong></p><ul><li><code>n_classes::Int64</code></li></ul><p>: Number of mixtures (latent classes) to consider [def: 3]</p><ul><li><code>initial_probmixtures::Vector{Float64}</code></li></ul><p>: Initial probabilities of the categorical distribution (n_classes x 1) [default: <code>[]</code>]</p><ul><li><code>mixtures::Union{Type, Vector{var&quot;#s792&quot;} where var&quot;#s792&quot;&lt;:AbstractMixture}</code></li></ul><p>: An array (of length <code>n_classes</code><code>) of the mixtures to employ (see the [</code>?GMM<code>](@ref GMM) module).     Each mixture object can be provided with or without its parameters (e.g. mean and variance for the gaussian ones). Fully qualified mixtures are useful only if the</code>initialisation<em>strategy<code>parameter is  set to &quot;gived&quot;</code>     This parameter can also be given symply in term of a _type</em>. In this case it is automatically extended to a vector of <code>n_classes</code><code>mixtures of the specified type.     Note that mixing of different mixture types is not currently supported.     [def:</code>[DiagonalGaussian() for i in 1:n_classes]`]</p><ul><li><code>tol::Float64</code></li></ul><p>: Tolerance to stop the algorithm [default: 10^(-6)]</p><ul><li><code>minimum_variance::Float64</code></li></ul><p>: Minimum variance for the mixtures [default: 0.05]</p><ul><li><code>minimum_covariance::Float64</code></li></ul><p>: Minimum covariance for the mixtures with full covariance matrix [default: 0]. This should be set different than minimum_variance (see notes).</p><ul><li><code>initialisation_strategy::String</code></li></ul><p>: The computation method of the vector of the initial mixtures.     One of the following:     - &quot;grid&quot;: using a grid approach     - &quot;given&quot;: using the mixture provided in the fully qualified <code>mixtures</code> parameter     - &quot;kmeans&quot;: use first kmeans (itself initialised with a &quot;grid&quot; strategy) to set the initial mixture centers [default]     Note that currently &quot;random&quot; and &quot;shuffle&quot; initialisations are not supported in gmm-based algorithms.</p><ul><li><code>maximum_iterations::Int64</code></li></ul><p>: Maximum number of iterations [def: <code>typemax(Int64)</code>, i.e. ∞]</p><ul><li><code>rng::Random.AbstractRNG</code></li></ul><p>: Random Number Generator [deafult: <code>Random.GLOBAL_RNG</code>]</p><p><strong>Example:</strong></p><pre><code class="language-julia hljs">julia&gt; using MLJ

julia&gt; modelType                   = @load MultitargetGaussianMixtureRegressor pkg = &quot;BetaML&quot;
[ Info: For silent loading, specify `verbosity=0`. 
import BetaML ✔
BetaML.GMM.MultitargetGaussianMixtureRegressor
julia&gt; model                       = modelType()
MultitargetGaussianMixtureRegressor(
  n_classes = 3, 
  initial_probmixtures = Float64[], 
  mixtures = BetaML.GMM.DiagonalGaussian{Float64}[BetaML.GMM.DiagonalGaussian{Float64}(nothing, nothing), BetaML.GMM.DiagonalGaussian{Float64}(nothing, nothing), BetaML.GMM.DiagonalGaussian{Float64}(nothing, nothing)], 
  tol = 1.0e-6, 
  minimum_variance = 0.05, 
  minimum_covariance = 0.0, 
  initialisation_strategy = &quot;kmeans&quot;, 
  maximum_iterations = 9223372036854775807, 
  rng = Random._GLOBAL_RNG())

julia&gt; X, y                        = @load_boston;

julia&gt; ydouble = hcat(y,y);

julia&gt; (fitResults, cache, report) = MLJ.fit(model, 0, X, ydouble);

julia&gt; y_est                       = predict(model, fitResults, X)
506×2 Matrix{Float64}:
 24.5785  24.5785
 24.5785  24.5785
 24.5785  24.5785
  ⋮       
 17.0039  17.0039
 17.0039  17.0039
 17.0039  17.0039</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/sylvaticus/BetaML.jl/blob/cd5245a06ffcf35225cc9b50c11daf5a3695435f/src/GMM/GMM_MLJ.jl#L202">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="BetaML.GMM.init_mixtures!-Union{Tuple{T}, Tuple{Vector{T}, Any}} where T&lt;:BetaML.GMM.AbstractGaussian" href="#BetaML.GMM.init_mixtures!-Union{Tuple{T}, Tuple{Vector{T}, Any}} where T&lt;:BetaML.GMM.AbstractGaussian"><code>BetaML.GMM.init_mixtures!</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">init_mixtures!(mixtures::Array{T,1}, X; minimum_variance=0.25, minimum_covariance=0.0, initialisation_strategy=&quot;grid&quot;,rng=Random.GLOBAL_RNG)</code></pre><p>The parameter <code>initialisation_strategy</code> can be <code>grid</code>, <code>kmeans</code> or <code>given</code>:</p><ul><li><code>grid</code>: Uniformly cover the space observed by the data</li><li><code>kmeans</code>: Use the kmeans algorithm. If the data contains missing values, a first run of <code>predictMissing</code> is done under init=<code>grid</code> to impute the missing values just to allow the kmeans algorithm. Then the em algorithm is used with the output of kmean as init values.</li><li><code>given</code>: Leave the provided set of initial mixtures</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/sylvaticus/BetaML.jl/blob/cd5245a06ffcf35225cc9b50c11daf5a3695435f/src/GMM/Mixtures.jl#L99-L108">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="BetaML.GMM.lpdf-Tuple{DiagonalGaussian, Any, Any}" href="#BetaML.GMM.lpdf-Tuple{DiagonalGaussian, Any, Any}"><code>BetaML.GMM.lpdf</code></a> — <span class="docstring-category">Method</span></header><section><div><p>lpdf(m::DiagonalGaussian,x,mask) - Log PDF of the mixture given the observation <code>x</code></p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/sylvaticus/BetaML.jl/blob/cd5245a06ffcf35225cc9b50c11daf5a3695435f/src/GMM/Mixtures.jl#L200">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="BetaML.GMM.lpdf-Tuple{FullGaussian, Any, Any}" href="#BetaML.GMM.lpdf-Tuple{FullGaussian, Any, Any}"><code>BetaML.GMM.lpdf</code></a> — <span class="docstring-category">Method</span></header><section><div><p>lpdf(m::FullGaussian,x,mask) - Log PDF of the mixture given the observation <code>x</code></p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/sylvaticus/BetaML.jl/blob/cd5245a06ffcf35225cc9b50c11daf5a3695435f/src/GMM/Mixtures.jl#L209">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="BetaML.GMM.lpdf-Tuple{SphericalGaussian, Any, Any}" href="#BetaML.GMM.lpdf-Tuple{SphericalGaussian, Any, Any}"><code>BetaML.GMM.lpdf</code></a> — <span class="docstring-category">Method</span></header><section><div><p>lpdf(m::SphericalGaussian,x,mask) - Log PDF of the mixture given the observation <code>x</code></p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/sylvaticus/BetaML.jl/blob/cd5245a06ffcf35225cc9b50c11daf5a3695435f/src/GMM/Mixtures.jl#L190">source</a></section></article></article><nav class="docs-footer"><a class="docs-footer-prevpage" href="Clustering.html">« Clustering</a><a class="docs-footer-nextpage" href="Imputation.html">Imputation »</a><div class="flexbox-break"></div><p class="footer-message">Powered by <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> and the <a href="https://julialang.org/">Julia Programming Language</a>.</p></nav></div><div class="modal" id="documenter-settings"><div class="modal-background"></div><div class="modal-card"><header class="modal-card-head"><p class="modal-card-title">Settings</p><button class="delete"></button></header><section class="modal-card-body"><p><label class="label">Theme</label><div class="select"><select id="documenter-themepicker"><option value="documenter-light">documenter-light</option><option value="documenter-dark">documenter-dark</option></select></div></p><hr/><p>This document was generated with <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> version 0.27.23 on <span class="colophon-date" title="Monday 5 December 2022 17:03">Monday 5 December 2022</span>. Using Julia version 1.6.7.</p></section><footer class="modal-card-foot"></footer></div></div></div></body></html>
