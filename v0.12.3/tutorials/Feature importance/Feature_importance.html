<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>Understanding variable importance in black-box machine learning models · BetaML.jl Documentation</title><meta name="title" content="Understanding variable importance in black-box machine learning models · BetaML.jl Documentation"/><meta property="og:title" content="Understanding variable importance in black-box machine learning models · BetaML.jl Documentation"/><meta property="twitter:title" content="Understanding variable importance in black-box machine learning models · BetaML.jl Documentation"/><meta name="description" content="Documentation for BetaML.jl Documentation."/><meta property="og:description" content="Documentation for BetaML.jl Documentation."/><meta property="twitter:description" content="Documentation for BetaML.jl Documentation."/><script async src="https://www.googletagmanager.com/gtag/js?id=G-JYKX8QY5JW"></script><script>  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());
  gtag('config', 'G-JYKX8QY5JW', {'page_path': location.pathname + location.search + location.hash});
</script><script data-outdated-warner src="../../assets/warner.js"></script><link href="https://cdnjs.cloudflare.com/ajax/libs/lato-font/3.0.0/css/lato-font.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/juliamono/0.050/juliamono.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/fontawesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/solid.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/brands.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.16.8/katex.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL="../.."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" data-main="../../assets/documenter.js"></script><script src="../../search_index.js"></script><script src="../../siteinfo.js"></script><script src="../../../versions.js"></script><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/catppuccin-mocha.css" data-theme-name="catppuccin-mocha"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/catppuccin-macchiato.css" data-theme-name="catppuccin-macchiato"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/catppuccin-frappe.css" data-theme-name="catppuccin-frappe"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/catppuccin-latte.css" data-theme-name="catppuccin-latte"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/documenter-dark.css" data-theme-name="documenter-dark" data-theme-primary-dark/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/documenter-light.css" data-theme-name="documenter-light" data-theme-primary/><script src="../../assets/themeswap.js"></script></head><body><div id="documenter"><nav class="docs-sidebar"><a class="docs-logo" href="../../index.html"><img src="../../assets/logo.png" alt="BetaML.jl Documentation logo"/></a><div class="docs-package-name"><span class="docs-autofit"><a href="../../index.html">BetaML.jl Documentation</a></span></div><button class="docs-search-query input is-rounded is-small is-clickable my-2 mx-auto py-1 px-2" id="documenter-search-query">Search docs (Ctrl + /)</button><ul class="docs-menu"><li><a class="tocitem" href="../../index.html">Index</a></li><li><span class="tocitem">Tutorial</span><ul><li><a class="tocitem" href="../Betaml_tutorial_getting_started.html">Getting started</a></li><li><input class="collapse-toggle" id="menuitem-2-2" type="checkbox"/><label class="tocitem" for="menuitem-2-2"><span class="docs-label">Classification - cars</span><i class="docs-chevron"></i></label><ul class="collapsed"><li><a class="tocitem" href="../Classification - cars/betaml_tutorial_classification_cars.html">A classification task when labels are known - determining the country of origin of cars given the cars characteristics</a></li></ul></li><li><input class="collapse-toggle" id="menuitem-2-3" type="checkbox"/><label class="tocitem" for="menuitem-2-3"><span class="docs-label">Regression - bike sharing</span><i class="docs-chevron"></i></label><ul class="collapsed"><li><a class="tocitem" href="../Regression - bike sharing/betaml_tutorial_regression_sharingBikes.html">A regression task: the prediction of  bike  sharing demand</a></li></ul></li><li><input class="collapse-toggle" id="menuitem-2-4" type="checkbox"/><label class="tocitem" for="menuitem-2-4"><span class="docs-label">Clustering - Iris</span><i class="docs-chevron"></i></label><ul class="collapsed"><li><a class="tocitem" href="../Clustering - Iris/betaml_tutorial_cluster_iris.html">A clustering task: the prediction of  plant species from floreal measures (the iris dataset)</a></li></ul></li><li><input class="collapse-toggle" id="menuitem-2-5" type="checkbox"/><label class="tocitem" for="menuitem-2-5"><span class="docs-label">Multi-branch neural network</span><i class="docs-chevron"></i></label><ul class="collapsed"><li><a class="tocitem" href="../Multi-branch neural network/betaml_tutorial_multibranch_nn.html">A deep neural network with multi-branch architecture</a></li></ul></li><li><input class="collapse-toggle" id="menuitem-2-6" type="checkbox" checked/><label class="tocitem" for="menuitem-2-6"><span class="docs-label">Feature importance</span><i class="docs-chevron"></i></label><ul class="collapsed"><li class="is-active"><a class="tocitem" href="Feature_importance.html">Understanding variable importance in black-box machine learning models</a><ul class="internal"><li><a class="tocitem" href="#Example-with-synthetic-data"><span>Example with synthetic data</span></a></li><li><a class="tocitem" href="#Determinant-of-house-prices-in-the-Boston-alrea"><span>Determinant of house prices in the Boston alrea</span></a></li></ul></li></ul></li></ul></li><li><span class="tocitem">API (Reference manual)</span><ul><li><input class="collapse-toggle" id="menuitem-3-1" type="checkbox"/><label class="tocitem" for="menuitem-3-1"><span class="docs-label">API V2 (current)</span><i class="docs-chevron"></i></label><ul class="collapsed"><li><a class="tocitem" href="../../Api_v2_user.html">Introduction for user</a></li><li><input class="collapse-toggle" id="menuitem-3-1-2" type="checkbox"/><label class="tocitem" for="menuitem-3-1-2"><span class="docs-label">For developers</span><i class="docs-chevron"></i></label><ul class="collapsed"><li><a class="tocitem" href="../../Api_v2_developer.html">API implementation</a></li><li><a class="tocitem" href="../../StyleGuide_templates.html">Style guide</a></li></ul></li><li><a class="tocitem" href="../../Api.html">The Api module</a></li></ul></li><li><a class="tocitem" href="../../Perceptron.html">Perceptron</a></li><li><a class="tocitem" href="../../Trees.html">Trees</a></li><li><a class="tocitem" href="../../Nn.html">Nn</a></li><li><a class="tocitem" href="../../Clustering.html">Clustering</a></li><li><a class="tocitem" href="../../GMM.html">GMM</a></li><li><a class="tocitem" href="../../Imputation.html">Imputation</a></li><li><a class="tocitem" href="../../Utils.html">Utils</a></li></ul></li><li><a class="tocitem" href="../../MLJ_interface.html">MLJ interface</a></li></ul><div class="docs-version-selector field has-addons"><div class="control"><span class="docs-label button is-static is-size-7">Version</span></div><div class="docs-selector control is-expanded"><div class="select is-fullwidth is-size-7"><select id="documenter-version-selector"></select></div></div></div></nav><div class="docs-main"><header class="docs-navbar"><a class="docs-sidebar-button docs-navbar-link fa-solid fa-bars is-hidden-desktop" id="documenter-sidebar-button" href="#"></a><nav class="breadcrumb"><ul class="is-hidden-mobile"><li><a class="is-disabled">Tutorial</a></li><li><a class="is-disabled">Feature importance</a></li><li class="is-active"><a href="Feature_importance.html">Understanding variable importance in black-box machine learning models</a></li></ul><ul class="is-hidden-tablet"><li class="is-active"><a href="Feature_importance.html">Understanding variable importance in black-box machine learning models</a></li></ul></nav><div class="docs-right"><a class="docs-navbar-link" href="https://github.com/sylvaticus/BetaML.jl" title="View the repository on GitHub"><span class="docs-icon fa-brands"></span><span class="docs-label is-hidden-touch">GitHub</span></a><a class="docs-navbar-link" href="https://github.com/sylvaticus/BetaML.jl/blob/master/docs/src/tutorials/Feature importance/Feature_importance.jl" title="Edit source on GitHub"><span class="docs-icon fa-solid"></span></a><a class="docs-settings-button docs-navbar-link fa-solid fa-gear" id="documenter-settings-button" href="#" title="Settings"></a><a class="docs-article-toggle-button fa-solid fa-chevron-up" id="documenter-article-toggle-button" href="javascript:;" title="Collapse all docstrings"></a></div></header><article class="content" id="documenter-page"><h1 id="variable_importance_tutorial"><a class="docs-heading-anchor" href="#variable_importance_tutorial">Understanding variable importance in black-box machine learning models</a><a id="variable_importance_tutorial-1"></a><a class="docs-heading-anchor-permalink" href="#variable_importance_tutorial" title="Permalink"></a></h1><p>Often we want to understand the contribution of the different variables (x columns) to the prediction accuracy of a black-box machine learning model. To this end, BetaML 0.12 introduces <a href="../../Utils.html#BetaML.Utils.FeatureRanker"><code>FeatureRanker</code></a>, a flexible variable ranking estimator that employs multiple variable importance metrics. <code>FeatureRanker</code> helps to determine the importance of features in predictions from any black-box machine learning model (not necessarily the BetaML suit), internally using cross-validation to assess the quality of the predictions (<code>metric=&quot;mda&quot;</code>), or the contribution of the variable to the variance of the predictions (<code>metric=&quot;sobol&quot;</code>), with or without a given variable.</p><p>By default, it ranks variables (columns) in a single pass without retraining on each one. However, it is possible to specify the model to use multiple passes (where on each pass the less important variable is permuted). This helps to assess importance in the presence of highly correlated variables. While the default strategy is to simply (temporarily) permute the &quot;test&quot; variable and predict the modified data set, it is possible to refit the model to be evaluated on each variable (&quot;permute and relearn&quot;), of course at a much higher computational cost. However, if the ML model to be evaluated supports ignoring variables during prediction (as BetaML tree models do), it is possible to specify the keyword argument for such an option in the target model prediction function and avoid refitting.</p><p>In this tutorial we will use <code>FeatureRanker</code> first with some synthetic data, and then with the Boston dataset to determine the most important variables in determining house prices. We will compare the results with Shapley values using the <a href="https://github.com/nredell/ShapML.jl"><code>ShapML</code></a> package.</p><p>Let&#39;s start by activating the local environment specific to the BetaML documentation and loading the necessary packages:</p><pre><code class="language-julia hljs">using Pkg
Pkg.activate(joinpath(@__DIR__,&quot;..&quot;,&quot;..&quot;,&quot;..&quot;))
using  Statistics, Random, Pipe, StableRNGs, HTTP, CSV, DataFrames, Plots, BetaML
import Distributions: Normal, Uniform, quantile
import ShapML
Random.seed!(123)</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">MersenneTwister(123)</code></pre><h2 id="Example-with-synthetic-data"><a class="docs-heading-anchor" href="#Example-with-synthetic-data">Example with synthetic data</a><a id="Example-with-synthetic-data-1"></a><a class="docs-heading-anchor-permalink" href="#Example-with-synthetic-data" title="Permalink"></a></h2><p>In this example, we generate a dataset of 5 random variables, where <code>x1</code> is the most important in determining <code>y</code>, <code>x2</code> is somewhat less important, <code>x3</code> has interaction effects with <code>x1</code>, while <code>x4</code> and <code>x5</code> do not contribute at all to the calculation of <code>y</code>. We also add <code>x6</code> as a highly correlated variable to <code>x1</code>, but note that <code>x4</code> also does not contribute to <code>y</code>:</p><pre><code class="language-julia hljs">N     = 2000
xa    = rand(Uniform(0.0,10.0),N,5)
xb    = xa[:,1] .* rand.(Normal(1,0.5))
x     = hcat(xa,xb)
y     = [10*r[1]-r[2]+0.1*r[3]*r[1] for r in eachrow(x) ];</code></pre><p>Aside of <code>y</code>, that is numerical, we create also a categorical version to test classification and a further one-hot version to test neural networks models that, for classification tasks, work using one-hot encoded variables:</p><pre><code class="language-julia hljs">ysort = sort(y)
ycat  = [(i &lt; ysort[Int(round(N/3))]) ?  &quot;c&quot; :  ( (i &lt; ysort[Int(round(2*N/3))]) ? &quot;a&quot; : &quot;b&quot;)  for i in y]
yoh    = fit!(OneHotEncoder(),ycat);</code></pre><p>We run this example using a Random Forest regressor. The BetaML <code>RandomForestEstimator</code> model supports a <code>predict</code> function with the option to ignore specific dimensions. This allow us to &quot;test&quot; the various variables without retraining the model:</p><pre><code class="language-julia hljs">fr = FeatureRanker(model=RandomForestEstimator(),nsplits=5,nrepeats=1,recursive=false,metric=&quot;mda&quot;,ignore_dims_keyword=&quot;ignore_dims&quot;)</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">FeatureRanker - A meta-model to extract variable importance of an arbitrary regressor/classifier (unfitted)</code></pre><p>We can now fit the <code>FeatureRanker</code> to our data. Note that, as for the other BetaML models, <code>fit!</code> by default returns the predictions, in this case the ranking, avoiding a separate <code>predict</code> call. The returned raking goes from the lowest to the most important variable, according to the given metric.</p><pre><code class="language-julia hljs">rank = fit!(fr,x,y)</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">6-element Vector{Int64}:
 5
 4
 3
 2
 6
 1</code></pre><p>As expected, the ranking shows <code>x1</code> as the most important variable. Let&#39;s look in detail at the metrics that we can obtain by querying the model with <code>info(fr)</code>:</p><pre><code class="language-julia hljs">loss_by_col        = info(fr)[&quot;loss_by_col&quot;]
sobol_by_col       = info(fr)[&quot;sobol_by_col&quot;]
loss_by_col_sd     = info(fr)[&quot;loss_by_col_sd&quot;]
sobol_by_col_sd    = info(fr)[&quot;sobol_by_col_sd&quot;]
loss_fullmodel     = info(fr)[&quot;loss_all_cols&quot;]
loss_fullmodel_sd  = info(fr)[&quot;loss_all_cols_sd&quot;]
ntrials_per_metric = info(fr)[&quot;ntrials_per_metric&quot;]</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">5</code></pre><p>Since we choosed <code>mda</code> as the reported metric, we must have that the reported rank is equal to the sortperm of <code>loss_by_col</code>:</p><pre><code class="language-julia hljs">sortperm(loss_by_col) == rank</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">true</code></pre><p>We can plot the loss per (omitted) column...</p><pre><code class="language-julia hljs">bar(string.(rank),loss_by_col[rank],label=&quot;Loss by col&quot;, yerror=quantile(Normal(1,0),0.975) .* (loss_by_col_sd[rank]./sqrt(ntrials_per_metric)))</code></pre><img src="Feature_importance-747e787f.svg" alt="Example block output"/><p>..and the sobol values:</p><pre><code class="language-julia hljs">bar(string.(sortperm(sobol_by_col)),sobol_by_col[sortperm(sobol_by_col)],label=&quot;Sobol index by col&quot;, yerror=quantile(Normal(1,0),0.975) .* (sobol_by_col_sd[sortperm(sobol_by_col)]./sqrt(ntrials_per_metric)))</code></pre><img src="Feature_importance-37e57243.svg" alt="Example block output"/><p>As we can see from the graphs, the model did a good job of identifying the first variable as the most important one, ignoring the others and even giving a very low importance to the correlated one.</p><h3 id="Comparision-with-the-Shapley-values"><a class="docs-heading-anchor" href="#Comparision-with-the-Shapley-values">Comparision with the Shapley values</a><a id="Comparision-with-the-Shapley-values-1"></a><a class="docs-heading-anchor-permalink" href="#Comparision-with-the-Shapley-values" title="Permalink"></a></h3><p>For Shapley values we need first to have a trained model</p><pre><code class="language-julia hljs">m = RandomForestEstimator()
fit!(m,x,y);</code></pre><p>We need then to wrap the predict function, accounting with the fact that BetaML models works with standard arrays, while <code>ShapML</code> assume data in DataFrame format:</p><pre><code class="language-julia hljs">function predict_function(model, data)
  data_pred = DataFrame(y_pred = BetaML.predict(model, Matrix(data)))
  return data_pred
end</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">predict_function (generic function with 1 method)</code></pre><p>We set up other data related to the simulation..</p><pre><code class="language-julia hljs">explain   = DataFrame(x[1:300, :],:auto)
reference = DataFrame(x,:auto)

sample_size = 60 ; # Number of Monte Carlo samples.</code></pre><p>...and finally compute the stochastic Shapley values per individual record:</p><pre><code class="language-julia hljs">data_shap = ShapML.shap(explain = explain,
                        reference = reference,
                        model = m,
                        predict_function = predict_function,
                        sample_size = sample_size,
                        seed = 1
                        );</code></pre><p>We aggregate the Shape values by feature and plot:</p><pre><code class="language-julia hljs">shap_aggregated =combine(groupby(data_shap,[:feature_name])) do subdf
            (mean_effect = mean(abs.(subdf.shap_effect)), std = std(abs.(subdf.shap_effect)), n = size(subdf,1)  )
end
shap_values = shap_aggregated.mean_effect

bar(string.(sortperm(shap_values)),shap_values[sortperm(shap_values)],label=&quot;Shapley values by col&quot;, yerror=quantile(Normal(1,0),0.975) .* (shap_aggregated.std[sortperm(shap_values)]./ sqrt.(shap_aggregated.n)))</code></pre><img src="Feature_importance-280c191c.svg" alt="Example block output"/><p>Note that the output using the Sobol index and the Shapley values are very similar. This shoudn&#39;t come as a surprice, as the two metrics are related.</p><h3 id="Classifications"><a class="docs-heading-anchor" href="#Classifications">Classifications</a><a id="Classifications-1"></a><a class="docs-heading-anchor-permalink" href="#Classifications" title="Permalink"></a></h3><p>For classification tasks, the usage of <code>FeatureRanker</code> doesn&#39;t change:</p><pre><code class="language-julia hljs">fr = FeatureRanker(model=RandomForestEstimator(),nsplits=3,nrepeats=2,recursive=true,metric=&quot;mda&quot;,ignore_dims_keyword=&quot;ignore_dims&quot;)
rank = fit!(fr,x,ycat)</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">6-element Vector{Int64}:
 5
 4
 3
 2
 6
 1</code></pre><pre><code class="language-julia hljs">fr = FeatureRanker(model=NeuralNetworkEstimator(verbosity=NONE),nsplits=3,nrepeats=1,recursive=false,metric=&quot;sobol&quot;,refit=false)
rank = fit!(fr,x,yoh)</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">6-element Vector{Int64}:
 5
 3
 4
 6
 2
 1</code></pre><h2 id="Determinant-of-house-prices-in-the-Boston-alrea"><a class="docs-heading-anchor" href="#Determinant-of-house-prices-in-the-Boston-alrea">Determinant of house prices in the Boston alrea</a><a id="Determinant-of-house-prices-in-the-Boston-alrea-1"></a><a class="docs-heading-anchor-permalink" href="#Determinant-of-house-prices-in-the-Boston-alrea" title="Permalink"></a></h2><p>We start this example by first loading the data from a CSV file and splitting the data in features and labels:</p><pre><code class="language-julia hljs">data = CSV.File(joinpath(@__DIR__,&quot;data&quot;,&quot;housing.data&quot;), delim=&#39; &#39;, header=false, ignorerepeated=true) |&gt; DataFrame

var_names = [
  &quot;CRIM&quot;,    # per capita crime rate by town
  &quot;ZN&quot;,      # proportion of residential land zoned for lots over 25,000 sq.ft.
  &quot;INDUS&quot;,   # proportion of non-retail business acres per town
  &quot;CHAS&quot;,    # Charles River dummy variable (= 1 if tract bounds river; 0 otherwise)
  &quot;NOX&quot;,     # nitric oxides concentration (parts per 10 million)
  &quot;RM&quot;,      # average number of rooms per dwelling
  &quot;AGE&quot;,     # proportion of owner-occupied units built prior to 1940
  &quot;DIS&quot;,     # weighted distances to five Boston employment centres
  &quot;RAD&quot;,     # index of accessibility to radial highways
  &quot;TAX&quot;,     # full-value property-tax rate per $10,000
  &quot;PTRATIO&quot;, # pupil-teacher ratio by town
  &quot;B&quot;,       # 1000(Bk - 0.63)^2 where Bk is the proportion of blacks by town
  &quot;LSTAT&quot;,   # % lower status of the population
]
y_name = &quot;MEDV&quot; ;# Median value of owner-occupied homes in $1000&#39;s</code></pre><p>Our features are a set of 13 explanatory variables, while the label that we want to estimate is the average housing prices:</p><pre><code class="language-julia hljs">x = Matrix(data[:,1:13])
y = data[:,14];</code></pre><p>We use a Random Forest model as regressor and we compute the variable importance for this model as we did for the synthetic data:</p><pre><code class="language-julia hljs">fr = FeatureRanker(model=RandomForestEstimator(),nsplits=3,nrepeats=2,recursive=false)
rank = fit!(fr,x,y)

loss_by_col        = info(fr)[&quot;loss_by_col&quot;]
sobol_by_col       = info(fr)[&quot;sobol_by_col&quot;]
loss_by_col_sd     = info(fr)[&quot;loss_by_col_sd&quot;]
sobol_by_col_sd    = info(fr)[&quot;sobol_by_col_sd&quot;]
loss_fullmodel     = info(fr)[&quot;loss_all_cols&quot;]
loss_fullmodel_sd  = info(fr)[&quot;loss_all_cols_sd&quot;]
ntrials_per_metric = info(fr)[&quot;ntrials_per_metric&quot;]</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">6</code></pre><p>Finally we can plot the variable importance:</p><pre><code class="language-julia hljs">bar(var_names[sortperm(loss_by_col)], loss_by_col[sortperm(loss_by_col)],label=&quot;Loss by var&quot;, permute=(:x,:y), yerror=quantile(Normal(1,0),0.975) .* (loss_by_col_sd[sortperm(loss_by_col)]./sqrt(ntrials_per_metric)), yrange=[0,0.6])
vline!([loss_fullmodel], label=&quot;Loss with all vars&quot;,linewidth=2)
vline!([loss_fullmodel-quantile(Normal(1,0),0.975) * loss_fullmodel_sd/sqrt(ntrials_per_metric),
        loss_fullmodel+quantile(Normal(1,0),0.975) * loss_fullmodel_sd/sqrt(ntrials_per_metric),
], label=nothing,linecolor=:black,linestyle=:dot,linewidth=1)</code></pre><img src="Feature_importance-cf841b12.svg" alt="Example block output"/><pre><code class="language-julia hljs">bar(var_names[sortperm(sobol_by_col)],sobol_by_col[sortperm(sobol_by_col)],label=&quot;Sobol index by col&quot;, permute=(:x,:y), yerror=quantile(Normal(1,0),0.975) .* (sobol_by_col_sd[sortperm(sobol_by_col)]./sqrt(ntrials_per_metric)), yrange=[0,0.5])</code></pre><img src="Feature_importance-d8ebcd39.svg" alt="Example block output"/><p>As we can see, the two analyses agree on the most important variables, showing that the size of the house (number of rooms), the percentage of low-income population in the neighbourhood and, to a lesser extent, the distance to employment centres are the most important explanatory variables of house price in the Boston area.</p><p><a href="Feature_importance.jl">View this file on Github</a>.</p><hr/><p><em>This page was generated using <a href="https://github.com/fredrikekre/Literate.jl">Literate.jl</a>.</em></p></article><nav class="docs-footer"><a class="docs-footer-prevpage" href="../Multi-branch neural network/betaml_tutorial_multibranch_nn.html">« A deep neural network with multi-branch architecture</a><a class="docs-footer-nextpage" href="../../Api_v2_user.html">Introduction for user »</a><div class="flexbox-break"></div><p class="footer-message">Powered by <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> and the <a href="https://julialang.org/">Julia Programming Language</a>.</p></nav></div><div class="modal" id="documenter-settings"><div class="modal-background"></div><div class="modal-card"><header class="modal-card-head"><p class="modal-card-title">Settings</p><button class="delete"></button></header><section class="modal-card-body"><p><label class="label">Theme</label><div class="select"><select id="documenter-themepicker"><option value="auto">Automatic (OS)</option><option value="documenter-light">documenter-light</option><option value="documenter-dark">documenter-dark</option><option value="catppuccin-latte">catppuccin-latte</option><option value="catppuccin-frappe">catppuccin-frappe</option><option value="catppuccin-macchiato">catppuccin-macchiato</option><option value="catppuccin-mocha">catppuccin-mocha</option></select></div></p><hr/><p>This document was generated with <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> version 1.14.1 on <span class="colophon-date" title="Tuesday 16 September 2025 14:11">Tuesday 16 September 2025</span>. Using Julia version 1.6.7.</p></section><footer class="modal-card-foot"></footer></div></div></div></body></html>
